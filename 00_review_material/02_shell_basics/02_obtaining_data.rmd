# Obtaining Data

The Internet provides without a doubt the largest resource for data. This data is available in various forms, using various protocols. The command-line tool cURL [@curl] can be considered the command line's Swiss Army knife when it comes to downloading data from the Internet.

When you access a URL, which stands for *uniform resource locator*, through your browser, the data that is being downloaded can be interpreted. For example, an HTML file is rendered as a website, an MP3 file may be automatically played, and a PDF file may be automatically downloaded or opened by a viewer. However, when cURL is used to access a URL, the data is downloaded as is printed to standard output. Other command-line tools may then be used to process this data further.

The easiest invocation of cURL is to simply specify a URL as a command-line argument. For example, to download the book *Adventures of Huckleberry Finn* by Mark Twain from Project Gutenberg, we can run the following command:

```{bash, eval=FALSE}
curl -s http://www.gutenberg.org/files/76/76-0.txt | head -n 10
```

By default, cURL outputs a progress meter that shows how the download rate and the expected time of completion. If you are piping the output directly to another command-line tool, such as `head`, be sure to specify the `-s` command-line argument, which stands for *silent*, so that the progress meter is disabled. Compare, for example, the output with the following command:

```{bash, eval=FALSE}
curl http://www.gutenberg.org/files/76/76-0.txt | head -n 10
```

Note that the output of the second command, where we do not disable the progress meter, contains the unwanted text and even an error message. If you save the data to a file, then you do not need to necessarily specify the `-s` option:

```{bash, eval=FALSE}
curl http://www.gutenberg.org/files/76/76-0.txt > data/finn.txt
```

You can also save the data by explicitly specifying the output file with the `-o` option:

```{bash, eval=FALSE}
curl -s http://www.gutenberg.org/files/76/76-0.txt -o data/finn.txt
```

When downloading data from the Internet, the URL will most likely use the protocols HTTP or HTTPS. To download from an FTP server, which stands for File Transfer Protocol, you use cURL in exactly the same way. When the URL is password protected, you can specify a username and a password as follows:

```{bash, eval=FALSE}
curl -u username:password ftp://host/file
```

If the specified URL is a directory, `curl` will list the contents of that directory.

When you access a shortened URL, such as the ones that start with *http://bit.ly/* or *http://t.co/*, your browser automatically redirects you to the correct location. With `curl`, however, you need to specify the `-L` or `--location` option in order to be redirected:

```{bash, eval=FALSE}
curl -L j.mp/locatbbar
```

If you do not specify the `-L` or `--location` option, you may get something like:

```{bash, eval=FALSE}
curl j.mp/locatbbar
```

By specifying the `-I` or `--head` option, `curl` fetches only the HTTP header of the response:

```{bash, eval=FALSE}
curl -I j.mp/locatbbar
```

The first line indicates the HTTP status code, which is 301 (moved permanently) in this case. You can also see the location this URL redirects to: <http://en.wikipedia.org/wiki/List_of_countries_and_territories_by_border/area_ratio>. Inspecting the header and getting the status code is a useful debugging tool in case `curl` does not give you the expected result. Other common HTTP status codes include 404 (not found) and 403 (forbidden). This page lists all HTTP status codes: <http://en.wikipedia.org/wiki/List_of_HTTP_status_codes>.

To conclude this section, cURL is a straight-forward command-line tool for downloading data from the Internet. Its three most common command-line arguments are `-s` to suppress the progress meter, `-u` to specify a username and password, and `-L` to automatically follow redirects. See its man page for more information.

## Calling a Web API 

In the previous section we explained how to download individual files from the Internet. Another way data can come from the Internet is through a web API, which stands for *Application Programming Interface*. The number of APIs that are being offered by organizations is growing at increasing rate, which means a lot of interesting data for us data scientists.

Web APIs are not meant to be presented in nice layout, such as websites. Instead, most web APIs return data in a structured format, such as JSON or XML. Having data in a structured form has the advantage that the data can be easily processed by other tools, such as `jq`. Which can be downloaded [here](https://stedolan.github.io/jq/). For example, the API from <https://randomuser.me> returns data in the following JSON structure.

```{bash, eval=FALSE}
curl -s https://randomuser.me/api/1.2/ | jq .
```

The data is piped to a command-line tool `jq` in order to display it in a nice way. 

Some web APIs return data in a streaming manner. This means that once you connect to it, the data will continue to pour in forever. A well-known example is the Twitter "firehose", which constantly streams all the tweets being sent around the world. Luckily, most command-line tools that we use also operate in a streaming matter, so that we also use this kind of data.

Some APIs require you to log in using the OAuth protocol. There is a handy command-line tool called `curlicue` [@curlicue] that assists in performing the so-called "OAuth dance". Once this has been set up, it `curlicue` will call `curl` with the correct headers. First, you set things up once for a particular API with `curlicue-setup`, and then you can call that API using `curlicue`. For example, to use `curlicue` with the Twitter API you would run:

```{bash, eval=FALSE}
 curlicue-setup \
> 'https://api.twitter.com/oauth/request_token' \
> 'https://api.twitter.com/oauth/authorize?oauth_token=oauth_token' \
> 'https://api.twitter.com/oauth/access_token' \
> credentials
 curlicue -f credentials \
> 'https://api.twitter.com/1/statuses/home_timeline.xml'
```

For more popular APIs, there are specialized command-line tools available. These are wrappers that provide a convenient way to connect to the API. 
