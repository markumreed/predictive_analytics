[["index.html", "Predictive Analytics Section 1 Predictive Analytics", " Predictive Analytics Markum Reed 2021-01-09 Section 1 Predictive Analytics This course aims to go beyond the classical statistical methods. As computing power has increased many new, highly computational, regression, or “Machine Learning,” methods have been developed. There has been a significant expansion of the number of possible approaches. Since these methods are so new, the business community is generally unaware of their huge potential. With the explosion of “Big Data” problems, machine learning has become a hot field in many scientific areas as well as marketing, finance and other business disciplines. People with machine learning skills are in high demand. 1.0.1 Course Text In this course we will be following James, Gareth, et al. An introduction to statistical learning. Vol. 112. New York: springer, 2013.. 1.0.2 Course Material Course Material Reading Assignment Labs Due Date [Course Introduction] Review Material Statistical Learning Ch. 1,2 Linear Regression: Theory Ch. 3 Linear Regression: Practice [Lab: Linear Regression] Classification: Theory Ch. 4 Classification: Practice [Lab: Logistic Regression] [Lab: KNN] Resampling: Theory Ch. 5 Resampling: Practice [Lab: Cross-Validation and the Bootstrap] Linear Model Selection and Regularization: Theory Ch. 6 Linear Model Selection and Regularization: Practice [Lab: Subset Selection Methods] [Lab: Elastic Net Regression] Tree Based Methods: Theory Ch. 8 Tree Based Methods: Practice [Lab: Decision Trees] Support Vector Machines: Theory Ch. 9 Support Vector Machines: Practice [Lab: Support Vector Machines] Unsupervised Learning: Theory Ch. 10 Unsupervised Learning: Practice [Lab: Principal Components Analysis] [Lab: Clustering] "],["references.html", "References", " References "],["introduction-types-expressions.html", "Section 2 Introduction, Types, &amp; Expressions 2.1 Why Learn to Code? 2.2 Why Python? 2.3 Storing and Computing Data 2.4 Types", " Section 2 Introduction, Types, &amp; Expressions 2.1 Why Learn to Code? 2.1.1 Outcomes Fluency: (Python) procedural programming Use assignments, conditionals, &amp; loops Create Python modules and programs Competency: object-oriented programming Recognize and use objects and classes Knowledge: Foundations for Data Science 2.2 Why Python? Low overhead little to learn before you start ‘doing’ easier for beginners designed with ‘rapid prototyping’ in mind Highly relevant to non-CS majors NumPy, SciPy and Pandas heavily used Modern language Popular for web applications Applicable to mobile app development Data Scientists Toolkit 2.3 Storing and Computing Data 2.3.1 Expressions An expression represents something Python evaluates it (turns it into a value) Similar to a calculator 2.3 # Literal (Evaluates to self) 2.3 (3*7 + 2) * 0.1 # An expression with four literals and some operators 2.3000000000000003 2.4 Types A set of values and operations on these values Examples of operations: +,-,/,* Meaning of operations depend on type MEMORIZE THIS DEFINITION 2.4.1 How to tell the type of a value Command: type(&lt;value&gt;) type(2) int 2.4.2 float (floating point) Values: (approximations of) real numbers - With a “.”: a float literal (e.g., 2.0) - Without a decimal: an int literal (e.g., 2) Operations: + ,- ,* ,/ ,** - Notice: operator meaning can change from type to type Exponent notation useful for large (or small) values \\(-22.51e6\\) is \\(-22.51 * 10^6\\) or \\(-22510000\\) \\(22.51e-6\\) is \\(-22.51 * 10^{-6}\\) or \\(0.00002251\\) 2.4.3 int (integers) Values: \\(\\dots , -3, -2, -1, 0, 1, 2, 3, 4, 5, \\dots\\) Operations: + ,- ,* , ** ,/ , // , % 2.4.4 bool (boolean) Values: True, False - Booleans literals True and False (MUST BE CAPITALIZED) Operations: not, and, or - not b: True if b is false and False if b is true - b and c: True if both b and c are true; False otherwise - b or c: True if b is true or c is true; False otherwise Often come from comparing int or float values - Order comparisons: \\(i&lt;j\\) \\(i&lt;=j\\) \\(i&gt;=j\\) \\(i&gt;j\\) - Equality, inequality: \\(i == j\\) \\(i!=j\\) 2.4.5 str (string) for text Values: any sequence of characters Operation(s): + (contenation, or concatenation) - operator + changes from type to type String literal: sequence of characters in quotes - Double quotes: “abc#&amp;$g&lt;” or “Hello World!” - Single quotes: ‘Hello World’ Concatenation applies only to strings - “ab” + “cd” evaluates to “abcd” - “ab” + 2 produces an ERROR "],["dictionaries.html", "Section 3 Dictionaries 3.1 Dictionary as a collection of counters 3.2 Looping and dictionaries 3.3 Reverse Lookup", " Section 3 Dictionaries A dictionary is like a list, but more general. In a list, the indices have to be integers; in a dictionary they can be (almost) any type. A dictionary contains a collection of indices, which are called keys, and a collection of values Each key is associated with a single value. The association of a key and a value is called a key-value pair Dictionaries represent a mapping from keys to values, so you can also say that each key “maps to” a value The function dict creates a new dictionary with no items en2ch = dict() en2ch {} squiggly-brackets, {}, represent an empty dictionary. To add items to the dictionary, you can use square brackets: en2ch[&#39;one&#39;] = &#39;yi&#39; This line creates an item that maps from the key ‘one’ to the value ‘yi.’ If we print the dictionary again, we see a key-value pair with a colon between the key and value: en2ch {&#39;one&#39;: &#39;yi&#39;} This output format is also an input format. For example, you can create a new dictionary with three items: en2ch = {&#39;one&#39;:&#39;yi&#39;,&#39;two&#39;:&#39;er&#39;,&#39;three&#39;:&#39;san&#39;} en2ch {&#39;one&#39;: &#39;yi&#39;, &#39;two&#39;: &#39;er&#39;, &#39;three&#39;: &#39;san&#39;} The order of the key-value pairs might not be the same If you type the same example on your computer, you might get a different result The order of items in a dictionary is unpredictable. Not a problem since the elements of a dictionary are never indexed with integer indices Instead, you use the keys to look up the corresponding values: en2ch[&#39;two&#39;] &#39;er&#39; The key ‘two’ always maps to the value ‘er’ so the order of the items doesn’t matter. If the key isn’t in the dictionary, you get an exception: en2ch[&#39;four&#39;] --------------------------------------------------------------------------- KeyError Traceback (most recent call last) &lt;ipython-input-14-bf386d070566&gt; in &lt;module&gt;() ----&gt; 1 en2ch[&#39;four&#39;] KeyError: &#39;four&#39; The len function works on dictionaries; it returns the number of key-value pairs: len(en2ch) 3 The in operator works on dictionaries; it tells you whether something appears as a key in the dictionary &#39;one&#39; in en2ch True &#39;yi&#39; in en2ch False To see whether something appears as a value in a dictionary, you can use the method values, which returns a collection of values, and then use the in operator: &#39;yi&#39; in en2ch.values() True en2ch.values() dict_values([&#39;yi&#39;, &#39;er&#39;, &#39;san&#39;]) 3.1 Dictionary as a collection of counters Suppose you are given a string and you want to count how many times each letter appears. An advantage of a dictionary implementation is that we don’t have to know ahead of time which letters appear in the string and we only have to make room for the letters that do appear def alpha_count(s): d = dict() for c in s: if c not in d: d[c] = 1 else: d[c] += 1 return d The first line of the function creates an empty dictionary. The for loop traverses the string. Each time through the loop, if the character c is not in the dictionary, we create a new item with key c and the initial value 1 (since we have seen this letter once). If c is already in the dictionary we increment d[c] h = alpha_count(&#39;the quick brown fox jumped over the lazy dog&#39;) h {&#39;t&#39;: 2, &#39;h&#39;: 2, &#39;e&#39;: 4, &#39; &#39;: 8, &#39;q&#39;: 1, &#39;u&#39;: 2, &#39;i&#39;: 1, &#39;c&#39;: 1, &#39;k&#39;: 1, &#39;b&#39;: 1, &#39;r&#39;: 2, &#39;o&#39;: 4, &#39;w&#39;: 1, &#39;n&#39;: 1, &#39;f&#39;: 1, &#39;x&#39;: 1, &#39;j&#39;: 1, &#39;m&#39;: 1, &#39;p&#39;: 1, &#39;d&#39;: 2, &#39;v&#39;: 1, &#39;l&#39;: 1, &#39;a&#39;: 1, &#39;z&#39;: 1, &#39;y&#39;: 1, &#39;g&#39;: 1} h indicates that the letters ‘t,’ ‘h’ appeared twice; ‘e’ appears four times; etc. Dictionaries have a method called get that takes a key and a default value. If the key appears in the dictionary, get returns the corresponding value; otherwise it returns the default value: h = alpha_count(&#39;a&#39;) h {&#39;a&#39;: 1} h.get(&#39;a&#39;,0) # Find &#39;a&#39;, if not return 0 1 h.get(&#39;b&#39;,0) 0 3.1.0.1 Exercise Use get to write alpha_count more concisely. def alpha_count(s): d = dict() for c in s: if c not in d: d[c] = 1 else: d[c] += 1 return d Hint: You should be able to eliminate the if statement. def alpha_count(s): d = dict() for c in s: d[c] = d.get(c, 0) + 1 return d alpha_count(&#39;carrot&#39;) {&#39;c&#39;: 1, &#39;a&#39;: 1, &#39;r&#39;: 2, &#39;o&#39;: 1, &#39;t&#39;: 1} 3.2 Looping and dictionaries If you use a dictionary in a for statement, it traverses the keys of the dictionary. For example, print_count prints each key and the corresponding value: def print_count(h): for k in h: print(k, h[k]) h = alpha_count(&#39;parrot&#39;) h {&#39;p&#39;: 1, &#39;a&#39;: 1, &#39;r&#39;: 2, &#39;o&#39;: 1, &#39;t&#39;: 1} print_count(h) p 1 a 1 r 2 o 1 t 1 The keys are in no particular order. To traverse the keys in sorted order, you can use the built-in function sorted: for key in sorted(h): print(key, h[key]) a 1 o 1 p 1 r 2 t 1 3.2.0.1 Exercise Use the sorted function to have print_count print a sorted dictionary. def print_count(h): for c in h: print(c, h[c]) def print_count(h): for c in sorted(h): print(c, h[c]) h = alpha_count(&#39;Pparrot&#39;.lower()) print_count(h) a 1 o 1 p 2 r 2 t 1 3.3 Reverse Lookup Given a dictionary d and a key k, it is easy to find the corresponding value v = d[k]. This operation is called a lookup. But what if you have v and you want to find k? You have two problems: 1. There might be more than one key that maps to the value v. 2. There is no simple syntax to do a reverse lookup; you have to search. Here is a function that takes a value and returns the first key that maps to that value: def reverse_lookup(d, v): for k in d: if d[k] == v: return k raise LookupError(&#39;value does not appear in the dictionary&#39;) h {&#39;p&#39;: 1, &#39;a&#39;: 1, &#39;r&#39;: 2, &#39;o&#39;: 1, &#39;t&#39;: 1} reverse_lookup(h,2) &#39;r&#39; reverse_lookup(h,1) &#39;p&#39; A reverse lookup is much slower than a forward lookup; if you have to do it often, or if the dictionary gets big, the performance of your program will suffer. "],["functions-part-1.html", "Section 4 Functions: Part 1 4.1 Function calls 4.2 Math functions 4.3 New Functions 4.4 Definitions and Uses 4.5 Parameters and arguments 4.6 Variables and parameters are local 4.7 Why functions", " Section 4 Functions: Part 1 function is a named sequence of statements that performs a computation. When you define a function, you specify the name and the sequence of statements. Later, you can “call” the function by name. 4.1 Function calls We’ve already see a function call: type(42) int The name of the function is type. The expression in parentheses is called the argument of the function. The result, for this function, is the type of the argument. It is common to say that a function “takes” an argument and “returns” a result. The result is also called the return value. int(&#39;32&#39;) # string to int 32 int(&#39;Hello&#39;) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-3-6765ce49acfe&gt; in &lt;module&gt;() ----&gt; 1 int(&#39;Hello&#39;) ValueError: invalid literal for int() with base 10: &#39;Hello&#39; float(32) # int / string to float 32.0 float(&#39;3.14&#39;) 3.14 str(31) # int / float to string &#39;31&#39; 4.2 Math functions Python has a math module that provides most of the familiar mathematical functions. A module is a file that contains a collection of related functions. Before we can use the functions in a module, we have to import it with an import statement: import math This statement creates a module object named math. math &lt;module &#39;math&#39; from &#39;/anaconda3/lib/python3.6/lib-dynload/math.cpython-36m-darwin.so&#39;&gt; The module object contains the functions and variables defined in the module. To access one of the functions, you have to specify the name of the module and the name of the function, separated by a period. This format is called dot notation 4.2.1 Example degrees = 45 radians = degrees / 180.0 * math.pi math.sin(radians) 0.7071067811865475 The expression math.pi gets the variable pi from the math module. Its value is a floating-point approximation of \\(\\pi\\), accurate to about 15 digits. math.pi 3.141592653589793 4.3 New Functions A function definition specifies the name of a new function and the sequence of statements that run when the function is called. def print_lyrics(): print(&quot;Hello darkness my old friend&quot;) print(&quot;Pink fluffy unicorns!&quot;) def is a keyword that indicates that this is a function definition. Defining a function creates a function object, which has type function: type(print_lyrics) function print_lyrics() Hello darkness my old friend Pink fluffy unicorns! Once you have defined a function, you can use it inside another function. def repeat_lyrics(): print_lyrics() print_lyrics() repeat_lyrics() Hello darkness my old friend Pink fluffy unicorns! Hello darkness my old friend Pink fluffy unicorns! 4.4 Definitions and Uses Pulling together the code fragments from the previous section, the whole program looks like this: def print_lyrics(): print(&quot;I&#39;m a lumberjack, and I&#39;m okay.&quot;) print(&quot;I sleep all night and I work all day.&quot;) def repeat_lyrics(): print_lyrics() print_lyrics() repeat_lyrics() This program contains two function definitions: print_lyrics and repeat_lyrics. You have to create a function before you can run it. In other words, the function definition has to run before the function gets called. 4.5 Parameters and arguments Some of the functions we have seen require arguments. Inside the function, the arguments are assigned to variables called parameters. Here is a definition for a function that takes an argument: def print_twice(param): print(param) print(param) print_twice(&#39;Hello&#39;) Hello Hello print_twice(42) 42 42 print_twice(math.pi) 3.141592653589793 3.141592653589793 print_twice(&#39;Spam &#39; * 10) Spam Spam Spam Spam Spam Spam Spam Spam Spam Spam Spam Spam Spam Spam Spam Spam Spam Spam Spam Spam print_twice(math.cos(math.pi)) -1.0 -1.0 The argument is evaluated before the function is called, so in the examples the expressions 'Spam '*10 and math.cos(math.pi) are only evaluated once. spam = &#39;Spam is the king of breakfast!&#39; print_twice(spam) Spam is the king of breakfast! Spam is the king of breakfast! 4.6 Variables and parameters are local When you create a variable inside a function, it is local, which means that it only exists inside the function. For example: def cat_twice(part1, part2): cat = part1 + part2 print_twice(cat) This function takes two arguments, concatenates them, and prints the result twice. Here is an example that uses it: line1 = &#39;Hello Darkness! &#39; line2 = &#39;Big Fluffy Unicorns!&#39; cat_twice(line1, line2) Hello Darkness! Big Fluffy Unicorns! Hello Darkness! Big Fluffy Unicorns! When cat_twice terminates, the variable cat is destroyed. If we try to print it, we get an exception: print(cat) --------------------------------------------------------------------------- NameError Traceback (most recent call last) &lt;ipython-input-28-34599fba884e&gt; in &lt;module&gt;() ----&gt; 1 print(cat) NameError: name &#39;cat&#39; is not defined 4.7 Why functions It may not be clear why it is worth the trouble to divide a program into functions. There are several reasons: Creating a new function gives you an opportunity to name a group of statements, which makes your program easier to read and debug. Functions can make a program smaller by eliminating repetitive code. Later, if you make a change, you only have to make it in one place. Dividing a long program into functions allows you to debug the parts one at a time and then assemble them into a working whole. Well-designed functions are often useful for many programs. Once you write and debug one, you can reuse it. "],["functions-part-2.html", "Section 5 Functions: Part 2 5.1 Fruitful Functions (For Business) 5.2 Return values 5.3 Boolean Functions 5.4 Recursion (Factorial Example) 5.5 Fibonacci Example 5.6 Checking types: Factorial Example 2", " Section 5 Functions: Part 2 5.1 Fruitful Functions (For Business) Many of the Python functions we have used (the math functions) produce return values. But the functions we’ve written are all void: they have an effect, like printing a value, but they don’t have a return value Today you’ll learn to write fruitful functions 5.2 Return values Calling the function generates a return value, which we (usually) assign to a variable or use as part of an expression Today we are going to write fruitful functions The first exampe is cash_flow, which returns your cash flow given income and expenses def cash_flow(income, expenses): cf = income - expenses return cf cash_flow(10000, 9000.0) 1000.0 We have seen the return statement before, but in a fruitful function the return statement includes an expression. This statement means: “Return immediately from this function and use the following expression as a return value.” We could have written this function more concisely: def cash_flow(income, expenses): return income - expenses 5.2.0.0.1 Incremental development As you write larger functions, you might find yourself spending more time debugging. To deal with increasingly complex programs, you might want to try a process called incremental development. 5.2.1 Example Suppose you want to calculate compound interest: Compound interest refers to calculating the compounded interest, not just the interest gained on the principal invested or borrowed amount. \\[A = P(1+\\frac{r}{n})^{nt}\\] where A = New Principal (principal + interest) P = Original Principal Amount r = Annual Nominal Interest Rate (Float) t = Overall length of time the interest is applied n = Compounding frequency 5.2.1.1 Step 1 The first step is to consider what a comp_interest function should look like. - What are the inputs (parameters) and what is the output (return value)? - In this case, the inputs are four numbers. - The return value is the accrued amount (principal + interest) AND the total compound interest (represented by a floating-point value) 5.2.1.2 Step 2 Write an outline of the function: def comp_interest(principal, rate, n, t): return 0.0 Obviously, this version doesn’t compute our values; it always returns zero. But it is syntactically correct, and it runs, which means that you can test it before you make it more complicated. comp_interest(5000, 0.08, 4, 2) # Easy to solve 0.0 At this point we have confirmed that the function is syntactically correct, and we can start adding code to the body. 5.2.1.3 Step 3 A reasonable next step is to find the subvalues. def comp_interest(principal, rate, n, t): r_n = 1 + (rate / n) nt = n * t print(&#39;Compound rate: &#39;, r_n) print(&#39;Frequency: &#39;, nt) return 0.0 If the function is working, it should display: comp_interest(5000,0.08,4,2) Compound rate: 1.02 Frequency: 8 5.2.1.4 Step 4 Next compute the result def comp_interest(principal, rate, n, t): r_n = 1 + (rate / n) nt = n * t result = principal*r_n**nt return round(result,2) When you start out, you should add only a line or two of code at a time. As you gain more experience, you might find yourself writing and debugging bigger chunks. Either way, incremental development can save you a lot of debugging time. The key aspects of the process are: Start with a working program and make small incremental changes. At any point, if there is an error, you should have a good idea where it is. Use variables to hold intermediate values so you can display and check them. Once the program is working, you might want to remove some of the scaffolding or consolidate multiple statements into compound expressions, but only if it does not make the program difficult to read. 5.2.2 Example 2 \\[B = R [\\frac{1-(1+i)^{-(n-x)}}{i}]\\] def rem_bal(reg_pay, i, num_pay, pay_made): return 0.0 def rem_bal(reg_pay, i, num_pay, pay_made): a = 1+i b = -(num_pay-pay_made) c = 1 - a**b d = c / i result = reg_pay * d return round(result,2) rem_bal(5000, 0.08, 4, 2) 8916.32 5.3 Boolean Functions Functions can return booleans, which is often convenient for hiding complicated tests inside functions. For example: def is_divisible(x, y): if x % y == 0: return True else: return False It is common to give boolean functions names that sound like yes/no questions The result of the == operator is a boolean, so we can write the function more concisely by returning it directly: def is_divisible(x, y): return x % y == 0 5.4 Recursion (Factorial Example) The factorial of a non-negative integer n, denoted by n!, is the product of all positive integers less than or equal to n. \\[0! = 1\\] \\[n! = n(n-1)!\\] If you can write a recursive definition of something, you can write a Python program to evaluate it 5.4.1 Step 1: Decide on parameters factorial takes an integer def factorial(n): 5.4.2 Step 2: Add basic conditional argument if the argument happens to be 0, we return 1: def factorial(n): if n == 0: return 1 5.4.3 Step 3: Make it recursive Otherwise, we have to make a recursive call to 1. find the factorial of n-1 2. multiply it by n def factorial(n): if n == 0: return 1 else: recurse = factorial(n-1) result = n * recurse return result This should look similar to our countdown example def factorial(n): if n == 0: return 1 else: recurse = factorial(n-1) result = n * recurse return result factorial(500) 1220136825991110068701238785423046926253574342803192842192413588385845373153881997605496447502203281863013616477148203584163378722078177200480785205159329285477907571939330603772960859086270429174547882424912726344305670173270769461062802310452644218878789465754777149863494367781037644274033827365397471386477878495438489595537537990423241061271326984327745715546309977202781014561081188373709531016356324432987029563896628911658974769572087926928871281780070265174507768410719624390394322536422605234945850129918571501248706961568141625359056693423813008856249246891564126775654481886506593847951775360894005745238940335798476363944905313062323749066445048824665075946735862074637925184200459369692981022263971952597190945217823331756934581508552332820762820023402626907898342451712006207714640979456116127629145951237229913340169552363850942885592018727433795173014586357570828355780158735432768888680120399882384702151467605445407663535984174430480128938313896881639487469658817504506926365338175055478128640000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 5.5 Fibonacci Example fibonacci(0) = 0 fibonacci(1) = 1 fibonacci(n) = fibonacci(n−1) + fibonacci(n−2) Translated into a Python function: def fibonacci(n): if n ==0: return 0 elif n == 1: return 1 else: return fibonacci(n-1) + fibonacci(n-2) fibonacci(4) 3 5.6 Checking types: Factorial Example 2 What happens if we call factorial and give it 1.5 as an argument? factorial(1.5) RuntimeError: Maximum recursion depth exceeded It looks like an infinite recursion. How can that be? The function has a base case—when n == 0. But if n is not an integer, we can miss the base case and recurse forever 5.6.1 Solution Use the built-in function isinstance to verify the type of the argument def factorial(n): if not isinstance(n, int): print(&#39;Factorial is only defined for integers.&#39;) return None elif n &lt; 0: print(&#39;Factorial is not defined for negative integers.&#39;) return None elif n == 0: return 1 else: return n * factorial(n-1) factorial(&#39;bob&#39;) Factorial is only defined for integers. factorial(-2) Factorial is not defined for negative integers. factorial(0) 1 "],["iteration.html", "Section 6 Iteration 6.1 Reassignment 6.2 Updating variables 6.3 while statement 6.4 break 6.5 Example: Square roots", " Section 6 Iteration iteration: the ability to run a block of statements repeatedly Saw a kind of iteration with recursion and using for loop 6.1 Reassignment Python uses the equal sign = for assignment Legal to make more than one assignment to the same variable New assignment makes an existing variable refer to a new value (and stop referring to the old value) x = 5 x 5 x = 7 x 7 a = 5 b = a # a &amp; b are equal a = 3 # a &amp; b are NOT equal b 5 Reassigning variables is useful, but you should use it with caution If the values of variables change frequently, it can make the code difficult to read and debug 6.2 Updating variables Common reassignment is an update, where the new value depends on the old c = c + 1 --------------------------------------------------------------------------- NameError Traceback (most recent call last) &lt;ipython-input-5-511f86f677ea&gt; in &lt;module&gt;() ----&gt; 1 c = c + 1 NameError: name &#39;c&#39; is not defined Before you can update a variable, you have to initialize it, usually with a simple assignment: z = 0 z = z + 1 Updating a variable by adding 1 is called an increment; subtracting 1 is called a decrement. 6.3 while statement Repeating identical or similar tasks without making errors is something that computers do well and people do poorly Since iteration is so common: Python provides features to make it easier def countdown(n): while n &gt; 0: print(n) n = n - 1 print(&#39;Blastoff!&#39;) countdown(3) 3 2 1 Blastoff! def countdown(n): while n &gt; 0: print(n) n = n - 1 print(&#39;Blastoff!&#39;) Here is the flow of execution for a while statement: Determine whether the condition is true or false. If false, exit the while statement and continue execution at the next statement. If the condition is true, run the body and then go back to step 1. This type of flow is called a loop because the third step loops back around to the top. def countdown(n): while n &gt; 0: print(n) n = n - 1 print(&#39;Blastoff!&#39;) The body of the loop should change the value of one or more variables so that the condition becomes false eventually and the loop terminates. Otherwise the loop will repeat forever, which is called an infinite loop. 6.4 break Sometimes you don’t know it’s time to end a loop until you get half way through the body In that case you can use the break statement to jump out of the loop. Suppose you want to take input from the user until they type done. You could write: while True: line = input(&#39;&gt; &#39;) if line == &#39;done&#39;: break print(line) print(&#39;Done&#39;) &gt; not done not done &gt; done Done while True: line = input(&#39;&gt; &#39;) if line == &#39;done&#39;: break print(line) print(&#39;Done&#39;) The loop condition is True, which is always true, so the loop runs until it hits the break statement This way of writing while loops is common because you can check the condition anywhere in the loop (not just at the top) and you can express the stop condition affirmatively rather than negatively E.g. - “stop when this happens” - “keep going until that happens” 6.5 Example: Square roots Loops are often used in programs that compute numerical results by starting with an approximate answer and iteratively improving it Suppose that you want to know the square root of a If you start with almost any estimate, x, you can compute a better estimate with the following formula: \\[y=\\frac{x+a/x}{2}\\] For example, if a is 4 and x is 3: a = 4 x = 3 y = (x + a/x) / 2 y The result is closer to the correct answer (√4 = 2). If we repeat the process with the new estimate, it gets even closer: # Run this a few times # y will get closer to 2 x = y y = (x + a/x) / 2 y 2.0 In general we don’t know ahead of time how many steps it takes to get to the right answer, but we know when we get there because the estimate stops changing When y == x, we can stop. Here is a loop that starts with an initial estimate, x, and improves it until it stops changing: while True: print(x) y = (x + a/x) / 2 if y == x: break x = y 2.0000000000262146 2.0 while True: print(x) y = (x + a/x) / 2 if y == x: break x = y def mysquart(a, x): while True: y = (x + a/x) / 2 if y == x: break x = y return x mysquart(4, 3) 2.1666666666666665 "],["strings.html", "Section 7 Strings 7.1 A string is a sequence 7.2 len 7.3 Traversal with a for loop 7.4 Example: Concatenation 7.5 Example: In-class question 7.6 String slices 7.7 Strings are immutable 7.8 Search 7.9 Looping and counting 7.10 String methods 7.11 in operator 7.12 Example 7.13 String comparison", " Section 7 Strings Strings are not like integers, floats, and booleans. A string is a sequence, which means it is an ordered collection of other values. 7.1 A string is a sequence A string is a sequence of characters You can access the characters one at a time with the bracket operator: fruit = &#39;coconut&#39; letter = fruit[1] letter &#39;o&#39; The expression in brackets is called an index. The index indicates which character in the sequence you want (hence the name). fruit = &#39;coconut&#39; letter = fruit[1] letter &#39;o&#39; For most people, the first letter of ‘coconut’ is c, not o. But for computer scientists, the index is an offset from the beginning of the string, and the offset of the first letter is zero. letter = fruit[0] letter &#39;c&#39; 7.2 len len is a built-in function that returns the number of characters in a string: fruit = &#39;lime&#39; len(fruit) 4 To get the last letter of a string, you might be tempted to try something like this: length = len(fruit) fruit[4] --------------------------------------------------------------------------- IndexError Traceback (most recent call last) &lt;ipython-input-10-9c7dbe8f62db&gt; in &lt;module&gt;() 1 length = len(fruit) ----&gt; 2 fruit[4] IndexError: string index out of range The reason for the IndexError is that there is no letter in ’banana’ with the index 6. Since we started counting at zero, the six letters are numbered 0 to 5. To get the last character, you have to subtract 1 from length: fruit[len(fruit) - 1] &#39;a&#39; Or you can use negative indices, which count backward from the end of the string. fruit[-1] 7.3 Traversal with a for loop A lot of computations involve processing a string one character at a time Often they start at the beginning, select each character in turn, do something to it, and continue until the end This pattern of processing is called a traversal. One way to write a traversal is with a while loop: fruit = &#39;kiwi&#39; index = 0 while index &lt; len(fruit): letter = fruit[index] print(letter) index = index + 1 Another way to write a traversal is with a for loop: for letter in fruit: print(letter) 7.4 Example: Concatenation The following example shows how to use concatenation (string addition) and a for loop to generate an abecedarian series (that is, in alphabetical order). In Robert McCloskey’s book Make Way for Ducklings, the names of the ducklings are Jack, Kack, Lack, Mack, Nack, Ouack, Pack, and Quack. This loop outputs these names in order: prefixes = &#39;JKLMNOPQ&#39; suffix = &#39;ack&#39; for letter in prefixes: print(letter + suffix) Jack Kack Lack Mack Nack Oack Pack Qack 7.5 Example: In-class question Notice that Ouack and Quack are misspelled: How do you fix this? prefixes = &#39;JKLMNOPQ&#39; suffix = &#39;ack&#39; for letter in prefixes: if letter == &#39;O&#39; or letter == &#39;Q&#39;: print(letter + &#39;u&#39; + suffix) else: print(letter + suffix) Jack Kack Lack Mack Nack Ouack Pack Quack 7.6 String slices Slice: a segment of a string. - Selecting a slice is similar to selecting a character - The operator [n:m] returns the part of the string from the \\(n^{th}\\) character to the \\(m^{th}\\) character - including the first but excluding the last. s = &#39;Henderson Reddies&#39; s[0:9] s[10:17] If you omit the first index (before the colon), the slice starts at the beginning of the string. If you omit the second index, the slice goes to the end of the string: fruit = &#39;peach&#39; fruit[:3] fruit[3:] 7.7 Strings are immutable You cannot use the [] operator to change characters in a string: greeting = &#39;Hello, students!&#39; greeting[0] = &#39;J&#39; The reason for the error is that strings are immutable, which means you can’t change an existing string. The best you can do is create a new string that is a variation on the original: greeting = &#39;Hello, students!&#39; new_greeting = &#39;J&#39; + greeting[1:] new_greeting This example concatenates a new first letter onto a slice of greeting. It has no effect on the original string. 7.8 Search What does the following function do? def find(word, letter): index = 0 while index &lt; len(word): if word[index] == letter: return index index = index + 1 return -1 In a sense, find is the inverse of the [] operator. Instead of taking an index and extracting the corresponding character, it takes a character and finds the index where that character appears. If the character is not found, the function returns -1. This pattern of computation—traversing a sequence and returning when we find what we are looking for—is called a search. 7.9 Looping and counting This program counts the number of times the letter a appears in a string: word = &#39;mississippi&#39; count = 0 for letter in word: if letter == &#39;i&#39;: count = count + 1 print(count) This program demonstrates another pattern of computation called a counter. The variable count is initialized to 0 and then incremented each time an a is found. When the loop exits, count contains the result—the total number of a’s. 7.10 String methods Strings provide methods that perform a variety of useful operations. A method is similar to a function—it takes arguments and returns a value—but the syntax is different. Instead of the function syntax upper(word), it uses the method syntax word.upper(). word = &#39;reddie&#39; new_word = word.upper() new_word new_word = word.upper() This form of dot notation specifies the name of the method, upper, and the name of the string to apply the method to, word. The empty parentheses indicate that this method takes no arguments. A method call is called an invocation; 7.11 in operator The word in is a boolean operator that takes two strings and returns True if the first appears as a substring in the second: &#39;e&#39; in &#39;reddie&#39; &#39;f&#39; in &#39;reddie&#39; 7.12 Example The following function prints all the letters from word1 that also appear in word2: def in_both(word1, word2): for letter in word1: if letter in word2: print(letter) in_both(&#39;Henderson&#39;, &#39;Reddies&#39;) 7.13 String comparison The relational operators work on strings. To see if two strings are equal: word = &#39;apple&#39; if word == &#39;banana&#39;: print(&#39;All right, bananas.&#39;) Other relational operations are useful for putting words in alphabetical order: word = &#39;banana&#39; # Try apple and pineapple if word &lt; &#39;banana&#39;: print(&#39;Your word, &#39; + word + &#39;, comes before banana.&#39;) elif word &gt; &#39;banana&#39;: print(&#39;Your word, &#39; + word + &#39;, comes after banana.&#39;) else: print(&#39;All right, bananas.&#39;) "],["conditionals-and-recursion.html", "Section 8 Conditionals and Recursion 8.1 Floor division and modulus 8.2 Boolean expressions 8.3 Logical operators 8.4 Conditional execution 8.5 Alternative execution 8.6 Chained conditionals 8.7 Nested conditionals 8.8 Recursion 8.9 Infinite recursion 8.10 Keyboard Input", " Section 8 Conditionals and Recursion if statement executes code depending on the state of the program 8.1 Floor division and modulus The floor division operator, //, divides two numbers and rounds down to an integer. Suppose you have a movie with a runtime of 105 minutes. You might want to know how long that is in hours. Conventional division returns a floating-point minutes = 105 minutes / 60 1.75 But we don’t normally write hours with decimal points. Floor division returns the integer number of hours, rounding down: hours = minutes // 60 hours 1 Modulus operator, %, which divides two numbers and returns the remainder. remainder = minutes % 60 remainder 45 def movie_time(minutes): &quot;&quot;&quot;Converts movie time from minutes to hours minutes. Output: ========= X hrs Y mins &quot;&quot;&quot; hrs = minutes // 60 mins = minutes % 60 print(str(hrs) + &#39; hrs &#39; + str(mins) +&#39; mins&#39;) movie_time(105) 1 hrs 45 mins 8.2 Boolean expressions A boolean expression is an expression that is either true or false. The following examples use the operator ==, which compares two operands and produces True if they are equal and False otherwise: 5 == 5 True 5 == 6 False type(True) bool type(False) bool The == operator is one of the relational operators; the others are: Relational Operator Description x != y x is not equal to y x &gt; y x is greater than y x &lt; y x is less than y x &gt;= y x is greater than or equal to y x &lt;= y x is less than or equal to y 8.3 Logical operators There are three logical operators: - and, or, and not. The meaning of these operators is similar to their meaning in English. For example, x &gt; 0 and x &lt; 10 is true only if x is greater than 0 and less than 10. 5 &gt; 0 and 5 &lt; 10 True 20 &gt; 0 and 20 &lt; 10 False n%2 == 0 or n%3 == 0 is true if either or both of the conditions is true, that is, if the number is divisible by 2 or 3 4 % 2 == 0 or 4 % 3 == 0 True 9 % 2 == 0 or 9 % 3 == 0 True 9 % 2 == 0 or 10 % 3 == 0 False the not operator negates a boolean expression, so not (x &gt; y) is true if x &gt; y is false, that is, if x is less than or equal to y. not (10 &gt; 5) False (10 &gt; 5) True 8.4 Conditional execution In order to write useful programs, we almost always need the ability to check conditions and change the behavior of the program accordingly. Conditional statements give us this ability. The simplest form is the if statement: x = 10 if x &gt; 0: print(&#39;x is positive&#39;) x is positive The boolean expression after if is called the condition. If it is true, the indented statement runs. If not, nothing happens. if statements have the same structure as function definitions: a header followed by an indented body. Statements like this are called compound statements. 8.5 Alternative execution A second form of the if statement is “alternative execution,” in which there are two possibilities and the condition determines which one runs. The alternatives are called branches, because they are branches in the flow of execution. x = 10 if x % 2 ==0: print(&#39;x is even&#39;) else: print(&#39;x is odd&#39;) x is even x = 9 if x % 2 ==0: print(&#39;x is even&#39;) else: print(&#39;x is odd&#39;) 8.6 Chained conditionals Sometimes there are more than two possibilities and we need more than two branches. elif is an abbreviation of “else if.” Exactly one branch will run. There is no limit on the number of elif statements. If there is an else clause, it has to be at the end, but there doesn’t have to be one. One way to express a computation like that is a chained conditional: x = 10 y = 11 if x &lt; y: print(&#39;x is less than y&#39;) elif x &gt; y: print(&#39;x is greater than y&#39;) else: print(&#39;x and y are equal&#39;) x is less than y x = 12 y = 11 if x &lt; y: print(&#39;x is less than y&#39;) elif x &gt; y: print(&#39;x is greater than y&#39;) else: print(&#39;x and y are equal&#39;) x = 12 y = 12 if x &lt; y: print(&#39;x is less than y&#39;) elif x &gt; y: print(&#39;x is greater than y&#39;) else: print(&#39;x and y are equal&#39;) 8.7 Nested conditionals One conditional can also be nested within another. The outer conditional contains two branches. The first branch contains a simple statement. The second branch contains another if statement, which has two branches of its own. Those two branches are both simple statements, although they could have been conditional statements as well. x = 12 y = 12 if x == y: print(&#39;x and y are equal&#39;) else: if x &lt; y: print(&#39;x is less than y&#39;) else: print(&#39;x is greater than y&#39;) x and y are equal x = 12 y = 11 if x == y: print(&#39;x and y are equal&#39;) else: if x &lt; y: print(&#39;x is less than y&#39;) else: print(&#39;x is greater than y&#39;) x = 10 y = 11 if x == y: print(&#39;x and y are equal&#39;) else: if x &lt; y: print(&#39;x is less than y&#39;) else: print(&#39;x is greater than y&#39;) Although the indentation of the statements makes the structure apparent, nested conditionals become difficult to read very quickly. It is a good idea to avoid them when you can. 8.8 Recursion It is legal for one function to call another; it is also legal for a function to call itself. It may not be obvious why that is a good thing, but it turns out to be one of the most magical things a program can do. 8.8.1 What is recursion in Python? Recursion is the process of defining something in terms of itself. A physical world example would be to place two parallel mirrors facing each other. Any object in between them would be reflected recursively. def countdown(n): if n &lt;= 0: print(&#39;Blastoff!&#39;) else: print(n) countdown(n-1) If n is 0 or negative, it outputs the word, “Blastoff!” Otherwise, it outputs n and then calls a function named countdown—itself—passing n-1 as an argument. countdown(5) 5 4 3 2 1 Blastoff! 8.9 Infinite recursion Infinite recursion is when a recursion never reaches a base case, it goes on making recursive calls forever, and the program never terminates. In most programming environments, a program with infinite recursion does not really run forever. Python reports an error message when the maximum recursion depth is reached: def recursion(): recursion() recursion() --------------------------------------------------------------------------- RecursionError Traceback (most recent call last) &lt;ipython-input-58-c6e0f7eb0cde&gt; in &lt;module&gt;() ----&gt; 1 recursion() &lt;ipython-input-57-d9b9ba688751&gt; in recursion() 1 def recursion(): ----&gt; 2 recursion() ... last 1 frames repeated, from the frame below ... &lt;ipython-input-57-d9b9ba688751&gt; in recursion() 1 def recursion(): ----&gt; 2 recursion() RecursionError: maximum recursion depth exceeded 8.10 Keyboard Input Python provides a built-in function called input that stops the program and waits for the user to type something. When the user presses Return or Enter, the program resumes and input returns what the user typed as a string. text = input() blah text &#39;blah&#39; number = input(&#39;Pick a number between 1 and 3:\\n&#39;) Pick a number between 1 and 3: 2 number &#39;2&#39; int(number) 2 What if they typed out the digits instead? number = input(&#39;Pick a number between 1 and 3:\\n&#39;) Pick a number between 1 and 3: two int(number) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-65-23a75fe4b6c6&gt; in &lt;module&gt;() ----&gt; 1 int(number) ValueError: invalid literal for int() with base 10: &#39;two&#39; "],["lists.html", "Section 9 Lists 9.1 List == Sequence 9.2 Lists are Mutable 9.3 Traversing a list 9.4 List operations 9.5 List slices 9.6 List methods 9.7 Map, filter, reduce 9.8 Deleting Elements 9.9 Lists and strings 9.10 List arguments 9.11 Debugging", " Section 9 Lists 9.1 List == Sequence list: sequence of values Values inside of lists are elements or items There are several ways to create a new list; the simplest is to enclose the elements in square brackets [] [10,20,30,40] # list of 4 integers [10, 20, 30, 40] [&#39;Eli&#39;,&#39;Xu&#39;,&#39;Markum&#39;] # list of 3 strings [&#39;Eli&#39;, &#39;Xu&#39;, &#39;Markum&#39;] Elements of a list do NOT have to be the same type: [&#39;string&#39;,2.0,5,[10,20], True] # list of string, float, integer, another list [&#39;string&#39;, 2.0, 5, [10, 20], True] A list within another list is nested A list that contains no items is an empty list [] [] You can assign values to variables people = [&#39;Eli&#39;,&#39;Xu&#39;,&#39;Markum&#39;] numbers = [3,6,9] empty = [] print(people, numbers, empty) [&#39;Eli&#39;, &#39;Xu&#39;, &#39;Markum&#39;] [3, 6, 9] [] 9.2 Lists are Mutable Syntax for accessing the elements of a list is the same as for accessing the characters of a string—the bracket operator. The expression inside the brackets specifies the index. Remember that the indices start at 0: people[0] &#39;Eli&#39; pep = [&#39;e&#39;,&#39;x&#39;,&#39;m&#39;,[0,&#39;target&#39;]] pep[3][1] &#39;target&#39; Unlike strings, lists are mutable. When the bracket operator appears on the left side of an assignment, it identifies the element of the list that will be assigned. numbers = [10,20,30] numbers[1] = 21 numbers [10, 21, 30] List indices work the same way as string indices: Any integer expression can be used as an index. If you try to read or write an element that does not exist, you get an IndexError. If an index has a negative value, it counts backward from the end of the list. The in operator also works on lists: &#39;Xu&#39; in people True &#39;Bob&#39; in people False 9.3 Traversing a list The most common way to traverse the elements of a list is with a for loop. The syntax is the same as for strings: for p in people: print(&#39;Hello, &#39;, p) Hello, Eli Hello, Xu Hello, Markum people [&#39;Eli&#39;, &#39;Xu&#39;, &#39;Markum&#39;] for num in [1,2,3]: num2 = num * 2 print(num2) 2 4 6 This works well if you only need to read the elements of the list. But if you want to write or update the elements, you need the indices. A common way to do that is to combine the built-in functions range and len: numbers = [10, 20, 30] for i in range(len(numbers)): numbers[i] = numbers[i] * 2 numbers [20, 40, 60] How would we translate the above code into a function? Call the function double_it that takes a list, t. def double_it(t): &quot;&quot;&quot; double_it takes a list and doubles the values inside. t: list &quot;&quot;&quot; for i in range(len(t)): t[i] = t[i] * 2 return t double_it([1,2,3]) [2, 4, 6] 9.4 List operations The + operator concatenates lists: a = [1,2,3] b = [4,5,6] c = a + b c [1, 2, 3, 4, 5, 6] The * operator repeats a list a given number of times: [0] * 6 [0, 0, 0, 0, 0, 0] [1, 2, 3] * 3 # Waltz sequence [1, 2, 3, 1, 2, 3, 1, 2, 3] 9.5 List slices t = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;] t[1:3] [&#39;b&#39;, &#39;c&#39;] t[:4] [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;] t[2:] [&#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;] t[:] [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;] A slice operator on the left side of an assignment can update multiple elements: t[1:3] = [&#39;Change&#39;, &#39;This&#39;] t [3, &#39;Change&#39;, &#39;This&#39;, &#39;test&#39;, &#39;test&#39;, &#39;test&#39;] 9.6 List methods Python provides methods that operate on lists. - append adds a new element to the end of a list: t = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;] t.append(&#39;d&#39;) t [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;] extend takes a list as an argument and appends all of the elements: t1 = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;] t2 = [&#39;d&#39;,&#39;e&#39;] t1.extend(t2) t1 [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;] 9.7 Map, filter, reduce 9.7.1 Reduce An operation that combines a sequence of elements into a single value is called reduce. To add up all the number in a list, you can use a loop: def add_all(t): total = 0 for x in t: total += x return total add_all([1,2,3]) 6 total is initialized to 0. Each time through the loop, x gets one element from the list. The += operator provides a short way to update a variable. total += x is equivalent to total = total + x Adding up the elements of a list is such a common operation that Python provides it as a built-in function, sum: t = [1,2,3] sum(t) 6 9.7.2 Map Sometimes you want to traverse one list while building another. For example, the following function takes a list of strings and returns a new list that contains capitalized strings: def capitalize_all(t): res = [] for s in t: res.append(s.capitalize()) return res capitalize_all([&#39;this&#39;,&#39;is&#39;,&#39;neat&#39;]) [&#39;This&#39;, &#39;Is&#39;, &#39;Neat&#39;] res is initialized with an empty list; each time through the loop, we append the next element. An operation like capitalize_all is called a map because it “maps” a function onto each of the elements in a sequence. 9.7.3 Filter An operation to select some of the elements from a list and return a sublist. For example, the following function takes a list of strings and returns a list that contains only the uppercase strings: def only_upper(t): res = [] for s in t: if s.isupper(): res.append(s) return res only_upper([&#39;A&#39;,&#39;b&#39;,&#39;C&#39;]) [&#39;A&#39;, &#39;C&#39;] isupper is a string method that returns True if the string contains only upper case letters. An operation like only_upper is called a filter because it selects some of the elements and filters out the others. 9.8 Deleting Elements There are several ways to delete elements from a list. If you know the index of the element you want, you can use pop: t = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;] x = t.pop(1) t [&#39;a&#39;, &#39;c&#39;] x &#39;b&#39; pop modifies the list and returns the element that was removed. If you don’t provide an index, it deletes and returns the last element. If you don’t need the removed value, you can use the del operator: t = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;] del t[1] t [&#39;a&#39;, &#39;c&#39;] If you know the element you want to remove (but not the index), you can use remove: t = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;] t.remove(&#39;b&#39;) t [&#39;a&#39;, &#39;c&#39;] To remove more than one element, you can use del with a slice index: t = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;] del t[1:5] t [&#39;a&#39;, &#39;f&#39;] 9.9 Lists and strings A string is a sequence of characters and a list is a sequence of values, but a list of characters is not the same as a string. To convert from a string to a list of characters, you can use list: test = &#39;test&#39; t = list(test) t [&#39;t&#39;, &#39;e&#39;, &#39;s&#39;, &#39;t&#39;] The list function breaks a string into individual letters. If you want to break a string into words, you can use the split method: test = &#39;this is a test&#39; t = test.split() t [&#39;this&#39;, &#39;is&#39;, &#39;a&#39;, &#39;test&#39;] An optional argument called a delimiter specifies which characters to use as word boundaries. The following example uses a hyphen as a delimiter: hyp = &#39;H-E-L-L-O&#39; t = hyp.split(&quot;-&quot;) t [&#39;H&#39;, &#39;E&#39;, &#39;L&#39;, &#39;L&#39;, &#39;O&#39;] join is the inverse of split. It takes a list of strings and concatenates the elements. join is a string method, so you have to invoke it on the delimiter and pass the list as a parameter: t = [&#39;H&#39;, &#39;E&#39;, &#39;L&#39;, &#39;L&#39;, &#39;O&#39;] &#39;-&#39;.join(t) &#39;H-E-L-L-O&#39; &#39;&amp;&#39;.join(t) &#39;H&amp;E&amp;L&amp;L&amp;O&#39; &#39; &#39;.join(t) &#39;H E L L O&#39; 9.10 List arguments When you pass a list to a function, the function gets a reference to the list. If the function modifies the list, the caller sees the change. For example, delete_head removes the first element from a list: def delete_head(t): del t[0] The parameter t and the variable letters are aliases for the same object. letters = list(&#39;abcd&#39;) letters [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;] delete_head(letters) letters [&#39;b&#39;, &#39;c&#39;, &#39;d&#39;] It is important to distinguish between operations that modify lists and operations that create new lists. For example, the append method modifies a list, but the + operator creates a new list. t1 = [1, 2] t1.append(3) t1 [1, 2, 3] t2 # Returns None The return value from append is None. t3 = t1 + [4] t1 [1, 2, 3] t3 [1, 2, 3, 4] The result of the operator is a new list, and the original list is unchanged. This difference is important when you write functions that are supposed to modify lists. For example, this function does not delete the head of a list: def bad_delete_head(t): t = t[1:] # WRONG! The slice operator creates a new list and the assignment makes t refer to it, but that doesn’t affect the caller. t4 = [1, 2, 3] bad_delete_head(t4) t4 [1, 2, 3] At the beginning of bad_delete_head, t and t4 refer to the same list. At the end, t refers to a new list, but t4 still refers to the original, unmodified list. An alternative is to write a function that creates and returns a new list. For example, tail returns all but the first element of a list: def tail(t): return t[1:] This function leaves the original list unmodified. Here’s how it is used: letters = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;] rest = tail(letters) rest [&#39;b&#39;, &#39;c&#39;] letters = list(&#39;abcdefghi&#39;) letters[::-1] [&#39;i&#39;, &#39;h&#39;, &#39;g&#39;, &#39;f&#39;, &#39;e&#39;, &#39;d&#39;, &#39;c&#39;, &#39;b&#39;, &#39;a&#39;] x = [1,2,3] x[::-1] [3, 2, 1] 9.11 Debugging Careless use of lists can lead to long hours of debugging. 9.11.1 Most list methods modify the argument and return None This is the opposite of the string methods, which return a new string and leave the original alone. If you are used to writing string code like this: word = word.strip() It is tempting to write list code like this: t = t.sort() # WRONG! Sort returns None, the next operation you perform with t is likely to fail. 9.11.2 Pick an idiom and stick with it Part of the problem with lists is that there are too many ways to do things. For example, to remove an element from a list, you can use pop, remove, del, or slice To add an element, you can use the append method or the + operator. Assuming that t is a list and x is a list element, These are correct: t.append(x) t = t + [x] t += [x] And these are wrong: t.append([x]) # WRONG! t = t.append(x) # WRONG! t + [x] # WRONG! t = t + x # WRONG! Try out each of these examples to make sure you understand what they do. Notice that only the last one causes a runtime error; the other three are legal, but they do the wrong thing. 9.11.3 Make copies to avoid aliasing. If you want to use a method like sort that modifies the argument, but you need to keep the original list as well, you can make a copy. t = [3, 1, 2] t2 = t[:] t2.sort() t [3, 1, 2] t2 [1, 2, 3] In this example you could also use the built-in function sorted, which returns a new, sorted list and leaves the original alone. t2 = sorted(t) t [3, 1, 2] t2 [1, 2, 3] "],["python-basics.html", "Section 10 Python Basics", " Section 10 Python Basics Here is the introduction to python programming series. It is based on Downey, A. (2012). Think Python. \" O’Reilly Media, Inc.\".. Introduction to Python Dictionaries Functions: Part 1 Functions: Part 2 Iteration Strings Conditionals and Recursios Lists "],["review-material.html", "Section 11 Review Material 11.1 Python Basics 11.2 Pandas Basics 11.3 Data Science at the Command Line 11.4 SQL Basics", " Section 11 Review Material Here are the basic programming concepts that for students to review: 11.1 Python Basics This section is based on (Downey et al. 2012). Introduction, Types, &amp; Expressions Dictionaries Functions: Part 1 Functions: Part 2 Iteration Strings Conditionals and Recursion Lists 11.2 Pandas Basics This section is based on (McKinney 2012) and (VanderPlas 2016). NumPy Wrangling Introduction to Pandas DataFrames Data Import Data Selection Missing Data: Part 1 Missing Data: Part 2 Tidy Data Operations Merging Data: Part 1 Merging Data: Part 2 Groupby: Part 1 Groupby: Part 2 Reshaping Time Series Categorical Data Plotting Data with Pandas Data Input/Output: Part 1 Data Input/Output: Part 2 11.3 Data Science at the Command Line This section is based on (Janssens 2014). Introduction to the Shell Getting Started with the Command Line Obtaining Data Reusable Command-line Tools 11.4 SQL Basics This section is based on (Nield 2016). Why Learn SQL? Using SQL in R Markdown Using SQL in Jupyter Notebooks SELECT Statements WHERE Statements GROUP BY and ORDER BY CASE Statements JOIN Statements SQL Practice References "],["numpy.html", "Section 12 NumPy 12.1 Using NumPy 12.2 Numpy Arrays 12.3 Creating NumPy Arrays 12.4 Built-in Methods 12.5 eye 12.6 Random 12.7 Array Attributes and Methods 12.8 Reshape 12.9 Shape 12.10 NumPy Indexing and Selection 12.11 Bracket Indexing and Selection 12.12 Broadcasting 12.13 Broadcasting (DANGERS) 12.14 Copying 12.15 Indexing a 2D array (matrices) 12.16 More Indexing Help 12.17 Selection 12.18 NumPy Operations 12.19 Arithmetic 12.20 Universal Array Functions", " Section 12 NumPy NumPy (or Numpy) is a Linear Algebra Library for Python, the reason it is so important for Data Science with Python is that almost all of the libraries in the PyData Ecosystem rely on NumPy as one of their main building blocks. Numpy is also incredibly fast, as it has bindings to C libraries. We will only learn the basics of NumPy 12.1 Using NumPy Import it as a library: import numpy as np Numpy has many built-in functions and capabilities. We won’t cover them all but instead we will focus on some of the most important aspects of Numpy: vectors,arrays,matrices, and number generation. Let’s start by discussing arrays. 12.2 Numpy Arrays NumPy arrays are the main way we will use Numpy throughout the course. Numpy arrays essentially come in two flavors: vectors and matrices. Vectors are strictly 1-d arrays and matrices are 2-d (but you should note a matrix can still have only one row or one column). Let’s begin our introduction by exploring how to create NumPy arrays. 12.3 Creating NumPy Arrays 12.3.1 From a Python List We can create an array by directly converting a list or list of lists: my_list = [1,2,3] my_list [1, 2, 3] np.array(my_list) array([1, 2, 3]) my_matrix = [[1,2,3],[4,5,6],[7,8,9]] my_matrix [[1, 2, 3], [4, 5, 6], [7, 8, 9]] np.array(my_matrix) array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) 12.4 Built-in Methods There are lots of built-in ways to generate Arrays 12.4.1 arange Return evenly spaced values within a given interval. np.arange(0,10) array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) np.arange(0,11,2) array([ 0, 2, 4, 6, 8, 10]) 12.4.2 zeros and ones Generate arrays of zeros or ones np.zeros(3) array([0., 0., 0.]) np.zeros((5,5)) array([[ 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0.]]) np.ones(3) array([ 1., 1., 1.]) np.ones((3,3)) array([[ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.]]) 12.4.3 linspace Return evenly spaced numbers over a specified interval. np.linspace(0,10,3) array([ 0., 5., 10.]) np.linspace(0,10,50) array([ 0. , 0.20408163, 0.40816327, 0.6122449 , 0.81632653, 1.02040816, 1.2244898 , 1.42857143, 1.63265306, 1.83673469, 2.04081633, 2.24489796, 2.44897959, 2.65306122, 2.85714286, 3.06122449, 3.26530612, 3.46938776, 3.67346939, 3.87755102, 4.08163265, 4.28571429, 4.48979592, 4.69387755, 4.89795918, 5.10204082, 5.30612245, 5.51020408, 5.71428571, 5.91836735, 6.12244898, 6.32653061, 6.53061224, 6.73469388, 6.93877551, 7.14285714, 7.34693878, 7.55102041, 7.75510204, 7.95918367, 8.16326531, 8.36734694, 8.57142857, 8.7755102 , 8.97959184, 9.18367347, 9.3877551 , 9.59183673, 9.79591837, 10. ]) 12.5 eye Creates an identity matrix np.eye(4) array([[ 1., 0., 0., 0.], [ 0., 1., 0., 0.], [ 0., 0., 1., 0.], [ 0., 0., 0., 1.]]) 12.6 Random Numpy also has lots of ways to create random number arrays: 12.6.1 rand Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1). np.random.rand(2) array([ 0.11570539, 0.35279769]) np.random.rand(5,5) array([[ 0.66660768, 0.87589888, 0.12421056, 0.65074126, 0.60260888], [ 0.70027668, 0.85572434, 0.8464595 , 0.2735416 , 0.10955384], [ 0.0670566 , 0.83267738, 0.9082729 , 0.58249129, 0.12305748], [ 0.27948423, 0.66422017, 0.95639833, 0.34238788, 0.9578872 ], [ 0.72155386, 0.3035422 , 0.85249683, 0.30414307, 0.79718816]]) 12.6.2 randn Return a sample (or samples) from the “standard normal” distribution. Unlike rand which is uniform: np.random.randn(2) array([-0.27954018, 0.90078368]) np.random.randn(5,5) array([[ 0.70154515, 0.22441999, 1.33563186, 0.82872577, -0.28247509], [ 0.64489788, 0.61815094, -0.81693168, -0.30102424, -0.29030574], [ 0.8695976 , 0.413755 , 2.20047208, 0.17955692, -0.82159344], [ 0.59264235, 1.29869894, -1.18870241, 0.11590888, -0.09181687], [-0.96924265, -1.62888685, -2.05787102, -0.29705576, 0.68915542]]) 12.6.3 randint Return random integers from low (inclusive) to high (exclusive). np.random.randint(1,100) 44 np.random.randint(1,100,10) array([13, 64, 27, 63, 46, 68, 92, 10, 58, 24]) 12.7 Array Attributes and Methods Let’s discuss some useful attributes and methods or an array: arr = np.arange(25) ranarr = np.random.randint(0,50,10) arr array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]) ranarr array([10, 12, 41, 17, 49, 2, 46, 3, 19, 39]) 12.8 Reshape Returns an array containing the same data with a new shape. arr.reshape(5,5) array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]]) 12.8.1 max,min,argmax,argmin These are useful methods for finding max or min values. Or to find their index locations using argmin or argmax ranarr array([10, 12, 41, 17, 49, 2, 46, 3, 19, 39]) ranarr.max() 49 ranarr.argmax() 4 ranarr.min() 2 ranarr.argmin() 5 12.9 Shape Shape is an attribute that arrays have (not a method): # Vector arr.shape (25,) # Notice the two sets of brackets arr.reshape(1,25) array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]]) arr.reshape(1,25).shape (1, 25) arr.reshape(25,1) array([[ 0], [ 1], [ 2], [ 3], [ 4], [ 5], [ 6], [ 7], [ 8], [ 9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24]]) arr.reshape(25,1).shape (25, 1) 12.9.1 dtype You can also grab the data type of the object in the array: arr.dtype dtype(&#39;int64&#39;) 12.10 NumPy Indexing and Selection In this lecture we will discuss how to select elements or groups of elements from an array. #Creating sample array arr = np.arange(0,11) #Show arr array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) 12.11 Bracket Indexing and Selection The simplest way to pick one or some elements of an array looks very similar to python lists: #Get a value at an index arr[8] 8 #Get values in a range arr[1:5] array([1, 2, 3, 4]) #Get values in a range arr[0:5] array([0, 1, 2, 3, 4]) 12.12 Broadcasting Numpy arrays differ from a normal Python list because of their ability to broadcast: #Setting a value with index range (Broadcasting) arr[0:5]=100 #Show arr array([100, 100, 100, 100, 100, 5, 6, 7, 8, 9, 10]) # Reset array arr = np.arange(0,11) #Show arr array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) #Important notes on Slices slice_of_arr = arr[0:6] #Show slice slice_of_arr array([0, 1, 2, 3, 4, 5]) #Change Slice slice_of_arr[:]=99 #Show Slice again slice_of_arr array([99, 99, 99, 99, 99, 99]) 12.13 Broadcasting (DANGERS) Now note the changes also occur in our original array! arr array([99, 99, 99, 99, 99, 99, 6, 7, 8, 9, 10]) Data is not copied, it’s a view of the original array! This avoids memory problems! 12.14 Copying #To get a copy, need to be explicit arr_copy = arr.copy() arr_copy array([99, 99, 99, 99, 99, 99, 6, 7, 8, 9, 10]) 12.15 Indexing a 2D array (matrices) The general format is arr_2d[row][col] or arr_2d[row,col]. I recommend usually using the comma notation for clarity. arr_2d = np.array(([5,10,15],[20,25,30],[35,40,45])) #Show arr_2d array([[ 5, 10, 15], [20, 25, 30], [35, 40, 45]]) #Indexing row arr_2d[1] array([20, 25, 30]) # Format is arr_2d[row][col] or arr_2d[row,col] # Getting individual element value arr_2d[1][0] 20 # Getting individual element value arr_2d[1,0] 20 # 2D array slicing #Shape (2,2) from top right corner arr_2d[:2,1:] array([[10, 15], [25, 30]]) #Shape bottom row arr_2d[2] array([35, 40, 45]) #Shape bottom row arr_2d[2,:] array([35, 40, 45]) 12.15.1 Fancy Indexing Fancy indexing allows you to select entire rows or columns out of order,to show this, let’s quickly build out a numpy array: #Set up matrix arr2d = np.zeros((10,10)) #Length of array arr_length = arr2d.shape[1] #Set up array for i in range(arr_length): arr2d[i] = i arr2d array([[ 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [ 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [ 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.], [ 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.], [ 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.], [ 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.], [ 6., 6., 6., 6., 6., 6., 6., 6., 6., 6.], [ 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.], [ 8., 8., 8., 8., 8., 8., 8., 8., 8., 8.], [ 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.]]) 12.15.2 Fancy Indexing Fancy indexing allows the following arr2d[[2,4,6,8]] array([[ 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.], [ 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.], [ 6., 6., 6., 6., 6., 6., 6., 6., 6., 6.], [ 8., 8., 8., 8., 8., 8., 8., 8., 8., 8.]]) #Allows in any order arr2d[[6,4,2,7]] array([[ 6., 6., 6., 6., 6., 6., 6., 6., 6., 6.], [ 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.], [ 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.], [ 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.]]) 12.16 More Indexing Help Indexing a 2d matrix can be a bit confusing at first, especially when you start to add in step size. 12.17 Selection Let’s briefly go over how to use brackets for selection based off of comparison operators. arr = np.arange(1,11) arr array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) arr &gt; 4 array([False, False, False, False, True, True, True, True, True, True]) bool_arr = arr&gt;4 # Sometimes called a mask bool_arr array([False, False, False, False, True, True, True, True, True, True]) arr[bool_arr] array([ 5, 6, 7, 8, 9, 10]) arr[arr&gt;2] array([ 3, 4, 5, 6, 7, 8, 9, 10]) x = 2 arr[arr&gt;x] array([ 3, 4, 5, 6, 7, 8, 9, 10]) 12.18 NumPy Operations 12.19 Arithmetic You can easily perform array with array arithmetic, or scalar with array arithmetic. Let’s see some examples: arr = np.arange(0,10) arr + arr array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]) arr * arr array([ 0, 1, 4, 9, 16, 25, 36, 49, 64, 81]) arr - arr array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # Warning on division by zero, but not an error! # Just replaced with nan arr/arr /home/markumreed/anaconda3/envs/ds4b/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide This is separate from the ipykernel package so we can avoid doing imports until array([nan, 1., 1., 1., 1., 1., 1., 1., 1., 1.]) # Also warning, but not an error instead infinity 1/arr /home/markumreed/anaconda3/envs/ds4b/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in true_divide array([ inf, 1. , 0.5 , 0.33333333, 0.25 , 0.2 , 0.16666667, 0.14285714, 0.125 , 0.11111111]) arr**3 array([ 0, 1, 8, 27, 64, 125, 216, 343, 512, 729]) 12.20 Universal Array Functions Numpy comes with many universal array functions, which are essentially just mathematical operations you can use to perform the operation across the array. Let’s show some common ones: #Taking Square Roots np.sqrt(arr) array([ 0. , 1. , 1.41421356, 1.73205081, 2. , 2.23606798, 2.44948974, 2.64575131, 2.82842712, 3. ]) #Calcualting exponential (e^) np.exp(arr) array([ 1.00000000e+00, 2.71828183e+00, 7.38905610e+00, 2.00855369e+01, 5.45981500e+01, 1.48413159e+02, 4.03428793e+02, 1.09663316e+03, 2.98095799e+03, 8.10308393e+03]) np.max(arr) #same as arr.max() 9 np.sin(arr) array([ 0. , 0.84147098, 0.90929743, 0.14112001, -0.7568025 , -0.95892427, -0.2794155 , 0.6569866 , 0.98935825, 0.41211849]) np.log(arr) /home/markumreed/anaconda3/envs/ds4b/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log &quot;&quot;&quot;Entry point for launching an IPython kernel. array([ -inf, 0. , 0.69314718, 1.09861229, 1.38629436, 1.60943791, 1.79175947, 1.94591015, 2.07944154, 2.19722458]) "],["wrangling.html", "Section 13 Wrangling", " Section 13 Wrangling In this part of the book, you’ll learn about data wrangling, the art of getting your data into R in a useful form for visualisation and modelling. Data wrangling is very important: without it you can’t work with your own data! There are three main parts to data wrangling: Data Scince Wrangling Workflow: Import, Tidy, Transform This part of the book proceeds as follows: In DataFrames, you’ll learn about the variant of the data frame that we use in this book: the tibble. You’ll learn what makes them different from regular data frames, and how you can construct them “by hand.” In data import, you’ll learn how to get your data from disk and into R. We’ll focus on plain-text rectangular formats, but will give you pointers to packages that help with other types of data. In tidy data, you’ll learn about tidy data, a consistent way of storing your data that makes transformation, visualisation, and modelling easier. You’ll learn the underlying principles, and how to get your data into a tidy form. Data wrangling also encompasses data transformation, which you’ve already learned a little about. Now we’ll focus on new skills for three specific types of data you will frequently encounter in practice: Relational data will give you tools for working with multiple interrelated datasets. Strings will introduce regular expressions, a powerful tool for manipulating strings. Categoricals are how Python stores categorical data. They are used when a variable has a fixed set of possible values, or when you want to use a non-alphabetical ordering of a string. Time series will give you the key tools for working with dates and date-times. "],["dataframes.html", "Section 14 DataFrames 14.1 Introduction 14.2 Creating a dataframe using List: 14.3 Creating DataFrame from dict of ndarray/lists: 14.4 Dealing with Rows and Columns 14.5 Select Multiple Columns 14.6 Row Selection: 14.7 Indexing and Selecting Data", " Section 14 DataFrames 14.1 Introduction Throughout this book we work with DataFrames. A DataFrame is a two-dimensional data structure, i.e., data is aligned in a tabular fashion in rows and columns. Pandas DataFrame consists of three principal components, the data, rows, and columns. DataFrame Anatomy We will get a brief insight on all these basic operation which can be performed on Pandas DataFrame : Creating a DataFrame Dealing with Rows and Columns Indexing and Selecting Data Working with Missing Data Iterating over rows and columns In the real world, a Pandas DataFrame will be created by loading the datasets from existing storage, storage can be SQL Database, CSV file, and Excel file. Pandas DataFrame can be created from the lists, dictionary, and from a list of dictionary etc. Dataframe can be created in different ways here are some ways by which we create a dataframe: 14.2 Creating a dataframe using List: DataFrame can be created using a single list or a list of lists. import pandas as pd Create a list of strings #@title Introducing Lists { display-mode: &quot;form&quot; } #@markdown This 4-minute video gives an overview of the key features of Booleans: from IPython.display import YouTubeVideo YouTubeVideo(&#39;BCN4PRoQnI4&#39;, width=600, height=400) state_lst = [&#39;California&#39;,&#39;Texas&#39;,&#39;New York&#39;,&#39;Florida&#39;,&#39;Illinois&#39;] Call the DataFrame constructor on the list. df = pd.DataFrame(state_lst) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 0 California 1 Texas 2 New York 3 Florida 4 Illinois 14.3 Creating DataFrame from dict of ndarray/lists: To create DataFrame from dict of narray/list, all the narray must be of same length. If index is passed then the length index should be equal to the length of arrays. If no index is passed, then by default, index will be range(n) where n is the array length. Intialise dictionary of lists. data = {&#39;state&#39;:[&#39;California&#39;,&#39;Texas&#39;,&#39;New York&#39;,&#39;Florida&#39;,&#39;Illinois&#39;], &#39;pop&#39;:[3833252,26448193,19651127,19552860,12882135], &#39;area&#39;: [423967,695662,141297,170312,149995]} #@title Introducing Dictionaries { display-mode: &quot;form&quot; } #@markdown This 3-minute video gives an overview of the key features of Booleans: from IPython.display import YouTubeVideo YouTubeVideo(&#39;1LRepvqzXzM&#39;, width=600, height=400) df = pd.DataFrame(data) df.set_index(keys=&#39;state&#39;, inplace=True) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pop area state California 3833252 423967 Texas 26448193 695662 New York 19651127 141297 Florida 19552860 170312 Illinois 12882135 149995 14.4 Dealing with Rows and Columns A Data frame is a two-dimensional data structure, i.e., data is aligned in a tabular fashion in rows and columns. We can perform basic operations on rows/columns like selecting, deleting, adding, and renaming. 14.4.1 Column Selection: The individual Series that make up the columns of the DataFrame can be accessed via dictionary-style indexing of the column name: df[&#39;area&#39;] state_name California 423967 Texas 695662 New York 141297 Florida 170312 Illinois 149995 Name: area, dtype: int64 Equivalently, we can use attribute-style access with column names that are strings: df.area state_name California 423967 Texas 695662 New York 141297 Florida 170312 Illinois 149995 Name: area, dtype: int64 Though this is a useful shorthand, keep in mind that it does not work for all cases! For example, if the column names are not strings, or if the column names conflict with methods of the DataFrame, this attribute-style access is not possible. For example, the DataFrame has a pop() method, so data.pop will point to this rather than the “pop” column: df.pop is df[&#39;pop&#39;] False In particular, you should avoid the temptation to try column assignment via attribute (i.e., use data['pop'] = z rather than data.pop = z). Like with the Series objects discussed earlier, this dictionary-style syntax can also be used to modify the object, in this case adding a new column: df[&#39;density&#39;] = df[&#39;pop&#39;] / df[&#39;area&#39;] df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pop area density state California 3833252 423967 9.041392 Texas 26448193 695662 38.018740 New York 19651127 141297 139.076746 Florida 19552860 170312 114.806121 Illinois 12882135 149995 85.883763 14.5 Select Multiple Columns df[[&#39;density&#39;,&#39;area&#39;]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } density area state California 9.041392 423967 Texas 38.018740 695662 New York 139.076746 141297 Florida 114.806121 170312 Illinois 85.883763 149995 14.6 Row Selection: Pandas provide a unique method to retrieve rows from a Data frame. DataFrame.loc[] method is used to retrieve rows from Pandas DataFrame. Rows can also be selected by passing integer location to an iloc[] function. df.loc[&quot;California&quot;] pop 3.833252e+06 area 4.239670e+05 density 9.041392e+00 Name: California, dtype: float64 df.loc[&quot;Texas&quot;] pop 2.644819e+07 area 6.956620e+05 density 3.801874e+01 Name: Texas, dtype: float64 14.7 Indexing and Selecting Data Indexing in pandas means simply selecting particular rows and columns of data from a DataFrame. Indexing could mean selecting all the rows and some of the columns, some of the rows and all of the columns, or some of each of the rows and columns. Indexing can also be known as Subset Selection. 14.7.1 Indexing a Dataframe using indexing operator [] : Indexing operator is used to refer to the square brackets following an object. The .loc and .iloc indexers also use the indexing operator to make selections. In this indexing operator to refer to df[]. 14.7.1.1 Selecting Single Columns In order to select a single column, we simply put the name of the column in-between the brackets df[&#39;density&#39;] state California 9.041392 Texas 38.018740 New York 139.076746 Florida 114.806121 Illinois 85.883763 Name: density, dtype: float64 14.7.1.2 Selecting Multiple Columns df[[&#39;area&#39;,&#39;density&#39;]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area density state California 423967 9.041392 Texas 695662 38.018740 New York 141297 139.076746 Florida 170312 114.806121 Illinois 149995 85.883763 14.7.2 Indexing a DataFrame using .loc[ ] : This function selects data by the label of the rows and columns. The df.loc indexer selects data in a different way than just the indexing operator. It can select subsets of rows or columns. It can also simultaneously select subsets of rows and columns. 14.7.2.1 Selecting a single row In order to select a single row using .loc[], we put a single row label in a .loc function. df.loc[&quot;Florida&quot;] pop 1.955286e+07 area 1.703120e+05 density 1.148061e+02 Name: Florida, dtype: float64 14.7.2.2 Selecting multiple rows df.loc[[&#39;Florida&#39;,&#39;Illinois&#39;]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pop area density state Florida 19552860 170312 114.806121 Illinois 12882135 149995 85.883763 14.7.3 Indexing a DataFrame using .iloc[ ] : This function allows us to retrieve rows and columns by position. In order to do that, we’ll need to specify the positions of the rows that we want, and the positions of the columns that we want as well. The df.iloc indexer is very similar to df.loc but only uses integer locations to make its selections. 14.7.3.1 Selecting a single row In order to select a single row using .iloc[], we can pass a single integer to .iloc[] function. df.iloc[2] pop 1.965113e+07 area 1.412970e+05 density 1.390767e+02 Name: New York, dtype: float64 14.7.3.2 Selecting multiple rows df.iloc[[0,2]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pop area density state California 3833252 423967 9.041392 New York 19651127 141297 139.076746 Next Section: Data Import "],["introduction-to-pandas.html", "Section 15 Introduction to Pandas 15.1 Overview of Pandas Capabilities 15.2 Pandas 15.3 DataFrame Creation 15.4 Viewing Data 15.5 Selection", " Section 15 Introduction to Pandas Pandas is an open source library built on top of NumPy Allows fast analysis, cleaning and preparation of data High performance and productivity Built-in visualization capability Work with data from many sources 15.1 Overview of Pandas Capabilities Series DataFrames Selection Missing Data Operations Merging, Joining, and Concatenating GroupBy Reshaping Data and Pivot Tables Time Series Data Input/Output 15.2 Pandas This is a basic introduction to the pandas module. First we start off with the customary imports. import numpy as np import pandas as pd np.random.seed(42) E# Object Creation 15.2.1 Series Similar to NumPy array Built on top of it Can have axis labels Here we will show a few ways to create series Throughout the course we will be primarily dealing with DataFrames DataFrames will be discussed shortly ## Series Series can hold a variety of object types Numbers, strings, etc Create a Series by passing a list, letting pandas create a default index value. s = pd.Series([1,2,3,np.nan, 4,5]) s 0 1.0 1 2.0 2 3.0 3 NaN 4 4.0 5 5.0 dtype: float64 Key to using a series is understanding its index Pandas makes use of these index names/numbers Allows fast lookups of information Works like a hash table or dictionary 15.2.2 Series Examples s_1 = pd.Series([1,2,3,4], [&#39;USA&#39;, &#39;Germany&#39;, &#39;China&#39;, &#39;Japan&#39;]) s_1 USA 1 Germany 2 China 3 Japan 4 dtype: int64 s_2 = pd.Series([1,2,5,6],[&#39;USA&#39;, &#39;Germany&#39;, &#39;Italy&#39;, &#39;China&#39;]) s_2 USA 1 Germany 2 Italy 5 China 6 dtype: int64 s_2[&#39;China&#39;] # Indexing is type dependent 6 s_2.index Index([&#39;USA&#39;, &#39;Germany&#39;, &#39;Italy&#39;, &#39;China&#39;], dtype=&#39;object&#39;) labels = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;] s_3 = pd.Series(data=labels) s_3 0 a 1 b 2 c dtype: object s_3[2] &#39;c&#39; Matches operation off of the index Creates NaN object where missing matches Integers convert to floats s_1 USA 1 Germany 2 China 3 Japan 4 dtype: int64 s_2 USA 1 Germany 2 Italy 5 China 6 dtype: int64 s_1 + s_2 China 9.0 Germany 4.0 Italy NaN Japan NaN USA 2.0 dtype: float64 15.3 DataFrame Creation Creating a DataFrame by passing a NumPy array, with a datetime index and labeled columns. dates = pd.date_range(&#39;20190101&#39;,periods=10) dates DatetimeIndex([&#39;2019-01-01&#39;, &#39;2019-01-02&#39;, &#39;2019-01-03&#39;, &#39;2019-01-04&#39;, &#39;2019-01-05&#39;, &#39;2019-01-06&#39;, &#39;2019-01-07&#39;, &#39;2019-01-08&#39;, &#39;2019-01-09&#39;, &#39;2019-01-10&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) df = pd.DataFrame(np.random.randn(10,4), index=dates, columns=list(&#39;ABCD&#39;)) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2019-01-01 0.496714 -0.138264 0.647689 1.523030 2019-01-02 -0.234153 -0.234137 1.579213 0.767435 2019-01-03 -0.469474 0.542560 -0.463418 -0.465730 2019-01-04 0.241962 -1.913280 -1.724918 -0.562288 2019-01-05 -1.012831 0.314247 -0.908024 -1.412304 2019-01-06 1.465649 -0.225776 0.067528 -1.424748 2019-01-07 -0.544383 0.110923 -1.150994 0.375698 2019-01-08 -0.600639 -0.291694 -0.601707 1.852278 2019-01-09 -0.013497 -1.057711 0.822545 -1.220844 2019-01-10 0.208864 -1.959670 -1.328186 0.196861 Creating a DataFrame by passing a dictionary that can be converted to a series: df2 = pd.DataFrame({&#39;A&#39;:1., &#39;B&#39;: pd.Timestamp(&#39;20190101&#39;), &#39;C&#39;:pd.Series(1, index=list(range(4)),dtype=&#39;float32&#39;), &#39;D&#39;:np.array([3] * 4, dtype=&#39;int32&#39;), &#39;E&#39;: pd.Categorical([&#39;test&#39;,&#39;train&#39;,&#39;test&#39;,&#39;train&#39;]), &#39;F&#39;:&#39;foo&#39;}) df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D E F 0 1.0 2019-01-01 1.0 3 test foo 1 1.0 2019-01-01 1.0 3 train foo 2 1.0 2019-01-01 1.0 3 test foo 3 1.0 2019-01-01 1.0 3 train foo The columns of the DataFrame have different dtypes. df2.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 4 entries, 0 to 3 Data columns (total 6 columns): A 4 non-null float64 B 4 non-null datetime64[ns] C 4 non-null float32 D 4 non-null int32 E 4 non-null category F 4 non-null object dtypes: category(1), datetime64[ns](1), float32(1), float64(1), int32(1), object(1) memory usage: 260.0+ bytes 15.4 Viewing Data Here is how to view the top and bottom rows of the frame: df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2019-01-01 0.496714 -0.138264 0.647689 1.523030 2019-01-02 -0.234153 -0.234137 1.579213 0.767435 2019-01-03 -0.469474 0.542560 -0.463418 -0.465730 2019-01-04 0.241962 -1.913280 -1.724918 -0.562288 2019-01-05 -1.012831 0.314247 -0.908024 -1.412304 df.tail(2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2019-01-09 -0.013497 -1.057711 0.822545 -1.220844 2019-01-10 0.208864 -1.959670 -1.328186 0.196861 Display the index, columns: df.index DatetimeIndex([&#39;2019-01-01&#39;, &#39;2019-01-02&#39;, &#39;2019-01-03&#39;, &#39;2019-01-04&#39;, &#39;2019-01-05&#39;, &#39;2019-01-06&#39;, &#39;2019-01-07&#39;, &#39;2019-01-08&#39;, &#39;2019-01-09&#39;, &#39;2019-01-10&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) df.columns Index([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;], dtype=&#39;object&#39;) df.shape (10, 4) We can convert our DataFrame (of floating-points) to a NumPy array. df.to_numpy() # This can be a taxing operation in not all floats # df.values array([[ 0.49671415, -0.1382643 , 0.64768854, 1.52302986], [-0.23415337, -0.23413696, 1.57921282, 0.76743473], [-0.46947439, 0.54256004, -0.46341769, -0.46572975], [ 0.24196227, -1.91328024, -1.72491783, -0.56228753], [-1.01283112, 0.31424733, -0.90802408, -1.4123037 ], [ 1.46564877, -0.2257763 , 0.0675282 , -1.42474819], [-0.54438272, 0.11092259, -1.15099358, 0.37569802], [-0.60063869, -0.29169375, -0.60170661, 1.85227818], [-0.01349722, -1.05771093, 0.82254491, -1.22084365], [ 0.2088636 , -1.95967012, -1.32818605, 0.19686124]]) df2.to_numpy() # This has multiple dtypes array([[1.0, Timestamp(&#39;2019-01-01 00:00:00&#39;), 1.0, 3, &#39;test&#39;, &#39;foo&#39;], [1.0, Timestamp(&#39;2019-01-01 00:00:00&#39;), 1.0, 3, &#39;train&#39;, &#39;foo&#39;], [1.0, Timestamp(&#39;2019-01-01 00:00:00&#39;), 1.0, 3, &#39;test&#39;, &#39;foo&#39;], [1.0, Timestamp(&#39;2019-01-01 00:00:00&#39;), 1.0, 3, &#39;train&#39;, &#39;foo&#39;]], dtype=object) describe() shows a quick statistic summary of your data: df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D count 10.000000 10.000000 10.000000 10.000000 mean -0.046179 -0.485280 -0.306027 -0.037061 std 0.701907 0.874335 1.060584 1.181041 min -1.012831 -1.959670 -1.724918 -1.424748 25% -0.525656 -0.866207 -1.090251 -1.056205 50% -0.123825 -0.229957 -0.532562 -0.134434 75% 0.233688 0.048626 0.502648 0.669501 max 1.465649 0.542560 1.579213 1.852278 Transposing your data: df.T .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2019-01-01 2019-01-02 2019-01-03 2019-01-04 2019-01-05 2019-01-06 2019-01-07 2019-01-08 2019-01-09 2019-01-10 A 0.496714 -0.234153 -0.469474 0.241962 -1.012831 1.465649 -0.544383 -0.600639 -0.013497 0.208864 B -0.138264 -0.234137 0.542560 -1.913280 0.314247 -0.225776 0.110923 -0.291694 -1.057711 -1.959670 C 0.647689 1.579213 -0.463418 -1.724918 -0.908024 0.067528 -1.150994 -0.601707 0.822545 -1.328186 D 1.523030 0.767435 -0.465730 -0.562288 -1.412304 -1.424748 0.375698 1.852278 -1.220844 0.196861 Sorting by an axis: df.sort_index(axis=0, ascending=False) # axis = 1 Columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2019-01-10 0.208864 -1.959670 -1.328186 0.196861 2019-01-09 -0.013497 -1.057711 0.822545 -1.220844 2019-01-08 -0.600639 -0.291694 -0.601707 1.852278 2019-01-07 -0.544383 0.110923 -1.150994 0.375698 2019-01-06 1.465649 -0.225776 0.067528 -1.424748 2019-01-05 -1.012831 0.314247 -0.908024 -1.412304 2019-01-04 0.241962 -1.913280 -1.724918 -0.562288 2019-01-03 -0.469474 0.542560 -0.463418 -0.465730 2019-01-02 -0.234153 -0.234137 1.579213 0.767435 2019-01-01 0.496714 -0.138264 0.647689 1.523030 Sorting by values: df.sort_values(by=&#39;B&#39;, ascending=False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2019-01-03 -0.469474 0.542560 -0.463418 -0.465730 2019-01-05 -1.012831 0.314247 -0.908024 -1.412304 2019-01-07 -0.544383 0.110923 -1.150994 0.375698 2019-01-01 0.496714 -0.138264 0.647689 1.523030 2019-01-06 1.465649 -0.225776 0.067528 -1.424748 2019-01-02 -0.234153 -0.234137 1.579213 0.767435 2019-01-08 -0.600639 -0.291694 -0.601707 1.852278 2019-01-09 -0.013497 -1.057711 0.822545 -1.220844 2019-01-04 0.241962 -1.913280 -1.724918 -0.562288 2019-01-10 0.208864 -1.959670 -1.328186 0.196861 15.5 Selection Selecting a single columns yields a Series df[&#39;A&#39;] 2019-01-01 0.496714 2019-01-02 -0.234153 2019-01-03 -0.469474 2019-01-04 0.241962 2019-01-05 -1.012831 2019-01-06 1.465649 2019-01-07 -0.544383 2019-01-08 -0.600639 2019-01-09 -0.013497 2019-01-10 0.208864 Freq: D, Name: A, dtype: float64 Selecting with [] slices the rows df[0:3] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2019-01-01 0.496714 -0.138264 0.647689 1.523030 2019-01-02 -0.234153 -0.234137 1.579213 0.767435 2019-01-03 -0.469474 0.542560 -0.463418 -0.465730 df[&#39;20190101&#39;:&#39;20190103&#39;] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2019-01-01 0.496714 -0.138264 0.647689 1.523030 2019-01-02 -0.234153 -0.234137 1.579213 0.767435 2019-01-03 -0.469474 0.542560 -0.463418 -0.465730 "],["data-import.html", "Section 16 Data Import 16.1 Obtaining Data 16.2 Importing Data with pandas 16.3 Getting started 16.4 Parsing a list 16.5 Numbers 16.6 Strings 16.7 Categoricals in Pandas 16.8 Dates, date-times, and times 16.9 Parsing a file 16.10 Writing to a file 16.11 Other types of data", " Section 16 Data Import 16.1 Obtaining Data Without any data, there is little data science you can do. So the first step is the obtain data. Unless you are fortunate enough to already possess data, you may need to do one or more of the following: Download data from another location (e.g., a webpage or server) Query data from a database or API (e.g., MySQL or Twitter) Extract data from another file (e.g., an HTML file or spreadsheet) Generate data yourself (e.g., reading sensors or taking surveys) 16.2 Importing Data with pandas Working with data provided by Python packages is a great way to learn the tools of data science, but at some point you want to stop learning and start working with your own data. In this chapter, you’ll learn how to read plain-text rectangular files into Python. Here, we’ll only scratch the surface of data import, but many of the principles will translate to other forms of data. We’ll finish with a few pointers to packages that are useful for other types of data. Importing data is one of the most essential and very first steps in any data related problem. The ability to import the data correctly is a must-have skill for every aspiring data scientist. Data exists in many different forms, and not only should you know how to import various data formats but also how to analyze and manipulate the data to gain useful insights. pandas is an open source Python library which is easy-to-use, provides high-performance, and a data analysis tool for various data formats. It gives you the capability to read various types of data formats like CSV, JSON, Excel, Pickle, etc. It allows you to represent your data in a row and column tabular fashion, which makes the data readable and presentable. pandas represent the data in a DataFrame form and provide you with extensive usage for data analysis and data manipulation. Once you start making sense out of the data using the various functionalities in pandas, you can then use this data for analyzing, forecasting, classifying, and more. pandas has an input and output API which has a set of top-level reader and writer functions. The reader function is accessed with pandas.read_json() that returns a pandas object, and the writer function is accessed with pandas.to_json() which is an object method. DataFrame has a Reader and a Writer function. The Reader function allows you to read the different data formats, while the Writer function enables you to save the data in a particular format. To get other types of data into Python, keep following the pd.read_ syntax. Below are data formats that DataFrame supports, which means if your data is in any of the below forms, you can use pandas to load that data format and even write into a particular format. Format Type Data Description Reader Writer text CSV read_csv to_csv text JSON read_json to_json text HTML read_html to_html text Local clipboard read_clipboard to_clipboard binary MS Excel read_excel to_excel binary OpenDocument read_excel binary HDF5 Format read_hdf to_hdf binary Feather Format read_feather to_feather binary Parquet Format read_parquet to_parquet binary Msgpack read_msgpack to_msgpack binary Stata read_stata to_stata binary SAS read_sas binary Python Pickle Format read_pickle to_pickle SQL SQL read_sql to_sql SQL Google Big Query read_gbq to_gbq 16.2.1 Prerequisites In this chapter, you’ll learn how to load flat files in Python with the pandas package. import pandas as pd 16.3 Getting started Most of pandas.read_ functions are concerned with turning files into dataframes. These functions all have similar syntax: once you’ve mastered one, you can use the others with ease. For the rest of this chapter we’ll focus on pandas.read_csv(). Not only are csv files one of the most common forms of data storage, but once you understand pandas.read_csv(), you can easily apply your knowledge to all the other functions pd.read_csv() reads comma delimited files, by changes the sep argument it also reads semicolon separated files (common in countries where , is used as the decimal place), reads tab delimited files, and reads in files with any delimiter. The first argument to read_csv() is the most important: it’s the path to the file to read. df = pd.read_csv(&quot;heights.csv&quot;,) df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } earn height sex ed age race 0 50000.0 74.424439 male 16 45 white 1 60000.0 65.537543 female 16 58 white 2 30000.0 63.629198 female 16 29 white 3 50000.0 63.108562 female 16 91 other 4 51000.0 63.402484 female 17 39 white read_csv() uses the first line of the data for the column names, which is a very common convention. There are two cases where you might want to tweak this behaviour: Sometimes there are a few lines of metadata at the top of the file. You can use skiprows = n to skip the first n lines; or use comment = \"#\" to drop all lines that start with (e.g.) # The data might not have column names. You can use header = None to tell read_csv() not to treat the first row as headings, and instead label them sequentially from 0 to n: pd.read_csv(&quot;heights.csv&quot;, header=None) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 0 earn height sex ed age race 1 50000.0 74.4244387818035 male 16 45 white 2 60000.0 65.5375428255647 female 16 58 white 3 30000.0 63.6291977374349 female 16 29 white 4 50000.0 63.108561675297096 female 16 91 other … … … … … … … 1188 19000.0 72.1657330563758 male 12 29 white 1189 15000.0 61.135799531126395 female 18 82 white 1190 8000.0 63.6641635315027 female 12 33 white 1191 60000.0 71.9258358024526 male 12 50 white 1192 6000.0 68.3684862144291 male 12 27 white 1193 rows × 6 columns You can pass names a list of strings which will be used as the column names: Another option that commonly needs tweaking is na_values: this specifies the value (or values) that are used to represent missing values in your file: pd.read_csv() 16.3.1 Exercises What function would you use to read a file where fields were separated with “|?” Apart from those mentioned in this chapter, what other arguments does read_csv() have? Import the following files using the correct pd.read_ syntax. 16.4 Parsing a list 16.5 Numbers 16.6 Strings 16.7 Categoricals in Pandas 16.8 Dates, date-times, and times 16.8.1 Exercises 16.9 Parsing a file 16.9.1 Strategy 16.9.2 Problems 16.9.3 Other Strategies 16.10 Writing to a file 16.11 Other types of data "],["data-selection.html", "Section 17 Data Selection 17.1 Selection by label 17.2 Selection by position 17.3 Boolean indexing 17.4 Setting", " Section 17 Data Selection 17.1 Selection by label For getting a cross section using a label: import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline dates = pd.date_range(&#39;20190101&#39;,periods=10) df = pd.DataFrame(np.random.randn(10,4), index=dates, columns=list(&#39;ABCD&#39;)) dates[0] Timestamp(&#39;2019-01-01 00:00:00&#39;, freq=&#39;D&#39;) df.loc[dates[0]] A -1.158573 B -0.941688 C 2.568583 D 0.494481 Name: 2019-01-01 00:00:00, dtype: float64 Selecting on a mutli-axis by label df.loc[:, [&#39;A&#39;,&#39;D&#39;]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A D 2019-01-01 -1.158573 0.494481 2019-01-02 -0.683202 -0.187010 2019-01-03 0.564102 -0.146018 2019-01-04 -0.939398 0.034436 2019-01-05 1.135158 -2.690404 2019-01-06 -0.953653 2.677651 2019-01-07 1.677311 0.462660 2019-01-08 -0.612544 0.027003 2019-01-09 1.106210 -0.057844 2019-01-10 0.165680 -0.706584 Showing label slicing, both endpoints are included df.loc[&#39;20190102&#39;:&#39;20190104&#39;, [&#39;A&#39;, &#39;B&#39;]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 2019-01-02 -0.683202 0.056144 2019-01-03 0.564102 1.448998 2019-01-04 -0.939398 2.276844 Reduction in the dimensions of the returns object df.loc[&#39;20190102&#39;,[&#39;A&#39;,&#39;B&#39;]] A -0.683202 B 0.056144 Name: 2019-01-02 00:00:00, dtype: float64 For getting a scalar value: df.loc[dates[0], &#39;A&#39;] -1.1585727488744095 17.2 Selection by position Select with the position of the passed integers: df.iloc[2] A 0.564102 B 1.448998 C 1.036779 D -0.146018 Name: 2019-01-03 00:00:00, dtype: float64 By integer slices, acting similar to numpy/python: df.iloc[2:4, 0:2] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 2019-01-03 0.564102 1.448998 2019-01-04 -0.939398 2.276844 By lists of integer position locations, similar to the numpy/python style: df.iloc[[1,2,4],[0,2]] For slicing rows explicitly: df.iloc[1:3, :] For slicing columns explicitly: df.iloc[:,1:3] For getting a value explicitly: df.iloc[1,1] 17.3 Boolean indexing Using a single column’s values to select data. mask = df[&#39;A&#39;] &gt; 0 mask.head() 2019-01-01 False 2019-01-02 False 2019-01-03 True 2019-01-04 False 2019-01-05 True Freq: D, Name: A, dtype: bool df[mask] # ~ negates the mask .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2019-01-03 0.564102 1.448998 1.036779 -0.146018 2019-01-05 1.135158 0.720135 -0.771679 -2.690404 2019-01-07 1.677311 -0.572894 0.405523 0.462660 2019-01-09 1.106210 -0.385144 1.194672 -0.057844 2019-01-10 0.165680 0.786776 -1.079196 -0.706584 Selecting values from a DataFrame where a boolean condition is met. df[df &gt; 0] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2019-01-01 NaN NaN 2.568583 0.494481 2019-01-02 NaN 0.056144 0.677926 NaN 2019-01-03 0.564102 1.448998 1.036779 NaN 2019-01-04 NaN 2.276844 NaN 0.034436 2019-01-05 1.135158 0.720135 NaN NaN 2019-01-06 NaN 0.296899 NaN 2.677651 2019-01-07 1.677311 NaN 0.405523 0.462660 2019-01-08 NaN NaN NaN 0.027003 2019-01-09 1.106210 NaN 1.194672 NaN 2019-01-10 0.165680 0.786776 NaN NaN Using the isin() method for filtering: df2 = df.copy() df2[&#39;E&#39;] = [&#39;one&#39;,&#39;one&#39;,&#39;two&#39;,&#39;three&#39;,&#39;four&#39;,&#39;three&#39;,&#39;two&#39;,&#39;three&#39;,&#39;four&#39;,&#39;four&#39;] df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D E 2019-01-01 -1.158573 -0.941688 2.568583 0.494481 one 2019-01-02 -0.683202 0.056144 0.677926 -0.187010 one 2019-01-03 0.564102 1.448998 1.036779 -0.146018 two 2019-01-04 -0.939398 2.276844 -0.400286 0.034436 three 2019-01-05 1.135158 0.720135 -0.771679 -2.690404 four 2019-01-06 -0.953653 0.296899 -1.022055 2.677651 three 2019-01-07 1.677311 -0.572894 0.405523 0.462660 two 2019-01-08 -0.612544 -1.102798 -0.829374 0.027003 three 2019-01-09 1.106210 -0.385144 1.194672 -0.057844 four 2019-01-10 0.165680 0.786776 -1.079196 -0.706584 four df2[df2[&#39;E&#39;].isin([&#39;two&#39;, &#39;four&#39;])] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D E 2019-01-03 0.564102 1.448998 1.036779 -0.146018 two 2019-01-05 1.135158 0.720135 -0.771679 -2.690404 four 2019-01-07 1.677311 -0.572894 0.405523 0.462660 two 2019-01-09 1.106210 -0.385144 1.194672 -0.057844 four 2019-01-10 0.165680 0.786776 -1.079196 -0.706584 four 17.4 Setting Setting a new column automatically aligns the data by the indexes. s1 = pd.Series([1, 2, 3, 4, 5, 6,7,8,9,10], index=pd.date_range(&#39;20190101&#39;, periods=10)) s1 2019-01-01 1 2019-01-02 2 2019-01-03 3 2019-01-04 4 2019-01-05 5 2019-01-06 6 2019-01-07 7 2019-01-08 8 2019-01-09 9 2019-01-10 10 Freq: D, dtype: int64 df[&#39;F&#39;] = s1 df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D F 2019-01-01 -1.158573 -0.941688 2.568583 0.494481 1 2019-01-02 -0.683202 0.056144 0.677926 -0.187010 2 2019-01-03 0.564102 1.448998 1.036779 -0.146018 3 2019-01-04 -0.939398 2.276844 -0.400286 0.034436 4 2019-01-05 1.135158 0.720135 -0.771679 -2.690404 5 2019-01-06 -0.953653 0.296899 -1.022055 2.677651 6 2019-01-07 1.677311 -0.572894 0.405523 0.462660 7 2019-01-08 -0.612544 -1.102798 -0.829374 0.027003 8 2019-01-09 1.106210 -0.385144 1.194672 -0.057844 9 2019-01-10 0.165680 0.786776 -1.079196 -0.706584 10 Setting values by label: df.loc[dates[0]] = 0 Setting values by position df.iloc[0,1] = 0 Setting by assigning with a NumPy array: df.loc[:, &#39;D&#39;] = np.array([5] * len(df)) The result of the prior setting operations. df A where operation with setting. df2 = df.copy() df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D F 2019-01-01 0.000000 0.000000 0.000000 5 0 2019-01-02 -0.683202 0.056144 0.677926 5 2 2019-01-03 0.564102 1.448998 1.036779 5 3 2019-01-04 -0.939398 2.276844 -0.400286 5 4 2019-01-05 1.135158 0.720135 -0.771679 5 5 2019-01-06 -0.953653 0.296899 -1.022055 5 6 2019-01-07 1.677311 -0.572894 0.405523 5 7 2019-01-08 -0.612544 -1.102798 -0.829374 5 8 2019-01-09 1.106210 -0.385144 1.194672 5 9 2019-01-10 0.165680 0.786776 -1.079196 5 10 df2[df2 &gt; 0] = -df2 df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D F 2019-01-01 0.000000 0.000000 0.000000 -5 0 2019-01-02 -0.683202 -0.056144 -0.677926 -5 -2 2019-01-03 -0.564102 -1.448998 -1.036779 -5 -3 2019-01-04 -0.939398 -2.276844 -0.400286 -5 -4 2019-01-05 -1.135158 -0.720135 -0.771679 -5 -5 2019-01-06 -0.953653 -0.296899 -1.022055 -5 -6 2019-01-07 -1.677311 -0.572894 -0.405523 -5 -7 2019-01-08 -0.612544 -1.102798 -0.829374 -5 -8 2019-01-09 -1.106210 -0.385144 -1.194672 -5 -9 2019-01-10 -0.165680 -0.786776 -1.079196 -5 -10 "],["missing-data-part-1.html", "Section 18 Missing Data: Part 1", " Section 18 Missing Data: Part 1 pandas primarily uses the value np.nan to represent missing data. Reindexing allows you to change/add/delete the index on a specified axis. This returns a copy of the data. import pandas as pd import numpy as np dates = pd.date_range(&#39;20190101&#39;,periods=10) df = pd.DataFrame(np.random.randn(10,4), index=dates, columns=list(&#39;ABCD&#39;)) df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + [&#39;E&#39;]) df1.loc[dates[0]:dates[1], &#39;E&#39;] = 1 df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D E 2019-01-01 0.199886 0.312388 1.394258 -0.311931 1.0 2019-01-02 0.259445 -0.377668 -1.481911 1.805175 1.0 2019-01-03 1.452134 -2.576209 -0.246738 -1.127367 NaN 2019-01-04 2.026428 0.183045 1.275433 -0.834084 NaN To drop any rows that have missing data. df1.dropna(how=&#39;any&#39;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D E 2019-01-01 0.199886 0.312388 1.394258 -0.311931 1.0 2019-01-02 0.259445 -0.377668 -1.481911 1.805175 1.0 Filling missing data. df1.fillna(value=&#39;FILL VALUE&#39;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D E 2019-01-01 0.199886 0.312388 1.394258 -0.311931 1 2019-01-02 0.259445 -0.377668 -1.481911 1.805175 1 2019-01-03 1.452134 -2.576209 -0.246738 -1.127367 FILL VALUE 2019-01-04 2.026428 0.183045 1.275433 -0.834084 FILL VALUE To get the boolean mask where values are nan. pd.isna(df1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D E 2019-01-01 False False False False False 2019-01-02 False False False False False 2019-01-03 False False False False True 2019-01-04 False False False False True "],["missing-data-part-2.html", "Section 19 Missing Data: Part 2 19.1 Dealing with Missing Data 19.2 Creating Missing Data 19.3 Drop Missing Data 19.4 Keep rows at Threshold 19.5 Fill Missing Values", " Section 19 Missing Data: Part 2 19.1 Dealing with Missing Data import numpy as np import pandas as pd np.random.seed(42) df = pd.DataFrame(np.random.randn(5,3), index=[&#39;a&#39;,&#39;c&#39;,&#39;d&#39;,&#39;f&#39;,&#39;g&#39;], columns=[&#39;one&#39;,&#39;two&#39;,&#39;three&#39;]) df[&#39;four&#39;] = &#39;blah&#39; df[&#39;five&#39;] = df[&#39;one&#39;] &gt; 0 19.2 Creating Missing Data df.iloc[2,2] = np.nan df.iloc[3,4] = np.nan # Notice what happens here! df.iloc[3,3] = np.nan df.iloc[4,4] = np.nan df.iloc[1,1] = np.nan df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } one two three four five a 0.496714 -0.138264 0.647689 blah 1.0 c 1.523030 NaN -0.234137 blah 1.0 d 1.579213 0.767435 NaN blah 1.0 f 0.542560 -0.463418 -0.465730 NaN NaN g 0.241962 -1.913280 -1.724918 blah NaN 19.3 Drop Missing Data df.dropna() # Keeps only complete rows .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } one two three four five a 0.496714 -0.138264 0.647689 blah 1.0 df.dropna(axis=1) # Keeps only complete columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } one a -0.719844 c 0.343618 d -0.385082 f 1.031000 g -0.309212 19.4 Keep rows at Threshold df.dropna(thresh=4) # Keeps rows that have AT LEAST 4 non-na values .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } one two three four five a 0.496714 -0.138264 0.647689 blah 1.0 c 1.523030 NaN -0.234137 blah 1.0 d 1.579213 0.767435 NaN blah 1.0 g 0.241962 -1.913280 -1.724918 blah NaN 19.5 Fill Missing Values df.fillna(value=&quot;PINK FLUFFY UNICORN&quot;) # Fill with whatever you want .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } one two three four five a 0.496714 -0.138264 0.647689 blah 1 c 1.523030 PINK FLUFFY UNICORN -0.234137 blah 1 d 1.579213 0.767435 PINK FLUFFY UNICORN blah 1 f 0.542560 -0.463418 -0.46573 PINK FLUFFY UNICORN PINK FLUFFY UNICORN g 0.241962 -1.91328 -1.72492 blah PINK FLUFFY UNICORN df[&#39;two&#39;].fillna(value=df[&#39;two&#39;].mean()) a -0.460639 c 0.031246 d -0.676922 f 0.931280 g 0.331263 Name: two, dtype: float64 "],["done.html", "Section 20 DONE!", " Section 20 DONE! "],["tidy-data.html", "Section 21 Tidy Data 21.1 Introduction 21.2 Spreading and gathering 21.3 Pivot (Spreading) 21.4 Separating and uniting 21.5 Separate 21.6 Missing Values 21.7 Case Study 21.8 Non-tidy data", " Section 21 Tidy Data 21.1 Introduction In this chapter, you will learn a consistent way to organise your data in Python, an organisation called tidy data. Getting your data into this format requires some upfront work, but that work pays off in the long term. Once you have tidy data and the tidy tools provided by pandas, you will spend much less time munging data from one representation to another, allowing you to spend more time on the analytic questions at hand. This chapter will give you a practical introduction to tidy data and the accompanying tools in pandas. If you’d like to learn more about the underlying theory, you might enjoy the Tidy Data paper published in the Journal of Statistical Software, http://www.jstatsoft.org/v59/i10/paper. 21.1.1 Prerequisties In this chapter we’ll focus on pandas, a package that provides tools to help tidy up your messy datasets. import pandas as pd import numpy as np You can represent the same underlying data in multiple ways. The example below shows the same data organised in four different ways. Each dataset shows the same values of four variables country, year, population, and cases, but each dataset organises the values in a different way. Table 1 country year cases population Afghanistan 1999 745 19987071 Afghanistan 2000 2666 20595360 Brazil 1999 37737 172006362 Brazil 2000 80488 174504898 China 1999 212258 1272915272 China 2000 213766 1280428583 Table 2 country year type count Afghanistan 1999 cases 745 Afghanistan 1999 population 19987071 Afghanistan 2000 cases 2666 Afghanistan 2000 population 20595360 Brazil 1999 cases 37737 Brazil 1999 population 172006362 Table 3 country year rate Afghanistan 1999 745/19987071 Afghanistan 2000 2666/20595360 Brazil 1999 37737/172006362 Brazil 2000 80488/174504898 China 1999 212258/1272915272 China 2000 213766/1280428583 Data Spread accross two tables. Table 4a: Cases country 1999 2000 Afghanistan 745 2666 Brazil 37737 80488 China 212258 213766 Table 4b: Population country 1999 2000 Afghanistan 19987071 20595360 Brazil 172006362 174504898 China 1272915272 1280428583 These are all representations of the same underlying data, but they are not equally easy to use. One dataset, the tidy dataset, will be much easier to work with inside the tidyverse. There are three interrelated rules which make a dataset tidy: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. Visual Representation of the rules Following three rules makes a dataset tidy: variables are in columns, observations are in rows, and values are in cells. These three rules are interrelated because it’s impossible to only satisfy two of the three. That interrelationship leads to an even simpler set of practical instructions: Put each dataset in a DataFrame. Put each variable in a column. In this example, only Table 1 is tidy. It’s the only representation where each column is a variable. Why ensure that your data is tidy? There are two main advantages: There’s a general advantage to picking one consistent way of storing data. If you have a consistent data structure, it’s easier to learn the tools that work with it because they have an underlying uniformity. There’s a specific advantage to placing variables in columns because it allows pandas’s vectorised nature to shine. As you learned, most built-in pandas functions work with vectors of values. That makes transforming tidy data feel particularly natural. !ll the other functions/methods in pandas are designed to work with tidy data. Here are a couple of small examples showing how you might work with from io import StringIO table1 = StringIO(&quot;&quot;&quot;country | year | cases | population Afghanistan | 1999 | 745 | 19987071 Afghanistan | 2000 | 2666 | 20595360 Brazil | 1999 | 37737 | 172006362 Brazil | 2000 | 80488 | 174504898 China | 1999 | 212258 | 1272915272 China | 2000 | 213766 | 1280428583&quot;&quot;&quot;) table1 = pd.read_csv(table1, sep=&quot;|&quot;, ) table1.columns = table1.columns.str.strip() Compute rate per 10,000 table1.assign(rate = lambda x: x[&quot;cases&quot;] / x[&quot;population&quot;] * 10000) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year cases population rate 0 Afghanistan 1999 745 19987071 0.372741 1 Afghanistan 2000 2666 20595360 1.294466 2 Brazil 1999 37737 172006362 2.193930 3 Brazil 2000 80488 174504898 4.612363 4 China 1999 212258 1272915272 1.667495 5 China 2000 213766 1280428583 1.669488 Compute cases per year table1.groupby(&#39;year&#39;)[&#39;cases&#39;].count() year 1999 3 2000 3 Name: cases, dtype: int64 Visualise changes over time import seaborn as sns sns.lineplot(x=&#39;year&#39;,y=&#39;cases&#39;,hue=&#39;country&#39;,data=table1); png 21.1.2 Exercises Using prose, describe how the variables and observations are organised in each of the sample tables. Compute the rate for table2, and table4a + table4b. You will need to perform four operations: Extract the number of TB cases per country per year. Extract the matching population per country per year. Divide cases by population, and multiply by 10000. Store back in the appropriate place. Which representation is easiest to work with? Which is hardest? Why? Recreate the plot showing change in cases over time using table2 instead of table1. What do you need to do first? 21.2 Spreading and gathering The principles of tidy data seem so obvious that you might wonder if you’ll ever encounter a dataset that isn’t tidy. Unfortunately, however, most data that you will encounter will be untidy. There are two main reasons: Most people aren’t familiar with the principles of tidy data, and it’s hard to derive them yourself unless you spend a lot of time working with data. Data is often organised to facilitate some use other than analysis. For example, data is often organised to make entry as easy as possible. This means for most real analyses, you’ll need to do some tidying. The first step is always to figure out what the variables and observations are. Sometimes this is easy; other times you’ll need to consult with the people who originally generated the data. The second step is to resolve one of two common problems: One variable might be spread across multiple columns. One observation might be scattered across multiple rows. Typically a dataset will only suffer from one of these problems; it’ll only suffer from both if you’re really unlucky! To fix these problems, you’ll need the two most important functions in pandas: melt() and pivot_table(). 21.2.1 Melt (Gathering) A common problem is a dataset where some of the column names are not names of variables, but values of a variable. Take table4a: the column names 1999 and 2000 represent values of the year variable, and each row represents two observations, not one. E.g., we are unpivoting a DataFrame from wide format to long format, optionally leaving identifier variables set. This function is useful to massage a DataFrame into a format where one or more columns are identifier variables (id_vars), while all other columns, considered measured variables (value_vars), are “unpivoted” to the row axis, leaving just two non-identifier columns, ‘variable’ and ‘value.’ table4a = StringIO(&quot;&quot;&quot;country|1999|2000 Afghanistan | 745 | 2666 Brazil | 37737 | 80488 China | 212258 | 213766 &quot;&quot;&quot;) table4a = pd.read_csv(table4a, sep=&quot;|&quot;) table4a .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country 1999 2000 0 Afghanistan 745 2666 1 Brazil 37737 80488 2 China 212258 213766 To tidy a dataset like this, we need to melt (gather) those columns into a new pair of variables. To describe that operation we need three parameters: The set of columns that represent values, not variables. In this example, those are the columns 1999 and 2000. The name of the identifier variable(s) is the id_vars, and here it is country. The name of the variable whose values form the column names. We call that the var_name, and here it is year. The name of the variable whose values are spread over the cells. We call that value_name, and here it’s the number of cases. Together those parameters generate the call to pd.melt() table4a = pd.melt(table4a, id_vars=[&quot;country&quot;], value_vars=[&quot;1999&quot;,&quot;2000&quot;], var_name = &quot;year&quot;, value_name=&quot;cases&quot; ) table4a .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year cases 0 Afghanistan 1999 745 1 Brazil 1999 37737 2 China 1999 212258 3 Afghanistan 2000 2666 4 Brazil 2000 80488 5 China 2000 213766 Here there are only two columns, so we list them individually. Note that “1999” and “2000” are non-syntactic names (because they don’t start with a letter) so we have to surround them in backticks. To refresh your memory of the other ways to select columns. Gathering table4 into a tidy form. In the final result, the gathered columns are dropped, and we get new key and value columns. Otherwise, the relationships between the original variables are preserved. We can use melt() to tidy table4b in a similar fashion. table4b = StringIO(&quot;&quot;&quot;country|1999|2000 Afghanistan | 19987071 | 20595360 Brazil | 172006362 | 174504898 China | 1272915272 | 1280428583&quot;&quot;&quot;) table4b = pd.read_csv(table4b, sep=&quot;|&quot;) table4b = pd.melt(table4b, id_vars=[&quot;country&quot;], value_vars=[&quot;1999&quot;,&quot;2000&quot;], var_name = &quot;year&quot;, value_name=&quot;population&quot; ) table4b .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year population 0 Afghanistan 1999 19987071 1 Brazil 1999 172006362 2 China 1999 1272915272 3 Afghanistan 2000 20595360 4 Brazil 2000 174504898 5 China 2000 1280428583 To combine the tidied versions of table4a and table4b into a single tibble, we need to use merge(), which you’ll learn about in relational data. pd.merge(table4a, table4b, how=&quot;left&quot;, on=[&quot;country&quot;,&quot;year&quot;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year cases population 0 Afghanistan 1999 745 19987071 1 Brazil 1999 37737 172006362 2 China 1999 212258 1272915272 3 Afghanistan 2000 2666 20595360 4 Brazil 2000 80488 174504898 5 China 2000 213766 1280428583 21.3 Pivot (Spreading) Pivoting (spreading) is the opposite of melting (gathering). You use it when an observation is scattered across multiple rows. For example, take table2: an observation is a country in a year, but each observation is spread across two rows. table2 = StringIO(&quot;&quot;&quot;country|year|type|count Afghanistan | 1999 | cases | 745 Afghanistan | 1999 | population | 19987071 Afghanistan | 2000 | cases | 2666 Afghanistan | 2000 | population | 20595360 Brazil | 1999 | cases | 37737 Brazil | 1999 | population | 172006362 &quot;&quot;&quot;) table2 = pd.read_csv(table2, sep=&quot;|&quot;) table2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year type count 0 Afghanistan 1999 cases 745 1 Afghanistan 1999 population 19987071 2 Afghanistan 2000 cases 2666 3 Afghanistan 2000 population 20595360 4 Brazil 1999 cases 37737 5 Brazil 1999 population 172006362 To tidy this up, we first analyse the representation in similar way to pivot_table(). pd.pivot_table(table2, index=[&quot;country&quot;,&quot;year&quot;],values=&quot;count&quot;, columns=&quot;type&quot;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type cases population country year Afghanistan 1999 745 19987071 2000 2666 20595360 Brazil 1999 37737 172006362 Spreading table2 makes it tidy. As you might have guessed that melt() and pivot_table() are complements. melt() makes wide tables narrower and longer; pivot_table() makes long tables shorter and wider. 21.3.1 Exercises 21.4 Separating and uniting So far you’ve learned how to tidy table2 and table4, but not table3. table3 has a different problem: we have one column (rate) that contains two variables (cases and population). To fix this problem, we’ll need the split() function. You’ll also learn about the complement of split(): cat(), which you use if a single variable is spread across multiple columns. 21.5 Separate str.split() pulls apart one column into multiple columns, by splitting whatever a separator character appears. The expand=True parameter converts the separated values into new columns. The astype(int) converts the string into integer columns. Take table3: table3 = StringIO(&quot;&quot;&quot;country|year|rate Afghanistan | 1999 | 745/19987071 Afghanistan | 2000 | 2666/20595360 Brazil | 1999 | 37737/172006362 Brazil | 2000 | 80488/174504898 China | 1999 | 212258/1272915272 China | 2000 | 213766/1280428583 &quot;&quot;&quot;) table3 = pd.read_csv(table3, sep=&quot;|&quot;) table3[[&#39;cases&#39;,&#39;population&#39;]] = table3[&#39;rate&#39;].str.split(&quot;/&quot;, expand=True).astype(int) Separating table3 makes it tidy By default, split() will split values by ,. If you wish to use a specific character to separate a column, you can pass the character to the sep argument. Formally, sep is a regular expression, which you’ll learn more about in strings table3[&quot;century&quot;] = table3[&quot;year&quot;].astype(str).str[:2] table3[&quot;year&quot;] = table3[&quot;year&quot;].astype(str).str[2:] table3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year rate cases population century 0 Afghanistan 99 745/19987071 745 19987071 19 1 Afghanistan 00 2666/20595360 2666 20595360 20 2 Brazil 99 37737/172006362 37737 172006362 19 3 Brazil 00 80488/174504898 80488 174504898 20 4 China 99 212258/1272915272 212258 1272915272 19 5 China 00 213766/1280428583 213766 1280428583 20 21.5.1 Unite cat() is the inverse of split()1: it combines multiple columns into a single column. You’ll need it much less frequently thansplit()`, but it’s still a useful tool to have in your back pocket. table3[&quot;new&quot;] = table3[&quot;century&quot;].str.cat(table3[&quot;year&quot;]) table3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year rate cases population century new 0 Afghanistan 99 745/19987071 745 19987071 19 1999 1 Afghanistan 00 2666/20595360 2666 20595360 20 2000 2 Brazil 99 37737/172006362 37737 172006362 19 1999 3 Brazil 00 80488/174504898 80488 174504898 20 2000 4 China 99 212258/1272915272 212258 1272915272 19 1999 5 China 00 213766/1280428583 213766 1280428583 20 2000 21.5.2 Exercises 21.6 Missing Values Changing the representation of a dataset brings up an important subtlety of missing values. Surprisingly, a value can be missing in one of two possible ways: 1. Explicitly, i.e. flagged with NaN 1. Implicitly, i.e. simply not present in the data. Let’s illustrate this idea with a very simple dataset: stocks = pd.DataFrame( { &quot;year&quot;:[2015, 2015, 2015, 2015, 2016, 2016, 2016], &quot;qtr&quot;:[1, 2, 3, 4, 2, 3, 4], &quot;return&quot;:[1.88, 0.59, 0.35, np.nan, 0.92, 0.17, 2.66] } ) stocks .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year qtr return 0 2015 1 1.88 1 2015 2 0.59 2 2015 3 0.35 3 2015 4 NaN 4 2016 2 0.92 5 2016 3 0.17 6 2016 4 2.66 There are two missing values in this dataset: The return for the fourth quarter of 2015 is explicitly missing, because the cell where its value should be instead contains NaN. The return for the first quarter of 2016 is implicitly missing, because it simply does not appear in the dataset. One way to think about the difference is with this Zen-like koan: An explicit missing value is the presence of an absence; an implicit missing value is the absence of a presence. The way that a dataset is represented can make implicit values explicit. For example, we can make the implicit missing value explicit by putting years in the columns: stocks_pivot = stocks.pivot(index=&quot;qtr&quot;, columns=&quot;year&quot;,values=&quot;return&quot;) stocks_pivot .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year 2015 2016 qtr 1 1.88 NaN 2 0.59 0.92 3 0.35 0.17 4 NaN 2.66 Because these explicit missing values may not be important in other representations of the data, you can use .dropna() in melt() to turn explicit missing values implicit: stocks_explicit = pd.melt(stocks_pivot.reset_index(), id_vars=[&#39;qtr&#39;], value_vars=[2015,2016]) stocks_explicit .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } qtr year value 0 1 2015 1.88 1 2 2015 0.59 2 3 2015 0.35 3 4 2015 NaN 4 1 2016 NaN 5 2 2016 0.92 6 3 2016 0.17 7 4 2016 2.66 stocks_implicit = pd.melt(stocks_pivot.reset_index(), id_vars=[&#39;qtr&#39;], value_vars=[2015,2016]).dropna() stocks_implicit .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } qtr year value 0 1 2015 1.88 1 2 2015 0.59 2 3 2015 0.35 5 2 2016 0.92 6 3 2016 0.17 7 4 2016 2.66 There’s one other important tool that you should know for working with missing values. To drop any rows that have missing data. stocks_explicit.dropna() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } qtr year value 0 1 2015 1.88 1 2 2015 0.59 2 3 2015 0.35 5 2 2016 0.92 6 3 2016 0.17 7 4 2016 2.66 Filling missing data. stocks_explicit.fillna(&quot;FILL VALUE&quot;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } qtr year value 0 1 2015 1.88 1 2 2015 0.59 2 3 2015 0.35 3 4 2015 FILL VALUE 4 1 2016 FILL VALUE 5 2 2016 0.92 6 3 2016 0.17 7 4 2016 2.66 stocks_explicit.fillna(0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } qtr year value 0 1 2015 1.88 1 2 2015 0.59 2 3 2015 0.35 3 4 2015 0.00 4 1 2016 0.00 5 2 2016 0.92 6 3 2016 0.17 7 4 2016 2.66 stocks_explicit.fillna(method=&quot;bfill&quot;) # backfill .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } qtr year value 0 1 2015 1.88 1 2 2015 0.59 2 3 2015 0.35 3 4 2015 0.92 4 1 2016 0.92 5 2 2016 0.92 6 3 2016 0.17 7 4 2016 2.66 stocks_explicit.fillna(method=&quot;ffill&quot;) # forward fill .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } qtr year value 0 1 2015 1.88 1 2 2015 0.59 2 3 2015 0.35 3 4 2015 0.35 4 1 2016 0.35 5 2 2016 0.92 6 3 2016 0.17 7 4 2016 2.66 There are a variety of other methods but we’ll leave those for another time. 21.6.1 Exercise 21.7 Case Study To finish off the chapter, let’s pull together everything you’ve learned to tackle a realistic data tidying problem. WHO dataset contains tuberculosis (TB) cases broken down by year, country, age, gender, and diagnosis method. The data comes from the 2014 World Health Organization Global Tuberculosis Report, available at http://www.who.int/tb/country/data/download/en/. There’s a wealth of epidemiological information in this dataset, but it’s challenging to work with the data in the form that it’s provided: url = &quot;https://extranet.who.int/tme/generateCSV.asp?ds=notifications&quot; who = pd.read_csv(url) This is a very typical real-life example dataset. It contains redundant columns, odd variable codes, and many missing values. In short, who is messy, and we’ll need multiple steps to tidy it. That means in real-life situations you’ll usually need to string together multiple functions into a pipeline. The best place to start is almost always to gather together the columns that are not variables. Let’s have a look at what we’ve got: It looks like country, iso2, and iso3 are three variables that redundantly specify the country. year is clearly also a variable. We don’t know what all the other columns are yet, but given the structure in the variable names (e.g. new_sp_m014, new_ep_m014, new_ep_f014) these are likely to be values, not variables. So we need to gather together all the columns from new_sp_m014 to newrel_f65. We don’t know what those values represent yet, so we’ll give them the generic name “key.” We know the cells represent the count of cases, so we’ll use the variable cases. There are a lot of missing values in the current representation, so for now we’ll use .dropna() just so we can focus on the values that are present. who1 = pd.melt(who, id_vars=[&quot;country&quot;,&quot;iso2&quot;,&quot;iso3&quot;,&quot;year&quot;], var_name=&quot;key&quot;, value_name=&quot;cases&quot;).dropna() We can get some hint of the structure of the values in the new key column by counting them: who1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country iso2 iso3 year key cases 0 Afghanistan AF AFG 1980 iso_numeric 4 1 Afghanistan AF AFG 1981 iso_numeric 4 2 Afghanistan AF AFG 1982 iso_numeric 4 3 Afghanistan AF AFG 1983 iso_numeric 4 4 Afghanistan AF AFG 1984 iso_numeric 4 … … … … … … … 1325717 Zambia ZM ZMB 2015 hiv_reg_new2 3888 1325719 Zambia ZM ZMB 2017 hiv_reg_new2 199278 1325755 Zimbabwe ZW ZWE 2014 hiv_reg_new2 215379 1325757 Zimbabwe ZW ZWE 2016 hiv_reg_new2 168968 1325758 Zimbabwe ZW ZWE 2017 hiv_reg_new2 164963 214489 rows × 6 columns You might be able to parse this out by yourself with a little thought and some experimentation, but luckily we have the data dictionary handy. It tells us: The first three letters of each column denote whether the column contains new or old cases of TB. In this dataset, each column contains new cases. The next two letters describe the type of TB: rel stands for cases of relapse ep stands for cases of extrapulmonary TB sn stands for cases of pulmonary TB that could not be diagnosed by a pulmonary smear (smear negative) sp stands for cases of pulmonary TB that could be diagnosed be a pulmonary smear (smear positive) The sixth letter gives the sex of TB patients. The dataset groups cases by males (m) and females (f). The remaining numbers gives the age group. The dataset groups cases into seven age groups: 014 = 0 – 14 years old 1524 = 15 – 24 years old 2534 = 25 – 34 years old 3544 = 35 – 44 years old 4554 = 45 – 54 years old 5564 = 55 – 64 years old 65 = 65 or older We need to make a minor fix to the format of the column names: unfortunately the names are slightly inconsistent because instead of new_rel we have newrel (it’s hard to spot this here but if you don’t fix it we’ll get errors in subsequent steps). You’ll learn aboutreplace() in strings, but the basic idea is pretty simple: replace the characters “newrel” with “new_rel.” This makes all variable names consistent. who2 = who1.assign(key= who1[&quot;key&quot;].str.replace(&quot;newrel&quot;,&quot;new_rel&quot;).str.replace(&quot;newret&quot;,&quot;new_ret&quot;)) We can separate the values in each code with two passes. The first pass will split the codes at each underscore. who2.key.str.split(&quot;_&quot;, expand=True).rename({0:&quot;new&quot;, 1:&quot;type&quot;,2:&quot;sexage&quot;,3:&quot;unk&quot;}, axis=1).unk.unique() array([None, &#39;flg&#39;, &#39;events&#39;, &#39;all&#39;], dtype=object) 21.7.1 Exercises 21.8 Non-tidy data "],["operations.html", "Section 22 Operations 22.1 Stats", " Section 22 Operations 22.1 Stats Operations in general exclude missing data. Performing a descriptive statistic: import pandas as pd import numpy as np dates = pd.date_range(&#39;20190101&#39;,periods=10) df = pd.DataFrame(np.random.randn(10,4), index=dates, columns=list(&#39;ABCD&#39;)) df = df.reindex(index=dates[0:4], columns=list(df.columns) + [&#39;E&#39;]) df.loc[dates[0]:dates[1], &#39;E&#39;] = 1 df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D E 2019-01-01 -0.914745 -1.169413 0.445567 -1.612202 1.0 2019-01-02 1.382350 1.083761 -1.132513 -0.282442 1.0 2019-01-03 -1.022387 2.451375 1.505668 0.344774 NaN 2019-01-04 0.907028 -0.532535 -0.730059 -0.608755 NaN df.mean() A 0.088062 B 0.458297 C 0.022166 D -0.539656 E 1.000000 dtype: float64 Same operation on the other axis: df.mean(axis=1) 2019-01-01 -0.450158 2019-01-02 0.410231 2019-01-03 0.819858 2019-01-04 -0.241081 Freq: D, dtype: float64 "],["apply.html", "Section 23 Apply", " Section 23 Apply Applying functions to the data: df.apply(np.cumsum) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D E 2019-01-01 -0.914745 -1.169413 0.445567 -1.612202 1.0 2019-01-02 0.467606 -0.085651 -0.686946 -1.894644 2.0 2019-01-03 -0.554781 2.365724 0.818722 -1.549870 NaN 2019-01-04 0.352247 1.833188 0.088663 -2.158625 NaN df.apply(lambda x: x.max() - x.min()) A 2.404737 B 3.620788 C 2.638181 D 1.956975 E 0.000000 dtype: float64 "],["histogramming.html", "Section 24 Histogramming", " Section 24 Histogramming s = pd.Series(np.random.randint(0, 7, size=10)) s 0 4 1 0 2 5 3 5 4 3 5 2 6 2 7 5 8 4 9 2 dtype: int64 s.value_counts() 5 3 2 3 4 2 3 1 0 1 dtype: int64 "],["string-methods-1.html", "Section 25 String Methods", " Section 25 String Methods Series is equipped with a set of string processing methods in the str attribute that make it easy to operate on each element of the array, as in the code snippet below. Note that pattern-matching in str generally uses regular expressions by default. s = pd.Series([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;Aaba&#39;, &#39;Baca&#39;, np.nan, &#39;CABA&#39;, &#39;dog&#39;, &#39;cat&#39;]) s 0 A 1 B 2 C 3 Aaba 4 Baca 5 NaN 6 CABA 7 dog 8 cat dtype: object s.str.title() 0 A 1 B 2 C 3 Aaba 4 Baca 5 NaN 6 Caba 7 Dog 8 Cat dtype: object "],["merging-data-part-1.html", "Section 26 Merging Data: Part 1 26.1 Merge 26.2 Concat", " Section 26 Merging Data: Part 1 26.1 Merge 26.2 Concat pandas provides various facilities for easily combining together Series and DataFrame objects with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations. Concatenating pandas objects together with concat(): import pandas as pd import numpy as np np.random.seed() df = pd.DataFrame(np.random.randn(10, 4)) df # DataFrame .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 0 -0.493947 1.625480 0.244771 -0.185388 1 0.198916 -1.376590 -1.215013 -1.194330 2 -0.119116 0.843964 -0.320494 -2.380751 3 0.024455 1.921114 -0.142516 0.262443 4 -0.871334 -0.456046 -0.653551 1.771860 5 1.058511 -2.091128 1.171792 1.563637 6 -0.192952 1.377423 0.552583 0.288304 7 1.480328 -0.757365 -0.272379 -0.362830 8 -0.033634 -0.371534 1.738937 -1.010350 9 -0.206187 -1.458660 0.538586 -1.532622 df[:3] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 0 -0.493947 1.625480 0.244771 -0.185388 1 0.198916 -1.376590 -1.215013 -1.194330 2 -0.119116 0.843964 -0.320494 -2.380751 pieces = [df[:3], df[3:7], df[7:]] # Break it into pieces pieces [ 0 1 2 3 0 -0.493947 1.625480 0.244771 -0.185388 1 0.198916 -1.376590 -1.215013 -1.194330 2 -0.119116 0.843964 -0.320494 -2.380751, 0 1 2 3 3 0.024455 1.921114 -0.142516 0.262443 4 -0.871334 -0.456046 -0.653551 1.771860 5 1.058511 -2.091128 1.171792 1.563637 6 -0.192952 1.377423 0.552583 0.288304, 0 1 2 3 7 1.480328 -0.757365 -0.272379 -0.362830 8 -0.033634 -0.371534 1.738937 -1.010350 9 -0.206187 -1.458660 0.538586 -1.532622] pd.concat(pieces) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 0 -0.493947 1.625480 0.244771 -0.185388 1 0.198916 -1.376590 -1.215013 -1.194330 2 -0.119116 0.843964 -0.320494 -2.380751 3 0.024455 1.921114 -0.142516 0.262443 4 -0.871334 -0.456046 -0.653551 1.771860 5 1.058511 -2.091128 1.171792 1.563637 6 -0.192952 1.377423 0.552583 0.288304 7 1.480328 -0.757365 -0.272379 -0.362830 8 -0.033634 -0.371534 1.738937 -1.010350 9 -0.206187 -1.458660 0.538586 -1.532622 "],["join.html", "Section 27 Join", " Section 27 Join SQL style merges. left = pd.DataFrame({&#39;key&#39;: [&#39;foo&#39;, &#39;foo&#39;], &#39;lval&#39;: [1, 2]}) right = pd.DataFrame({&#39;key&#39;: [&#39;foo&#39;, &#39;foo&#39;], &#39;rval&#39;: [4, 5]}) left .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key lval 0 foo 1 1 foo 2 right .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key rval 0 foo 4 1 foo 5 pd.merge(left, right, on=&#39;key&#39;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key lval rval 0 foo 1 4 1 foo 1 5 2 foo 2 4 3 foo 2 5 Another Example left = pd.DataFrame({&#39;key&#39;: [&#39;foo&#39;, &#39;bar&#39;], &#39;lval&#39;: [1, 2]}) right = pd.DataFrame({&#39;key&#39;: [&#39;foo&#39;, &#39;bar&#39;], &#39;rval&#39;: [4, 5]}) left .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key lval 0 foo 1 1 bar 2 right .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key rval 0 foo 4 1 bar 5 pd.merge(left, right, on=&#39;key&#39;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key lval rval 0 foo 1 4 1 bar 2 5 "],["append.html", "Section 28 Append", " Section 28 Append Append rows to a dataframe. df = pd.DataFrame(np.random.randn(8, 4), columns=[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 -0.784375 0.511805 0.323780 -0.065874 1 -1.777933 0.149912 -1.727348 0.197836 2 2.382810 0.501593 1.274527 -0.456674 3 0.003459 0.793957 1.188773 0.999236 4 -0.874190 0.413551 0.020734 -0.300775 5 -1.520646 1.291041 0.561644 -0.705396 6 0.487781 0.191630 0.788804 1.912556 7 0.612815 0.366643 -1.120432 -1.543861 s = df.iloc[3] s A 0.003459 B 0.793957 C 1.188773 D 0.999236 Name: 3, dtype: float64 df.append(s, ignore_index=True) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 -0.784375 0.511805 0.323780 -0.065874 1 -1.777933 0.149912 -1.727348 0.197836 2 2.382810 0.501593 1.274527 -0.456674 3 0.003459 0.793957 1.188773 0.999236 4 -0.874190 0.413551 0.020734 -0.300775 5 -1.520646 1.291041 0.561644 -0.705396 6 0.487781 0.191630 0.788804 1.912556 7 0.612815 0.366643 -1.120432 -1.543861 8 0.003459 0.793957 1.188773 0.999236 "],["merging-data-part-2.html", "Section 29 Merging Data: Part 2 29.1 Merging, Joining, and Concatenating 29.2 Create DataFrame Examples Using List Comprehension 29.3 Concatenation 29.4 Example DataFrames with Keys 29.5 Merging 29.6 Joining", " Section 29 Merging Data: Part 2 29.1 Merging, Joining, and Concatenating There are 3 methods to combine DataFrames: 1. Merging (`pd.merge()`) 2. Joining (`df_left.join(df_right)`) 3. Concatenating (`pd.concat()`) 29.2 Create DataFrame Examples Using List Comprehension import numpy as np import pandas as pd l = [let + str(num) for let in [&#39;A&#39;,&#39;B&#39;,&#39;C&#39;, &#39;D&#39;] for num in range(12)] l [&#39;A0&#39;, &#39;A1&#39;, &#39;A2&#39;, &#39;A3&#39;, &#39;A4&#39;, &#39;A5&#39;, &#39;A6&#39;, &#39;A7&#39;, &#39;A8&#39;, &#39;A9&#39;, &#39;A10&#39;, &#39;A11&#39;, &#39;B0&#39;, &#39;B1&#39;, &#39;B2&#39;, &#39;B3&#39;, &#39;B4&#39;, &#39;B5&#39;, &#39;B6&#39;, &#39;B7&#39;, &#39;B8&#39;, &#39;B9&#39;, &#39;B10&#39;, &#39;B11&#39;, &#39;C0&#39;, &#39;C1&#39;, &#39;C2&#39;, &#39;C3&#39;, &#39;C4&#39;, &#39;C5&#39;, &#39;C6&#39;, &#39;C7&#39;, &#39;C8&#39;, &#39;C9&#39;, &#39;C10&#39;, &#39;C11&#39;, &#39;D0&#39;, &#39;D1&#39;, &#39;D2&#39;, &#39;D3&#39;, &#39;D4&#39;, &#39;D5&#39;, &#39;D6&#39;, &#39;D7&#39;, &#39;D8&#39;, &#39;D9&#39;, &#39;D10&#39;, &#39;D11&#39;] n = 4 d = [l[i:i + n] for i in range(0, len(l), n)] df_1 = pd.DataFrame({&#39;A&#39;:d[0],&#39;B&#39;:d[3], &#39;C&#39;:d[6],&#39;D&#39;:d[9]}, index=[0,1,2,3]) df_2 = pd.DataFrame({&#39;A&#39;:d[1],&#39;B&#39;:d[4], &#39;C&#39;:d[7],&#39;D&#39;:d[10]}, index=[4,5,6,7]) df_3 = pd.DataFrame({&#39;A&#39;:d[2],&#39;B&#39;:d[5], &#39;C&#39;:d[8],&#39;D&#39;:d[11]}, index=[8,9,10,11]) df_1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 df_2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 4 A4 B4 C4 D4 5 A5 B5 C5 D5 6 A6 B6 C6 D6 7 A7 B7 C7 D7 df_3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 8 A8 B8 C8 D8 9 A9 B9 C9 D9 10 A10 B10 C10 D10 11 A11 B11 C11 D11 29.3 Concatenation Concatenation glues together DataFrames The dimensions of each series should match Use pd.concat() with list of DataFrames pd.concat([df_1,df_2,df_3]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 4 A4 B4 C4 D4 5 A5 B5 C5 D5 6 A6 B6 C6 D6 7 A7 B7 C7 D7 8 A8 B8 C8 D8 9 A9 B9 C9 D9 10 A10 B10 C10 D10 11 A11 B11 C11 D11 pd.concat([df_1,df_2,df_3],axis=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D A B C D A B C D 0 A0 B0 C0 D0 NaN NaN NaN NaN NaN NaN NaN NaN 1 A1 B1 C1 D1 NaN NaN NaN NaN NaN NaN NaN NaN 2 A2 B2 C2 D2 NaN NaN NaN NaN NaN NaN NaN NaN 3 A3 B3 C3 D3 NaN NaN NaN NaN NaN NaN NaN NaN 4 NaN NaN NaN NaN A4 B4 C4 D4 NaN NaN NaN NaN 5 NaN NaN NaN NaN A5 B5 C5 D5 NaN NaN NaN NaN 6 NaN NaN NaN NaN A6 B6 C6 D6 NaN NaN NaN NaN 7 NaN NaN NaN NaN A7 B7 C7 D7 NaN NaN NaN NaN 8 NaN NaN NaN NaN NaN NaN NaN NaN A8 B8 C8 D8 9 NaN NaN NaN NaN NaN NaN NaN NaN A9 B9 C9 D9 10 NaN NaN NaN NaN NaN NaN NaN NaN A10 B10 C10 D10 11 NaN NaN NaN NaN NaN NaN NaN NaN A11 B11 C11 D11 29.4 Example DataFrames with Keys left = pd.DataFrame({&#39;key&#39;: [&#39;K0&#39;, &#39;K1&#39;, &#39;K2&#39;, &#39;K3&#39;], &#39;A&#39;: d[0], &#39;B&#39;: d[3]}) right = pd.DataFrame({&#39;key&#39;: [&#39;K0&#39;, &#39;K1&#39;, &#39;K2&#39;, &#39;K3&#39;], &#39;C&#39;: d[6], &#39;D&#39;: d[9]}) left .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key A B 0 K0 A0 B0 1 K1 A1 B1 2 K2 A2 B2 3 K3 A3 B3 right .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key C D 0 K0 C0 D0 1 K1 C1 D1 2 K2 C2 D2 3 K3 C3 D3 29.5 Merging pd.merge() allows you to merge DataFrames together This is similar to merging SQL Tables pd.merge(left,right,how=&#39;inner&#39;,on=&#39;key&#39;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key A B C D 0 K0 A0 B0 C0 D0 1 K1 A1 B1 C1 D1 2 K2 A2 B2 C2 D2 3 K3 A3 B3 C3 D3 Or to show a more complicated example: left = pd.DataFrame({&#39;key1&#39;: [&#39;K0&#39;, &#39;K0&#39;, &#39;K1&#39;, &#39;K2&#39;], &#39;key2&#39;: [&#39;K0&#39;, &#39;K1&#39;, &#39;K0&#39;, &#39;K1&#39;], &#39;A&#39;: d[0], &#39;B&#39;: d[3]}) right = pd.DataFrame({&#39;key1&#39;: [&#39;K0&#39;, &#39;K1&#39;, &#39;K1&#39;, &#39;K2&#39;], &#39;key2&#39;: [&#39;K0&#39;, &#39;K0&#39;, &#39;K0&#39;, &#39;K0&#39;], &#39;C&#39;: d[6], &#39;D&#39;: d[9]}) pd.merge(left, right, on=[&#39;key1&#39;, &#39;key2&#39;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key1 key2 A B C D 0 K0 K0 A0 B0 C0 D0 1 K1 K0 A2 B2 C1 D1 2 K1 K0 A2 B2 C2 D2 pd.merge(left, right, how=&#39;outer&#39;, on=[&#39;key1&#39;, &#39;key2&#39;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key1 key2 A B C D 0 K0 K0 A0 B0 C0 D0 1 K0 K1 A1 B1 NaN NaN 2 K1 K0 A2 B2 C1 D1 3 K1 K0 A2 B2 C2 D2 4 K2 K1 A3 B3 NaN NaN 5 K2 K0 NaN NaN C3 D3 pd.merge(left, right, how=&#39;right&#39;, on=[&#39;key1&#39;, &#39;key2&#39;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key1 key2 A B C D 0 K0 K0 A0 B0 C0 D0 1 K1 K0 A2 B2 C1 D1 2 K1 K0 A2 B2 C2 D2 3 K2 K0 NaN NaN C3 D3 pd.merge(left, right, how=&#39;left&#39;, on=[&#39;key1&#39;, &#39;key2&#39;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key1 key2 A B C D 0 K0 K0 A0 B0 C0 D0 1 K0 K1 A1 B1 NaN NaN 2 K1 K0 A2 B2 C1 D1 3 K1 K0 A2 B2 C2 D2 4 K2 K1 A3 B3 NaN NaN 29.6 Joining left.join(right) combines the columns of two (potentially differently) indexed DataFrames into a single DataFrame left = pd.DataFrame({&#39;A&#39;: d[0], &#39;B&#39;: d[3]}, index=[&#39;K0&#39;, &#39;K1&#39;, &#39;K2&#39;, &#39;K4&#39;]) right = pd.DataFrame({&#39;C&#39;: d[6], &#39;D&#39;: d[9]}, index=[&#39;K0&#39;, &#39;K2&#39;, &#39;K3&#39;, &#39;K5&#39;]) left.join(right) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D K0 A0 B0 C0 D0 K1 A1 B1 NaN NaN K2 A2 B2 C1 D1 K4 A3 B3 NaN NaN left.join(right, how=&#39;outer&#39;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D K0 A0 B0 C0 D0 K1 A1 B1 NaN NaN K2 A2 B2 C1 D1 K3 NaN NaN C2 D2 K4 A3 B3 NaN NaN K5 NaN NaN C3 D3 "],["done-1.html", "Section 30 DONE", " Section 30 DONE "],["groupby-part-1.html", "Section 31 Groupby: Part 1", " Section 31 Groupby: Part 1 By “group by” we are referring to a process involving one or more of the following steps: Splitting the data into groups based on some criteria Applying a function to each group independently Combining the results into a data structure df = pd.DataFrame({&#39;A&#39;: [&#39;foo&#39;, &#39;bar&#39;, &#39;foo&#39;, &#39;bar&#39;, &#39;foo&#39;, &#39;bar&#39;, &#39;foo&#39;, &#39;foo&#39;], &#39;B&#39;: [&#39;one&#39;, &#39;one&#39;, &#39;two&#39;, &#39;three&#39;, &#39;two&#39;, &#39;two&#39;, &#39;one&#39;, &#39;three&#39;], &#39;C&#39;: np.random.randn(8), &#39;D&#39;: np.random.randn(8)}) df Grouping and then applying the sum() function to the resulting groups. df.groupby(&#39;A&#39;).sum() Grouping by multiple columns forms a hierarchical index, and again we can apply the sum function. df.groupby([&#39;A&#39;, &#39;B&#39;]).sum() "],["groupby-part-2.html", "Section 32 Groupby: Part 2 32.1 groupby() 32.2 Groupby and Aggregation Methods 32.3 Groupby + describe method 32.4 Groupby + describe method 32.5 Groupby + describe method (multiple numeric values)", " Section 32 Groupby: Part 2 .groupby method allow syou to group rows of data together and call aggregate functions import numpy as np import pandas as pd data = {&#39;school&#39;: [&#39;HSU&#39;, &#39;HSU&#39;,&#39;HSU&#39;, &#39;OBU&#39;,&#39;OBU&#39;, &#39;SIU&#39;,&#39;SIU&#39;, &#39;SEU&#39;, &#39;SEU&#39;], &#39;professor&#39;: [&#39;Bob&#39;,&#39;Jeff&#39;,&#39;Angela&#39;,&#39;Susan&#39;, &#39;Albert&#39;,&#39;Zelda&#39;,&#39;Alexa&#39;, &#39;Heather&#39;,&#39;Rebecca&#39;], &#39;publication&#39;: [10,2,30,25,0,80,4,30,15]} df = pd.DataFrame(data) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } school professor publication 0 HSU Bob 10 1 HSU Jeff 2 2 HSU Angela 30 3 OBU Susan 25 4 OBU Albert 0 5 SIU Zelda 80 6 SIU Alexa 4 7 SEU Heather 30 8 SEU Rebecca 15 32.1 groupby() use the .groupby() method to group rows together based off of a column name Create a group based off of school df.groupby(&#39;school&#39;) #creates a DataFrameGroupBy object &lt;pandas.core.groupby.groupby.DataFrameGroupBy object at 0x119d4db00&gt; by_school = df.groupby(&#39;school&#39;) 32.2 Groupby and Aggregation Methods Use the aggregation methods on the grouped object by_school.mean() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } publication school HSU 14.0 OBU 12.5 SEU 22.5 SIU 42.0 df.groupby(&#39;school&#39;).mean() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } publication school HSU 14.0 OBU 12.5 SEU 22.5 SIU 42.0 by_school.std() # standard deviation .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } publication school HSU 14.422205 OBU 17.677670 SEU 10.606602 SIU 53.740115 by_school.min() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } professor publication school HSU Angela 2 OBU Albert 0 SEU Heather 15 SIU Alexa 4 by_school.max() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } professor publication school HSU Jeff 30 OBU Susan 25 SEU Rebecca 30 SIU Zelda 80 by_school.count() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } professor publication school HSU 3 3 OBU 2 2 SEU 2 2 SIU 2 2 32.3 Groupby + describe method by_school.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } publication count mean std min 25% 50% 75% max school HSU 3.0 14.0 14.422205 2.0 6.00 10.0 20.00 30.0 OBU 2.0 12.5 17.677670 0.0 6.25 12.5 18.75 25.0 SEU 2.0 22.5 10.606602 15.0 18.75 22.5 26.25 30.0 SIU 2.0 42.0 53.740115 4.0 23.00 42.0 61.00 80.0 32.4 Groupby + describe method type(by_school.describe().transpose()) pandas.core.frame.DataFrame 32.5 Groupby + describe method (multiple numeric values) data = {&#39;school&#39;: [&#39;HSU&#39;, &#39;HSU&#39;,&#39;HSU&#39;, &#39;OBU&#39;,&#39;OBU&#39;,&#39;SIU&#39;,&#39;SIU&#39;, &#39;SEU&#39;, &#39;SEU&#39;], &#39;professor&#39;: [&#39;Bob&#39;,&#39;Jeff&#39;,&#39;Angela&#39;,&#39;Susan&#39;,&#39;Albert&#39;,&#39;Zelda&#39;,&#39;Alexa&#39;,&#39;Heather&#39;,&#39;Rebecca&#39;], &#39;publication&#39;: [10,2,30,25,0,80,4,30,15], &#39;years&#39;:[2,8,14,3,4,25,7,2,5]} df = pd.DataFrame(data) df.groupby(&#39;school&#39;).describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } publication years count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max school HSU 3.0 14.0 14.422205 2.0 6.00 10.0 20.00 30.0 3.0 8.0 6.000000 2.0 5.00 8.0 11.00 14.0 OBU 2.0 12.5 17.677670 0.0 6.25 12.5 18.75 25.0 2.0 3.5 0.707107 3.0 3.25 3.5 3.75 4.0 SEU 2.0 22.5 10.606602 15.0 18.75 22.5 26.25 30.0 2.0 3.5 2.121320 2.0 2.75 3.5 4.25 5.0 SIU 2.0 42.0 53.740115 4.0 23.00 42.0 61.00 80.0 2.0 16.0 12.727922 7.0 11.50 16.0 20.50 25.0 df.groupby(&#39;school&#39;).describe().transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } school HSU OBU SEU SIU publication count 3.000000 2.000000 2.000000 2.000000 mean 14.000000 12.500000 22.500000 42.000000 std 14.422205 17.677670 10.606602 53.740115 min 2.000000 0.000000 15.000000 4.000000 25% 6.000000 6.250000 18.750000 23.000000 50% 10.000000 12.500000 22.500000 42.000000 75% 20.000000 18.750000 26.250000 61.000000 max 30.000000 25.000000 30.000000 80.000000 years count 3.000000 2.000000 2.000000 2.000000 mean 8.000000 3.500000 3.500000 16.000000 std 6.000000 0.707107 2.121320 12.727922 min 2.000000 3.000000 2.000000 7.000000 25% 5.000000 3.250000 2.750000 11.500000 50% 8.000000 3.500000 3.500000 16.000000 75% 11.000000 3.750000 4.250000 20.500000 max 14.000000 4.000000 5.000000 25.000000 "],["done-2.html", "Section 33 DONE", " Section 33 DONE "],["reshaping.html", "Section 34 Reshaping 34.1 Stack 34.2 Removing Columns 34.3 Remove/Drop data (For real) 34.4 Dropping rows 34.5 Why 0 for row; 1 for column", " Section 34 Reshaping 34.1 Stack tuples = list(zip(*[[&#39;bar&#39;, &#39;bar&#39;, &#39;baz&#39;, &#39;baz&#39;, &#39;foo&#39;, &#39;foo&#39;, &#39;qux&#39;, &#39;qux&#39;], [&#39;one&#39;, &#39;two&#39;, &#39;one&#39;, &#39;two&#39;, &#39;one&#39;, &#39;two&#39;, &#39;one&#39;, &#39;two&#39;]])) index = pd.MultiIndex.from_tuples(tuples, names=[&#39;first&#39;, &#39;second&#39;]) df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=[&#39;A&#39;, &#39;B&#39;]) df2 = df[:4] df2 The stack() method “compresses” a level in the DataFrame’s columns. stacked = df2.stack() stacked With a “stacked” DataFrame or Series (having a MultiIndex as the index), the inverse operation of stack() is unstack(), which by default unstacks the last level: stacked.unstack() stacked.unstack(1) stacked.unstack(0) ## Creating New Columns df[&#39;new&#39;] = df[&#39;A&#39;] + df[&#39;B&#39;] df 34.2 Removing Columns df.drop(&#39;new&#39;, axis=&#39;columns&#39;) # could also use axis = 1 df # new is still there!! 34.3 Remove/Drop data (For real) Use inplace to remove PERMENANTLY df.drop(&#39;new&#39;, axis=&#39;columns&#39;, inplace=True) 34.4 Dropping rows Default drop is row (or use axis = 0) df.drop(&#39;second&#39;, axis=&#39;rows&#39;) 34.5 Why 0 for row; 1 for column df.shape # location 0 for row; 1 for column "],["pivot-tables.html", "Section 35 Pivot tables", " Section 35 Pivot tables df = pd.DataFrame({&#39;A&#39;: [&#39;one&#39;, &#39;one&#39;, &#39;two&#39;, &#39;three&#39;] * 3, &#39;B&#39;: [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;] * 4, &#39;C&#39;: [&#39;foo&#39;, &#39;foo&#39;, &#39;foo&#39;, &#39;bar&#39;, &#39;bar&#39;, &#39;bar&#39;] * 2, &#39;D&#39;: np.random.randn(12), &#39;E&#39;: np.random.randn(12)}) df We can produce pivot tables from this data very easily: pd.pivot_table(df, values=&#39;D&#39;, index=[&#39;A&#39;, &#39;B&#39;], columns=[&#39;C&#39;]) "],["time-series.html", "Section 36 Time Series", " Section 36 Time Series pandas has simple, powerful, and efficient functionality for performing resampling operations during frequency conversion (e.g., converting secondly data into 5-minutely data). This is extremely common in, but not limited to, financial applications. rng = pd.date_range(&#39;1/1/2012&#39;, periods=100, freq=&#39;S&#39;) ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng) ts.resample(&#39;5Min&#39;).sum() Time zone representation: rng = pd.date_range(&#39;3/6/2012 00:00&#39;, periods=5, freq=&#39;D&#39;) ts = pd.Series(np.random.randn(len(rng)), rng) ts ts_utc = ts.tz_localize(&#39;UTC&#39;) ts_utc Converting to another time zone: ts_utc.tz_convert(&#39;US/Eastern&#39;) Converting between time span representations: rng = pd.date_range(&#39;1/1/2012&#39;, periods=5, freq=&#39;M&#39;) ts = pd.Series(np.random.randn(len(rng)), index=rng) ts ps = ts.to_period() ps ps.to_timestamp() Converting between period and timestamp enables some convenient arithmetic functions to be used. In the following example, we convert a quarterly frequency with year ending in November to 9am of the end of the month following the quarter end: prng = pd.period_range(&#39;1990Q1&#39;, &#39;2000Q4&#39;, freq=&#39;Q-NOV&#39;) ts = pd.Series(np.random.randn(len(prng)), prng) ts.index = (prng.asfreq(&#39;M&#39;, &#39;e&#39;) + 1).asfreq(&#39;H&#39;, &#39;s&#39;) + 9 ts.head() "],["categorical-data.html", "Section 37 Categorical Data", " Section 37 Categorical Data pandas can include categorical data in a DataFrame. df = pd.DataFrame({&quot;id&quot;: [1, 2, 3, 4, 5, 6], &quot;raw_grade&quot;: [&#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;a&#39;, &#39;a&#39;, &#39;e&#39;]}) Convert the raw grades to a categorical data type. df[&quot;grade&quot;] = df[&quot;raw_grade&quot;].astype(&quot;category&quot;) df[&#39;grade&#39;] Rename the categories to more meaningful names (assigning to Series.cat.categories is inplace!). df[&quot;grade&quot;].cat.categories = [&quot;very good&quot;, &quot;good&quot;, &quot;very bad&quot;] df[&#39;grade&#39;] Reorder the categories and simultaneously add the missing categories (methods under Series .cat return a new Series by default). df[&quot;grade&quot;] = df[&quot;grade&quot;].cat.set_categories([&quot;very bad&quot;, &quot;bad&quot;, &quot;medium&quot;, &quot;good&quot;, &quot;very good&quot;]) df[&#39;grade&#39;] df.sort_values(by=&quot;grade&quot;) Grouping by a categorical column also shows empty categories. df.groupby(&quot;grade&quot;).size() "],["plotting-data-with-pandas.html", "Section 38 Plotting Data with Pandas", " Section 38 Plotting Data with Pandas ts = pd.Series(np.random.randn(1000), index=pd.date_range(&#39;1/1/2000&#39;, periods=1000)) ts = ts.cumsum() ts.plot(); On a DataFrame, the plot() method is a convenience to plot all of the columns with labels: df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;]) df = df.cumsum() df.plot(); "],["data-inputoutput-part-1.html", "Section 39 Data Input/Output: Part 1 39.1 CSV 39.2 HDF5 39.3 Excel", " Section 39 Data Input/Output: Part 1 39.1 CSV Writing a csv file. df.to_csv(&#39;foo.csv&#39;) Reading from a csvs file. pd.read_csv(&#39;foo.csv&#39;) 39.2 HDF5 Writing to a HDF5 Store. df.to_hdf(&#39;foo.h5&#39;,&#39;df&#39;) Reading a HDF5 Store pd.read_hdf(&#39;foo.h5&#39;, &#39;df&#39;) 39.3 Excel Writing an excel file df.to_excel(&#39;foo.xlsx&#39;, sheet_name=&#39;Sheet1&#39;) Reading from an excel file pd.read_excel(&#39;foo.xlsx&#39;,&#39;Sheet1&#39;,index_col=None, na_values=[&#39;NA&#39;]) "],["data-inputoutput-part-2.html", "Section 40 Data Input/Output: Part 2 40.1 CSV 40.2 Excel", " Section 40 Data Input/Output: Part 2 Let’s take a look at importing and exporting data with Pandas import numpy as np import pandas as pd 40.1 CSV 40.1.1 CSV Input df = pd.read_csv(&#39;../../../data/diabetes.csv&#39;) df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id chol stab.glu hdl ratio glyhb location age gender height weight frame bp.1s bp.1d bp.2s bp.2d waist hip time.ppn 0 1000 203.0 82 56.0 3.6 4.31 Buckingham 46 female 62.0 121.0 medium 118.0 59.0 NaN NaN 29.0 38.0 720.0 1 1001 165.0 97 24.0 6.9 4.44 Buckingham 29 female 64.0 218.0 large 112.0 68.0 NaN NaN 46.0 48.0 360.0 2 1002 228.0 92 37.0 6.2 4.64 Buckingham 58 female 61.0 256.0 large 190.0 92.0 185.0 92.0 49.0 57.0 180.0 3 1003 78.0 93 12.0 6.5 4.63 Buckingham 67 male 67.0 119.0 large 110.0 50.0 NaN NaN 33.0 38.0 480.0 4 1005 249.0 90 28.0 8.9 7.72 Buckingham 64 male 68.0 183.0 medium 138.0 80.0 NaN NaN 44.0 41.0 300.0 40.1.2 CSV Output df.to_csv(&#39;diabetes_copy.csv&#39;,index=False) # Index False is important 40.2 Excel Pandas can read and write excel files, keep in mind, this only imports data. Not formulas or images, having images or macros may cause this read_excel method to crash. pd.read_excel(&#39;../../../data/diabetes.xls&#39;) 40.2.1 Excel Output df.to_excel(&#39;diabetes_copy.xlsx&#39;,sheet_name=&#39;new_name&#39;) 40.2.2 HTML Input Pandas read_html function will read tables off of a webpage and return a list of DataFrame objects: df = pd.read_html(&#39;http://www.fdic.gov/bank/individual/failed/banklist.html&#39;) df[0] # since read_html creates list of dataframes "],["operations-1.html", "Section 41 Operations 41.1 Unique 41.2 Select Data 41.3 Applying Functions 41.4 Columns 41.5 Sort and Order DataFrames 41.6 Null Values", " Section 41 Operations There are many operations you can use with pandas We’ll cover the most common ones bank = df[0] bank.info() bank.tail(10) 41.1 Unique bank[&#39;ST&#39;].unique() # unique obs bank[&#39;ST&#39;].nunique() # Number of unique obs bank[&#39;ST&#39;].value_counts() # counts each of the unique obs 41.2 Select Data bank.head() bank[(bank[&#39;CERT&#39;] &lt; 2000) &amp; (bank[&#39;ST&#39;] == &#39;IL&#39;)] 41.3 Applying Functions def divide_1000(x): return x / 1000 bank[&#39;CERT&#39;].apply(lambda x: x / 1000) bank[&#39;CERT&#39;].apply(np.log) bank[&#39;CERT&#39;].sum() 41.4 Columns bank.columns # list columns del bank[&#39;Updated Date&#39;] # permanently remove column bank.columns bank.index 41.5 Sort and Order DataFrames bank.head() bank.sort_values(by=&#39;ST&#39;) # inplace = False by default 41.6 Null Values bank.isnull() bank.dropna() # Seen this before "],["sql.html", "Section 42 SQL", " Section 42 SQL The pandas.io.sql module provides a collection of query wrappers to both facilitate data retrieval and to reduce dependency on DB-specific API. Database abstraction is provided by SQLAlchemy if installed. In addition you will need a driver library for your database. Examples of such drivers are psycopg2 for PostgreSQL or pymysql for MySQL. For SQLite this is included in Python’s standard library by default. You can find an overview of supported drivers for each SQL dialect in the SQLAlchemy docs. If SQLAlchemy is not installed, a fallback is only provided for sqlite (and for mysql for backwards compatibility, but this is deprecated and will be removed in a future version). This mode requires a Python database adapter which respect the Python DB-API. See also some cookbook examples for some advanced strategies. The key functions are: read_sql_table(table_name, con[, schema, ...]) Read SQL database table into a DataFrame. read_sql_query(sql, con[, index_col, ...]) Read SQL query into a DataFrame. read_sql(sql, con[, index_col, ...]) Read SQL query or database table into a DataFrame. DataFrame.to_sql(name, con[, flavor, ...]) Write records stored in a DataFrame to a SQL database. from sqlalchemy import create_engine engine = create_engine(&#39;sqlite:///:memory:&#39;) df.to_sql(&#39;temp&#39;, engine) sql_df = pd.read_sql(&#39;data&#39;,con=engine) sql_df "],["pandas-basics.html", "Section 43 Pandas Basics", " Section 43 Pandas Basics NumPy Wrangling Pandas Introduction DataFrames Importing Data Data Selection Missing Data: Part 1 Missing Data: Part 2 Tidy Data Operations Merging Data: Part 1 Merging Data: Part 2 Groupby: Part 1 Groupby: Part 2 Reshaping Time Series Categorical Data Ploting Data with Pandas Data Input/Output: Part 1 Data Input/Output: Part 2 "],["introduction-to-the-shell.html", "Section 44 Introduction to the Shell", " Section 44 Introduction to the Shell In this section you’ll learn: A practical definition of data science What the command line is exactly and how you can use it Why the command line is a wonderful environment for doing data science "],["why-data-science-at-the-command-line.html", "Section 45 Why Data Science at the Command Line? 45.1 Obtaining Data", " Section 45 Why Data Science at the Command Line? The command line has many great advantages that can really make you a more efficient and productive data scientist. Roughly grouping the advantages, the command line is: agile, augmenting, scalable, extensible, and ubiquitous. Define data science according to the following five steps: 1. obtaining data, 1. scrubbing data, 1. exploring data, 1. modeling data, 1. interpreting data. Together, these steps form the OSEMN model (which is pronounced as awesome). 45.1 Obtaining Data Without any data, there is little data science you can do. So the first step is the obtain data. Unless you are fortunate enough to already possess data, you may need to do one or more of the following: Download data from another location (e.g., a webpage or server) Query data from a database or API (e.g., MySQL or Twitter) Extract data from another file (e.g., an HTML file or spreadsheet) Generate data yourself (e.g., reading sensors or taking surveys) "],["scrubbing-data.html", "Section 46 Scrubbing Data 46.1 Exploring Data 46.2 Modeling Data 46.3 Interpreting Data", " Section 46 Scrubbing Data It is not uncommon that the obtained data has missing values, inconsistencies, errors, weird characters, or uninteresting columns. In that case, you have to scrub, or clean, the data before you can do anything interesting with it. Common scrubbing operations include: Filtering lines Extracting certain columns Replacing values Extracting words Handling missing values Converting data from one format to another 46.1 Exploring Data Once you have scrubbed your data, you are ready to explore it. This is where it gets interesting because here you will get really into your data. You’ll see how the command line can be used to: Look at your data Derive statistics from your data Create interesting visualizations 46.2 Modeling Data If you want to explain the data or predict what will happen, you probably want to create a statistical model of your data. Techniques to create a model include clustering, classification, regression, and dimensionality reduction. The command line is not suitable for implementing a new model from scratch. It is, however, very useful to be able to build a model from the command line. 46.3 Interpreting Data The final and perhaps most important step in the OSEMN model is interpreting data. This step involves: Drawing conclusions from your data Evaluating what your results mean Communicating your result To be honest, the computer is of little use here, and the command line does not really come into play at this stage. Once you have reached this step, it is up to you. "],["getting-started-with-the-command-line.html", "Section 47 Getting Started with the Command Line 47.1 Executing a Command-line Tool 47.2 Combining Command-line Tools 47.3 Redirecting Input and Output 47.4 Working With Files 47.5 Help", " Section 47 Getting Started with the Command Line 47.1 Executing a Command-line Tool Now that you have a basic understanding of the environment, it is high time that you try out some commands. Type the following in your terminal (without the dollar sign) and press &lt;Enter&gt;: pwd Sometimes we are using commands and pipelines that are too long to fit on the page. In that case you’ll see something like the following: echo &#39;Hello&#39;\\ &#39; world&#39; | wc The greater-than sign is the continuation prompt, which indicates that this line is a continuation of the previous one. A long command can be broken up with either a backslash or a pipe symbol. Be sure to first match any quotation marks. The following command is exactly the same: echo &#39;Hello world&#39; | wc This is as simple as it gets. You just executed a command that contained a single command-line tool. The command-line tool pwd (pwd?) prints the name of the directory where you currently are. By default, when you login, this is your home directory. You can view the contents of this directory with ls (ls?): ls The command-line tool cd, which is a Bash builtin, allows you to navigate to a different directory: cd book/ch02/ cd data pwd /home/vagrant/book/ch02/data cd .. pwd /home/vagrant/book/ch02/ The part after cd specifies to which directory you want to navigate to. Values that come after the command are called command-line arguments or options. The two dots refer to the parent directory. Let’s try a different command: head -n 3 data/movies.txt Matrix Star Wars Home Alone Here we pass three command-line arguments to head (head?). The first one is an option. The second one is a value that belongs to the option. The third one is a filename. This particular command outputs the first three lines of file book/ch02/data/movies.txt. 47.1.1 Five Types of Command-line Tools We employ the term command-line tool a lot, but so far, we have not yet explained what we actually mean by it. We use it as an umbrella term for anything that can be executed from the command line. Under the hood, each command-line tool is one of the following five types: A binary executable. A shell builtin. An interpreted script. A shell function. An alias. It’s good to know the difference between the types. The other three types (interpreted script, shell function, and alias) allow us to further build up our data science toolbox and become more efficient and more productive data scientists. 47.1.2 Binary Executable Binary executables are programs in the classical sense. A binary executable is created by compiling source code to machine code. This means that when you open the file in a text editor you cannot read it. 47.1.3 Shell Builtin Shell builtins are command-line tools provided by the shell, which is Bash in our case. Examples include cd and help. These cannot be changed. Shell builtins may differ between shells. Like binary executables, they cannot be easily inspected or changed. 47.1.4 Interpreted Script An interpreted script is a text file that is executed by a binary executable. Examples include: Python, R, and Bash scripts. One great advantage of an interpreted script is that you can read and change it. Example ?? shows a script named ~/book/ch02/fac.py. This script is interpreted by Python not because of the file extension .py, but because the first line of the script defines the binary that should execute it. \\BeginKnitrBlock{example}\\iffalse{-91-80-121-116-104-111-110-32-115-99-114-105-112-116-32-116-104-97-116-32-99-111-109-112-117-116-101-115-32-116-104-101-32-102-97-99-116-111-114-105-97-108-32-111-102-32-97-110-32-105-110-116-101-103-101-114-93-}\\fi{}&lt;div class=&quot;example&quot;&gt;&lt;span class=&quot;example&quot; id=&quot;exm:script-fac&quot;&gt;&lt;strong&gt;(\\#exm:script-fac) \\iffalse (Python script that computes the factorial of an integer) \\fi{} &lt;/strong&gt;&lt;/span&gt;&lt;/div&gt;\\EndKnitrBlock{example} ```python ##!/usr/bin/env python def factorial(x): result = 1 for i in xrange(2, x + 1): result *= i return result if __name__ == &quot;__main__&quot;: import sys x = int(sys.argv[1]) print factorial(x) ``` This script computes the factorial of the integer that we pass as a parameter. It can be invoked from the command line as follows: ```bash book/ch02/fac.py 5 120 ``` 47.1.5 Shell Function A shell function is a function that is, in our case executed by Bash. They provide similar functionality to a Bash script, but they are usually (but not necessarily) smaller than scripts. They also tend to be more personal. The following command defines a function called fac, which, just like the interpreted Python script above, computes the factorial of the integer we pass as a parameter. It does by generating a list of numbers using seq, putting those numbers on one line with * as the delimiter using paste (paste?), and passing this equation into bc (bc?), which is evaluates it and outputs the result. ```bash fac() { (echo 1; seq 1) | paste -s -d\\* - | bc; } fac 5 120 ``` The file .bashrc, which is a configuration file for Bash, is a good place to define your shell functions, so that they are always available. 47.1.6 Alias Aliases are like macros. If you often find yourself executing a certain command with the same parameters (or a part of it), you can define an alias for this. Aliases are also very useful when you continue to misspell a certain command (see https://github.com/chrishwiggins/mise/blob/master/sh/aliases-public.sh for a long list of useful aliases). The following command defines such an alias: ```bash alias l=&#39;ls -1 --group-directories-first&#39; alias moer=more ``` Now, if you type the following on the command line, the shell will replace each alias it finds with its value: ```bash cd ~ l book ``` 47.2 Combining Command-line Tools Because most command-line tools adhere to the UNIX philosophy, they are designed to do only thing, and do it really well. For example, the command-line tool grep (grep?) can filter lines, wc (wc?) can count lines, and sort (sort?) can sort lines. The power of the command line comes from its ability to combine these small, yet powerful command-line tools. The most important way of combining command-line tools is through a so-called pipe. The output from the first tool is passed to the second tool. There are virtually no limits to this. Consider, for example, the command-line tool seq, which generates a sequence of numbers. Let us generate a sequence of five numbers. seq 5 The output of a command-line tool is by default passed on to the terminal, which displays it on our screen. We can pipe the ouput of seq to a second tool, called grep, which can be used to filter lines. Imagine that we only want to see numbers that contain a “3.” We can combine seq and grep as follows: seq 30 | grep 3 And if we wanted to know how many numbers between 1 and 100 that contain a three, we can use wc, that is very good at counting things: seq 100 | grep 3 | wc -l The option -l specifies that wc should only output the number of lines that are pass into it. By default it also returns the number of characters and words. You may start to see that combining command-line tools is a very powerful concept. In the rest of the book you will be introduced to many more tools and the functionality they offer when combining them. 47.3 Redirecting Input and Output We mentioned that, by default, the output of the last command-line tool in the pipeline is outputted to the terminal. You can also save this output to a file. This is called output redirection, and works as follows: seq 10 &gt; data/ten-numbers Here, we save the output of the seq tool to a file named ten-numbers in the directory ~/book/ch02/data. If this file does not exist yet, it is created. If this file already did exist, its contents would have been overwritten. You can also append the output to a file with &gt;&gt;, meaning the output is put after the original contents: echo -n &quot;Hello&quot; &gt; hello-world echo &quot; World&quot; &gt;&gt; hello-world The tool echo just outputs the value you specify. The -n option specifies that echo should not output a trailing newline. Saving the output to a file is useful if you need to store intermediate results, for example for continuing with your analysis at a later stage. To use the contents of the file hello-world again, we can use cat (cat?), which reads a file prints it. cat hello-world | wc -w (Note that the -w option indicates wc to only count words.) The same result can be achieved with the following notation: &lt; hello-world wc -w This way, you are directly passing the file to the standard input of wc without running an additional process. If the command-line tool also allows files to be specified as command-line arguments, which many do, you can also do the following for wc: wc -w hello-world 47.4 Working With Files As data scientists, we work with a lot of data. This data is often stored in files. It is important to know how to work with files (and the directories they live in) on the command line. Every action that you can do using a graphical user interface, you can do with command-line tools (and much more). In this section we introduce the most important ones to create, move, copy, rename, and delete files and directories. You have already seen how we can create new files by redirecting the output with either &gt; or &gt;&gt;. In case you need to move a file to a different directory you can use mv (mv?): mv hello.txt ~/book/ch02/data/ You can also rename files with mv: cd data mv hello.txt bye.txt You can also rename or move entire directories. In case you no longer need a file, you delete (or remove) it with rm (rm?): rm bye.txt In case you want to remove an entire directory with all its contents, specify the -r option, which stands for recursive: rm -r book/ch02/data/old In case you want to copy a file, use cp (cp?). This is useful for creating backups: cp server.log server.log.bak You can create directories using mkdir (mkdir?): cd data mkdir logs Using the command-line tools to manage your files can be scary at first, because you have no graphical overview of the file system to provide immediate feedback. All of the above command-line tools accept the -v option, which stands for verbose, so that they output what’s going on. All but mkdir accept the -i option, which stands for interactive, and causes the tools to ask you for confirmation. 47.5 Help As you are finding your way around the command-line, it may happen that you need help. Even the most-seasoned Linux users need help at some point. It is impossible to remember all the different command-line tools and their possible arguments. Fortunately, the command line offers severals ways to get help. The most important command to get help is perhaps man (man?), which is short for manual. It contains information for most command-line tools. Imagine that we forgot the different options to the tool cat. You can access its man page using: man cat | head -n 20 Sometimes you’ll see us use head, fold, or cut at the end of a command. This is only to ensure that the output of the command fits on the page; you don’t have to type these. For example, head -n 5 only prints the first five lines, fold wraps long lines to 80 characters, and cut -c1-80 trims lines that are long than 80 characters. Not every command-line tool has a man page. For shell builtins, such as cd, you need to use the help command-line tool: help cd | head -n 20 This help also covers other topics of Bash, in case you are interested (try help without command-line arguments for a list of topics). Remember that you can use type to figure out the kind of a specific command-line tool. Newer tools that can be used from the command-line, often lack a man page as well. In that case, your best bet is to invoke the tool with the --help option (and sometimes the -h option). For example: jq --help Specifying the --help option also works for the GNU command-line tools such as cat. However, the corresponding man page often provides more information. If, after trying these three approaches, you are still stuck, then it is perfectly acceptable to consult the Internet. In the appendix, there’s a list of all command-line tools used in this book. Besides how each command-line tool can be installed, it also shows how you can get help. "],["obtaining-data-2.html", "Section 48 Obtaining Data 48.1 Calling a Web API", " Section 48 Obtaining Data The Internet provides without a doubt the largest resource for data. This data is available in various forms, using various protocols. The command-line tool cURL (curl?) can be considered the command line’s Swiss Army knife when it comes to downloading data from the Internet. When you access a URL, which stands for uniform resource locator, through your browser, the data that is being downloaded can be interpreted. For example, an HTML file is rendered as a website, an MP3 file may be automatically played, and a PDF file may be automatically downloaded or opened by a viewer. However, when cURL is used to access a URL, the data is downloaded as is printed to standard output. Other command-line tools may then be used to process this data further. The easiest invocation of cURL is to simply specify a URL as a command-line argument. For example, to download the book Adventures of Huckleberry Finn by Mark Twain from Project Gutenberg, we can run the following command: curl -s http://www.gutenberg.org/files/76/76-0.txt | head -n 10 By default, cURL outputs a progress meter that shows how the download rate and the expected time of completion. If you are piping the output directly to another command-line tool, such as head, be sure to specify the -s command-line argument, which stands for silent, so that the progress meter is disabled. Compare, for example, the output with the following command: curl http://www.gutenberg.org/files/76/76-0.txt | head -n 10 Note that the output of the second command, where we do not disable the progress meter, contains the unwanted text and even an error message. If you save the data to a file, then you do not need to necessarily specify the -s option: curl http://www.gutenberg.org/files/76/76-0.txt &gt; data/finn.txt You can also save the data by explicitly specifying the output file with the -o option: curl -s http://www.gutenberg.org/files/76/76-0.txt -o data/finn.txt When downloading data from the Internet, the URL will most likely use the protocols HTTP or HTTPS. To download from an FTP server, which stands for File Transfer Protocol, you use cURL in exactly the same way. When the URL is password protected, you can specify a username and a password as follows: curl -u username:password ftp://host/file If the specified URL is a directory, curl will list the contents of that directory. When you access a shortened URL, such as the ones that start with http://bit.ly/ or http://t.co/, your browser automatically redirects you to the correct location. With curl, however, you need to specify the -L or --location option in order to be redirected: curl -L j.mp/locatbbar If you do not specify the -L or --location option, you may get something like: curl j.mp/locatbbar By specifying the -I or --head option, curl fetches only the HTTP header of the response: curl -I j.mp/locatbbar The first line indicates the HTTP status code, which is 301 (moved permanently) in this case. You can also see the location this URL redirects to: http://en.wikipedia.org/wiki/List_of_countries_and_territories_by_border/area_ratio. Inspecting the header and getting the status code is a useful debugging tool in case curl does not give you the expected result. Other common HTTP status codes include 404 (not found) and 403 (forbidden). This page lists all HTTP status codes: http://en.wikipedia.org/wiki/List_of_HTTP_status_codes. To conclude this section, cURL is a straight-forward command-line tool for downloading data from the Internet. Its three most common command-line arguments are -s to suppress the progress meter, -u to specify a username and password, and -L to automatically follow redirects. See its man page for more information. 48.1 Calling a Web API In the previous section we explained how to download individual files from the Internet. Another way data can come from the Internet is through a web API, which stands for Application Programming Interface. The number of APIs that are being offered by organizations is growing at increasing rate, which means a lot of interesting data for us data scientists. Web APIs are not meant to be presented in nice layout, such as websites. Instead, most web APIs return data in a structured format, such as JSON or XML. Having data in a structured form has the advantage that the data can be easily processed by other tools, such as jq. Which can be downloaded here. For example, the API from https://randomuser.me returns data in the following JSON structure. curl -s https://randomuser.me/api/1.2/ | jq . The data is piped to a command-line tool jq in order to display it in a nice way. Some web APIs return data in a streaming manner. This means that once you connect to it, the data will continue to pour in forever. A well-known example is the Twitter “firehose,” which constantly streams all the tweets being sent around the world. Luckily, most command-line tools that we use also operate in a streaming matter, so that we also use this kind of data. Some APIs require you to log in using the OAuth protocol. There is a handy command-line tool called curlicue (curlicue?) that assists in performing the so-called “OAuth dance.” Once this has been set up, it curlicue will call curl with the correct headers. First, you set things up once for a particular API with curlicue-setup, and then you can call that API using curlicue. For example, to use curlicue with the Twitter API you would run: curlicue-setup \\ &gt; &#39;https://api.twitter.com/oauth/request_token&#39; \\ &gt; &#39;https://api.twitter.com/oauth/authorize?oauth_token=oauth_token&#39; \\ &gt; &#39;https://api.twitter.com/oauth/access_token&#39; \\ &gt; credentials curlicue -f credentials \\ &gt; &#39;https://api.twitter.com/1/statuses/home_timeline.xml&#39; For more popular APIs, there are specialized command-line tools available. These are wrappers that provide a convenient way to connect to the API. "],["reusable-command-line-tools.html", "Section 49 Reusable Command-line Tools", " Section 49 Reusable Command-line Tools "],["why-learn-sql.html", "Section 50 Why Learn SQL? 50.1 What Is SQL and Why Is It Marketable? 50.2 Databases 50.3 Exploring Relational Databases 50.4 SQLite", " Section 50 Why Learn SQL? 50.1 What Is SQL and Why Is It Marketable? It is an obvious statement that the business landscape is shifting rapidly. Companies are investing vast amounts of capital to gather and warehouse data. SQL, which stands for Structured Query Language, comes in. It provides a means to access and manipulate this data in meaningful ways and provide business insights not possible before. Businesses are gathering data at exponential rates, and there is an equally growing need for people who know how to analyze and manage it. In recent years, data has suddenly become ubiquitous — yet few people know how to access it meaningfully, which has put SQL talent in high demand. 50.2 Databases 50.2.1 What Is a Database? A database is anything that collects and organizes data. A spreadsheet holding customer bookings is a database A plain-text file containing flight schedule data Professionally we are referring to a relational database management system (RDBMS). RDBMS is simply a type of database that holds one or more tables that may have relationships to each other. 50.3 Exploring Relational Databases A table should be a familiar concept. It has columns and rows to store data, much like a spreadsheet. These tables can have relationships to each other, such as an ORDER table that refers to a CUSTOMER table for customer information. Suppose we have an ORDER table with a field called CUSTOMER_ID ORDER_ID ORDER_DATE SHIP_DATE CUSTOMER_ID PRODUCT_ID ORDER_QTY SHIPPED 3 2018-04-20 2018-04-23 3 3 300 false 4 2018-04-18 2018-04-22 5 5 375 false 1 2018-04-15 2018-04-18 1 1 450 false 5 2018-04-17 2018-04-20 3 3 500 false 2 2018-04-18 2018-04-21 3 3 600 false We can reasonably expect another table called CUSTOMER, which holds customer information CUSTOMER_ID NAME REGION STREET_ADDRESS CITY STATE ZIP 1 LITE Industrial Southwest 123 Ravine Way Irving TX 75014 2 REX Tooling Inc Southwest 456 Collie Blvd Dallas TX 75201 3 Re-Barre Construction Southwest 789 Windy Dr Irving TX 75032 4 Prairie Construction Southwest 890 Long Rd Moore OK 62104 5 Marsh Lane Metals Southeast 345 Marsh Ln Avondale LA 79782 When we go through the ORDER table, we can use the CUSTOMER_ID to look up the customer information in the CUSTOMER table. This is the fundamental idea behind a “relational database,” where tables may have fields that point to information in other tables. 50.3.1 Why Separate Tables? The motivation is normalization, which is separating the different types of data into their own tables rather than putting them in one table. If we had all information in a single table, it would be redundant, bloated, and very difficult to maintain. 50.4 SQLite 50.4.1 What is SQLite? SQLite is the most widely distributed database in the world. It is put on iPhones, iPads, Android devices, Windows phones, thermostats, car consoles, satellites, and many other modern devices that need to store and retrieve data easily. It excels where simplicity and low overhead is needed. It is also great for prototyping business databases. But every technology has a trade-off. Because it has no server managing access to it, it fails in multiuser environments where multiple people can simultaneously edit the SQLite file. Still, for our training purposes, SQLite is perfect. "],["using-sql-in-r-markdown.html", "Section 51 Using SQL in R Markdown", " Section 51 Using SQL in R Markdown First let’s use the command line and git to grab the necessary data. We will be following Nield (2016) the data for this section can be found here. git clone https://github.com/thomasnield/oreilly_getting_started_with_sql.git library(DBI) db = dbConnect(RSQLite::SQLite(), dbname =&quot;oreilly_getting_started_with_sql/rexon_metals.db&quot;) knitr::opts_chunk$set(connection = &quot;db&quot;) SELECT * FROM CUSTOMER; Table 51.1: 5 records CUSTOMER_ID NAME REGION STREET_ADDRESS CITY STATE ZIP 1 LITE Industrial Southwest 729 Ravine Way Irving TX 75014 2 Rex Tooling Inc Southwest 6129 Collie Blvd Dallas TX 75201 3 Re-Barre Construction Southwest 9043 Windy Dr Irving TX 75032 4 Prairie Construction Southwest 264 Long Rd Moore OK 62104 5 Marsh Lane Metal Works Southeast 9143 Marsh Ln Avondale LA 79782 References "],["using-sql-in-jupyter-notebooks.html", "Section 52 Using SQL in Jupyter Notebooks", " Section 52 Using SQL in Jupyter Notebooks First let’s use the command line and git to grab the necessary data. We will be following Nield (2016) the data for this section can be found here. git clone https://github.com/thomasnield/oreilly_getting_started_with_sql.git !pip install ipython-sql !git clone https://github.com/thomasnield/oreilly_getting_started_with_sql.git %load_ext sql %sql sqlite:///oreilly_getting_started_with_sql/rexon_metals.db Using %%sql allows you to use SQL in Jupyter Notebook cells while using python in other cells. %%sql SELECT * FROM CUSTOMER; References "],["select-statements.html", "Section 53 SELECT Statements 53.1 Retrieving Data with SQL 53.2 Expression in SELECT Statements 53.3 Text Concatenation 53.4 Summary", " Section 53 SELECT Statements When working with databases and SQL, the most common task is to request data from one or more tables and display it. The SELECT statement accomplishes this. 53.1 Retrieving Data with SQL Let’s write our first SQL statement. The most common SQL operation is a SELECT statement, which pulls data from a table and then displays the results. Write the following statement: SELECT * FROM CUSTOMER; Let’s break down exactly what happened. A SELECT statement allows you to choose which columns to pull from a table. So the first part of the SQL shown here should be read as “Select all columns,” where * is a placeholder to specify all columns. And you are getting these columns from the CUSTOMER table. You do not have to pull all columns in a SELECT statement. You can also pick and choose only the columns you are interested in. The following query will only pull the CUSTOMER_ID and NAME columns: SELECT CUSTOMER_ID, NAME FROM CUSTOMER; 53.2 Expression in SELECT Statements The SELECT statement can do far more than simply select columns. You can also do calculations on one or more columns and include them in your query result. Let’s work with another table called PRODUCT. SELECT * FROM PRODUCT; Suppose we wanted to generate a calculated column called TAXED_PRICE that is 7% higher than PRICE. We could use a SELECT query to dynamically calculate this for us SELECT PRODUCT_ID, DESCRIPTION, PRICE, PRICE * 1.07 AS TAXED_PRICE FROM PRODUCT; Notice how the TAXED_PRICE column was dynamically calculated in the SELECT query. This column is not stored in the table, but rather calculated and displayed to us every time we run this query. Let’s take a look at our TAXED_PRICE column and break down how it was created. We first see the PRICE is multiplied by 1.07 to calculate the taxed amount. We generate this TAXED_PRICE value for every record. Notice too that we gave this calculated value a name using an AS statement (this is known as an alias): We can use aliases to give names to expressions. We can also use aliases to apply a new name to an existing column within the query. For example, we can alias the PRICE column to UNTAXED_PRICE. This does not actually change the name of the column in the table, but it gives it a new name within the scope of our SELECT statement: SELECT PRODUCT_ID, DESCRIPTION, PRICE AS UNTAXED_PRICE, PRICE * 1.07 AS TAXED_PRICE FROM PRODUCT To round the TAXED_PRICE to two decimal places, we can pass the multiplication expression PRICE * 1.07 as the first argument, and a 2 as the second: SELECT PRODUCT_ID, DESCRIPTION, PRICE AS UNTAXED_PRICE, round(PRICE * 1.07,2) AS TAXED_PRICE FROM PRODUCT Here are the mathematical operators you can use in SQL: Operator Description Example + Adds two numbers STOCK + NEW_SHIPMENT - Subtracts two numbers STOCK - DEFECTS * Multiplies two numbers PRICE * 1.07 / Divides two numbers STOCK / PALLET_SIZE % Divides two, numbers but return the remainder STOCK % PALLET_SIZE 53.3 Text Concatenation The concatenate operator is specified by a double pipe (||), and you put the data values to concatenate on both sides of it. SELECT NAME, CITY || &#39;, &#39; || STATE AS LOCATION FROM CUSTOMER; SELECT NAME, STREET_ADDRESS || &#39; &#39; || CITY || &#39;, &#39; || STATE || &#39; &#39; || ZIP AS SHIP_ADDRESS FROM CUSTOMER; 53.4 Summary In this section, we covered how to use the SELECT statement, the most common SQL operation. It retrieves and transforms data from a table without affecting the table itself. We also learned how to select columns and write expressions. Within expressions, we can use operators and functions to do tasks such as rounding, math, and concatenation. "],["where-statements.html", "Section 54 WHERE Statements 54.1 Filtering Records 54.2 AND, OR, and IN Statements 54.3 Using Where on Text 54.4 WHERE on Booleans 54.5 Handling NULL 54.6 Grouping Conditions", " Section 54 WHERE Statements Over the next few weeks, we will be adding more functionalities to the SELECT statement. A very common task when working with data is filtering for records based on criteria, which can be done with a WHERE statement. We will be learning more functions and using them in the WHERE clause, but we can also use them in SELECT statements, as discussed in the previous chapter. For the most part, expressions and functions can be used in any part of a SQL statement. 54.1 Filtering Records SELECT * FROM station_data; SELECT * FROM station_data WHERE year = 2010; Conversely, you can use != or &lt;&gt; to get everything but 2010. SELECT * FROM station_data WHERE year != 2010; We can also qualify inclusive ranges using a BETWEEN statement: SELECT * FROM station_data WHERE year BETWEEN 2005 AND 2010; 54.2 AND, OR, and IN Statements A BETWEEN can alternatively be expressed using greater than or equal to and less than or equal to expressions and an AND statement. SELECT * FROM station_data WHERE year &gt;= 2005 AND year &lt;= 2010; If we wanted everything between 2005 and 2010 exclusively — i.e., not including those two years — we would just get rid of the = characters. Only 2006, 2007, 2008, and 2009 would then qualify: SELECT * FROM station_data WHERE year &gt; 2005 AND year &lt; 2010; We also have the option of using OR. In an OR statement, at least one of the criteria must be true for the record to qualify. If we wanted only records with months 3, 6, 9, or 12, we could use an OR to accomplish that: SELECT * FROM station_data WHERE MONTH = 3 OR MONTH = 6 OR MONTH = 9 OR MONTH = 12; A more efficient way is to use the IN statement. SELECT * FROM station_data WHERE MONTH IN (3,6,9,12); If we want everything except 3,6,9,12, we use the NOT IN: SELECT * FROM station_data WHERE MONTH NOT IN (3,6,9,12); We could also leverage some logic: SELECT * FROM station_data WHERE MONTH % 3 = 0; 54.3 Using Where on Text The rules for qualifying text fields follow the same structure, although there are subtle differences. You can use =, AND, OR, and IN statements with text. However, when using text, you must wrap literals (or text values you specify) in single quotes. For example, if you wanted to filter for a specific report_code, you could run this query: SELECT * FROM station_data WHERE report_code = &#39;513A63&#39;; This single-quote rule applies to all text operations, including this IN operation: SELECT * FROM station_data WHERE report_code IN (&#39;513A63&#39;,&#39;1F8A7B&#39;,&#39;EF616A&#39;); The length function will count the number of characters for a given value: SELECT * FROM station_data WHERE length(report_code) != 6; Another common operation is to use wildcards with a LIKE expression, where % is any number of characters and _ is any single character. Any other character is interpreted literally. So, if you wanted to find all report codes that start with the letter “A,” you would run this statement to find “A” followed by any characters: SELECT * FROM station_data WHERE report_code LIKE &#39;A%&#39;; If you wanted to find all report codes that have a “B” as the first character and a “C” as the third character, you would specify an underscore (_) for the second position, and follow with any number of characters after the “C”: SELECT * FROM station_data WHERE report_code LIKE &#39;B_C%&#39; 54.4 WHERE on Booleans SELECT * FROM station_data WHERE tornado = 1 AND hail = 1; SQLite only supports using 1 for true and 0 for false. SELECT * FRPM station_data WHERE tornado =0 AND hail = 1; 54.5 Handling NULL Some columns such as station_pressure and snow_depth, have null values. A null is a value that has no value. It is the complete absence of any content. SELECT * FROM station_data WHERE snow_depth IS NULL; Nulls can be hard to handle when composing WHERE statements. If you wanted to query all records where `precipitation is less than 0.5, you could write: SELECT * FROM station_data WHERE precipitation &lt;= 0.5; But you must consider the null values. If you wanted the nulls to be included you need to use an OR statement. SELECT * FROM station_data WHERE precipitation IS NULL OR precipitation &lt;= 0.5; A more elegant way is to use coalesce(). If you wanted nulls to be treated as 0 within our condition, we could coalesce() the precipitation field to convert null to 0: SELECT * FROM station_data WHERE coalesce(precipitation, 0) &lt;= 0.5; 54.6 Grouping Conditions When chaining AND and OR together, it is good to group them. Make sure that you organize each set of conditions between each OR in a way that groups related conditions. SELECT * FROM station_data WHERE rain = 1 AND temperature &lt;=32 OR snow_depth&gt;0; While this technically works, there is a degree of ambiguity. SELECT * FROM station_data WHERE (rain=1 AND temperature &lt;=32) OR snow_depth &gt;0; Here we group the expression within parentheses so it is calculated as a single unit, and temperature is not mixed up with the OR operator and accidentally mangled with the snow_depth. "],["group-by-and-order-by.html", "Section 55 GROUP BY and ORDER BY 55.1 Grouping Records 55.2 Ordering Records 55.3 HAVING 55.4 Getting Distinct Records", " Section 55 GROUP BY and ORDER BY Aggregating is creating a \" total form a number of records. Sum, min, max, count, and average are common aggregate operations. In SQL we group these totals on a any column, allowing you to control the scope of these aggregations. 55.1 Grouping Records The simplest aggregation: count the number of records in a table. SELECT COUNT(*) as record_count FROM station_data SELECT COUNT(*) AS record_count FROM station_data WHERE tornado=1; You should see 3000 records. If we want to separate the count by year: SELECT year, COUNT(*) as record_count FROM station_data WHERE tornado=1 GROUP BY year; This data becomes more meaningful. We specify that we are grouping by year. This is what effectively allows us to count the number of records by year. SELECT year, month, COUNT(*) AS record_count FROM station_data WHERE tornado=1 GROUP BY year, month; 55.2 Ordering Records Notice that the month column is not in an order we’d expect. If we wanted to sort by year, and then month: SELECT year, month, COUNT(*) AS record_count FROM station_data WHERE tornado=1 GROUP BY year, month ORDER BY year, month; What if we want to more recent data at the top? By default, sorting is done with the ASC operator, which orders the data in ascending order. If you want to sort in descending order, apply the DESC operator to the ordering of year to make more recent records appear at the top: SELECT year, month, COUNT(*) AS records_count FROM station_data WHERE tornado=1 GROUP BY year, month ORDER BY year DESC, month; SUM is another common aggregate operation. To find the sum of snow depth by year since 2000: SELECT year, SUM(snow_depth) AS total_snow FROM station_data WHERE year &gt;= 2000 GROUP BY year; There is no limit on how many aggregations you use: SELECT year, SUM(snow_depth) as total_snow, SUM(precipitation) as total_precipitation, MAX(precipitation) as max_precipitation FROM station_data WHERE year &gt;=2000 GROUP BY year; We can also achieve very specific aggregations by leveraging WHERE SELECT year, SUM(precipitation) as tornado_precipitation FROM station_data WHERE tornado=1 GROUP BY year; 55.3 HAVING Suppose you want to filter out records based on an aggregateded value. While your first instinct may be to use the WHERE statement, this actually will not work. You cannot filter aggregated fields with WHERE. You must use the HAVING keyword to accomplish this. If we want to filter on the SUM() value, we would need to filter to take place after it is calculated. This is where HAVING can be applied: SELECT year, SUM(precipitation) as total_precipitation FROM station_data GROUP BY year HAVING total_precipitation &gt; 30 HAVING is the aggregated equivalent to WHERE. The WHERE keyword filters individual records, but HAVING filters aggregations. 55.4 Getting Distinct Records It is not uncommon to want to set of distinct results from a query. We know that there are some 28,000 reconfds in our station_data table. But suppose we want to get a distinct list of the station_number values? SELECT DISTINCT station_number FROM station_data You can also get distinct results from more than one column. If you need to distinct station_number and year, just include both those columns. SELECT DISTINCT station_number, year FROM station_data "],["case-statements.html", "Section 56 CASE Statements 56.1 Grouping CASE Statements 56.2 The “Zero/Null” CASE Trick", " Section 56 CASE Statements A CASE statement allows us to map one or more conditions to a corresponding value for each condition. You start a CASE statement with the word CASE and conclude it with an END. Between those keywords, you specify each condition with the a WHEN [condition] THEN [value]. After specifying the condition-value pairs, you can have a catch-all value to default to if none of the conditions where met, which is specified in the ELSE. SELECT report_code, year, month, day, wind_speed, CASE WHEN wind_speed &gt;= 40 THEN &#39;HIGH&#39; WHEN wind_speed &gt;= 30 AND wind_speed &lt; 40 THEN &#39;MODERATE&#39; ELSE &#39;LOW&#39; END as wind_severity FROM station_data LIMIT 10; 56.1 Grouping CASE Statements When you create CAST statements and group them, you can create some very powerful transformations. Converting values based on one or more conditions before aggregating them gives us even more possibilities to slice data in interesting ways. SELECT year, CASE WHEN wind_speed &gt;= 40 THEN &#39;HIGHT&#39; WHEN wind_speed &gt;= 40 THEN &#39;MODERATE&#39; ELSE &#39;LOW&#39; END as wind_seversity, COUNT(*) as record_count FROM station_data GROUP BY 1, 2 LIMIT 10; 56.2 The “Zero/Null” CASE Trick You can use tricks with the CASE statement. One simple but useful pattern is the “zero/null” CASE trick. This allows you to apply different filters for different aggregate values, all in a single SELECT query. SELECT year, month, round(SUM(CASE WHEN tornado = 1 THEN precipitation ELSE 0 END),2) as tornado_precipitation, round(SUM(CASE WHEN tornado = 0 THEN precipitation ELSE 0 END),2) as non_tornado_precipitation FROM station_data GROUP BY year, month LIMIT 10; The CASE statement can do an impressive amount of work, especially in complex aggregation task. By leverageing a condition to make a value 0 if the condition is not met, we effectively ignore that value and exclude it from the SUM (since adding 0 has no impact). You could so a similar calculation with MIN or MAX operations, and us a null instead of 0 to make sure values with certain coinditon are never considered: SELECT year, MAX(CASE WHEN tornado = 0 THEN precipitation ELSE NULL END) as max_non_tornado_precipitation, MAX(CASE WHEN tornado = 1 THEN precipitation ELSE NULL END) as max_tornado_precipitation FROM station_data WHERE year &gt;= 1990 GROUP BY year LIMIT 10; Just like the WHERE statement, you can use any Boolean expression in a CASE statement, in cluding function and AND, OR, and NOT statements. The following query will find the avarage temperatures by month when rain/hail was present versus not present after the year 2000: SELECT month, round(AVG(CASE WHEN rain OR hail THEN temperature ELSE null END),2) as avg_precipitation_temp, round(AVG(CASE WHEN NOT (rain OR hail) THEN temperature ELSE null END),2) as avg_non_precipitation_temp FROM station_data WHERE year &gt; 2000 GROUP BY month LIMIT 10; "],["join-statements.html", "Section 57 JOIN Statements 57.1 Stitching Tables Together 57.2 INNER JOIN 57.3 LEFT JOIN 57.4 Joining Mutliple Tables 57.5 Grouping JOINs", " Section 57 JOIN Statements 57.1 Stitching Tables Together Joining is the defining functionality of SQL an dsets it apart from other data technologies. Remember: Normalized databases have tables with fields that point to other tables. Sonciser the CUSTOMER_ORDER table, which has a CUSTOMER_ID field. SELECT * FROM customer LIMIT 10; This CUSTOMER_ID field gives us a key to loop up in the table CUSTOMER. Knowing this, it should be no surprise that the CUSTOMER table also has a CUSTOMER_ID field. We can retrieve customer if information from an order from this table. SELECT * FROM customer_order LIMIT 10; The other aspect to consider in a relationship is how many records in the child can be tited to a single record of the parent. Toake the CUSTOMER and CUSTOMER_ORDER tables and we see a one-to-many relationship, where a single customer record can line up with multiple orders. One-to-many is the most common type of relationship since it acconodates most business needs. 57.2 INNER JOIN Understanding table relationships, we can consider that it might be nice to stich two tables together, so we can see CUSTOMER and CUSTOMER_ORDER information side by side. We can avoid using many lookups by using the JOIN operations. The INNER JOIN allows us to merge two tables together. But if we are going to merge tables, we need to define a commonality between the two so records between the two line up. SELECT ORDER_ID, CUSTOMER.CUSTOMER_ID, ORDER_DATE, SHIP_DATE, NAME, STREET_ADDRESS, CITY, STATE, ZIP, PRODUCT_ID, ORDER_QTY FROM CUSTOMER INNER JOIN CUSTOMER_ORDER ON CUSTOMER.CUSTOMER_ID = CUSTOMER_ORDER.CUSTOMER_ID; Select the fields we want from the CUSTOMER and CUSTOMER_ORDER tables. The important part that temporarily merges two tables into one. 57.3 LEFT JOIN Two customers were excluded from the INNER JOIN on CUSTOMER_ID since they didn’t have any orders to join on. But suppose we wanted them included anyways. Modify the previous query to utilize the LEFT JOIN: SELECT ORDER_ID, CUSTOMER.CUSTOMER_ID, ORDER_DATE, SHIP_DATE, NAME, STREET_ADDRESS, CITY, STATE, ZIP, PRODUCT_ID, ORDER_QTY FROM CUSTOMER LEFT JOIN CUSTOMER_ORDER ON CUSTOMER.CUSTOMER_ID = CUSTOMER_ORDER.CUSTOMER_ID; The table specified on the “left” side of the LEFT JOIN operator will have all its records included, even if they do not have any child records in the “right” table. Notice we have two additional records for the customers that have no orders. It is common to use LEFT JOIN to find “orphaned” child records. SELECT CUSTOMER.CUSTOMER_ID, NAME as CUSTOMER_NAME FROM CUSTOMER LEFT JOIN CUSTOMER_ORDER ON CUSTOMER.CUSTOMER_ID = CUSTOMER_ORDER.CUSTOMER_ID WHERE ORDER_ID IS NULL; 57.4 Joining Mutliple Tables Relational databases can be complex in terms of relationships between tables. For example, we can supply not only CUSTOMER information to the CUSTOMER_ORDER table, but also PRODUCT information using PRODUCT_ID. We can use these relationships to execute a query that displays orders with customer information and product information simultaneously. All we do is define the two joins. SELECT ORDER_ID, CUSTOMER.CUSTOMER_ID, NAME as CUSTOMER_NAME, STREET_ADDRESS, CITY, STATE, ZIP, ORDER_DATE, PRODUCT.PRODUCT_ID, DESCRIPTION, ORDER_QTY, ORDER_QTY * PRICE as REVENUE FROM CUSTOMER INNER JOIN CUSTOMER_ORDER ON CUSTOMER.CUSTOMER_ID = CUSTOMER_ORDER.CUSTOMER_ID INNER JOIN PRODUCT ON CUSTOMER_ORDER.PRODUCT_ID = PRODUCT.PRODUCT_ID 57.5 Grouping JOINs We have orders with their revenue, thatnks to the join we built. But suppose we want to find the total_revenue by customer? We still need to use all three tables and merge them together with out current join setup, since we need the revenue we just calculated. But also we need to do a GROUP BY: SELECT CUSTOMER.CUSTOMER_ID, NAME as CUSTOMER_NAME, sum(ORDER_QTY * PRICE) as TOTAL_REVENUE FROM CUSTOMER_ORDER INNER JOIN CUSTOMER ON CUSTOMER.CUSTOMER_ID = CUSTOMER_ORDER.CUSTOMER_ID INNER JOIN PRODUCT ON CUSTOMER_ORDER.PRODUCT_ID = PRODUCT.PRODUCT_ID GROUP BY 1, 2 Since we want all customers we can use LEFT JOIN in place of INNER JOIN SELECT CUSTOMER.CUSTOMER_ID, NAME as CUSTOMER_NAME, coalesce(sum(ORDER_QTY * PRICE),0) as TOTAL_REVENUE FROM CUSTOMER_ORDER LEFT JOIN CUSTOMER ON CUSTOMER.CUSTOMER_ID = CUSTOMER_ORDER.CUSTOMER_ID LEFT JOIN PRODUCT ON CUSTOMER_ORDER.PRODUCT_ID = PRODUCT.PRODUCT_ID GROUP BY 1, 2 "],["sql-practice.html", "Section 58 SQL Practice 58.1 Question 1: 58.2 Question 2: 58.3 Question Prep 58.4 Question 3: 58.5 Question 4:", " Section 58 SQL Practice Get to know the three Tables in this database: ARTIST ALBUM TRACK Playlist PlaylistTrack %%sql SELECT name FROM track LIMIT(3); 58.1 Question 1: Create a table that contains the artists name, album title and tracks. Use the GROUP_CONCAT function to concat the Track names. Group by artist name and album title. SELECT ARTIST.NAME AS ARTIST_NAME, TITLE AS ALBUM_TITLE, GROUP_CONCAT(TRACK.Name,&#39;, &#39;) AS TRACKS FROM ARTIST INNER JOIN ALBUM ON ARTIST.ArtistId = ALBUM.ArtistId INNER JOIN TRACK ON TRACK.AlbumId = Album.AlbumId GROUP BY 1,2 LIMIT(5); 58.2 Question 2: Grunge Playlists: Join Playlist and PlaylistTrack, Track and Album, Album and Artist. Where the playlist is grunge: (HINT: Grunge = 16) SELECT Playlist.Name as PLAYLIST_NAME, Artist.Name as ARTIST_NAME, Track.Name as TRACK_NAME, Album.Title as ALBUM_TITLE FROM Playlist INNER JOIN PlaylistTrack ON Playlist.PlaylistId = PlaylistTrack.PlaylistId INNER JOIN Track ON PlaylistTrack.TrackId = Track.TrackId INNER JOIN Album ON Track.AlbumId = Album.AlbumId INNER JOIN Artist ON Album.ArtistId = Artist.ArtistId WHERE Playlist.PlaylistId = 16 58.3 Question Prep Get to know the following tables: Invoice Customer SELECT * FROM Invoice LIMIT(3); 58.4 Question 3: Find total sales by country, round to 2 decimal places. SELECT BillingCountry as COUNTRY, round(SUM(Total),2) as TOTAL_SALES FROM Invoice GROUP BY BillingCountry 58.5 Question 4: Calculate Total sales by customer, give their first and last names. Round to 2 decimals. SELECT Customer.CustomerId, FirstName, LastName, round(SUM(Total),2) as TOTAL_SALES FROM Invoice INNER JOIN Customer ON Invoice.CustomerId = Customer.CustomerId GROUP BY Customer.CustomerId, FirstName, LastName "],["sql-basics.html", "Section 59 SQL Basics 59.1 Lectures", " Section 59 SQL Basics This section is based on Getting Started with SQL by Thomas Nield. 59.1 Lectures Introduction to SQL Using SQL within Jupyter Select Statements Where Statements Group by and Order by Cases Joins !pip install pandas_profiling Requirement already satisfied: pandas_profiling in /usr/local/lib/python3.6/dist-packages (1.4.1) Requirement already satisfied: six&gt;=1.9 in /usr/local/lib/python3.6/dist-packages (from pandas_profiling) (1.12.0) Requirement already satisfied: jinja2&gt;=2.8 in /usr/local/lib/python3.6/dist-packages (from pandas_profiling) (2.10.1) Requirement already satisfied: matplotlib&gt;=1.4 in /usr/local/lib/python3.6/dist-packages (from pandas_profiling) (3.0.3) Requirement already satisfied: pandas&gt;=0.19 in /usr/local/lib/python3.6/dist-packages (from pandas_profiling) (0.24.2) Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2&gt;=2.8-&gt;pandas_profiling) (1.1.1) Requirement already satisfied: numpy&gt;=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=1.4-&gt;pandas_profiling) (1.16.5) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=1.4-&gt;pandas_profiling) (2.5.3) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=1.4-&gt;pandas_profiling) (2.4.2) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=1.4-&gt;pandas_profiling) (1.1.0) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=1.4-&gt;pandas_profiling) (0.10.0) Requirement already satisfied: pytz&gt;=2011k in /usr/local/lib/python3.6/dist-packages (from pandas&gt;=0.19-&gt;pandas_profiling) (2018.9) Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver&gt;=1.0.1-&gt;matplotlib&gt;=1.4-&gt;pandas_profiling) (41.2.0) import matplotlib.pyplot as plt import pandas as pd from sklearn import ( ensemble, preprocessing, tree, ) from sklearn.metrics import ( auc, confusion_matrix, roc_auc_score, roc_curve, ) from sklearn.model_selection import ( train_test_split, StratifiedKFold, ) from yellowbrick.classifier import ( ConfusionMatrix, ROCAUC ) from yellowbrick.model_selection import ( LearningCurve, ) url = ( &quot;http://biostat.mc.vanderbilt.edu/&quot; &quot;wiki/pub/Main/DataSets/titanic3.xls&quot; ) url &#39;http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.xls&#39; #!pip install xlrd df = pd.read_excel(url) orig_df = df df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1309 entries, 0 to 1308 Data columns (total 14 columns): pclass 1309 non-null int64 survived 1309 non-null int64 name 1309 non-null object sex 1309 non-null object age 1046 non-null float64 sibsp 1309 non-null int64 parch 1309 non-null int64 ticket 1309 non-null object fare 1308 non-null float64 cabin 295 non-null object embarked 1307 non-null object boat 486 non-null object body 121 non-null float64 home.dest 745 non-null object dtypes: float64(3), int64(4), object(7) memory usage: 143.2+ KB None #!pip install pandas_profiling import pandas_profiling pandas_profiling.ProfileReport(df) .variablerow { border: 1px solid #e1e1e8; border-top: hidden; padding-top: 2em; padding-bottom: 2em; padding-left: 1em; padding-right: 1em; } .headerrow { border: 1px solid #e1e1e8; background-color: #f5f5f5; padding: 2em; } .namecol { margin-top: -1em; overflow-x: auto; } .dl-horizontal dt { text-align: left; padding-right: 1em; white-space: normal; } .dl-horizontal dd { margin-left: 0; } .ignore { opacity: 0.4; } .container.pandas-profiling { max-width:975px; } .col-md-12 { padding-left: 2em; } .indent { margin-left: 1em; } .center-img { margin-left: auto !important; margin-right: auto !important; display: block; } /* Table example_values */ table.example_values { border: 0; } .example_values th { border: 0; padding: 0 ; color: #555; font-weight: 600; } .example_values tr, .example_values td{ border: 0; padding: 0; color: #555; } /* STATS */ table.stats { border: 0; } .stats th { border: 0; padding: 0 2em 0 0; color: #555; font-weight: 600; } .stats tr { border: 0; } .stats td{ color: #555; padding: 1px; border: 0; } /* Sample table */ table.sample { border: 0; margin-bottom: 2em; margin-left:1em; } .sample tr { border:0; } .sample td, .sample th{ padding: 0.5em; white-space: nowrap; border: none; } .sample thead { border-top: 0; border-bottom: 2px solid #ddd; } .sample td { width:100%; } /* There is no good solution available to make the divs equal height and then center ... */ .histogram { margin-top: 3em; } /* Freq table */ table.freq { margin-bottom: 2em; border: 0; } table.freq th, table.freq tr, table.freq td { border: 0; padding: 0; } .freq thead { font-weight: 600; white-space: nowrap; overflow: hidden; text-overflow: ellipsis; } td.fillremaining{ width:auto; max-width: none; } td.number, th.number { text-align:right ; } /* Freq mini */ .freq.mini td{ width: 50%; padding: 1px; font-size: 12px; } table.freq.mini { width:100%; } .freq.mini th { overflow: hidden; text-overflow: ellipsis; white-space: nowrap; max-width: 5em; font-weight: 400; text-align:right; padding-right: 0.5em; } .missing { color: #a94442; } .alert, .alert > th, .alert > td { color: #a94442; } /* Bars in tables */ .freq .bar{ float: left; width: 0; height: 100%; line-height: 20px; color: #fff; text-align: center; background-color: #337ab7; border-radius: 3px; margin-right: 4px; } .other .bar { background-color: #999; } .missing .bar{ background-color: #a94442; } .tooltip-inner { width: 100%; white-space: nowrap; text-align:left; } .extrapadding{ padding: 2em; } .pp-anchor{ } &lt;div class=&quot;row headerrow highlight&quot;&gt; &lt;h1&gt;Overview&lt;/h1&gt; &lt;/div&gt; &lt;div class=&quot;row variablerow&quot;&gt; &lt;div class=&quot;col-md-6 namecol&quot;&gt; &lt;p class=&quot;h4&quot;&gt;Dataset info&lt;/p&gt; &lt;table class=&quot;stats&quot; style=&quot;margin-left: 1em;&quot;&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;Number of variables&lt;/th&gt; &lt;td&gt;14 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Number of observations&lt;/th&gt; &lt;td&gt;1309 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Total Missing (%)&lt;/th&gt; &lt;td&gt;21.0% &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Total size in memory&lt;/th&gt; &lt;td&gt;143.2 KiB &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Average record size in memory&lt;/th&gt; &lt;td&gt;112.1 B &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;/div&gt; &lt;div class=&quot;col-md-6 namecol&quot;&gt; &lt;p class=&quot;h4&quot;&gt;Variables types&lt;/p&gt; &lt;table class=&quot;stats&quot; style=&quot;margin-left: 1em;&quot;&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;Numeric&lt;/th&gt; &lt;td&gt;6 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Categorical&lt;/th&gt; &lt;td&gt;7 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Boolean&lt;/th&gt; &lt;td&gt;1 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Date&lt;/th&gt; &lt;td&gt;0 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Text (Unique)&lt;/th&gt; &lt;td&gt;0 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Rejected&lt;/th&gt; &lt;td&gt;0 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Unsupported&lt;/th&gt; &lt;td&gt;0 &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;/div&gt; &lt;div class=&quot;col-md-12&quot; style=&quot;padding-left: 1em;&quot;&gt; &lt;p class=&quot;h4&quot;&gt;Warnings&lt;/p&gt; &lt;ul class=&quot;list-unstyled&quot;&gt;&lt;li&gt;&lt;a href=&quot;#pp_var_age&quot;&gt;&lt;code&gt;age&lt;/code&gt;&lt;/a&gt; has 263 / 20.1% missing values &lt;span class=&quot;label label-default&quot;&gt;Missing&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;#pp_var_boat&quot;&gt;&lt;code&gt;boat&lt;/code&gt;&lt;/a&gt; has 823 / 62.9% missing values &lt;span class=&quot;label label-default&quot;&gt;Missing&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;#pp_var_body&quot;&gt;&lt;code&gt;body&lt;/code&gt;&lt;/a&gt; has 1188 / 90.8% missing values &lt;span class=&quot;label label-default&quot;&gt;Missing&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;#pp_var_cabin&quot;&gt;&lt;code&gt;cabin&lt;/code&gt;&lt;/a&gt; has 1014 / 77.5% missing values &lt;span class=&quot;label label-default&quot;&gt;Missing&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;#pp_var_cabin&quot;&gt;&lt;code&gt;cabin&lt;/code&gt;&lt;/a&gt; has a high cardinality: 187 distinct values &lt;span class=&quot;label label-warning&quot;&gt;Warning&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;#pp_var_fare&quot;&gt;&lt;code&gt;fare&lt;/code&gt;&lt;/a&gt; has 17 / 1.3% zeros &lt;span class=&quot;label label-info&quot;&gt;Zeros&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;#pp_var_home.dest&quot;&gt;&lt;code&gt;home.dest&lt;/code&gt;&lt;/a&gt; has 564 / 43.1% missing values &lt;span class=&quot;label label-default&quot;&gt;Missing&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;#pp_var_home.dest&quot;&gt;&lt;code&gt;home.dest&lt;/code&gt;&lt;/a&gt; has a high cardinality: 370 distinct values &lt;span class=&quot;label label-warning&quot;&gt;Warning&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;#pp_var_name&quot;&gt;&lt;code&gt;name&lt;/code&gt;&lt;/a&gt; has a high cardinality: 1307 distinct values &lt;span class=&quot;label label-warning&quot;&gt;Warning&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;#pp_var_parch&quot;&gt;&lt;code&gt;parch&lt;/code&gt;&lt;/a&gt; has 1002 / 76.5% zeros &lt;span class=&quot;label label-info&quot;&gt;Zeros&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;#pp_var_sibsp&quot;&gt;&lt;code&gt;sibsp&lt;/code&gt;&lt;/a&gt; has 891 / 68.1% zeros &lt;span class=&quot;label label-info&quot;&gt;Zeros&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;#pp_var_ticket&quot;&gt;&lt;code&gt;ticket&lt;/code&gt;&lt;/a&gt; has a high cardinality: 939 distinct values &lt;span class=&quot;label label-warning&quot;&gt;Warning&lt;/span&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class=&quot;row headerrow highlight&quot;&gt; &lt;h1&gt;Variables&lt;/h1&gt; &lt;/div&gt; &lt;div class=&quot;row variablerow&quot;&gt; &lt;div class=&quot;col-md-3 namecol&quot;&gt; &lt;p class=&quot;h4 pp-anchor&quot; id=&quot;pp_var_age&quot;&gt;age&lt;br/&gt; &lt;small&gt;Numeric&lt;/small&gt; &lt;/p&gt; &lt;/div&gt;&lt;div class=&quot;col-md-6&quot;&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm-6&quot;&gt; &lt;table class=&quot;stats &quot;&gt; &lt;tr&gt; &lt;th&gt;Distinct count&lt;/th&gt; &lt;td&gt;99&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Unique (%)&lt;/th&gt; &lt;td&gt;7.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;alert&quot;&gt; &lt;th&gt;Missing (%)&lt;/th&gt; &lt;td&gt;20.1%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;alert&quot;&gt; &lt;th&gt;Missing (n)&lt;/th&gt; &lt;td&gt;263&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Infinite (%)&lt;/th&gt; &lt;td&gt;0.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Infinite (n)&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-6&quot;&gt; &lt;table class=&quot;stats &quot;&gt; &lt;tr&gt; &lt;th&gt;Mean&lt;/th&gt; &lt;td&gt;29.881&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Minimum&lt;/th&gt; &lt;td&gt;0.1667&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Maximum&lt;/th&gt; &lt;td&gt;80&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Zeros (%)&lt;/th&gt; &lt;td&gt;0.0%&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMgAAABLCAYAAAA1fMjoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD%2BnaQAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAATFJREFUeJzt3cGNglAUQNFxYkkWYU%2Bup6cpwp6wAXMDJoSnnrMn%2BZvL930gnpZlWX6Ap36PXgBMdj56AUe53P43X3P/u%2B6wEiazg0AQCASBQPjaGeQVW%2BcWM8v7s4NAEAgEgUAQCASBQBAIBIFAEAgEgUAQCASBQBAIBIFAEAgEr7vvyGe9788OAkEgEAQCQSAQBAJBIBAEAkEgEDwoHMbDxVnsIBAEAkEgEAQCQSAQBAJh3DGvY04mOU37l9tXAmEbN5T1/MSCIBAIAoEwbkhnfw5C1rODQBAIBIFAEAgEQzqrfOtgLxB28wlRjXvVBCYxg0AQCASBQBAIBIFAEAgEgUAQCASBQBAIBIFAEAgEgUAQCASBQBAIBIFAEAgEgUAQCASBQBAIBIFAEAgEgUB4ALFNJ5gUfuDGAAAAAElFTkSuQmCC&quot;&gt; &lt;a role=&quot;button&quot; data-toggle=&quot;collapse&quot; data-target=&quot;#descriptives4581289135072909039,#minihistogram4581289135072909039&quot; aria-expanded=&quot;false&quot; aria-controls=&quot;collapseExample&quot;&gt; Toggle details &lt;/a&gt; &lt;ul class=&quot;nav nav-tabs&quot; role=&quot;tablist&quot;&gt; &lt;li role=&quot;presentation&quot; class=&quot;active&quot;&gt;&lt;a href=&quot;#quantiles4581289135072909039&quot; aria-controls=&quot;quantiles4581289135072909039&quot; role=&quot;tab&quot; data-toggle=&quot;tab&quot;&gt;Statistics&lt;/a&gt;&lt;/li&gt; &lt;li role=&quot;presentation&quot;&gt;&lt;a href=&quot;#histogram4581289135072909039&quot; aria-controls=&quot;histogram4581289135072909039&quot; role=&quot;tab&quot; data-toggle=&quot;tab&quot;&gt;Histogram&lt;/a&gt;&lt;/li&gt; &lt;li role=&quot;presentation&quot;&gt;&lt;a href=&quot;#common4581289135072909039&quot; aria-controls=&quot;common4581289135072909039&quot; role=&quot;tab&quot; data-toggle=&quot;tab&quot;&gt;Common Values&lt;/a&gt;&lt;/li&gt; &lt;li role=&quot;presentation&quot;&gt;&lt;a href=&quot;#extreme4581289135072909039&quot; aria-controls=&quot;extreme4581289135072909039&quot; role=&quot;tab&quot; data-toggle=&quot;tab&quot;&gt;Extreme Values&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;div class=&quot;tab-content&quot;&gt; &lt;div role=&quot;tabpanel&quot; class=&quot;tab-pane active row&quot; id=&quot;quantiles4581289135072909039&quot;&gt; &lt;div class=&quot;col-md-4 col-md-offset-1&quot;&gt; &lt;p class=&quot;h4&quot;&gt;Quantile statistics&lt;/p&gt; &lt;table class=&quot;stats indent&quot;&gt; &lt;tr&gt; &lt;th&gt;Minimum&lt;/th&gt; &lt;td&gt;0.1667&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;5-th percentile&lt;/th&gt; &lt;td&gt;5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Q1&lt;/th&gt; &lt;td&gt;21&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Median&lt;/th&gt; &lt;td&gt;28&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Q3&lt;/th&gt; &lt;td&gt;39&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;95-th percentile&lt;/th&gt; &lt;td&gt;57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Maximum&lt;/th&gt; &lt;td&gt;80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Range&lt;/th&gt; &lt;td&gt;79.833&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Interquartile range&lt;/th&gt; &lt;td&gt;18&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;div class=&quot;col-md-4 col-md-offset-2&quot;&gt; &lt;p class=&quot;h4&quot;&gt;Descriptive statistics&lt;/p&gt; &lt;table class=&quot;stats indent&quot;&gt; &lt;tr&gt; &lt;th&gt;Standard deviation&lt;/th&gt; &lt;td&gt;14.413&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Coef of variation&lt;/th&gt; &lt;td&gt;0.48236&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Kurtosis&lt;/th&gt; &lt;td&gt;0.14695&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Mean&lt;/th&gt; &lt;td&gt;29.881&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;MAD&lt;/th&gt; &lt;td&gt;11.262&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;&quot;&gt; &lt;th&gt;Skewness&lt;/th&gt; &lt;td&gt;0.40767&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Sum&lt;/th&gt; &lt;td&gt;31256&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Variance&lt;/th&gt; &lt;td&gt;207.75&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Memory size&lt;/th&gt; &lt;td&gt;10.3 KiB&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;div role=&quot;tabpanel&quot; class=&quot;tab-pane col-md-8 col-md-offset-2&quot; id=&quot;histogram4581289135072909039&quot;&gt; &lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAlgAAAGQCAYAAAByNR6YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD%2BnaQAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3X1UlXW%2B//8XN8mkAoqKzpipZViIpnlDoR0Mi/FmNPMOnWNmx0odknSkMLNTLU1sqcdMXKbN2JlGV5Iuz3iTd5VZWc40ljYbJE%2BirpSDQcoOJSFurt8f/eTbFkuQD177unw%2B1nKZn2vz2e/X7L2Zl9e12QZYlmUJAAAAxgTaPQAAAIDbULAAAAAMo2ABAAAYRsECAAAwjIIFAABgGAULAADAMAoWAACAYRQsAAAAwyhYAAAAhlGwAAAADKNgAQAAGEbBAgAAMIyCBQAAYBgFCwAAwDAKFgAAgGEULAAAAMMoWAAAAIZRsAAAAAyjYAEAABhGwQIAADCMggUAAGAYBQsAAMAwChYAAIBhFCwAAADDKFgAAACGUbAAAAAMo2ABAAAYRsECAAAwjIIFAABgGAULAADAMAoWAACAYRQsAAAAwyhYAAAAhlGwAAAADKNgAQAAGEbBAgAAMIyCBQAAYBgFCwAAwDAKFgAAgGHBdg9wrSgsPGt0v8DAAEVENNGZMyWqqrKM7m03t2Zzay6JbE7k1lwS2ZyoIXO1ahVqdL/a4gyWQwUGBiggIECBgQF2j2KcW7O5NZdENidyay6JbE7kxlwULAAAAMMoWAAAAIZRsAAAAAyjYAEAABhGwQIAADCMggUAAGAYBQsAAMAwChYAAIBhFCwAAADDKFgAAACGUbAAAAAMo2ABAAAYRsECAAAwLNjuAQB/Nejlj%2B0eoU62T%2B9r9wgAgP8fZ7AAAAAMo2ABAAAYRsECAAAwjIIFAABgGAULAADAMAoWAACAYRQsAAAAwyhYAAAAhlGwAAAADKNgAQAAGEbBAgAAMIyCBQAAYBgFCwAAwDAKFgAAgGEULAAAAMMoWAAAAIZRsAAAAAyjYAEAABhGwQIAADCMggUAAGAYBQsAAMAw1xasvLw8JScnKzY2VnFxcZo1a5aKi4t18uRJde7cWV27dvX59ec//7n6a7dt26ahQ4eqR48eGjFihPbu3WtjEgAA4DTBdg/QUKZMmaKYmBjt3r1bZ8%2BeVXJysl566SVNnTpVkuTxeC75dTk5OUpLS1NGRobuvPNO7dy5U48//rh27NihNm3aXM0IAADAoVx5Bqu4uFgxMTGaOXOmmjRpojZt2uiBBx7Q/v37L/u169evV3x8vOLj4xUSEqJhw4YpKipKmzdvvgqTAwAAN3BlwQoLC1N6erpatmxZvZafn6/IyMjqPz/11FPq16%2Bf7rzzTi1evFjl5eWSpOzsbEVHR/vsFx0d/bNnvAAAAC7m2kuEP%2BXxeLRmzRqtWLFCjRo1Uo8ePXTffffpxRdfVE5OjqZNm6bg4GA98cQT8nq9Cg8P9/n68PBwHTlypNb3V1BQoMLCQp%2B14ODGPgWvvoKCAn1%2BdxM3Z2tIwcH2/e/l5sfMrdncmksimxO5MZfrC9Znn32mqVOnaubMmYqLi5MkrVu3rvp4t27dNHnyZK1cuVJPPPGEJMmyrHrdZ2ZmpjIyMnzWkpOTlZKSUq99LyUs7Hrje/oLN2drCPct%2BsjuEWpt/4sD7R6hztz6fHRrLolsTuSmXK4uWLt379aTTz6pZ599VsOHD//Z27Vt21bffvutLMtS8%2BbN5fV6fY57vV5FRETU%2Bn6TkpKUkJDgsxYc3FhFRSV1C/ALgoICFRZ2vYqLz6uyssrYvv7AzdnwI5OvhYbm1uejW3NJZHOihszVvHkTo/vVlmsL1ueff660tDQtXbpU/fr1q17ft2%2BfDh48WP3ThJJ09OhRtW3bVgEBAYqJiVFWVpbPXh6PR0OGDKn1fUdGRta4HFhYeFYVFeZfDJWVVQ2yrz9wc7ZrnRMfV7c%2BH92aSyKbE7kpl3sudv5ERUWF5syZo9TUVJ9yJUmhoaFavny5Nm3apPLycnk8Hv35z3/WuHHjJEljxozRJ598oj179qisrEwbNmzQ8ePHNWzYMDuiAAAAB3LlGayDBw8qNzdX8%2BbN07x583yO7dixQ0uWLFFGRob%2B8z//U6GhoXrwwQf10EMPSZKioqK0aNEipaenKy8vT506ddLKlSvVqlUrO6IAAAAHcmXB6tWrlw4fPvyzx9u2bav77rvvZ48nJiYqMTGxIUYDAADXAFdeIgQAALATBQsAAMAwChYAAIBhFCwAAADDKFgAAACGUbAAAAAMo2ABAAAYRsECAAAwjIIFAABgGAULAADAMAoWAACAYRQsAAAAwyhYAAAAhlGwAAAADKNgAQAAGEbBAgAAMIyCBQAAYBgFCwAAwDAKFgAAgGEULAAAAMMoWAAAAIZRsAAAAAyjYAEAABhGwQIAADCMggUAAGAYBQsAAMAwChYAAIBhFCwAAADDKFgAAACGUbAAAAAMo2ABAAAYRsECAAAwjIIFAABgGAULAADAMAoWAACAYRQsAAAAwyhYAAAAhlGwAAAADKNgAQAAGEbBAgAAMIyCBQAAYBgFCwAAwDAKFgAAgGEULAAAAMMoWAAAAIa5tmDl5eUpOTlZsbGxiouL06xZs1RcXCxJysnJ0fjx49WzZ08lJiZq9erVPl%2B7bds2DR06VD169NCIESO0d%2B9eOyIAAACHcm3BmjJlisLCwrR7925t3LhRX331lV566SWVlpZq8uTJuvPOO/XRRx9pyZIlWrlypXbt2iXpx/KVlpam1NRU/f3vf9fEiRP1%2BOOP69SpUzYnAgAATuHKglVcXKyYmBjNnDlTTZo0UZs2bfTAAw9o//792rNnj8rLyzV16lQ1btxYXbp00ejRo5WZmSlJWr9%2BveLj4xUfH6%2BQkBANGzZMUVFR2rx5s82pAACAU7iyYIWFhSk9PV0tW7asXsvPz1dkZKSys7PVuXNnBQUFVR%2BLjo5WVlaWJCk7O1vR0dE%2B%2B0VHR8vj8Vyd4QEAgOMF2z3A1eDxeLRmzRqtWLFC27dvV1hYmM/xZs2ayev1qqqqSl6vV%2BHh4T7Hw8PDdeTIkVrfX0FBgQoLC33WgoMbKzIy8spDXCQoKNDndzdxczb8KDjYOY%2BtW5%2BPbs0lkc2J3JjL9QXrs88%2B09SpUzVz5kzFxcVp%2B/btl7xdQEBA9X9bllWv%2B8zMzFRGRobPWnJyslJSUuq176WEhV1vfE9/4eZs17rmzZvYPUKdufX56NZcEtmcyE25XF2wdu/erSeffFLPPvushg8fLkmKiIjQ8ePHfW7n9XrVrFkzBQYGqnnz5vJ6vTWOR0RE1Pp%2Bk5KSlJCQ4LMWHNxYRUUlVxbkEoKCAhUWdr2Ki8%2BrsrLK2L7%2BwM3Z8COTr4WG5tbno1tzSWRzoobMZddf6FxbsD7//HOlpaVp6dKl6tevX/V6TEyM3nzzTVVUVCg4%2BMf4Ho9Ht99%2Be/XxC%2B/HusDj8WjIkCG1vu/IyMgalwMLC8%2BqosL8i6GysqpB9vUHbs52rXPi4%2BrW56Nbc0lkcyI35XLPxc6fqKio0Jw5c5SamupTriQpPj5eTZs21YoVK3T%2B/Hl98cUX2rBhg8aNGydJGjNmjD755BPt2bNHZWVl2rBhg44fP65hw4bZEQUAADiQK89gHTx4ULm5uZo3b57mzZvnc2zHjh169dVX9dxzz2nVqlVq2bKlZsyYof79%2B0uSoqKitGjRIqWnpysvL0%2BdOnXSypUr1apVKxuSAAAAJ3JlwerVq5cOHz78i7d58803f/ZYYmKiEhMTTY8FAACuEa68RAgAAGAnChYAAIBhFCwAAADDKFgAAACGUbAAAAAMo2ABAAAYRsECAAAwjIIFAABgGAULAADAMAoWAACAYRQsAAAAwyhYAAAAhlGwAAAADKNgAQAAGEbBAgAAMIyCBQAAYBgFCwAAwDAKFgAAgGEULAAAAMMoWAAAAIZRsAAAAAyjYAEAABhGwQIAADDM7wpWQkKCMjIylJ%2Bfb/coAAAAV8TvCtbIkSO1bds23XvvvXrkkUe0a9cuVVRU2D0WAABArfldwUpOTta2bdv01ltv6ZZbbtH8%2BfMVHx%2BvhQsX6tixY3aPBwAAcFl%2BV7Au6NKli9LS0vT%2B%2B%2B9r9uzZeuuttzR48GBNmjRJ//rXv%2BweDwAA4Gf5bcEqLy/Xtm3b9OijjyotLU2tW7fW008/rdtuu00TJ07Uli1b7B4RAADgkoLtHuBiubm52rBhg/72t7%2BppKREv/3tb/WXv/xFPXv2rL5N79699fzzz2vo0KE2TgoAAHBpflewhgwZoo4dO2ry5MkaPny4mjVrVuM28fHxOnPmjA3TAQAAXJ7fFaw33nhDffr0ueztvvjii6swDQAAQN353XuwOnfurClTpujdd9%2BtXvvv//5vPfroo/J6vTZOBgAAUDt%2BV7DS09N19uxZderUqXqtf//%2Bqqqq0oIFC2ycDAAAoHb87hLh3r17tWXLFjVv3rx6rUOHDlq0aJF%2B97vf2TgZAABA7fjdGazS0lKFhITUWA8MDNT58%2BdtmAgAAKBu/K5g9e7dWwsWLNB3331XvfbNN9/ohRde8PmoBgAAAH/ld5cIZ8%2Berf/4j//QXXfdpaZNm6qqqkolJSVq166d/vrXv9o9HgAAwGX5XcFq166d3n77bX344Yf6%2BuuvFRgYqI4dO6pfv34KCgqyezwABgx6%2BWO7R6iT/S8OtHsEAA7jdwVLkho1aqR7773X7jEAAACuiN8VrBMnTmjx4sX66quvVFpaWuP4e%2B%2B9Z8NUAAAAted3BWv27NkqKChQv3791LhxY7vHAQAAqDO/K1hZWVl67733FBERYfcoAAAAV8TvPqahRYsWnLkCAACO5ncFa/LkycrIyJBlWXaPAgAAcEX87hLhhx9%2BqM8//1wbN27UDTfcoMBA3w64bt06myYDAACoHb8rWE2bNtW//du/1Xufjz76SGlpaYqNjdWSJUuq1zdu3KjZs2fruuuu87n92rVr1a1bN1VVVWnp0qXaunWriouL1a1bNz3//PNq165dvWcCAADXBr8rWOnp6fXe47XXXtOGDRvUvn37Sx7v3bv3z34q/Nq1a7Vlyxa99tprat26tZYsWaLk5GRt2rRJAQEB9Z4NAAC4n9%2B9B0uSjh49qmXLlunpp5%2BuXjtw4ECtvz4kJOQXC9YvyczM1MSJE3XzzTeradOmmjFjhnJzc/XFF1/UeS8AAHBt8rszWPv27dOjjz6qjh076vjx40pPT9eJEyc0YcIEvfzyyxowYMBl95gwYcIvHs/Pz9fDDz%2BsrKwshYWFKSUlRffff79KS0t15MgRRUdHV9%2B2adOmat%2B%2BvTwej7p3716rDAUFBSosLPRZCw5urMjIyFp9fW0EBQX6/O4mbs4G53Lb89HNrzOyOY8bc/ldwVqyZImefPJJPfTQQ%2BrWrZukH/99wgULFmj58uW1Kli/JCIiQh06dNAf//hHderUSe%2B8846eeuopRUZG6qabbpJlWQoPD/f5mvDwcBUVFdX6PjIzM5WRkeGzlpycrJSUlHrNfilhYdcb39NfuDkbnMetz0e35pLI5kRuyuV3Bet///d/tWbNGknyec/TwIEDNXv27Hrv379/f/Xv37/6z0OGDNE777yjjRs3KjU1VZLq/RERSUlJSkhI8FkLDm6soqKSeu37U0FBgQoLu17FxedVWVllbF9/4OZscC63PR/d/Dojm/M0ZK7mzZsY3a%2B2/K5ghYaGqrS0VI0aNfJZLygoqLFmStu2bZWVlaVmzZopMDBQXq/X57jX61WLFi1qvV9kZGSNy4GFhWdVUWH%2BxVBZWdUg%2B/oDN2eD87j1%2BejWXBLZnMhNufzuYucdd9yh%2BfPn69y5c9Vrx44dU1pamu6666567//mm29q27ZtPmu5ublq166dQkJCdMsttyg7O7v6WHFxsb7%2B%2Buvqy5UAAACX43cF6%2Bmnn9aBAwcUGxursrIy3XHHHRo8eLC8Xq9mzZpV7/1/%2BOEHzZ07Vx6PR%2BXl5dq6das%2B/PBDjR07VpI0btw4vfHGG8rNzdW5c%2Be0aNEi3XbbberatWu97xsAAFwb/O4SYZs2bbR161Z98MEHOnbsmH71q1%2BpY8eO6tu3b60/h%2BpCGaqoqJAkvfvuu5Ikj8ejCRMmqKSkRE888YQKCwt1ww03aPny5YqJiZEkjR07VoWFhXrwwQdVUlKi2NjYGm9YBwAA%2BCUBFv/o31VRWHjW6H7BwYFq3ryJiopKXHO9%2BgJ/yTbo5Y9tu2/4l/0vDrT9%2BWiav7zOGgLZnKchc7VqFWp0v9ryuzNYCQkJv3im6r333ruK0wAAANSd3xWswYMH%2BxSsyspKHTt2TB6PRw899JCNkwEAANSO3xWsC59FdbGdO3fqH//4x1WeBgAAoO787qcIf869996rt99%2B2%2B4xAAAALssxBevQoUP1/oR1AACAq8HvLhFe%2BDyqnzp//rxyc3OVmJhow0QAAAB143cFq0OHDjV%2BijAkJESjRo3S6NGjbZoKAACg9vyuYC1YsMDuEQAAAOrF7wrW3/72t1rfdvjw4Q04CQAAwJXxu4L1zDPPqKqqqsYb2gMCAnzWAgICKFgAAMAv%2BV3B%2BtOf/qTVq1drypQp6ty5syzL0uHDh/Xaa69p/Pjxio2NtXtEAACAX%2BR3BWvBggVatWqVWrduXb3Wq1cvtWvXTpMmTdLWrVttnA4AAODy/O5zsI4fP67w8PAa62FhYcrLy7NhIgAAgLrxu4LVtm1bLViwQEVFRdVrxcXFWrx4sW688UYbJwMAAKgdv7tEOHv2bM2cOVOZmZlq0qSJAgMDde7cOf3qV7/S8uXL7R4PAADgsvyuYPXr10979uzRBx98oFOnTsmyLLVu3Vp33323QkND7R4PAADgsvyuYEnS9ddfrwEDBujUqVNq166d3eMAAADUid%2B9B6u0tFRpaWnq0aOHBg0aJOnH92A98sgjKi4utnk6AACAy/O7grVw4ULl5ORo0aJFCgz8f%2BNVVlZq0aJFNk4GAABQO35XsHbu3KlXXnlFAwcOrP5Hn8PCwpSenq5du3bZPB0AAMDl%2BV3BKikpUYcOHWqsR0RE6Pvvv7/6AwEAANSR3xWsG2%2B8Uf/4xz8kyeffHtyxY4d%2B85vf2DUWAABArfndTxH%2B/ve/17Rp0zRy5EhVVVXp9ddfV1ZWlnbu3KlnnnnG7vEAAAAuy%2B8KVlJSkoKDg7VmzRoFBQXp1VdfVceOHbVo0SINHDjQ7vEAAAAuy%2B8K1pkzZzRy5EiNHDnS7lEAAACuiN%2B9B2vAgAE%2B770CAABwGr8rWLGxsdq%2BfbvdYwAAAFwxv7tE%2BOtf/1ovvviiVq1apRtvvFHXXXedz/HFixfbNBkAAEDt%2BF3BOnLkiG666SZJUlFRkc3TAAAA1J3fFKwZM2ZoyZIl%2Butf/1q9tnz5ciUnJ9s4FQAAQN35zXuwdu/eXWNt1apVNkwCAABQP35TsC71k4P8NCEAAHAivylYF/5h58utAQAA%2BDu/KVgAAABuQcECAAAwzG9%2BirC8vFwzZ8687BqfgwUAAPyd3xSsnj17qqCg4LJrAAAA/s5vCtZPP/8KAADAyXgPFgAAgGEULAAAAMP85hIhAPirXs/ssHuEOtk%2Bva/dIwDXPM5gAQAAGEbBAgAAMIyCBQAAYJhrC9ZHH32kuLg4zZgxo8axbdu2aejQoerRo4dGjBihvXv3Vh%2BrqqrSkiVLNGDAAPXu3VuTJk3SiRMnruboAADA4VxZsF577TXNmzdP7du3r3EsJydHaWlpSk1N1d///ndNnDhRjz/%2BuE6dOiVJWrt2rbZs2aJVq1bp/fffV4cOHZScnCzLsq52DAAA4FCuLFghISHasGHDJQvW%2BvXrFR8fr/j4eIWEhGjYsGGKiorS5s2bJUmZmZmaOHGibr75ZjVt2lQzZsxQbm6uvvjii6sdAwAAOJQrC9aECRMUGhp6yWPZ2dmKjo72WYuOjpbH41FpaamOHDnic7xp06Zq3769PB5Pg84MAADc45r7HCyv16vw8HCftfDwcB05ckTfffedLMu65PGioqJa30dBQYEKCwt91oKDGysyMvLKB79IUFCgz%2B9u4uZswNUQHHz5146bX2dkcx435rrmCpaky76fqr7vt8rMzFRGRobPWnJyslJSUuq176WEhV1vfE9/4eZsQENq3rxJrW/r5tcZ2ZzHTbmuuYLVvHlzeb1enzWv16uIiAg1a9ZMgYGBlzzeokWLWt9HUlKSEhISfNaCgxurqKjkyge/SFBQoMLCrldx8XlVVlYZ29cfuDkbcDXU5nuNm19nZHOehsxVl79wmHTNFayYmBhlZWX5rHk8Hg0ZMkQhISG65ZZblJ2drT59%2BkiSiouL9fXXX6tbt261vo/IyMgalwMLC8%2BqosL8i6GysqpB9vUHbs4GNKS6vG7c/Dojm/O4KZd7LnbW0pgxY/TJJ59oz549Kisr04YNG3T8%2BHENGzZMkjRu3Di98cYbys3N1blz57Ro0SLddttt6tq1q82TAwAAp3DlGawLZaiiokKS9O6770r68UxVVFSUFi1apPT0dOXl5alTp05auXKlWrVqJUkaO3asCgsL9eCDD6qkpESxsbE13k8FAADwS1xZsC73kQqJiYlKTEy85LGAgAClpKQ0yBvSAQDAteGau0QIAADQ0ChYAAAAhlGwAAAADKNgAQAAGEbBAgAAMIyCBQAAYBgFCwAAwDAKFgAAgGEULAAAAMMoWAAAAIZRsAAAAAyjYAEAABhGwQIAADCMggUAAGAYBQsAAMAwChYAAIBhFCwAAADDKFgAAACGUbAAAAAMo2ABAAAYRsECAAAwjIIFAABgGAULAADAMAoWAACAYRQsAAAAw4LtHgD1c9%2Bij%2Bweoda2T%2B9r9wgAAFwVnMECAAAwjIIFAABgGAULAADAMAoWAACAYRQsAAAAwyhYAAAAhlGwAAAADONzsADAZQa9/LHdI9Qan48Ht%2BIMFgAAgGEULAAAAMO4RIirxkmXLQAAqA/OYAEAABhGwQIAADCMggUAAGAYBQsAAMAwChYAAIBhFCwAAADDKFgAAACGUbAAAAAMuyY/aLRz58667rrrFBAQUL02ZswYPfvss9q3b58WL16so0eP6te//rUmT56sYcOG2TgtAABwmmuyYEnSjh07dMMNN/isFRQU6A9/%2BIOeeeYZDR06VJ999pmmTp2qjh07qmvXrjZNCgAAnIZLhD%2BxZcsWdejQQaNGjVJISIji4uKUkJCg9evX2z0aAABwkGv2DNbixYt14MABnTt3ToMGDdKsWbOUnZ2t6Ohon9tFR0dr%2B/btddq7oKBAhYWFPmvBwY0VGRlZ77kvCAqiGwNwvuBg89/LLnx/dOP3Sbdmc2Oua7Jgde/eXXFxcXrppZd04sQJTZ8%2BXS%2B88IK8Xq9at27tc9tmzZqpqKioTvtnZmYqIyPDZy05OVkpKSn1nh0A3KR58yYNtndY2PUNtrfd3JrNTbmuyYKVmZlZ/d8333yzUlNTNXXqVPXs2dPI/klJSUpISPBZCw5urKKiEiP7Sz%2B2fDc9EQFcm0x%2BX7zgwvfH4uLzqqysMr6/ndyarSFzNWSJ/yXXZMG62A033KDKykoFBgbK6/X6HCsqKlJERESd9ouMjKxxObCw8KwqKtzzYgAAExry%2B2JlZZVrv%2B%2B6NZubcrnnYmctHTp0SAsWLPBZy83NVaNGjRQfH6%2BsrCyfY1lZWbr99tuv5ogAAMDhrrmC1aJFC2VmZmrVqlX64YcfdOzYMS1dulRJSUm6//77lZeXp/Xr16usrEwffPCBPvjgA40ZM8busQEAgINccwWrdevWWrVqlXbv3q3Y2FiNHTtWd999t5588km1aNFCK1eu1Jo1a9SzZ0/Nnz9fCxcu1K233mr32AAAwEGuyfdg9e7dW%2BvWrfvZY5s2bbrKEwEAADe55s5gAQAANDQKFgAAgGEULAAAAMOuyfdgAQD8w6CXP7Z7hDrZPr2v3SPAITiDBQAAYBgFCwAAwDAKFgAAgGEULAAAAMMoWAAAAIZRsAAAAAyjYAEAABhGwQIAADCMggUAAGAYBQsAAMAwChYAAIBhFCwAAADDKFgAAACGUbAAAAAMo2ABAAAYRsECAAAwjIIFAABgGAULAADAMAoWAACAYRQsAAAAwyhYAAAAhlGwAAAADKNgAQAAGEbBAgAAMIyCBQAAYBgFCwAAwDAKFgAAgGEULAAAAMMoWAAAAIZRsAAAAAyjYAEAABhGwQIAADCMggUAAGAYBQsAAMCwYLsHAADAKQa9/LHdI9TJ9ul97R7hmsUZLAAAAMMoWAAAAIZRsAAAAAyjYAEAABhGwQIAADCMggUAAGAYBesS8vLy9Nhjjyk2Nlb33HOPFi5cqKqqKrvHAgAADsHnYF3CtGnT1KVLF7377rs6ffq0Jk%2BerJYtW%2Brhhx%2B2ezQAAGrNSZ/btf/FgXaPYBRnsC7i8Xj05ZdfKjU1VaGhoerQoYMmTpyozMxMu0cDAAAOwRmsi2RnZ6tt27YKDw%2BvXuvSpYuOHTumc%2BfOqWnTppfdo6CgQIWFhT5rwcGNFRkZaWzOoCC6MQDAXdz0/20UrIt4vV6FhYX5rF0oW0VFRbUqWJmZmcrIyPBZe/zxxzVt2jRjcxYUFOgvf/mTtj2RZLS4%2BYOCggJlZmYqKcld2dyaSyKbE7k1l0Q2JyooKNCyZctclcs9VdEgy7Lq9fVJSUnauHGjz6%2BkpCRD0/2osLBQGRkZNc6UuYFbs7k1l0Q2J3JrLolsTuTGXJzBukhERIS8Xq/PmtfrVUBAgCIiImq1R2RkpGsaOAAAqDvOYF0kJiZG%2Bfn5OnPmTPWax%2BNRp06d1KRJExsnAwAATkHBukh0dLS6du2qxYsX69y5c8rNzdXrr7%2BucePG2T0aAABwiKDnn3/%2BebuH8Dd33323tm7dqrlz5%2Brtt9/WqFGjNGnSJAUEBNg9mo8mTZqoT58%2Brjyz5tZsbs0lkc2J3JpLIpsTuS1XgFXfd3QDAADAB5cIAQAADKNgAQAAGEbBAgAAMIyy58O3AAAJ%2B0lEQVSCBQAAYBgFCwAAwDAKFgAAgGEULAAAAMMoWAAAAIZRsAAAAAyjYDlQXl6eHnvsMcXGxuqee%2B7RwoULVVVVZfdYV%2BSjjz5SXFycZsyYUePYtm3bNHToUPXo0UMjRozQ3r17bZjwyuTl5Sk5OVmxsbGKi4vTrFmzVFxcLEnKycnR%2BPHj1bNnTyUmJmr16tU2T1s3X375pR566CH17NlTcXFxmj59ugoLCyVJ%2B/bt06hRo3THHXdoyJAh2rx5s83TXpn58%2Berc%2BfO1X92eq7OnTsrJiZGXbt2rf41d%2B5cSc7PJkkrVqxQv3791L17d02cOFEnT56U5Oxs//znP30er65duyomJqb6eenkbIcOHdKECRPUq1cv9e3bV6mpqTpz5owkZ%2BeqwYLjPPDAA9acOXOs4uJi69ixY1ZiYqK1evVqu8eqs1WrVlmJiYnW2LFjrenTp/scO3TokBUTE2Pt2bPHKi0ttTZt2mTdfvvtVn5%2Bvk3T1s3vfvc7a9asWda5c%2Bes/Px8a8SIEdbs2bOt8%2BfPW3fffbe1bNkyq6SkxMrKyrL69Olj7dy50%2B6Ra6WsrMy66667rIyMDKusrMw6ffq0NX78eOsPf/iD9c0331jdu3e31q9fb5WWlloff/yx1a1bN%2Btf//qX3WPXyaFDh6w%2BffpYUVFRlmVZrsgVFRVlnThxosa6G7KtWbPGGjhwoJWbm2udPXvWmjt3rjV37lxXZLvYihUrrCeeeMLR2crLy62%2BfftaixcvtsrKyqwzZ85YDz/8sDVt2jRH57oUzmA5jMfj0ZdffqnU1FSFhoaqQ4cOmjhxojIzM%2B0erc5CQkK0YcMGtW/fvsax9evXKz4%2BXvHx8QoJCdGwYcMUFRXliL/NFBcXKyYmRjNnzlSTJk3Upk0bPfDAA9q/f7/27Nmj8vJyTZ06VY0bN1aXLl00evRoxzx%2B58%2Bf14wZMzR58mQ1atRIERERuu%2B%2B%2B/TVV19py5Yt6tChg0aNGqWQkBDFxcUpISFB69evt3vsWquqqtJzzz2niRMnVq%2B5IdfPcUO21atXa8aMGbrpppvUtGlTzZkzR3PmzHFFtp/6v//7P73%2B%2But66qmnHJ2tsLBQhYWFuv/%2B%2B9WoUSM1b95c9913n3Jychyd61IoWA6TnZ2ttm3bKjw8vHqtS5cuOnbsmM6dO2fjZHU3YcIEhYaGXvJYdna2oqOjfdaio6Pl8Xiuxmj1EhYWpvT0dLVs2bJ6LT8/X5GRkcrOzlbnzp0VFBRUfSw6OlpZWVl2jFpn4eHhGj16tIKDgyVJR48e1f/8z/9o0KBBP/uYOSWbJK1bt04hISEaOnRo9ZobcknS4sWL1b9/f/Xq1UvPPvusSkpKHJ/tm2%2B%2B0cmTJ/Xdd99p8ODBio2NVUpKis6cOeP4bBdbunSpRo4cqd/85jeOzta6dWvddtttyszMVElJiU6fPq1du3apf//%2Bjs51KRQsh/F6vQoLC/NZu1C2ioqK7BipQXi9Xp8SKf2Y04kZPR6P1qxZo6lTp17y8WvWrJm8Xq%2Bj3keXl5enmJgYDR48WF27dlVKSsrPZnPKY/btt99q2bJleu6553zWnZ5Lkrp37664uDjt2rVLmZmZOnjwoF544QXHZzt16pQkaceOHXr99de1adMmnTp1SnPmzHF8tp86efKkdu3apYcffliSs5%2BTgYGBWrZsmd577z3dcccdiouLU0VFhWbOnOnoXJdCwXIgy7LsHuGqcEPOzz77TJMmTdLMmTMVFxf3s7cLCAi4ilPVX9u2beXxeLRjxw4dP35cTz31lN0j1Vt6erpGjBihTp062T2KcZmZmRo9erQaNWqkm2%2B%2BWampqdq6davKy8vtHq1eLnyPeOSRR9S6dWu1adNG06ZN0%2B7du22ezKy1a9cqMTFRrVq1snuUevvhhx80ZcoUDRw4UPv379eHH36o0NBQpaam2j2acRQsh4mIiJDX6/VZ83q9CggIUEREhE1Tmde8efNL5nRSxt27d%2Buxxx7T7NmzNWHCBEk/Pn4X/23M6/WqWbNmCgx01ssxICBAHTp00IwZM7R161YFBwfXeMyKiooc8Zjt27dPBw4cUHJyco1jl3ouOiXXz7nhhhtUWVmpwMBAR2e7cBn%2Bp2c92rZtK8uyVF5e7uhsP7Vz504lJCRU/9nJz8l9%2B/bp5MmT%2BuMf/6jQ0FC1bt1aKSkpeueddxz/fLyYs76jQzExMcrPz6/%2BkVbpx0tQnTp1UpMmTWyczKyYmJga1909Ho9uv/12myaqm88//1xpaWlaunSphg8fXr0eExOjw4cPq6KionrNSbn27dun3/72tz6XMy8Uw27dutV4zLKyshyRbfPmzTp9%2BrTuuecexcbGasSIEZKk2NhYRUVFOTaX9OOPxC9YsMBnLTc3V40aNVJ8fLyjs7Vp00ZNmzZVTk5O9VpeXp6uu%2B46x2e7ICcnR3l5eerbt2/1WteuXR2brbKyUlVVVT5XKH744QdJUlxcnGNzXZKtP8OIKzJ69Ghr9uzZ1tmzZ60jR45YCQkJ1po1a%2Bwe64qlpaXV%2BJiGw4cPW127drXef/99q7S01Fq/fr3Vo0cPq6CgwKYpa6%2B8vNwaNGiQtW7duhrHysrKrHvuucd65ZVXrO%2B//946ePCg1atXL%2Bv999%2B/%2BoNegeLiYisuLs5asGCB9f3331unT5%2B2Jk2aZP3%2B97%2B3vv32W6tHjx7WW2%2B9ZZWWllp79uyxunXrZuXk5Ng99mV5vV4rPz%2B/%2BteBAwesqKgoKz8/38rLy3NsLsuyrFOnTlndu3e3Vq5caZWVlVlHjx61Bg8ebM2dO9fRj9kF8%2BfPtwYMGGAdP37c%2Bvbbb62kpCRr1qxZrshmWZa1YcMGq0%2BfPj5rTs525swZq0%2BfPtZ//dd/Wd9//7115swZa8qUKda///u/OzrXpVCwHCg/P9965JFHrG7dullxcXHWK6%2B8YlVVVdk9Vp3FxMRYMTEx1q233mrdeuut1X%2B%2BYOfOnVZiYqLVpUsX6/7777c%2B/fRTG6etvX/%2B859WVFRUdZ6f/jp58qR1%2BPBha%2BzYsVZMTIzVv39/a%2B3atXaPXCdffvmlNX78eKtbt27WnXfeaU2fPt06deqUZVmW9emnn1rDhg2zunTpYiUmJjrm870uduLEierPwbIs5%2Bf69NNPraSkJKt79%2B5Wnz59rPT0dKu0tLT6mJOzlZWVWc8//7zVu3dvq3v37lZaWpp17tw5y7Kcn82yLOvVV1%2B1hgwZUmPdydk8Ho81fvx4q1evXlZcXJwrv4dYlmUFWJYL3kkMAADgR3gPFgAAgGEULAAAAMMoWAAAAIZRsAAAAAyjYAEAABhGwQIAADCMggUAAGAYBQsAAMAwChYAAIBhFCwAAADDKFgAAACGUbAAAAAMo2ABAAAY9v8BfSeVTQ5s9DIAAAAASUVORK5CYII%3D&quot;/&gt; &lt;/div&gt; &lt;div role=&quot;tabpanel&quot; class=&quot;tab-pane col-md-12&quot; id=&quot;common4581289135072909039&quot;&gt; Value Count Frequency (%) 24.0 47 3.6% 22.0 43 3.3% 21.0 41 3.1% 30.0 40 3.1% 18.0 39 3.0% 25.0 34 2.6% 28.0 32 2.4% 36.0 31 2.4% 27.0 30 2.3% 29.0 30 2.3% Other values (88) 679 51.9% (Missing) 263 20.1% &lt;/div&gt; &lt;div role=&quot;tabpanel&quot; class=&quot;tab-pane col-md-12&quot; id=&quot;extreme4581289135072909039&quot;&gt; &lt;p class=&quot;h4&quot;&gt;Minimum 5 values&lt;/p&gt; Value Count Frequency (%) 0.1667 1 0.1% 0.3333 1 0.1% 0.4167 1 0.1% 0.6667 1 0.1% 0.75 3 0.2% &lt;p class=&quot;h4&quot;&gt;Maximum 5 values&lt;/p&gt; Value Count Frequency (%) 70.5 1 0.1% 71.0 2 0.2% 74.0 1 0.1% 76.0 1 0.1% 80.0 1 0.1% &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;col-md-3 namecol&quot;&gt; &lt;p class=&quot;h4 pp-anchor&quot; id=&quot;pp_var_boat&quot;&gt;boat&lt;br/&gt; &lt;small&gt;Categorical&lt;/small&gt; &lt;/p&gt; &lt;/div&gt;&lt;div class=&quot;col-md-3&quot;&gt; &lt;table class=&quot;stats &quot;&gt; &lt;tr class=&quot;&quot;&gt; &lt;th&gt;Distinct count&lt;/th&gt; &lt;td&gt;29&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Unique (%)&lt;/th&gt; &lt;td&gt;2.2%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;alert&quot;&gt; &lt;th&gt;Missing (%)&lt;/th&gt; &lt;td&gt;62.9%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;alert&quot;&gt; &lt;th&gt;Missing (n)&lt;/th&gt; &lt;td&gt;823&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;table class=&quot;mini freq&quot;&gt; &lt;tr class=&quot;&quot;&gt; &lt;th&gt;13&lt;/th&gt; &lt;td&gt; &lt;div class=&quot;bar&quot; style=&quot;width:5%&quot; data-toggle=&quot;tooltip&quot; data-placement=&quot;right&quot; data-html=&quot;true&quot; data-delay=500 title=&quot;Percentage: 3.0%&quot;&gt; &amp;nbsp; &lt;/div&gt; 39 &lt;/td&gt; C &amp;nbsp; &lt;/div&gt; 38 &lt;/td&gt; 15 &amp;nbsp; &lt;/div&gt; 37 &lt;/td&gt; Other values (25) 372 &lt;/div&gt; &lt;/td&gt; (Missing) 823 &lt;/div&gt; &lt;/td&gt; &lt;/table&gt; &lt;a role=&quot;button&quot; data-toggle=&quot;collapse&quot; data-target=&quot;#freqtable-4263283471668388709, #minifreqtable-4263283471668388709&quot; aria-expanded=&quot;true&quot; aria-controls=&quot;collapseExample&quot;&gt; Toggle details &lt;/a&gt; Value Count Frequency (%) 13 39 3.0% C 38 2.9% 15 37 2.8% 14 33 2.5% 4 31 2.4% 10 29 2.2% 5 27 2.1% 3 26 2.0% 9 25 1.9% 11 25 1.9% Other values (18) 176 13.4% (Missing) 823 62.9% &lt;div class=&quot;col-md-3 namecol&quot;&gt; &lt;p class=&quot;h4 pp-anchor&quot; id=&quot;pp_var_body&quot;&gt;body&lt;br/&gt; &lt;small&gt;Numeric&lt;/small&gt; &lt;/p&gt; &lt;/div&gt;&lt;div class=&quot;col-md-6&quot;&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm-6&quot;&gt; &lt;table class=&quot;stats &quot;&gt; &lt;tr&gt; &lt;th&gt;Distinct count&lt;/th&gt; &lt;td&gt;122&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Unique (%)&lt;/th&gt; &lt;td&gt;9.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;alert&quot;&gt; &lt;th&gt;Missing (%)&lt;/th&gt; &lt;td&gt;90.8%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;alert&quot;&gt; &lt;th&gt;Missing (n)&lt;/th&gt; &lt;td&gt;1188&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Infinite (%)&lt;/th&gt; &lt;td&gt;0.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Infinite (n)&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-6&quot;&gt; &lt;table class=&quot;stats &quot;&gt; &lt;tr&gt; &lt;th&gt;Mean&lt;/th&gt; &lt;td&gt;160.81&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Minimum&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Maximum&lt;/th&gt; &lt;td&gt;328&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Zeros (%)&lt;/th&gt; &lt;td&gt;0.0%&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMgAAABLCAYAAAA1fMjoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD%2BnaQAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAAS5JREFUeJzt2tFJBDEUQNGsWJJF2JPf9mQR21P8FuTiLsxONp7zPxAClxcec5lzzgH86uXsA8DKXs8%2BAD%2B9fXzd/M318/2AkzCGCQJJIBAEAkEgEAQCYYstls0PRzFBIAgEwhZPrHvc8yy71U7PuEfc1xjr3ZkJAkEgEJZ7Yj1qlP9n7vjvTBAIAoEgEAgCgSAQCMttsXZiW/T8TBAIAoEgEAgCgSAQCAKBIBAIAoEgEAgCgSAQCAKBIBAI/ubdgL%2BGj2OCQBAIBIFAEAgEgUAQCASBQBAIBIFAEAgEgUAQCASBQBAIhMucc559CFiVCQJBIBAEAkEgEAQCQSAQBAJBIBAEAkEgEAQCQSAQBAJBIBAEAkEgEAQCQSAQBAJBIBAEAkEgEAQCQSAQBALhGyzqGjWSaoRBAAAAAElFTkSuQmCC&quot;&gt; &lt;a role=&quot;button&quot; data-toggle=&quot;collapse&quot; data-target=&quot;#descriptives-6722867098104776931,#minihistogram-6722867098104776931&quot; aria-expanded=&quot;false&quot; aria-controls=&quot;collapseExample&quot;&gt; Toggle details &lt;/a&gt; &lt;ul class=&quot;nav nav-tabs&quot; role=&quot;tablist&quot;&gt; &lt;li role=&quot;presentation&quot; class=&quot;active&quot;&gt;&lt;a href=&quot;#quantiles-6722867098104776931&quot; aria-controls=&quot;quantiles-6722867098104776931&quot; role=&quot;tab&quot; data-toggle=&quot;tab&quot;&gt;Statistics&lt;/a&gt;&lt;/li&gt; &lt;li role=&quot;presentation&quot;&gt;&lt;a href=&quot;#histogram-6722867098104776931&quot; aria-controls=&quot;histogram-6722867098104776931&quot; role=&quot;tab&quot; data-toggle=&quot;tab&quot;&gt;Histogram&lt;/a&gt;&lt;/li&gt; &lt;li role=&quot;presentation&quot;&gt;&lt;a href=&quot;#common-6722867098104776931&quot; aria-controls=&quot;common-6722867098104776931&quot; role=&quot;tab&quot; data-toggle=&quot;tab&quot;&gt;Common Values&lt;/a&gt;&lt;/li&gt; &lt;li role=&quot;presentation&quot;&gt;&lt;a href=&quot;#extreme-6722867098104776931&quot; aria-controls=&quot;extreme-6722867098104776931&quot; role=&quot;tab&quot; data-toggle=&quot;tab&quot;&gt;Extreme Values&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;div class=&quot;tab-content&quot;&gt; &lt;div role=&quot;tabpanel&quot; class=&quot;tab-pane active row&quot; id=&quot;quantiles-6722867098104776931&quot;&gt; &lt;div class=&quot;col-md-4 col-md-offset-1&quot;&gt; &lt;p class=&quot;h4&quot;&gt;Quantile statistics&lt;/p&gt; &lt;table class=&quot;stats indent&quot;&gt; &lt;tr&gt; &lt;th&gt;Minimum&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;5-th percentile&lt;/th&gt; &lt;td&gt;16&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Q1&lt;/th&gt; &lt;td&gt;72&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Median&lt;/th&gt; &lt;td&gt;155&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Q3&lt;/th&gt; &lt;td&gt;256&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;95-th percentile&lt;/th&gt; &lt;td&gt;307&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Maximum&lt;/th&gt; &lt;td&gt;328&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Range&lt;/th&gt; &lt;td&gt;327&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Interquartile range&lt;/th&gt; &lt;td&gt;184&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;div class=&quot;col-md-4 col-md-offset-2&quot;&gt; &lt;p class=&quot;h4&quot;&gt;Descriptive statistics&lt;/p&gt; &lt;table class=&quot;stats indent&quot;&gt; &lt;tr&gt; &lt;th&gt;Standard deviation&lt;/th&gt; &lt;td&gt;97.697&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Coef of variation&lt;/th&gt; &lt;td&gt;0.60753&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Kurtosis&lt;/th&gt; &lt;td&gt;-1.2541&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Mean&lt;/th&gt; &lt;td&gt;160.81&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;MAD&lt;/th&gt; &lt;td&gt;84.169&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;&quot;&gt; &lt;th&gt;Skewness&lt;/th&gt; &lt;td&gt;0.091739&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Sum&lt;/th&gt; &lt;td&gt;19458&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Variance&lt;/th&gt; &lt;td&gt;9544.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Memory size&lt;/th&gt; &lt;td&gt;10.3 KiB&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;div role=&quot;tabpanel&quot; class=&quot;tab-pane col-md-8 col-md-offset-2&quot; id=&quot;histogram-6722867098104776931&quot;&gt; &lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAlgAAAGQCAYAAAByNR6YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD%2BnaQAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xt8VOWdx/FvkjGBkAwk6sA2IqAUVkgQFIxG5BK5KYKAYKR1kVYU3EA0BQURF6gK7ELWtYTSgoKCXc2SsoIRgUUFgbJWtMhwkdUIW8iCmUJiSAyXJGf/4EXC5MbtIYcz83m/XrxinjPnyW9%2Bnnnmm3NmJiGWZVkCAACAMaF2FwAAABBoCFgAAACGEbAAAAAMI2ABAAAYRsACAAAwjIAFAABgGAELAADAMAIWAACAYQQsAAAAwwhYAAAAhhGwAAAADCNgAQAAGEbAAgAAMIyABQAAYBgBCwAAwDACFgAAgGEELAAAAMMIWAAAAIYRsAAAAAwjYAEAABhGwAIAADCMgAUAAGAYAQsAAMAwAhYAAIBhBCwAAADDCFgAAACGEbAAAAAMI2ABAAAYRsACAAAwjIAFAABgGAELAADAMAIWAACAYQQsAAAAwwhYAAAAhhGwAAAADCNgAQAAGEbAAgAAMIyABQAAYBgBCwAAwDCX3QUEC5/vuLG5QkNDFBvbRMeOlaiiwjI2rxPRiyr0ogq9OIM%2BVKEXVYKtF9dfH23Lz%2BUMlgOFhoYoJCREoaEhdpdiO3pRhV5UoRdn0Icq9KIKvWgYBCwAAADDCFgAAACGEbAAAAAMI2ABAAAYRsACAAAwjIAFAABgGAELAADAMAIWAACAYQQsAAAAw4I2YG3evFlJSUlKT0%2Bvse3777/XU089pc6dOyspKUkZGRmqqKiwoUoAAOBEQRmwFi9erJdfflmtWrWqsc2yLI0fP15xcXHasmWLli9frm3btumzzz6zoVIAAOBEQfnHniMiIpSdna1XXnlFJ0%2Be9Nv2%2Beef6%2BDBg/rDH/6g8PBwRUVFKTs726ZKAQCAEwVlwBo1alSd27744gu1a9dOr776qlauXKmoqCj9/Oc/1y9/%2BcsLnj8/P18%2Bn89vzOWKlMfjueSazxUWFur3NZjRiyr0ogq9OIM%2BVKEXVehFwwjKgFWfI0eOaMeOHerRo4c2btyoP//5zxo/frxuvPFG9enT54LmyMrKUmZmpt9Yamqq0tLSjNbqdjc2Op%2BT0Ysq9KKKqV50fWGtkXkayvZXBvh9zzFRhV5UoRdXFgGrGsuyFBsbqzFjxkiSevbsqb59%2B%2BrDDz%2B84ICVkpKi5ORkvzGXK1IFBSVGagwLC5Xb3VhFRaUqLw/uF9/Tiyr0okqw9%2BLsWhPsfTgXvagSbL2IiWliy88lYFVz/fXXKzo62m8sLi5OX3311QXP4fF4alwO9PmOq6zM7IFcXl5hfE6nohdV6EWVYO1F9fscrH2oDb2oQi%2BuLC7AVnPzzTfr4MGDKimpOtuUl5enuLg4G6sCAABOQsCqJjk5WW63W//yL/%2BiH3/8Udu2bdOGDRs0bNgwu0sDAAAOEZSXCBMSEiRJZWVlkqQNGzZIkrxerxo1aqTXX39d06dP15133qnY2FjNnDlT3bp1s61eAADgLEEZsLxeb73b27Vrp3feeaeBqgEAAIGGS4QAAACGEbAAAAAMI2ABAAAYRsACAAAwjIAFAABgGAELAADAMAIWAACAYQQsAAAAwwhYAAAAhhGwAAAADCNgAQAAGEbAAgAAMIyABQAAYBgBCwAAwDACFgAAgGEELAAAAMMIWAAAAIYRsAAAAAwjYAEAABhGwAIAADCMgAUAAGAYAQsAAMAwAhYAAIBhBCwAAADDCFgAAACGEbAAAAAMI2ABAAAYFrQBa/PmzUpKSlJ6enqdtykpKVGvXr00ZcqUBqwMAAA4ncvuAuywePFiZWdnq1WrVvXebv78%2BSouLm6gqgAAQKAIyjNYERER5w1YX3/9tXJycjR06NAGrAwAAASCoDyDNWrUqHq3W5alGTNmKD09Xf/3f/%2Bn48ePX9T8%2Bfn58vl8fmMuV6Q8Hs9F11qbsLBQv6/BjF5UoRdVgr0XLpf//Q/WPpyLXlShFw0jKAPW%2BWRlZSkkJETDhg1TZmbmJe1ffb/U1FSlpaWZKlGS5HY3Njrfldb1hbV2l3BRtr8ywO4SLonTjosrKVh7ERPTxO/7YO1DbehFFXpxZRGwqjl69Khee%2B01vfnmmwoJCbmkOVJSUpScnOw35nJFqqCgxESJCgsLldvdWEVFpSovrzAyJ2oy9f%2BroXBcVAn2Xpw9doO9D%2BeiF1WCrRfVf%2BFoKASsaubMmaMhQ4aoffv2lzyHx%2BOpcTnQ5zuusjKzB3J5eYXxOVHFqb3luKgSrL2ofp%2BDtQ%2B1oRdV6MWVRcCqZvXq1XK73Vq5cqUk6cSJE6qoqNAnn3yizz77zObqAACAExCwqtm0aZPf90uXLtWRI0f0/PPP21QRAABwmqAMWAkJCZKksrIySdKGDRskSV6vVy1atPC7bVRUlBo3blxjHAAAoC5BGbC8Xu8F33bChAlXsBIAABCI%2BBAMAAAAwwhYAAAAhhGwAAAADCNgAQAAGEbAAgAAMIyABQAAYBgBCwAAwDACFgAAgGEELAAAAMMIWAAAAIYRsAAAAAwjYAEAABgWlH/sGbgQ9/3bVrtLCFgfPnO33SUAl4R14coJtHWBM1gAAACGEbAAAAAMI2ABAAAYRsACAAAwjIAFAABgGAELAADAMAIWAACAYQQsAAAAwwhYAAAAhhGwAAAADCNgAQAAGEbAAgAAMIyABQAAYBgBCwAAwLCgDVibN29WUlKS0tPTa2xbv369Bg8erC5duqh///76j//4DxsqBAAATuWyuwA7LF68WNnZ2WrVqlWNbTt37tSkSZP0r//6r%2BrVq5e2bt2q1NRU3XTTTeratasN1QIAAKcJyjNYERERdQaswsJCjR07Vn369JHL5VLPnj3Vrl07bd%2B%2B3YZKAQCAEwXlGaxRo0bVua1Hjx7q0aNH5fdlZWXy%2BXxq3rz5Bc%2Bfn58vn8/nN%2BZyRcrj8Vx8sbUICwv1%2Bwo4jct1ZY/dYH%2BMnO1vsPfhXPTi6nel14WGFpQB62LMmzdPkZGRuv/%2B%2By94n6ysLGVmZvqNpaamKi0tzWhtbndjdX1hrdE5gYYQE9OkQX6O2924QX7O1aZ6f6/mPrCG4ayGWhcaCgGrDpZlad68ecrJydGyZcsUERFxwfumpKQoOTnZb8zlilRBQYmR2sLCQuV2N1ZRUamR%2BYCGZuqxUJdzHyPl5RVX9Gddjc72N9j7AGe5UuuCXcGNgFWLiooKPf/889q5c6feeecdtWzZ8qL293g8NS4H%2BnzHVVZmdoFjwYRTmX4s1KW8vKLBftbVpPp9DtY%2BwFkC7RglYNVi1qxZ%2Buabb/TOO%2B%2BoWbNmdpcDAAAchoBVzRdffKHVq1drzZo1hCsAAHBJgjJgJSQkSDrzDkFJ2rBhgyTJ6/Xqj3/8o44fP67evXv77dOtWzctWbKkYQsFAACOFJQBy%2Bv11rlt1qxZmjVrVgNWAwAAAk1gfegEAADAVYCABQAAYBgBCwAAwDACFgAAgGEELAAAAMMIWAAAAIYRsAAAAAwjYAEAABhGwAIAADCMgAUAAGAYAQsAAMAwAhYAAIBhBCwAAADDCFgAAACGEbAAAAAMI2ABAAAYRsACAAAwjIAFAABgGAELAADAMAIWAACAYQQsAAAAwwhYAAAAhhGwAAAADCNgAQAAGEbAAgAAMIyABQAAYBgBCwAAwLCgDVibN29WUlKS0tPTa2xbs2aNBg0apC5dumjYsGHasmWLDRUCAACnctldgB0WL16s7OxstWrVqsa2vXv3avLkycrMzNSdd96pdevWafz48Vq7dq1atGhhQ7UAAMBpHHcGKzk5WZmZmTp8%2BPAlzxEREVFnwFqxYoV69uypnj17KiIiQoMHD1a7du20evXqyykbAAAEEccFrIceekhr1qxRnz59NGbMGK1fv15lZWUXNceoUaMUHR1d67bdu3erQ4cOfmMdOnSQ1%2Bu95JoBAEBwcdwlwtTUVKWmpmr37t3KycnRrFmzNHPmTA0ZMkTDhw9XmzZtLmv%2BwsJCNW3a1G%2BsadOm%2Bvbbby94jvz8fPl8Pr8xlytSHo/nsmo7Kyws1O8r4DQu15U9doP9MXK2v8HeBzjLlV4XGprjAtZZHTt2VMeOHfXcc89pzZo1mjFjhpYsWaKkpCQ9/fTT6tSp0yXPbVnWZdWWlZWlzMxMv7HU1FSlpaVd1rzVud2Njc4HNJS%2B8zbbXUJAi4lp4vc9awWcoPpx63SODVinT5/Wf/3Xf2nlypX67//%2Bb7Vu3VoTJkxQfn6%2BRo8erZkzZ2rQoEEXPW9MTIwKCwv9xgoLCxUbG3vBc6SkpCg5OdlvzOWKVEFByUXXU5uwsFC53Y1VVFRqZD4AgeXsWnPuWlFeXmFzVUD9TD1HVmdXcHNcwMrNzVV2drbee%2B89lZSUqH///nrrrbd0%2B%2B23V96mW7dumjFjxiUFrPj4eO3atctvzOv1auDAgRc8h8fjqXE50Oc7rrIyswscCyaA2lRfa8rLK4yvP4BpgXaMOi5gDRw4UG3atNHYsWM1ZMgQNWvWrMZtevbsqWPHjl3S/A8//LCGDx%2BujRs36q677tL777%2BvAwcOaPDgwZdbOgAACBKOC1jLli3THXfccd7bffXVV3VuS0hIkKTKdx9u2LBB0pkzVe3atdO8efM0e/Zs5eXlqW3btvr973%2Bv66%2B/3kD1AAAgGDguYLVv317jxo3T8OHD1adPH0nSm2%2B%2Bqa1bt2ru3Lm1ntGq7nwfudCvXz/169fPSL0AACD4OO49kbNnz9bx48fVtm3byrFevXqpoqJCc%2BbMsbEyAACAMxx3BmvLli16//33FRMTUznWunVrzZs3Tw888ICNlQEAAJzhuDNYJ06cUERERI3x0NBQlZbysQUAAMB%2BjgtY3bp105w5c/TDDz9Ujn3//feaOXOm30c1AAAA2MVxlwinTp2qX/7yl7rrrrsUFRWliooKlZSUqGXLllq%2BfLnd5QEAADgvYLVs2VIffPCBPv30U/31r39VaGio2rRpo%2B7duyssLMzu8gAAAJwXsCQpPDy88iMaAAAArjaOC1gHDx5URkaGvvnmG504caLG9o8%2B%2BsiGqgAAAKo4LmBNnTpV%2Bfn56t69uyIjI%2B0uBwAAoAbHBaxdu3bpo48%2BUmxsrN2lAAAA1MpxH9Nw7bXXcuYKAABc1RwXsMaOHavMzExZlmV3KQAAALVy3CXCTz/9VF9%2B%2BaVWrlypG264QaGh/hnx3XfftakyAACAMxwXsKKiotSjRw%2B7ywAAAKiT4wLW7Nmz7S4BAACgXo57DZYkfffdd5o/f76ef/75yrG//OUvNlYEAABQxXEBa9u2bRo8eLDWr1%2BvnJwcSWc%2BfHTUqFF8yCgAALgqOC5gvfrqq3r22Wf1/vvvKyQkRNKZv084Z84cLViwwObqAAAAHBiw/ud//kcjR46UpMqAJUkDBgxQbm6uXWUBAABUclzAio6OrvVvEObn5ys8PNyGigAAAPw5LmDddtttmjVrloqLiyvH9u/fr8mTJ%2Buuu%2B6ysTIAAIAzHPcxDc8//7wee%2BwxJSYmqry8XLfddptKS0v105/%2BVHPmzLG7PAAAAOcFrBYtWignJ0ebNm3S/v371ahRI7Vp00Z3332332uyAAAA7OK4gCVJ11xzjfr06WN3GQAAALVyXMBKTk6u90wVn4UFAADs5riAdf/99/sFrPLycu3fv19er1ePPfaYjZUBAACc4biANWnSpFrH161bp88%2B%2B6yBqwEAAKjJcR/TUJc%2Bffrogw8%2BMDLXnj17NGrUKHXt2lV33323Jk2apGPHjhmZGwAABL6ACVh79uyRZVmXPU9ZWZmefPJJde7cWX/605%2BUk5OjY8eOacaMGZdfJAAACAqOu0T4yCOP1BgrLS1Vbm6u%2BvXrd9nz%2B3w%2B%2BXw%2BPfjggwoPD1d4eLj69u2rJUuWXPbcAAAgODguYLVu3brGuwgjIiI0fPhwjRgx4rLnb968uW655RZlZWXp6aef1okTJ7R%2B/Xr16tXrsucGAADBwXEB60p/WntoaKjmz5%2Bv0aNH66233pIk3XHHHZo4ceIFz5Gfny%2Bfz%2Bc35nJFyuPxGKkxLCzU7ysAnMvl8l8jWCvgBGeP20DhuID13nvvXfBthwwZctHznzp1SuPGjdOAAQM0btw4/fjjj5o5c6YmTZqkzMzMC5ojKyurxm1TU1OVlpZ20fXUx%2B1ubHQ%2BAIEhJqaJ3/esFXCC6set0zkuYL3wwguqqKio8YL2kJAQv7GQkJBLCljbtm3ToUOH9Ktf/UphYWGKjo5WWlqaHnzwQRUWFqpZs2bnnSMlJUXJycl%2BYy5XpAoKSi66ntqEhYXK7W6soqJSI/MBCCxn15pz14ry8gqbqwLqZ%2Bo5sjq7gpvjAtbrr7%2BuJUuWaNy4cWrfvr0sy9K%2Bffu0ePFiPfroo0pMTLys%2BcvLy2sEuFOnTl3UHB6Pp8blQJ/vuMrKzC5wLJgAalN9rSkvrzC%2B/gCmBdox6riANWfOHC1atEjNmzevHOvatatatmypxx9/XDk5OZc1f5cuXRQZGan58%2Bdr3LhxOnHihBYuXKhu3bpd0NkrAAAAx72i7MCBA2ratGmNcbfbrby8vMuePyYmRm%2B88Ya%2B/PJL9ejRQw888IAaNWqkjIyMy54bAAAEB8edwYqLi9OcOXP09NNPKyYmRpJUVFSk3/zmN7rxxhuN/Iz4%2BHgtX77cyFwAACD4OC5gTZ06VRMnTlRWVpaaNGmi0NBQFRcXq1GjRlqwYIHd5QEAADgvYHXv3l0bN27Upk2bdOTIEVmWpebNm%2Buee%2B5RdHS03eUBAAA4L2BJUuPGjXXvvffqyJEjatmypd3lAAAA%2BHHci9xPnDihyZMnq0uXLrrvvvsknXkN1pgxY1RUVGRzdQAAAA4MWHPnztXevXs1b948hYZWlV9eXq558%2BbZWBkAAMAZjgtY69at029%2B8xsNGDCg8o8%2Bu91uzZ49W%2BvXr7e5OgAAAAcGrJKSErVu3brGeGxsrH788ceGLwgAAKAaxwWsG2%2B8UZ999pkk%2Bf05m7Vr1%2BonP/mJXWUBAABUcty7CH/2s59pwoQJeuihh1RRUaGlS5dq165dWrdunV544QW7ywMAAHBewEpJSZHL5dLbb7%2BtsLAw/e53v1ObNm00b948DRgwwO7yAAAAnBewjh07poceekgPPfSQ3aUAAADUynGvwbr33nv9XnsFAABwtXFcwEpMTNSHH35odxkAAAB1ctwlwr/7u7/TK6%2B8okWLFunGG2/UNddc47c9IyPDpsoAAADOcFzA%2Bvbbb3XTTTdJkgoKCmyuBgAAoCbHBKz09HS9%2BuqrWr58eeXYggULlJqaamNVAAAANTnmNVgff/xxjbFFixbZUAkAAED9HBOwanvnIO8mBAAAVyPHBKyzf9j5fGMAAAB2c0zAAgAAcAoCFgAAgGGOeRfh6dOnNXHixPOO8TlYAADAbo4JWLfffrvy8/PPOwYAAGA3xwSscz//CgAA4GrGa7AAAAAMI2ABAAAYRsACAAAwjIBVh4ULF6p79%2B7q3LmzRo8erUOHDtldEgAAcAgCVi3%2B8Ic/aPXq1Vq2bJm2bNmitm3b6s0337S7LAAA4BCOeRdhQ1qyZIkmT56sm266SZI0bdo0mysCAABOwhmsar7//nsdOnRIP/zwg%2B6//34lJiYqLS1Nx44ds7s0AADgEJzBqubIkSOSpLVr12rp0qWyLEtpaWmaNm2afvvb317QHPn5%2BfL5fH5jLlekPB6PkRrDwkL9vgLAuVwu/zWCtQJOcPa4DRQErGosy5IkjRkzRs2bN5ckTZgwQU888YROnjypiIiI886RlZWlzMxMv7HU1FSlpaUZrdXtbmx0PgCBISamid/3rBVwgurHrdMRsKq57rrrJElut7tyLC4uTpZl6ejRo/rJT35y3jlSUlKUnJzsN%2BZyRaqgoMRIjWFhoXK7G6uoqNTIfAACy9m15ty1ory8wuaqgPqZeo6szq7gRsCqpkWLFoqKitLevXvVsWNHSVJeXp6uueaaC77E5/F4atzW5zuusjKzCxwLJoDaVF9ryssrjK8/gGmBdowG1gVPA1wul4YPH67f/e53%2Bt///V8dPXpUCxYs0KBBg%2BRykUcBAMD5kRhqMXHiRJ06dUojRozQ6dOn1b9/fz6qAQAAXDACVi3Cw8M1ffp0TZ8%2B3e5SAACAA3GJEAAAwDACFgAAgGEELAAAAMMIWAAAAIYRsAAAAAwjYAEAABhGwAIAADCMgAUAAGAYAQsAAMAwAhYAAIBhBCwAAADDCFgAAACGEbAAAAAMc9ldAADArPv%2BbavdJQBBjzNYAAAAhhGwAAAADCNgAQAAGEbAAgAAMIyABQAAYBgBCwAAwDACFgAAgGEELAAAAMMIWAAAAIYRsAAAAAwjYAEAABhGwAIAADCMgAUAAGAYAes8Zs2apfbt29tdBgAAcBACVj327t2rVatW2V0GAABwGAJWHSoqKjR9%2BnSNHj3a7lIAAIDDELDq8O677yoiIkKDBg2yuxQAAOAwLrsLuBr97W9/0/z587V8%2BfJL2j8/P18%2Bn89vzOWKlMfjMVGewsJC/b4CAOB0LldgPacRsGoxe/ZsDRs2TG3bttWhQ4cuev%2BsrCxlZmb6jaWmpiotLc1UiZIkt7ux0fkAALBLTEwTu0swioBVzbZt2/SXv/xFOTk5lzxHSkqKkpOT/cZcrkgVFJRcbnmSzpy5crsbq6io1Mh8AADYzdRzZHV2BTcCVjWrV6/W0aNH1bt3b0mSZVmSpMTERP3TP/2TBg4ceN45PB5PjcuBPt9xlZVVGK21vNzsfAAA2MX0c6TdCFjVTJkyRU8//XTl90eOHFFKSopWrVqlpk2b2lgZAABwCgJWNU2bNvULUmVlZZKkFi1a2FUSAABwmMB6yf4VcMMNN2jfvn12lwEAAByEgAUAAGAYAQsAAMAwAhYAAIBhBCwAAADDCFgAAACGEbAAAAAMI2ABAAAYRsACAAAwjIAFAABgGAELAADAMAIWAACAYQQsAAAAwwhYAAAAhhGwAAAADCNgAQAAGEbAAgAAMIyABQAAYBgBCwAAwDACFgAAgGEELAAAAMMIWAAAAIYRsAAAAAwjYAEAABhGwAIAADCMgAUAAGAYAQsAAMAwAhYAAIBhBKxa5OXlKTU1VYmJiUpKStKUKVNUVFRkd1kAAMAhCFi1GDdunNxutz7%2B%2BGOtXLlS33zzjf75n//Z7rIAAIBDELCqKSoqUnx8vCZOnKgmTZqoRYsWGjp0qLZv3253aQAAwCFcdhdwtXG73Zo9e7bf2OHDh%2BXxeGyqCAAAOA0B6zy8Xq/efvttLVy48IL3yc/Pl8/n8xtzuSKNhbSwsFC/rwAAOJ3LFVjPaQSsenzxxRd66qmnNHHiRCUlJV3wfllZWcrMzPQbS01NVVpamtH63O7GRucDAMAuMTFN7C7BKAJWHT7%2B%2BGM9%2B%2ByzevHFFzVkyJCL2jclJUXJycl%2BYy5XpAoKSozUFhYWKre7sYqKSo3MBwCA3Uw9R1ZnV3AjYNXiyy%2B/1OTJk/Xaa6%2Bpe/fuF72/x%2BOpcTnQ5zuusrIKUyVKksrLzc4HAIBdTD9H2i2wLngaUFZWpmnTpmnSpEmXFK4AAAAIWNXs2LFDubm5evnll5WQkOD3Ly8vz%2B7yAACAA3CJsJquXbtq3759dpcBAAAcjDNYAAAAhhGwAAAADCNgAQAAGEbAAgAAMIyABQAAYBgBCwAAwDACFgAAgGEELAAAAMMIWAAAAIYRsAAAAAwjYAEAABhGwAIAADCMgAUAAGAYAQsAAMAwAhYAAIBhBCwAAADDCFgAAACGEbAAAAAMI2ABAAAYRsACAAAwjIAFAABgGAELAADAMAIWAACAYQQsAAAAwwhYAAAAhhGwAAAADCNgAQAAGEbAAgAAMIyAVYu8vDw9%2BeSTSkxMVO/evTV37lxVVFTYXRYAAHAIl90FXI0mTJigjh07asOGDTp69KjGjh2r6667Tr/4xS/sLg0AADgAZ7Cq8Xq9%2BvrrrzVp0iRFR0erdevWGj16tLKysuwuDQAAOARnsKrZvXu34uLi1LRp08qxjh07av/%2B/SouLlZUVNR558jPz5fP5/Mbc7ki5fF4jNQYFhbq9xUAAKdzuQLrOY2AVU1hYaHcbrff2NmwVVBQcEEBKysrS5mZmX5j48eP14QJE4zUmJ%2Bfr7feel0pKSna/soAI3M6VX5%2BvrKyspSSkmIswDoVvahCL86gD1XoRRV60TACKy4aYlnWZe2fkpKilStX%2Bv1LSUkxVJ3k8/mUmZlZ4yxZMKIXVehFFXpxBn2oQi%2Bq0IuGwRmsamJjY1VYWOg3VlhYqJCQEMXGxl7QHB6Ph98KAAAIYpzBqiY%2BPl6HDx/WsWPHKse8Xq/atm2rJk2a2FgZAABwCgJWNR06dFBCQoIyMjJUXFys3NxcLV26VCNHjrS7NAAA4BBhM2bMmGF3EVebe%2B65Rzk5OXrppZf0wQcfaPjw4Xr88ccVEhJid2mVmjRpojvuuIOzaqIX56IXVejFGfShCr2oQi%2BuvBDrcl/RDQAAAD9cIgQAADCMgAUAAGAYAQsAAMAwAhYAAIBhBCwAAADDCFgAAACGEbAAAAAMI2ABAAAYRsACAAAwjIDlMHl5eXryySeVmJio3r17a%2B7cuaqoqLC7rAbRvn17xcfHKyEhofLfSy%2B9JEnatm2bhg8frttuu00DBw7U6tWrba7WrM2bNyspKUnp6ek1tq1Zs0aDBg1Sly5dNGzYMG3ZsqVyW0VFhV599VXde%2B%2B96tatmx5//HEdPHiwIUs3rq5erFy5Un//93/vd3wkJCRo586dkgKvF3l5eUpNTVViYqKSkpI0ZcoUFRUVSZL27t2rRx99VLfffrv69eunJUuW%2BO1b3zHjRHX14tChQ2rfvn2NY%2BKNN96o3DfQevH111/rscce0%2B23366kpCQ988wz8vl8ks6/Ti5btkz9%2B/fXbbfdppEjR2rXrl123IXUJ4uMAAAIz0lEQVTAYcFRhg4dak2bNs0qKiqy9u/fb/Xr189asmSJ3WU1iHbt2lkHDx6sMf79999bnTt3tlasWGGdOHHC2rp1q9WpUydr586dNlRp3qJFi6x%2B/fpZjzzyiPXMM8/4bduzZ48VHx9vbdy40Tpx4oS1atUq69Zbb7UOHz5sWZZlLVu2zOrdu7f17bffWsePH7d%2B/etfW4MGDbIqKirsuCuXrb5e/PGPf7QeffTROvcNtF488MAD1pQpU6zi4mLr8OHD1rBhw6ypU6dapaWl1j333GPNnz/fKikpsXbt2mXdcccd1rp16yzLOv8x40R19eLgwYNWu3bt6twv0Hpx8uRJ66677rIyMzOtkydPWkePHrUeffRR6x//8R/Pu05%2B9NFHVteuXa0dO3ZYpaWl1u9//3vr7rvvtkpKSmy%2BV87FGSwH8Xq9%2BvrrrzVp0iRFR0erdevWGj16tLKysuwuzVbvv/%2B%2BWrdureHDhysiIkJJSUlKTk7WihUr7C7NiIiICGVnZ6tVq1Y1tq1YsUI9e/ZUz549FRERocGDB6tdu3aVv5lmZWVp9OjRuvnmmxUVFaX09HTl5ubqq6%2B%2Baui7YUR9vTifQOpFUVGR4uPjNXHiRDVp0kQtWrTQ0KFDtX37dm3cuFGnT5/WU089pcjISHXs2FEjRoyoXCfOd8w4TX29OJ9A60VpaanS09M1duxYhYeHKzY2Vn379tU333xz3nUyKytLw4YN06233qpGjRppzJgxkqRPPvnEzrvkaAQsB9m9e7fi4uLUtGnTyrGOHTtq//79Ki4utrGyhpORkaFevXqpa9euevHFF1VSUqLdu3erQ4cOfrfr0KFDwJzeHjVqlKKjo2vdVtd993q9OnHihL799lu/7VFRUWrVqpW8Xu8VrflKqa8XknT48GH94he/ULdu3XTvvfdq1apVkhRwvXC73Zo9e7auu%2B66yrHDhw/L4/Fo9%2B7dat%2B%2BvcLCwiq3nft4qO%2BYcaL6enHWc889p%2B7du%2BvOO%2B9URkaGTp8%2BLSnwetG0aVONGDFCLpdLkvTdd9/pP//zP3Xfffedd52svj00NFS33HKLY3txNSBgOUhhYaHcbrff2NmwVVBQYEdJDapz585KSkrS%2BvXrlZWVpR07dmjmzJm19qVZs2ZB0ZPCwkK/wC2dOSYKCgr0ww8/yLKsOrcHmtjYWLVu3VrPPvustm7dql/96leaOnWqtm3bFvC98Hq9evvtt/XUU0/V%2BXgoLCxURUVFvcdMIDi3F%2BHh4erSpYv69u2rTz75RIsWLdLq1av129/%2BVlL9jx8ny8vLU3x8vO6//34lJCQoLS3tvOtkoPbCTgQsh7Esy%2B4SbJOVlaURI0YoPDxcN998syZNmqScnJzK30aD1fmOiWA5Znr16qXXX39dHTp0UHh4uAYOHKi%2Bfftq5cqVlbcJxF588cUXevzxxzVx4kQlJSXVebuQkJDK/w7EPkg1e%2BHxePTuu%2B%2Bqb9%2B%2Buuaaa9SpUyeNHTs24I%2BJuLg4eb1erV27VgcOHNBzzz13QfsFYi/sRMBykNjYWBUWFvqNFRYWKiQkRLGxsTZVZZ8bbrhB5eXlCg0NrdGXgoKCoOhJTExMrcdEbGysmjVrVmtvCgsLde211zZkmbaJi4tTfn5%2BwPbi448/1pNPPqmpU6dq1KhRks6sE9XPOhQWFlb2oL5jxslq60Vt4uLi9Le//U2WZQVsL6Qzgbp169ZKT09XTk6OXC5XvetkIPfCLgQsB4mPj9fhw4d17NixyjGv16u2bduqSZMmNlZ25e3Zs0dz5szxG8vNzVV4eLh69uxZ4/VWu3bt0q233tqQJdoiPj6%2Bxn33er269dZbFRERoZ/%2B9KfavXt35baioiL99a9/VadOnRq61CvunXfe0Zo1a/zGcnNz1bJly4DsxZdffqnJkyfrtdde05AhQyrH4%2BPjtW/fPpWVlVWOnT0mzm6v65hxqrp6sW3bNi1cuNDvtt99953i4uIUEhIScL3Ytm2b%2Bvfv7/fRPaGhZ57mO3XqVO86GR8f7/f4KC8v1549exzbi6sBActBOnTooISEBGVkZKi4uFi5ublaunSpRo4caXdpV9y1116rrKwsLVq0SKdOndL%2B/fv12muvKSUlRQ8%2B%2BKDy8vK0YsUKnTx5Ups2bdKmTZv08MMP2132Fffwww/rT3/6kzZu3KiTJ08qOztbBw4c0ODBgyVJI0eO1LJly5Sbm6vi4mLNmzdPt9xyixISEmyu3LxTp07ppZdektfr1enTp5WTk6NPP/1UjzzyiKTA6kVZWZmmTZumSZMmqXv37n7bevbsqaioKC1cuFClpaX66quvlJ2dXblOnO%2BYcZr6ehEdHa0FCxZo1apVOn36tLxer954442A7UV8fLyKi4s1d%2B5clZaW6tixY5o/f766du2qkSNH1rtOjhw5Uu%2B995527Nih0tJSLVy4UOHh4erVq5e9d8rBQiwuujrKkSNH9OKLL%2BrPf/6zoqKi9Mgjj2j8%2BPF%2Br68IVJ9//rkyMjK0b98%2BhYeHa%2BjQoUpPT1dERIQ%2B//xzvfzyy8rNzVVcXJwmTpyofv362V2yEWcDwNkzEmffIXT23T3r169XRkaG8vLy1LZtW73wwgvq1q2bpDOvqZg/f77effddlZSUKDExUb/%2B9a/VokULG%2B7J5auvF5ZlaeHChcrOzpbP59MNN9yg5557Tr1795YUWL3Yvn27fv7znys8PLzGtrVr16qkpETTp0/Xrl27dN111%2BmJJ57Qz372s8rb1HfMOM35erFnzx5lZmbqwIEDio6O1j/8wz/oiSeeqDyzE0i9kKR9%2B/bp5Zdf1s6dOxUZGak777xTU6ZMUfPmzc%2B7Tv77v/%2B7Fi1apKNHjyohIUEzZsxQu3btbLw3zkbAAgAAMIxLhAAAAIYRsAAAAAwjYAEAABhGwAIAADCMgAUAAGAYAQsAAMAwAhYAAIBhBCwAAADDCFgAAACGEbAAAAAMI2ABAAAYRsACAAAwjIAFAABg2P8DXjA0FYX7gOgAAAAASUVORK5CYII%3D&quot;/&gt; &lt;/div&gt; &lt;div role=&quot;tabpanel&quot; class=&quot;tab-pane col-md-12&quot; id=&quot;common-6722867098104776931&quot;&gt; Value Count Frequency (%) 142.0 1 0.1% 68.0 1 0.1% 47.0 1 0.1% 98.0 1 0.1% 188.0 1 0.1% 120.0 1 0.1% 156.0 1 0.1% 37.0 1 0.1% 58.0 1 0.1% 70.0 1 0.1% Other values (111) 111 8.5% (Missing) 1188 90.8% &lt;/div&gt; &lt;div role=&quot;tabpanel&quot; class=&quot;tab-pane col-md-12&quot; id=&quot;extreme-6722867098104776931&quot;&gt; &lt;p class=&quot;h4&quot;&gt;Minimum 5 values&lt;/p&gt; Value Count Frequency (%) 1.0 1 0.1% 4.0 1 0.1% 7.0 1 0.1% 9.0 1 0.1% 14.0 1 0.1% &lt;p class=&quot;h4&quot;&gt;Maximum 5 values&lt;/p&gt; Value Count Frequency (%) 312.0 1 0.1% 314.0 1 0.1% 322.0 1 0.1% 327.0 1 0.1% 328.0 1 0.1% &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;col-md-3 namecol&quot;&gt; &lt;p class=&quot;h4 pp-anchor&quot; id=&quot;pp_var_cabin&quot;&gt;cabin&lt;br/&gt; &lt;small&gt;Categorical&lt;/small&gt; &lt;/p&gt; &lt;/div&gt;&lt;div class=&quot;col-md-3&quot;&gt; &lt;table class=&quot;stats &quot;&gt; &lt;tr class=&quot;alert&quot;&gt; &lt;th&gt;Distinct count&lt;/th&gt; &lt;td&gt;187&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Unique (%)&lt;/th&gt; &lt;td&gt;14.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;alert&quot;&gt; &lt;th&gt;Missing (%)&lt;/th&gt; &lt;td&gt;77.5%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;alert&quot;&gt; &lt;th&gt;Missing (n)&lt;/th&gt; &lt;td&gt;1014&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;table class=&quot;mini freq&quot;&gt; &lt;tr class=&quot;&quot;&gt; &lt;th&gt;C23 C25 C27&lt;/th&gt; &lt;td&gt; &lt;div class=&quot;bar&quot; style=&quot;width:1%&quot; data-toggle=&quot;tooltip&quot; data-placement=&quot;right&quot; data-html=&quot;true&quot; data-delay=500 title=&quot;Percentage: 0.5%&quot;&gt; &amp;nbsp; &lt;/div&gt; 6 &lt;/td&gt; G6 &amp;nbsp; &lt;/div&gt; 5 &lt;/td&gt; B57 B59 B63 B66 &amp;nbsp; &lt;/div&gt; 5 &lt;/td&gt; Other values (183) 279 &lt;/div&gt; &lt;/td&gt; (Missing) 1014 &lt;/div&gt; &lt;/td&gt; &lt;/table&gt; &lt;a role=&quot;button&quot; data-toggle=&quot;collapse&quot; data-target=&quot;#freqtable6688113341693915533, #minifreqtable6688113341693915533&quot; aria-expanded=&quot;true&quot; aria-controls=&quot;collapseExample&quot;&gt; Toggle details &lt;/a&gt; Value Count Frequency (%) C23 C25 C27 6 0.5% G6 5 0.4% B57 B59 B63 B66 5 0.4% F4 4 0.3% F33 4 0.3% F2 4 0.3% C22 C26 4 0.3% C78 4 0.3% B96 B98 4 0.3% D 4 0.3% Other values (176) 251 19.2% (Missing) 1014 77.5% &lt;div class=&quot;col-md-3 namecol&quot;&gt; &lt;p class=&quot;h4 pp-anchor&quot; id=&quot;pp_var_embarked&quot;&gt;embarked&lt;br/&gt; &lt;small&gt;Categorical&lt;/small&gt; &lt;/p&gt; &lt;/div&gt;&lt;div class=&quot;col-md-3&quot;&gt; &lt;table class=&quot;stats &quot;&gt; &lt;tr class=&quot;&quot;&gt; &lt;th&gt;Distinct count&lt;/th&gt; &lt;td&gt;4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Unique (%)&lt;/th&gt; &lt;td&gt;0.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Missing (%)&lt;/th&gt; &lt;td&gt;0.2%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Missing (n)&lt;/th&gt; &lt;td&gt;2&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;table class=&quot;mini freq&quot;&gt; &lt;tr class=&quot;&quot;&gt; &lt;th&gt;S&lt;/th&gt; &lt;td&gt; &lt;div class=&quot;bar&quot; style=&quot;width:100%&quot; data-toggle=&quot;tooltip&quot; data-placement=&quot;right&quot; data-html=&quot;true&quot; data-delay=500 title=&quot;Percentage: 69.8%&quot;&gt; 914 &lt;/div&gt; &lt;/td&gt; C 270 &lt;/div&gt; &lt;/td&gt; Q &amp;nbsp; &lt;/div&gt; 123 &lt;/td&gt; (Missing) &amp;nbsp; &lt;/div&gt; 2 &lt;/td&gt; &lt;/table&gt; &lt;a role=&quot;button&quot; data-toggle=&quot;collapse&quot; data-target=&quot;#freqtable-5492184930412029605, #minifreqtable-5492184930412029605&quot; aria-expanded=&quot;true&quot; aria-controls=&quot;collapseExample&quot;&gt; Toggle details &lt;/a&gt; Value Count Frequency (%) S 914 69.8% C 270 20.6% Q 123 9.4% (Missing) 2 0.2% &lt;div class=&quot;col-md-3 namecol&quot;&gt; &lt;p class=&quot;h4 pp-anchor&quot; id=&quot;pp_var_fare&quot;&gt;fare&lt;br/&gt; &lt;small&gt;Numeric&lt;/small&gt; &lt;/p&gt; &lt;/div&gt;&lt;div class=&quot;col-md-6&quot;&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm-6&quot;&gt; &lt;table class=&quot;stats &quot;&gt; &lt;tr&gt; &lt;th&gt;Distinct count&lt;/th&gt; &lt;td&gt;282&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Unique (%)&lt;/th&gt; &lt;td&gt;21.5%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Missing (%)&lt;/th&gt; &lt;td&gt;0.1%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Missing (n)&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Infinite (%)&lt;/th&gt; &lt;td&gt;0.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Infinite (n)&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-6&quot;&gt; &lt;table class=&quot;stats &quot;&gt; &lt;tr&gt; &lt;th&gt;Mean&lt;/th&gt; &lt;td&gt;33.295&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Minimum&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Maximum&lt;/th&gt; &lt;td&gt;512.33&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;alert&quot;&gt; &lt;th&gt;Zeros (%)&lt;/th&gt; &lt;td&gt;1.3%&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMgAAABLCAYAAAA1fMjoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD%2BnaQAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAARNJREFUeJzt3bEJAkEQQFEVS7IIezK2J4uwpzUX%2BejCeYe%2Bly9M8hkm2v0YY%2ByAlw5rDwBbdlx7gGeny%2B3jN/freYFJwAaBJBAIAoEgEAgCgSAQCAKBIBAIAoEgEAgCgSAQCAKBIBAIAoEgEAgCgSAQCAKBIBAIAoEgEAgCgSAQCAKBIBAIAoEgEAgCgSAQCAKBIBAIAoEgEAgCgSAQCAKBIBAIAoGwuX/SZ/hbnaXYIBAEAkEgEAQC4SeO9BkOe97xt4F8w0yEM4S7nP0YY6w9BGyVGwSCQCAIBIJAIAgEgkAgCASCQCAIBIJAIAgEgkAgCASCQCAIBIJAIAgEgkAgCASCQCAIBIJAIAgEgkAgCATCA5JpFJU50JYtAAAAAElFTkSuQmCC&quot;&gt; &lt;a role=&quot;button&quot; data-toggle=&quot;collapse&quot; data-target=&quot;#descriptives-1641287166913019484,#minihistogram-1641287166913019484&quot; aria-expanded=&quot;false&quot; aria-controls=&quot;collapseExample&quot;&gt; Toggle details &lt;/a&gt; &lt;ul class=&quot;nav nav-tabs&quot; role=&quot;tablist&quot;&gt; &lt;li role=&quot;presentation&quot; class=&quot;active&quot;&gt;&lt;a href=&quot;#quantiles-1641287166913019484&quot; aria-controls=&quot;quantiles-1641287166913019484&quot; role=&quot;tab&quot; data-toggle=&quot;tab&quot;&gt;Statistics&lt;/a&gt;&lt;/li&gt; &lt;li role=&quot;presentation&quot;&gt;&lt;a href=&quot;#histogram-1641287166913019484&quot; aria-controls=&quot;histogram-1641287166913019484&quot; role=&quot;tab&quot; data-toggle=&quot;tab&quot;&gt;Histogram&lt;/a&gt;&lt;/li&gt; &lt;li role=&quot;presentation&quot;&gt;&lt;a href=&quot;#common-1641287166913019484&quot; aria-controls=&quot;common-1641287166913019484&quot; role=&quot;tab&quot; data-toggle=&quot;tab&quot;&gt;Common Values&lt;/a&gt;&lt;/li&gt; &lt;li role=&quot;presentation&quot;&gt;&lt;a href=&quot;#extreme-1641287166913019484&quot; aria-controls=&quot;extreme-1641287166913019484&quot; role=&quot;tab&quot; data-toggle=&quot;tab&quot;&gt;Extreme Values&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;div class=&quot;tab-content&quot;&gt; &lt;div role=&quot;tabpanel&quot; class=&quot;tab-pane active row&quot; id=&quot;quantiles-1641287166913019484&quot;&gt; &lt;div class=&quot;col-md-4 col-md-offset-1&quot;&gt; &lt;p class=&quot;h4&quot;&gt;Quantile statistics&lt;/p&gt; &lt;table class=&quot;stats indent&quot;&gt; &lt;tr&gt; &lt;th&gt;Minimum&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;5-th percentile&lt;/th&gt; &lt;td&gt;7.225&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Q1&lt;/th&gt; &lt;td&gt;7.8958&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Median&lt;/th&gt; &lt;td&gt;14.454&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Q3&lt;/th&gt; &lt;td&gt;31.275&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;95-th percentile&lt;/th&gt; &lt;td&gt;133.65&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Maximum&lt;/th&gt; &lt;td&gt;512.33&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Range&lt;/th&gt; &lt;td&gt;512.33&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Interquartile range&lt;/th&gt; &lt;td&gt;23.379&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;div class=&quot;col-md-4 col-md-offset-2&quot;&gt; &lt;p class=&quot;h4&quot;&gt;Descriptive statistics&lt;/p&gt; &lt;table class=&quot;stats indent&quot;&gt; &lt;tr&gt; &lt;th&gt;Standard deviation&lt;/th&gt; &lt;td&gt;51.759&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Coef of variation&lt;/th&gt; &lt;td&gt;1.5545&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Kurtosis&lt;/th&gt; &lt;td&gt;27.028&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Mean&lt;/th&gt; &lt;td&gt;33.295&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;MAD&lt;/th&gt; &lt;td&gt;29.799&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;&quot;&gt; &lt;th&gt;Skewness&lt;/th&gt; &lt;td&gt;4.3677&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Sum&lt;/th&gt; &lt;td&gt;43550&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Variance&lt;/th&gt; &lt;td&gt;2679&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Memory size&lt;/th&gt; &lt;td&gt;10.3 KiB&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;div role=&quot;tabpanel&quot; class=&quot;tab-pane col-md-8 col-md-offset-2&quot; id=&quot;histogram-1641287166913019484&quot;&gt; &lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAlgAAAGQCAYAAAByNR6YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD%2BnaQAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xt01PWd//FXkjGJEHJDA128hAoBQiDcUyMSCAoKghrCJRwWswsKEmVB6GK5HKRSwgocVMJB0i5WCrvGUJebIPyUS6WkdVuBTsJl5bZiNpgIGcMlQZJ8f39Yph2CEuWT72Qmz8c5nAOfz8x839/XCZxXvjP5EmBZliUAAAAYE%2BjtAQAAAPwNBQsAAMAwChYAAIBhFCwAAADDKFgAAACGUbAAAAAMo2ABAAAYRsECAAAwjIIFAABgGAULAADAMAoWAACAYRQsAAAAwyhYAAAAhlGwAAAADKNgAQAAGEbBAgAAMIyCBQAAYBgFCwAAwDAKFgAAgGEULAAAAMMoWAAAAIZRsAAAAAyjYAEAABhGwQIAADCMggUAAGAYBQsAAMAwChYAAIBhFCwAAADDKFgAAACGUbAAAAAMo2ABAAAYRsECAAAwjIIFAABgGAULAADAMAoWAACAYRQsAAAAwyhYAAAAhlGwAAAADKNgAQAAGObw9gBNRVnZBeOvGRgYoOjo5jp//pJqay3jr98Ukal5ZGoemTYMcjWvMWR6550tvHJcrmD5sMDAAAUEBCgwMMDbo/gNMjWPTM0j04ZBruY15UwpWAAAAIZRsAAAAAyjYAEAABhGwQIAADCMggUAAGAYBQsAAMAwChYAAIBhFCwAAADDKFgAAACGUbAAAAAMo2ABAAAYRsECAAAwjIIFAABgmMPbA%2BDW9JrzvrdHqLft0x7w9ggAANiCK1gAAACGUbAAAAAMo2ABAAAYRsECAAAwjIIFAABgGAULAADAMAoWAACAYRQsAAAAwyhYAAAAhlGwAAAADKNgAQAAGEbBAgAAMIyCBQAAYBgFCwAAwDAKFgAAgGE%2BXbA%2B%2BugjJScna/r06XX2tm3bpmHDhql79%2B5KS0vTvn373Hu1tbVavny5Bg4cqN69e2vChAk6c%2BaMe9/lcmnatGlKTk5W3759NWfOHFVVVdlyTgAAwPf5bMH65S9/qYULF%2Bree%2B%2Bts3fkyBHNmjVLM2fO1B/%2B8AdlZmbqueee09mzZyVJ69ev15YtW5Sbm6vdu3crNjZWWVlZsixLkjRv3jxVVlZq69at%2Bu1vf6sTJ05o6dKltp4fAADwXT5bsEJCQrRhw4YbFqz8/HylpKQoJSVFISEhGj58uOLi4rR582ZJUl5enjIzM3XfffcpLCxM06dP14kTJ3To0CF9%2BeWX%2BuCDDzR9%2BnRFR0erVatWmjJlin7729/q6tWrdp8mAADwQQ5vD/BDjR8//lv3ioqKlJKS4rEWHx8vp9OpqqoqHT9%2BXPHx8e69sLAw3XvvvXI6nbpw4YKCgoLUoUMH937nzp11%2BfJlnTx50mP925SWlqqsrMxjzeFoppiYmPqeXr0EBflWP3Y4Gv%2B81zL1tWwbMzI1j0wbBrma15Qz9dmC9V1cLpciIiI81iIiInT8%2BHF99dVXsizrhvvl5eWKjIxUWFiYAgICPPYkqby8vF7Hz8vLU05OjsdaVlaWpk6d%2BkNOx29ERTX39gj1Fh5%2Bu7dH8Dtkah6ZNgxyNa8pZuqXBUuS%2B/NUP2T/Zs%2B9mdGjRys1NdVjzeFopvLyS7f0utfzte8ITJ9/QwgKClR4%2BO2qqKhUTU2tt8fxC2RqHpk2DHI1rzFk6q1v7v2yYEVFRcnlcnmsuVwuRUdHKzIyUoGBgTfcb9mypaKjo3Xx4kXV1NQoKCjIvSdJLVu2rNfxY2Ji6rwdWFZ2QdXVTfsvrC%2Bdf01NrU/N6wvI1DwybRjkal5TzNS3LoHUU0JCggoLCz3WnE6nEhMTFRISovbt26uoqMi9V1FRoc8%2B%2B0xdu3ZVp06dZFmWjh496vHc8PBwtW3b1rZzAAAAvssvC9aoUaO0f/9%2B7dmzR1euXNGGDRt0%2BvRpDR8%2BXJKUkZGhtWvX6sSJE7p48aKWLl2qTp06qUuXLoqOjtbgwYP16quv6vz58zp79qxWrlyp9PR0ORx%2BecEPAAAY5rONoUuXLpKk6upqSdIHH3wg6ZurTXFxcVq6dKmys7NVXFysdu3aafXq1brzzjslSWPGjFFZWZn%2B8R//UZcuXVJSUpLHh9J//vOfa/78%2BRo4cKBuu%2B02PfbYYze8mSkAAMCNBFi3%2Bolu1EtZ2QXjr%2BlwBOrhpR8Zf92Gsn3aA94e4aYcjkBFRTVXefmlJvd5gYZCpuaRacMgV/MaQ6Z33tnCK8f1y7cIAQAAvImCBQAAYBgFCwAAwDAKFgAAgGEULAAAAMMoWAAAAIZRsAAAAAyjYAEAABhGwQIAADCMggUAAGAYBQsAAMAwChYAAIBhFCwAAADDKFgAAACGUbAAAAAMo2ABAAAYRsECAAAwjIIFAABgGAULAADAMAoWAACAYRQsAAAAwyhYAAAAhlGwAAAADKNgAQAAGEbBAgAAMIyCBQAAYBgFCwAAwDAKFgAAgGEULAAAAMMoWAAAAIZRsAAAAAyjYAEAABhGwQIAADCMggUAAGAYBQsAAMAwChYAAIBhFCwAAADDKFgAAACGUbAAAAAMo2ABAAAYRsECAAAwjIIFAABgGAULAADAMAoWAACAYRQsAAAAwyhYAAAAhvltwTp8%2BLDGjx%2BvXr166YEHHtDMmTN1/vx5SVJBQYHS09PVo0cPDR06VJs3b/Z47tq1azV48GD16NFDGRkZKiws9MYpAAAAH%2BWXBau6ulrPPPOMunXrpv3792vr1q06f/68XnrpJZWWlmrKlCkaM2aMCgoKNGfOHM2bN09Op1OStGvXLq1YsUKvvPKK9u/frwEDBmjy5Mm6fPmyl88KAAD4Cr8sWGVlZSorK9Pjjz%2Bu4OBgRUVF6eGHH9aRI0e0ZcsWxcbGKj09XSEhIUpOTlZqaqry8/MlSXl5eUpLS1NiYqJCQ0M1ceJESdLu3bu9eUoAAMCHOLw9QENo1aqVOnXqpLy8PP3Lv/yLqqqqtHPnTvXv319FRUWKj4/3eHx8fLy2b98uSSoqKtKQIUPce4GBgerUqZOcTqeGDh1ar%2BOXlpaqrKzMY83haKaYmJhbPDNPQUG%2B1Y8djsY/77VMfS3bxoxMzSPThkGu5jXlTP2yYAUGBmrFihXKzMzUW2%2B9JUnq06ePZsyYoSlTpqhVq1Yej4%2BMjFR5ebkkyeVyKSIiwmM/IiLCvV8feXl5ysnJ8VjLysrS1KlTf8jp%2BI2oqObeHqHewsNv9/YIfodMzSPThkGu5jXFTP2yYH399deaPHmyHnnkEffnpxYsWKCZM2fW6/mWZd3S8UePHq3U1FSPNYejmcrLL93S617P174jMH3%2BDSEoKFDh4beroqJSNTW13h7HL5CpeWTaMMjVvMaQqbe%2BuffLglVQUKDPP/9cL7zwgoKCgtSiRQtNnTpVjz/%2BuB588EG5XC6Px5eXlys6OlqSFBUVVWff5XKpffv29T5%2BTExMnbcDy8ouqLq6af%2BF9aXzr6mp9al5fQGZmkemDYNczWuKmfrWJZB6qqmpUW1trceVqK%2B//lqSlJycXOe2C4WFhUpMTJQkJSQkqKioyOO1Dh8%2B7N4HAAC4Gb8sWN27d1ezZs20YsUKVVZWqry8XKtWrVLv3r31%2BOOPq7i4WPn5%2Bbpy5Yr27t2rvXv3atSoUZKkjIwMbdy4UQcPHlRlZaVWrVql4OBg9e/f37snBQAAfIZfFqyoqCj9%2B7//uz755BP169dPjz32mEJDQ7Vs2TK1bNlSq1ev1rp169SzZ08tWrRIS5YsUceOHSVJ/fr10wsvvKBp06apT58%2B2r9/v3JzcxUaGurlswIAAL4iwLrVT3SjXsrKLhh/TYcjUA8v/cj46zaU7dMe8PYIN%2BVwBCoqqrnKyy81uc8LNBQyNY9MGwa5mtcYMr3zzhZeOa5fXsECAADwJgoWAACAYRQsAAAAwyhYAAAAhlGwAAAADKNgAQAAGEbBAgAAMIyCBQAAYBgFCwAAwDAKFgAAgGEULAAAAMMoWAAAAIZRsAAAAAyjYAEAABhGwQIAADCMggUAAGAYBQsAAMAwChYAAIBhFCwAAADDKFgAAACGUbAAAAAMo2ABAAAYRsECAAAwjIIFAABgGAULAADAMAoWAACAYRQsAAAAwyhYAAAAhtlesFJTU5WTk6OSkhK7Dw0AAGAL2wvWiBEjtG3bNj300EOaOHGidu7cqerqarvHAAAAaDC2F6ysrCxt27ZN77zzjtq3b69FixYpJSVFS5Ys0alTp%2BweBwAAwDivfQarc%2BfOmjVrlnbv3q3Zs2frnXfe0ZAhQzRhwgT95S9/8dZYAAAAt8xrBevq1avatm2bnn76ac2aNUutWrXSz372M3Xq1EmZmZnasmWLt0YDAAC4JQ67D3jixAlt2LBBGzdu1KVLlzR48GC99dZb6tmzp/sxvXv31ksvvaRhw4bZPR4AAMAts71gDR06VG3bttWkSZP0xBNPKDIyss5jUlJSdP78ebtHAwAAMML2grV27Vr16dPnpo87dOiQDdMAAACYZ/tnsDp06KDJkyfrgw8%2BcK/9%2Bte/1tNPPy2Xy2X3OAAAAMbZXrCys7N14cIFtWvXzr3Wv39/1dbWavHixXaPAwAAYJztbxHu27dPW7ZsUVRUlHstNjZWS5cu1WOPPWb3OAAAAMbZfgWrqqpKISEhdQcJDFRlZaXd4wAAABhne8Hq3bu3Fi9erK%2B%2B%2Bsq99sUXX2jBggUet2oAAADwVba/RTh79mz98z//s%2B6//36FhYWptrZWly5d0t13363f/OY3do8DAABgnO0F6%2B6779Z7772n3/3ud/rss88UGBiotm3bqm/fvgoKCrJ7HAAAAONsL1iSFBwcrIceesgbhwYAAGhwthesM2fOaNmyZfr0009VVVVVZ//DDz%2B0eyQAAACjvPIZrNLSUvXt21fNmjWz%2B/AAAAANzvaCVVhYqA8//FDR0dENfqxVq1Zp/fr1unjxorp166aFCxfqrrvuUkFBgZYtW6aTJ0/qRz/6kSZNmqThw4e7n7d27VqtX79eZWVl6tChg%2BbMmaOEhIQGnxcAAPgH22/T0LJlS1uuXK1fv16bN2/W2rVrtW/fPrVr106//vWvVVpaqilTpmjMmDEqKCjQnDlzNG/ePDmdTknSrl27tGLFCr3yyivav3%2B/BgwYoMmTJ%2Bvy5csNPjMAAPAPthesSZMmKScnR5ZlNehx1qxZo%2BnTp%2BvHP/6xwsLCNHfuXM2dO1dbtmxRbGys0tPTFRISouTkZKWmpio/P1%2BSlJeXp7S0NCUmJio0NFQTJ06UJO3evbtB5wUAAP7D9rcIf/e73%2BmTTz7Ru%2B%2B%2Bq7vuukuBgZ4d7%2B23377lY3zxxRf6/PPP9dVXX2nIkCE6d%2B6ckpKS9NJLL6moqEjx8fEej4%2BPj9f27dslSUVFRRoyZIh7LzAwUJ06dZLT6dTQoUPrdfzS0lKVlZV5rDkczRQTE3OLZ%2BYpKMj2fnxLHI7GP%2B%2B1TH0t28aMTM0j04ZBruY15UxtL1hhYWHq169fgx7j7NmzkqT3339fb775pizL0tSpUzV37lxVVVWpVatWHo%2BPjIxUeXm5JMnlcikiIsJjPyIiwr1fH3l5ecrJyfFYy8rK0tSpU3/I6fiNqKjm3h6h3sLDb/f2CH6HTM0j04ZBruY1xUxtL1jZ2dkNfoxrbz9OnDjRXaaef/55Pf3000pOTq7383%2Bo0aNHKzU11WPN4Wim8vJLt/S61/O17whMn39DCAoKVHj47aqoqFRNTa23x/ELZGoemTYMcjWvMWTqrW/uvXKj0ZMnT%2Bq9997T//3f/7kL14EDB9S9e3cjr3/HHXdIksLDw91rbdq0kWVZunr1qlwul8fjy8vL3T/VGBUVVWff5XKpffv29T5%2BTExMnbcDy8ouqLq6af%2BF9aXzr6mp9al5fQGZmkemDYNczWuKmdp%2BCaSgoEDDhw/Xzp07tXXrVknf3Hx0/Pjxxm4y2rp1a4WFhenIkSPuteLiYt12221KSUlRYWGhx%2BMLCwuVmJgoSUpISFBRUZF7r6amRocPH3bvAwAA3IztBWv58uX66U9/qi1btiggIEDSN/8/4eLFi7Vy5Uojx3A4HEpPT9cbb7yh//3f/9W5c%2Be0cuVKDRs2TE8%2B%2BaSKi4uVn5%2BvK1euaO/evdq7d69GjRolScrIyNDGjRt18OBBVVZWatWqVQoODlb//v2NzAYAAPyf7W8R/s///I/WrVsnSe6CJUmPPPKIZs%2Bebew4M2bM0Ndff62RI0fq6tWrGjx4sObOnavmzZtr9erVWrhwoRYsWKA2bdpoyZIl6tixoySpX79%2BeuGFFzRt2jSdO3dOXbp0UW5urkJDQ43NBgAA/JvtBatFixaqqqpScHCwx3ppaWmdtVsRHBys%2BfPna/78%2BXX2evfurU2bNn3rc8eOHauxY8camwUAADQttr9F2KNHDy1atEgXL150r506dUqzZs3S/fffb/c4AAAAxtl%2BBetnP/uZnnrqKSUlJammpkY9evRQZWWl2rdvr8WLF9s9DgAAgHG2F6zWrVtr69at2rt3r06dOqXQ0FC1bdtWDzzwgMdnsgAAAHyVV%2B6Dddttt%2Bmhhx7yxqEBAAAanO0FKzU19TuvVJm6FxYAAIC32F6whgwZ4lGwampqdOrUKTmdTj311FN2jwMAAGCc7QVr5syZN1zfsWOH/vjHP9o8DQAAgHmN5n8Lfuihh/Tee%2B95ewwAAIBb1mgK1uHDh2VZlrfHAAAAuGW2v0U4ZsyYOmuVlZU6ceKEBg0aZPc4AAAAxtlesGJjY%2Bv8FGFISIjS09M1cuRIu8cBAAAwzvaCxd3aAQCAv7O9YG3cuLHej33iiScacBIAAICGYXvBmjNnjmpra%2Bt8oD0gIMBjLSAggIIFAAB8ku0F61e/%2BpXWrFmjyZMnq0OHDrIsS8eOHdMvf/lLjRs3TklJSXaPBAAAYJRXPoOVm5urVq1audd69eqlu%2B%2B%2BWxMmTNDWrVvtHgkAAMAo2%2B%2BDdfr0aUVERNRZDw8PV3Fxsd3jAAAAGGd7wWrTpo0WL16s8vJy91pFRYWWLVume%2B65x%2B5xAAAAjLP9LcLZs2drxowZysvLU/PmzRUYGKiLFy8qNDRUK1eutHscAAAA42wvWH379tWePXu0d%2B9enT17VpZlqVWrVnrwwQfVokULu8cBAAAwzvaCJUm33367Bg4cqLNnz%2Bruu%2B/2xggAAAANxvbPYFVVVWnWrFnq3r27Hn30UUnffAZr4sSJqqiosHscAAAA42wvWEuWLNGRI0e0dOlSBQb%2B7fA1NTVaunSp3eMAAAAYZ3vB2rFjh15//XU98sgj7v/0OTw8XNnZ2dq5c6fd4wAAABhne8G6dOmSYmNj66xHR0fr8uXLdo8DAABgnO0F65577tEf//hHSfL4vwfff/99/cM//IPd4wAAABhn%2B08Rjh07Vs8//7xGjBih2tpavfnmmyosLNSOHTs0Z84cu8cBAAAwzvaCNXr0aDkcDq1bt05BQUF644031LZtWy1dulSPPPKI3eMAAAAYZ3vBOn/%2BvEaMGKERI0bYfWgAAABb2P4ZrIEDB3p89goAAMDf2F6wkpKStH37drsPCwAAYBvb3yL80Y9%2BpF/84hfKzc3VPffco9tuu81jf9myZXaPBAAAYJTtBev48eP68Y9/LEkqLy%2B3%2B/AAAAANzraCNX36dC1fvly/%2Bc1v3GsrV65UVlaWXSMAAADYwrbPYO3atavOWm5url2HBwAAsI1tBetGPznITxMCAAB/ZFvBuvYfO99sDQAAwNfZfpsGAAAAf0fBAgAAMMy2nyK8evWqZsyYcdM17oMFAAB8nW0Fq2fPniotLb3pGgAAgK%2BzrWD9/f2vAAAA/BmfwQIAADCMggUAAGAYBQsAAMAwChYAAIBhTaJgLVq0SB06dHD/uaCgQOnp6erRo4eGDh2qzZs3ezx%2B7dq1Gjx4sHr06KGMjAwVFhbaPTIAAPBhfl%2Bwjhw5ok2bNrn/XFpaqilTpmjMmDEqKCjQnDlzNG/ePDmdTknf/KfUK1as0CuvvKL9%2B/drwIABmjx5si5fvuytUwAAAD7GrwtWbW2t5s%2Bfr8zMTPfali1bFBsbq/T0dIWEhCg5OVmpqanKz8%2BXJOXl5SktLU2JiYkKDQ3VxIkTJUm7d%2B/2xikAAAAfZNt9sLzh7bffVkhIiIYNG6ZXX31VklRUVKT4%2BHiPx8XHx2v79u3u/SFDhrj3AgMD1alTJzmdTg0dOrRexy0tLVVZWZnHmsPRTDExMbdyOnUEBflWP3Y4Gv%2B81zL1tWwbMzI1j0wbBrma15Qz9duC9eWXX2rFihV1bnDqcrnUqlUrj7XIyEiVl5e79yMiIjz2IyIi3Pv1kZeXp5ycHI%2B1rKwsTZ069fucgt%2BJimru7RHqLTz8dm%2BP4HfI1DwybRjkal5TzNRvC1Z2drbS0tLUrl07ff7559/ruZZl3dKxR48erdTUVI81h6OZyssv3dLrXs/XviMwff4NISgoUOHht6uiolI1NbXeHscvkKl5ZNowyNW8xpCpt76598uCVVBQoAMHDmjr1q119qKiouRyuTzWysvLFR0d/a37LpdL7du3r/fxY2Ji6rwdWFZ2QdXVTfsvrC%2Bdf01NrU/N6wvI1DwybRjkal5TzNS3LoHU0%2BbNm3Xu3DkNGDBASUlJSktLkyQlJSUpLi6uzm0XCgsLlZiYKElKSEhQUVGRe6%2BmpkaHDx927wMAANyMXxasF198UTt27NCmTZu0adMm5ebmSpI2bdqkYcOGqbi4WPn5%2Bbpy5Yr27t2rvXv3atSoUZKkjIwMbdy4UQcPHlRlZaVWrVql4OBg9e/f34tnBAAAfIlfvkUYERHh8UH16upqSVLr1q0lSatXr9bChQu1YMECtWnTRkuWLFHHjh0lSf369dMLL7ygadOm6dy5c%2BrSpYtyc3MVGhpq/4kAAACf5JcF63p33XWXjh075v5z7969PW4%2Ber2xY8dq7NixdowGAAD8kF%2B%2BRQgAAOBNFCwAAADDKFgAAACGUbAAAAAMo2ABAAAYRsECAAAwjIIFAABgGAULAADAMAoWAACAYRQsAAAAwyhYAAAAhlGwAAAADKNgAQAAGEbBAgAAMIyCBQAAYBgFCwAAwDAKFgAAgGEULAAAAMMoWAAAAIZRsAAAAAyjYAEAABhGwQIAADCMggUAAGAYBQsAAMAwChYAAIBhFCwAAADDKFgAAACGUbAAAAAMo2ABAAAYRsECAAAwjIIFAABgGAULAADAMAoWAACAYRQsAAAAwyhYAAAAhlGwAAAADKNgAQAAGEbBAgAAMIyCBQAAYBgFCwAAwDAKFgAAgGEULAAAAMMoWAAAAIZRsAAAAAyjYAEAABhGwQIAADDMbwtWcXGxsrKylJSUpOTkZL344ouqqKiQJB05ckTjxo1Tz549NWjQIK1Zs8bjudu2bdOwYcPUvXt3paWlad%2B%2Bfd44BQAA4KP8tmBNnjxZ4eHh2rVrl9599119%2Bumn%2Brd/%2BzdVVVVp0qRJ%2BslPfqKPPvpIy5cv1%2BrVq7Vz505J35SvWbNmaebMmfrDH/6gzMxMPffcczp79qyXzwgAAPgKvyxYFRUVSkhI0IwZM9S8eXO1bt1aTz75pP70pz9pz549unr1qp599lk1a9ZMnTt31siRI5WXlydJys/PV0pKilJSUhQSEqLhw4crLi5Omzdv9vJZAQAAX%2BHw9gANITw8XNnZ2R5rJSUliomJUVFRkTp06KCgoCD3Xnx8vPLz8yVJRUVFSklJ8XhufHy8nE5nvY9fWlqqsrIyjzWHo5liYmK%2B76l8p6Ag3%2BrHDkfjn/dapr6WbWNGpuaRacMgV/OacqZ%2BWbCu53Q6tW7dOq1atUrbt29XeHi4x35kZKRcLpdqa2vlcrkUERHhsR8REaHjx4/X%2B3h5eXnKycnxWMvKytLUqVN/%2BEn4gaio5t4eod7Cw2/39gh%2Bh0zNI9OGQa7mNcVM/b5g/fnPf9azzz6rGTNmKDk5Wdu3b7/h4wICAty/tyzrlo45evRopaameqw5HM1UXn7pll73er72HYHp828IQUGBCg%2B/XRUVlaqpqfX2OH6BTM0j04ZBruY1hky99c29XxesXbt26ac//anmzZunJ554QpIUHR2t06dPezzO5XIpMjJSgYGBioqKksvlqrMfHR1d7%2BPGxMTUeTuwrOyCqqub9l9YXzr/mppan5rXF5CpeWTaMMjVvKaYqW9dAvkePvnkE82aNUuvvfaau1xJUkJCgo4dO6bq6mr3mtPpVGJionu/sLDQ47X%2Bfh8AAOBm/LJgVVdXa%2B7cuZo5c6b69u3rsZeSkqKwsDCtWrVKlZWVOnTokDZs2KCMjAxJ0qhRo7R//37t2bNHV65c0YYNG3T69GkNHz7cG6cCAAB8kF%2B%2BRXjw4EGdOHFCCxcu1MKFCz323n//fb3xxhuaP3%2B%2BcnNzdccdd2j69Onq37%2B/JCkuLk5Lly5Vdna2iouL1a5dO61evVp33nmnF84EAAD4Ir8sWL169dKxY8e%2B8zH/%2BZ//%2Ba17gwYN0qBBg0yPBQAAmgi/fIsQAADAmyhYAAAAhlGwAAAADKNgAQAAGEbBAgAAMIyCBQAAYJhf3qYBjdOjr/7e2yN8L9unPeDtEQAAPoorWAAAAIZRsAAAAAyjYAEAABhGwQIAADCMggUAAGAYBQsAAMAwChYAAIBhFCwAAADDKFgAAACGUbAAAAAMo2ABAAAYRsECAAAwjIIFAABgGAULAADAMAoWAACAYRQsAAAAwyhYAAAAhlGwAAAADKNgAQAAGEbBAgAAMIyCBQAAYBgFCwAAwDAKFgAAgGEULAAAAMMoWAAAAIZRsAAAAAyjYAEAABhGwQIAADCMggUAAGAYBQsAAMAwChYAAIBhDm8PADRWj776e2%2BP8L1sn/aAt0cAAPwVV7AAAAAMo2ABAAAYRsECAAAwjIIFAABgGAULAADAMH6KEIDt%2BAlNAP7rJ0iQAAAK7UlEQVSOK1gAAACGcQXrBoqLi7VgwQIdOnRIzZo105AhQzRjxgwFBtJHgaaIK24Avi8K1g08//zz6ty5sz744AOdO3dOkyZN0h133KF/%2Bqd/8vZoAADAB3BJ5jpOp1NHjx7VzJkz1aJFC8XGxiozM1N5eXneHg0AAPgIrmBdp6ioSG3atFFERIR7rXPnzjp16pQuXryosLCwm75GaWmpysrKPNYcjmaKiYkxOmtQEP0Yf%2BNrb2Oh4Tgc/NvwQ1z7N9Wf/m19eOlH3h6h3v7fzAe9PYJRFKzruFwuhYeHe6xdK1vl5eX1Klh5eXnKycnxWHvuuef0/PPPmxtU3xS5p1p/qtGjRxsvb01VaWmp8vLyyNQgMjWPTBtGaWmp3nrrV36V659%2B8YhXj9%2BUv1b9p6YbZFnWLT1/9OjRevfddz1%2BjR492tB0f1NWVqacnJw6V8vww5GpeWRqHpk2DHI1rylnyhWs60RHR8vlcnmsuVwuBQQEKDo6ul6vERMT0%2BSaOgAA%2BBuuYF0nISFBJSUlOn/%2BvHvN6XSqXbt2at68uRcnAwAAvoKCdZ34%2BHh16dJFy5Yt08WLF3XixAm9%2BeabysjI8PZoAADARwS99NJLL3l7iMbmwQcf1NatW/Xyyy/rvffeU3p6uiZMmKCAgABvj1ZH8%2BbN1adPH66uGUSm5pGpeWTaMMjVvKaaaYB1q5/oBgAAgAfeIgQAADCMggUAAGAYBQsAAMAwChYAAIBhFCwAAADDKFgAAACGUbAAAAAMo2ABAAAYRsECAAAwjILlg4qLi/XMM88oKSlJAwYM0JIlS1RbW%2BvtsRq9jz76SMnJyZo%2BfXqdvW3btmnYsGHq3r270tLStG/fPvdebW2tli9froEDB6p3796aMGGCzpw5Y%2BfojVZxcbGysrKUlJSk5ORkvfjii6qoqJAkHTlyROPGjVPPnj01aNAgrVmzxuO535V5U3b06FE99dRT6tmzp5KTkzVt2jSVlZVJkgoKCpSenq4ePXpo6NCh2rx5s8dz165dq8GDB6tHjx7KyMhQYWGhN06hUVu0aJE6dOjg/jOZ/nAdOnRQQkKCunTp4v718ssvSyJXSZIFn/Pkk09ac%2BfOtSoqKqxTp05ZgwYNstasWePtsRq13Nxca9CgQdaYMWOsadOmeewdPnzYSkhIsPbs2WNVVVVZmzZtshITE62SkhLLsixr7dq11oABA6zjx49bFy5csH7%2B859bw4YNs2pra71xKo3KY489Zr344ovWxYsXrZKSEistLc2aPXu2VVlZaT344IPWihUrrEuXLlmFhYVWnz59rB07dliWdfPMm6orV65Y999/v5WTk2NduXLFOnfunDVu3DhrypQp1hdffGF169bNys/Pt6qqqqzf//73VteuXa2//OUvlmVZ1ocffmj16tXLOnjwoFVZWWmtXr3aeuCBB6xLly55%2Bawaj8OHD1t9%2BvSx4uLiLMuyyPQWxcXFWWfOnKmzTq7f4AqWj3E6nTp69KhmzpypFi1aKDY2VpmZmcrLy/P2aI1aSEiINmzYoHvvvbfOXn5%2BvlJSUpSSkqKQkBANHz5ccXFx7u%2B48vLylJmZqfvuu09hYWGaPn26Tpw4oUOHDtl9Go1KRUWFEhISNGPGDDVv3lytW7fWk08%2BqT/96U/as2ePrl69qmeffVbNmjVT586dNXLkSPfX6c0yb6oqKys1ffp0TZo0ScHBwYqOjtbDDz%2BsTz/9VFu2bFFsbKzS09MVEhKi5ORkpaamKj8/X9I3X6dpaWlKTExUaGioJk6cKEnavXu3N0%2Bp0aitrdX8%2BfOVmZnpXiPThkGu36Bg%2BZiioiK1adNGERER7rXOnTvr1KlTunjxohcna9zGjx%2BvFi1a3HCvqKhI8fHxHmvx8fFyOp2qqqrS8ePHPfbDwsJ07733yul0NujMjV14eLiys7N1xx13uNdKSkoUExOjoqIidejQQUFBQe69%2BPh499sA35V5UxYREaGRI0fK4XBIkk6ePKn/%2Bq//0qOPPvqtmX1bpoGBgerUqVOTz/Sat99%2BWyEhIRo2bJh7jUxv3bJly9S/f3/16tVL8%2BbN06VLl8j1ryhYPsblcik8PNxj7VrZKi8v98ZIPs/lcnkUVumbTMvLy/XVV1/Jsqxv3cffOJ1OrVu3Ts8%2B%2B%2BwNv04jIyPlcrlUW1v7nZnjm8%2B2JSQkaMiQIerSpYumTp36rZley4xMv92XX36pFStWaP78%2BR7rZHprunXrpuTkZO3cuVN5eXk6ePCgFixYQK5/RcHyQZZleXsEv3OzTMn8u/35z3/WhAkTNGPGDCUnJ3/r4wICAty/J9Nv16ZNGzmdTr3//vs6ffq0/vVf/7VezyPTG8vOzlZaWpratWv3vZ9Lpt8uLy9PI0eOVHBwsO677z7NnDlTW7du1dWrV2/63KaQKwXLx0RHR8vlcnmsuVwuBQQEKDo62ktT%2BbaoqKgbZhodHa3IyEgFBgbecL9ly5Z2jtlo7dq1S88884xmz56t8ePHS/rm6/T670ZdLpc7z%2B/KHN8ICAhQbGyspk%2Bfrq1bt8rhcNTJrLy83J0Zmd5YQUGBDhw4oKysrDp7N8qMTH%2B4u%2B66SzU1NTf8N7Mp5krB8jEJCQkqKSnR%2BfPn3WtOp1Pt2rVT8%2BbNvTiZ70pISKjzI8JOp1OJiYkKCQlR%2B/btVVRU5N6rqKjQZ599pq5du9o9aqPzySefaNasWXrttdf0xBNPuNcTEhJ07NgxVVdXu9euZXpt/9syb8oKCgo0ePBgj9uuBAZ%2B8890165d62RWWFjokenff53W1NTo8OHDTT7TzZs369y5cxowYICSkpKUlpYmSUpKSlJcXByZ/kCHDx/W4sWLPdZOnDih4OBgpaSkkKvEbRp80ciRI63Zs2dbFy5csI4fP26lpqZa69at8/ZYPmHWrFl1btNw7Ngxq0uXLtbu3butqqoqKz8/3%2BrevbtVWlpqWZZl/cd//IfVv39/920a5s2bZ40YMcIb4zcqV69etR599FHr7bffrrN35coVa8CAAdbrr79uXb582Tp48KDVq1cva/fu3ZZl3TzzpqqiosJKTk62Fi9ebF2%2BfNk6d%2B6cNWHCBGvs2LHWl19%2BaXXv3t165513rKqqKmvPnj1W165drSNHjliWZVl79%2B61evbsaR04cMC6fPmytWLFCislJcWqrKz08ll5l8vlskpKSty/Dhw4YMXFxVklJSVWcXExmf5AZ8%2Betbp162atXr3aunLlinXy5ElryJAh1ssvv8zX6l9RsHxQSUmJNXHiRKtr165WcnKy9frrr3NPpptISEiwEhISrI4dO1odO3Z0//maHTt2WIMGDbI6d%2B5sPf7449bHH3/s3qutrbVee%2B016/7777e6du1qPf30003%2Bfk2WZVn//d//bcXFxbmz/Ptfn3/%2BuXXs2DFrzJgxVkJCgtW/f39r/fr1Hs//rsybsqNHj1rjxo2zunbtav3kJz%2Bxpk2bZp09e9ayLMv6%2BOOPreHDh1udO3e2Bg0a5L6v2DXr16%2B3UlJSrISEBCsjI8M6duyYN06hUTtz5oz7PliWRaa34uOPP7ZGjx5tdevWzerTp4%2BVnZ1tVVVVufeaeq4BltUEPmkGAABgIz6DBQAAYBgFCwAAwDAKFgAAgGEULAAAAMMoWAAAAIZRsAAAAAyjYAEAABhGwQIAADCMggUAAGAYBQsAAMAwChYAAIBhFCwAAADDKFgAAACG/X%2B6YhQJN6qbLgAAAABJRU5ErkJggg%3D%3D&quot;/&gt; &lt;/div&gt; &lt;div role=&quot;tabpanel&quot; class=&quot;tab-pane col-md-12&quot; id=&quot;common-1641287166913019484&quot;&gt; Value Count Frequency (%) 8.05 60 4.6% 13.0 59 4.5% 7.75 55 4.2% 26.0 50 3.8% 7.8958 49 3.7% 10.5 35 2.7% 7.775 26 2.0% 7.2292 24 1.8% 7.925 23 1.8% 26.55 22 1.7% Other values (271) 905 69.1% &lt;/div&gt; &lt;div role=&quot;tabpanel&quot; class=&quot;tab-pane col-md-12&quot; id=&quot;extreme-1641287166913019484&quot;&gt; &lt;p class=&quot;h4&quot;&gt;Minimum 5 values&lt;/p&gt; Value Count Frequency (%) 0.0 17 1.3% 3.1708 1 0.1% 4.0125 1 0.1% 5.0 1 0.1% 6.2375 1 0.1% &lt;p class=&quot;h4&quot;&gt;Maximum 5 values&lt;/p&gt; Value Count Frequency (%) 227.525 5 0.4% 247.5208 3 0.2% 262.375 7 0.5% 263.0 6 0.5% 512.3292 4 0.3% &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;col-md-3 namecol&quot;&gt; &lt;p class=&quot;h4 pp-anchor&quot; id=&quot;pp_var_home.dest&quot;&gt;home.dest&lt;br/&gt; &lt;small&gt;Categorical&lt;/small&gt; &lt;/p&gt; &lt;/div&gt;&lt;div class=&quot;col-md-3&quot;&gt; &lt;table class=&quot;stats &quot;&gt; &lt;tr class=&quot;alert&quot;&gt; &lt;th&gt;Distinct count&lt;/th&gt; &lt;td&gt;370&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Unique (%)&lt;/th&gt; &lt;td&gt;28.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;alert&quot;&gt; &lt;th&gt;Missing (%)&lt;/th&gt; &lt;td&gt;43.1%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;alert&quot;&gt; &lt;th&gt;Missing (n)&lt;/th&gt; &lt;td&gt;564&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;table class=&quot;mini freq&quot;&gt; &lt;tr class=&quot;&quot;&gt; &lt;th&gt;New York, NY&lt;/th&gt; &lt;td&gt; &lt;div class=&quot;bar&quot; style=&quot;width:10%&quot; data-toggle=&quot;tooltip&quot; data-placement=&quot;right&quot; data-html=&quot;true&quot; data-delay=500 title=&quot;Percentage: 4.9%&quot;&gt; &amp;nbsp; &lt;/div&gt; 64 &lt;/td&gt; London &amp;nbsp; &lt;/div&gt; 14 &lt;/td&gt; Montreal, PQ &amp;nbsp; &lt;/div&gt; 10 &lt;/td&gt; Other values (366) 657 &lt;/div&gt; &lt;/td&gt; (Missing) 564 &lt;/div&gt; &lt;/td&gt; &lt;/table&gt; &lt;a role=&quot;button&quot; data-toggle=&quot;collapse&quot; data-target=&quot;#freqtable4622614539229109798, #minifreqtable4622614539229109798&quot; aria-expanded=&quot;true&quot; aria-controls=&quot;collapseExample&quot;&gt; Toggle details &lt;/a&gt; Value Count Frequency (%) New York, NY 64 4.9% London 14 1.1% Montreal, PQ 10 0.8% Cornwall / Akron, OH 9 0.7% Paris, France 9 0.7% Philadelphia, PA 8 0.6% Wiltshire, England Niagara Falls, NY 8 0.6% Winnipeg, MB 8 0.6% Brooklyn, NY 7 0.5% Belfast 7 0.5% Other values (359) 601 45.9% (Missing) 564 43.1% &lt;div class=&quot;col-md-3 namecol&quot;&gt; &lt;p class=&quot;h4 pp-anchor&quot; id=&quot;pp_var_name&quot;&gt;name&lt;br/&gt; &lt;small&gt;Categorical&lt;/small&gt; &lt;/p&gt; &lt;/div&gt;&lt;div class=&quot;col-md-3&quot;&gt; &lt;table class=&quot;stats &quot;&gt; &lt;tr class=&quot;alert&quot;&gt; &lt;th&gt;Distinct count&lt;/th&gt; &lt;td&gt;1307&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Unique (%)&lt;/th&gt; &lt;td&gt;99.8%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Missing (%)&lt;/th&gt; &lt;td&gt;0.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Missing (n)&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;table class=&quot;mini freq&quot;&gt; &lt;tr class=&quot;&quot;&gt; &lt;th&gt;Connolly, Miss. Kate&lt;/th&gt; &lt;td&gt; &lt;div class=&quot;bar&quot; style=&quot;width:1%&quot; data-toggle=&quot;tooltip&quot; data-placement=&quot;right&quot; data-html=&quot;true&quot; data-delay=500 title=&quot;Percentage: 0.2%&quot;&gt; &amp;nbsp; &lt;/div&gt; 2 &lt;/td&gt; Kelly, Mr. James &amp;nbsp; &lt;/div&gt; 2 &lt;/td&gt; Risien, Mrs. Samuel (Emma) &amp;nbsp; &lt;/div&gt; 1 &lt;/td&gt; Other values (1304) 1304 &lt;/div&gt; &lt;/td&gt; &lt;/table&gt; &lt;a role=&quot;button&quot; data-toggle=&quot;collapse&quot; data-target=&quot;#freqtable2773139842415725495, #minifreqtable2773139842415725495&quot; aria-expanded=&quot;true&quot; aria-controls=&quot;collapseExample&quot;&gt; Toggle details &lt;/a&gt; Value Count Frequency (%) Connolly, Miss. Kate 2 0.2% Kelly, Mr. James 2 0.2% Risien, Mrs. Samuel (Emma) 1 0.1% Ilmakangas, Miss. Ida Livija 1 0.1% Ilett, Miss. Bertha 1 0.1% Douglas, Mr. Walter Donald 1 0.1% Coleff, Mr. Satio 1 0.1% Peters, Miss. Katie 1 0.1% Christmann, Mr. Emil 1 0.1% Hellstrom, Miss. Hilda Maria 1 0.1% Other values (1297) 1297 99.1% &lt;div class=&quot;col-md-3 namecol&quot;&gt; &lt;p class=&quot;h4 pp-anchor&quot; id=&quot;pp_var_parch&quot;&gt;parch&lt;br/&gt; &lt;small&gt;Numeric&lt;/small&gt; &lt;/p&gt; &lt;/div&gt;&lt;div class=&quot;col-md-6&quot;&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm-6&quot;&gt; &lt;table class=&quot;stats &quot;&gt; &lt;tr&gt; &lt;th&gt;Distinct count&lt;/th&gt; &lt;td&gt;8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Unique (%)&lt;/th&gt; &lt;td&gt;0.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Missing (%)&lt;/th&gt; &lt;td&gt;0.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Missing (n)&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Infinite (%)&lt;/th&gt; &lt;td&gt;0.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Infinite (n)&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-6&quot;&gt; &lt;table class=&quot;stats &quot;&gt; &lt;tr&gt; &lt;th&gt;Mean&lt;/th&gt; &lt;td&gt;0.38503&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Minimum&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Maximum&lt;/th&gt; &lt;td&gt;9&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;alert&quot;&gt; &lt;th&gt;Zeros (%)&lt;/th&gt; &lt;td&gt;76.5%&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMgAAABLCAYAAAA1fMjoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD%2BnaQAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAARZJREFUeJzt3cEJAjEQQFEVS7IIe/JsTxZhT/Eu8tHAuou%2Bdw/M5TPJKfsxxtgBLx3WHgC27Lj2AM9Ol9vHZ%2B7X8wKTgA0CSSAQBAJBIBAEAkEgEAQCQSAQBAJBIBAEAkEgEAQCQSAQBAJBIBAEAkEgEAQCQSAQBAJBIBAEAkEgEAQCQSAQBAJBIBAEAkEgEAQCQSAQBAJBIBAEAkEgEAQCQSAQNvcN9AxfR7MUGwSCQCD8xBVrhmsZ77BBIAgEgkAg/O0bZMbMu%2BUbvI2Wsx9jjLWHgK1yxYIgEAgCgSAQCAKBIBAIAoEgEAgCgSAQCAKBIBAIAoEgEAgCgSAQCAKBIBAIAoEgEAgCgSAQCAKBIBAIAoHwAHz7FJXKElCqAAAAAElFTkSuQmCC&quot;&gt; &lt;a role=&quot;button&quot; data-toggle=&quot;collapse&quot; data-target=&quot;#descriptives8522789940792607072,#minihistogram8522789940792607072&quot; aria-expanded=&quot;false&quot; aria-controls=&quot;collapseExample&quot;&gt; Toggle details &lt;/a&gt; &lt;ul class=&quot;nav nav-tabs&quot; role=&quot;tablist&quot;&gt; &lt;li role=&quot;presentation&quot; class=&quot;active&quot;&gt;&lt;a href=&quot;#quantiles8522789940792607072&quot; aria-controls=&quot;quantiles8522789940792607072&quot; role=&quot;tab&quot; data-toggle=&quot;tab&quot;&gt;Statistics&lt;/a&gt;&lt;/li&gt; &lt;li role=&quot;presentation&quot;&gt;&lt;a href=&quot;#histogram8522789940792607072&quot; aria-controls=&quot;histogram8522789940792607072&quot; role=&quot;tab&quot; data-toggle=&quot;tab&quot;&gt;Histogram&lt;/a&gt;&lt;/li&gt; &lt;li role=&quot;presentation&quot;&gt;&lt;a href=&quot;#common8522789940792607072&quot; aria-controls=&quot;common8522789940792607072&quot; role=&quot;tab&quot; data-toggle=&quot;tab&quot;&gt;Common Values&lt;/a&gt;&lt;/li&gt; &lt;li role=&quot;presentation&quot;&gt;&lt;a href=&quot;#extreme8522789940792607072&quot; aria-controls=&quot;extreme8522789940792607072&quot; role=&quot;tab&quot; data-toggle=&quot;tab&quot;&gt;Extreme Values&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;div class=&quot;tab-content&quot;&gt; &lt;div role=&quot;tabpanel&quot; class=&quot;tab-pane active row&quot; id=&quot;quantiles8522789940792607072&quot;&gt; &lt;div class=&quot;col-md-4 col-md-offset-1&quot;&gt; &lt;p class=&quot;h4&quot;&gt;Quantile statistics&lt;/p&gt; &lt;table class=&quot;stats indent&quot;&gt; &lt;tr&gt; &lt;th&gt;Minimum&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;5-th percentile&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Q1&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Median&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Q3&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;95-th percentile&lt;/th&gt; &lt;td&gt;2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Maximum&lt;/th&gt; &lt;td&gt;9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Range&lt;/th&gt; &lt;td&gt;9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Interquartile range&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;div class=&quot;col-md-4 col-md-offset-2&quot;&gt; &lt;p class=&quot;h4&quot;&gt;Descriptive statistics&lt;/p&gt; &lt;table class=&quot;stats indent&quot;&gt; &lt;tr&gt; &lt;th&gt;Standard deviation&lt;/th&gt; &lt;td&gt;0.86556&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Coef of variation&lt;/th&gt; &lt;td&gt;2.2481&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Kurtosis&lt;/th&gt; &lt;td&gt;21.541&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Mean&lt;/th&gt; &lt;td&gt;0.38503&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;MAD&lt;/th&gt; &lt;td&gt;0.58945&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;&quot;&gt; &lt;th&gt;Skewness&lt;/th&gt; &lt;td&gt;3.6691&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Sum&lt;/th&gt; &lt;td&gt;504&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Variance&lt;/th&gt; &lt;td&gt;0.74919&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Memory size&lt;/th&gt; &lt;td&gt;10.3 KiB&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;div role=&quot;tabpanel&quot; class=&quot;tab-pane col-md-8 col-md-offset-2&quot; id=&quot;histogram8522789940792607072&quot;&gt; &lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAlgAAAGQCAYAAAByNR6YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD%2BnaQAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3X9YlXWe//EXhyOg/BJKcNet0RYzEdeylB00Uax0NC2JRKZZY0czkuRSYS7LH5c124iNeNUsuI7OlOnYtTE4beLPHEMtV5q9rKYFpLYcu2bkkjjJOaGEP/jx/aNv7JzQjeLjfXPOeT6uq6u878O53/f14con97k5J6ijo6NDAAAAMMZh9wAAAAD%2BhsACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwzGn3AIHC5Tpn/DkdjiDFxoarsbFZ7e0dxp8f34w1sB9rYD/WwH6swdUNGBBpy3G5guXDHI4gBQUFyeEIsnuUgMUa2I81sB9rYD/WoPchsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAzz6cB66623lJKSoiVLlnTZt3fvXs2YMUO33Xab0tPTdfTo0c597e3teu655zR58mSNGTNG8%2BbN01/%2B8pfO/R6PR4sXL1ZKSorGjx%2BvFStW6MKFC5acEwAA8H0%2BG1i/%2BtWv9Mwzz%2Bh73/tel321tbVatmyZCgoK9Pbbbys7O1uPP/646uvrJUkvv/yydu3apc2bN%2BvQoUMaPHiwcnNz1dHx5ec3rVq1Si0tLdq9e7d%2B97vf6eTJkyoqKrL0/AAAgO/y2cAKDQ3Vjh07rhhYZWVlSk1NVWpqqkJDQzVz5kzdfPPNKi8vlySVlpYqOztbf//3f6%2BIiAgtWbJEJ0%2Be1Pvvv6/PPvtMBw8e1JIlSxQbG6v4%2BHgtXLhQv/vd73T58mWrTxMAAPggnw2suXPnKjLyyp%2BQXVNTo8TERK9tiYmJqqqq0oULF/Txxx977Y%2BIiND3vvc9VVVVqba2VsHBwRo2bFjn/hEjRuiLL77Qn/70p2tzMgAAwK847R7gWvB4PIqOjvbaFh0drY8//liff/65Ojo6rrjf7Xarf//%2BioiIUFBQkNc%2BSXK73d06fkNDg1wul9c2p7Of4uLivsvpXFVwsEN3rNhv9Dmvpd8X3Gn3CMYFBzu8/g3rsQb2Yw3sxxr0Pn4ZWJI676f6Lvu/6Wu/SWlpqUpKSry25ebmKi8vr0fP6%2BtiYsLtHuGaiYrqa/cIAY81sB9rYD/WoPfwy8CKiYmRx%2BPx2ubxeBQbG6v%2B/fvL4XBccf91112n2NhYnT9/Xm1tbQoODu7cJ0nXXXddt46fmZmptLQ0r21OZz%2B53c3f9ZSuyNd%2BUjF9/r1BcLBDUVF91dTUora2drvHCUisgf1YA/uxBldn1w/3fhlYSUlJqq6u9tpWVVWl6dOnKzQ0VEOHDlVNTY3Gjh0rSWpqatKf//xn/cM//IMGDRqkjo4OffDBBxoxYkTn10ZFRWnIkCHdOn5cXFyXlwNdrnNqbQ3sb3p/Pv%2B2tna/Pj9fwBrYjzWwH2vQe/jWJZBumj17to4dO6bDhw/r4sWL2rFjhz755BPNnDlTkpSVlaVt27bp5MmTOn/%2BvIqKijR8%2BHCNHDlSsbGxmjJlip5//nk1Njaqvr5eGzZsUEZGhpxOv%2BxRAABgmM8Ww8iRIyVJra2tkqSDBw9K%2BvJq080336yioiIVFhaqrq5OCQkJ2rRpkwYMGCBJmjNnjlwul/7pn/5Jzc3NSk5O9rpn6qc//alWr16tyZMnq0%2BfPrr33nuv%2BGamAAAAVxLU0dM7utEtLtc548/pdDp0d9Fbxp/3Wtm3eJzdIxjndDoUExMut7uZy/I2YQ3sxxrYjzW4ugEDrvyWTteaX75ECAAAYCcCCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDACCwAAwDC/DawTJ05o7ty5uuOOOzRu3DgVFBSosbFRklRZWamMjAyNHj1a06dPV3l5udfXbtu2TVOmTNHo0aOVlZWl6upqO04BAAD4KL8MrNbWVi1YsEC33nqrjh07pt27d6uxsVFPPfWUGhoatHDhQs2ZM0eVlZVasWKFVq1apaqqKklSRUWFiouL9fOf/1zHjh3TpEmTlJOToy%2B%2B%2BMLmswIAAL7CLwPL5XLJ5XLpvvvuU0hIiGJiYnT33XertrZWu3bt0uDBg5WRkaHQ0FClpKQoLS1NZWVlkqTS0lKlp6dr1KhRCgsL0/z58yVJhw4dsvOUAACAD/HLwIqPj9fw4cNVWlqq5uZmnT17VgcOHNDEiRNVU1OjxMREr8cnJiZ2vgz49f0Oh0PDhw/vvMIFAADwTZx2D3AtOBwOFRcXKzs7W1u3bpUkjR07Vvn5%2BVq4cKHi4%2BO9Ht%2B/f3%2B53W5JksfjUXR0tNf%2B6Ojozv3d0dDQIJfL5bXN6eynuLi473I6VxUc7Ft97HT61rzd8dUa%2BNpa%2BBPWwH6sgf1Yg97HLwPr0qVLysnJ0dSpUzvvn3r66adVUFDQra/v6Ojo0fFLS0tVUlLitS03N1d5eXk9el5fFxMTbvcI10xUVF%2B7Rwh4rIH9WAP7sQa9h18GVmVlpU6fPq2lS5cqODhYkZGRysvL03333ac777xTHo/H6/Fut1uxsbGSpJiYmC77PR6Phg4d2u3jZ2ZmKi0tzWub09lPbnfzdzyjK/O1n1RMn39vEBzsUFRUXzU1taitrd3ucQISa2A/1sB%2BrMHV2fXDvV8GVltbm9rb272uRF26dEmSlJKSov/4j//wenx1dbVGjRolSUpKSlJNTY1mzZrV%2BVwnTpxQRkZGt48fFxfX5eVAl%2BucWlsD%2B5ven8%2B/ra3dr8/PF7AG9mMN7Mca9B6%2BdQmkm2677Tb169dPxcXFamlpkdvt1saNGzVmzBjdd999qqurU1lZmS5evKgjR47oyJEjmj17tiQpKytLr732mv74xz%2BqpaVFGzduVEhIiCZOnGjvSQEAAJ/hl4EVExOjF154Qe%2B%2B%2B64mTJige%2B%2B9V2FhYVq/fr2uu%2B46bdq0Sdu3b9ftt9%2BuNWvWaN26dbrlllskSRMmTNDSpUu1ePFijR07VseOHdPmzZsVFhZm81kBAABfEdTR0zu60S0u1znjz%2Bl0OnR30VvGn/da2bd4nN0jGOd0OhQTEy63u5nL8jZhDezHGtiPNbi6AQMibTmuX17BAgAAsBOBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYJjlgZWWlqaSkhKdOXPG6kMDAABYwvLAeuCBB7R3717dddddmj9/vg4cOKDW1larxwAAALhmLA%2Bs3Nxc7d27V7/97W81dOhQrVmzRqmpqVq3bp1OnTpl9TgAAADG2XYP1ogRI7Rs2TIdOnRIy5cv129/%2B1tNmzZN8%2BbN03//93/bNRYAAECP2RZYly9f1t69e/XII49o2bJlio%2BP15NPPqnhw4crOztbu3btsms0AACAHnFafcCTJ09qx44deu2119Tc3KwpU6Zo69atuv322zsfM2bMGD311FOaMWOG1eMBAAD0mOVXsKZPn67Dhw/r0Ucf1Ztvvql169Z5xZUkpaamqrGxscfH2rhxo8aPH69bb71V2dnZOn36tCSpsrJSGRkZGj16tKZPn67y8nKvr9u2bZumTJmi0aNHKysrS9XV1T2eBQAABA7LA2vbtm3at2%2BfsrOz1b9//6s%2B7v333%2B/RcV5%2B%2BWWVl5dr27ZtOnr0qBISEvTSSy%2BpoaFBCxcu1Jw5c1RZWakVK1Zo1apVqqqqkiRVVFSouLhYP//5z3Xs2DFNmjRJOTk5%2BuKLL3o0DwAACByWB9awYcOUk5OjgwcPdm576aWX9Mgjj8jj8Rg7zosvvqglS5bopptuUkREhFauXKmVK1dq165dGjx4sDIyMhQaGqqUlBSlpaWprKxMklRaWqr09HSNGjVKYWFhmj9/viTp0KFDxmYDAAD%2BzfLAKiws1Llz55SQkNC5beLEiWpvb9fatWuNHOPTTz/V6dOn9fnnn2vatGlKTk5WXl6eGhsbVVNTo8TERK/HJyYmdr4M%2BPX9DodDw4cP77zCBQAA8E0sv8n96NGj2rVrl2JiYjq3DR48WEVFRbr33nuNHKO%2Bvl6StH//fm3ZskUdHR3Ky8vTypUrdeHCBcXHx3s9vn///nK73ZIkj8ej6Ohor/3R0dGd%2B7ujoaFBLpfLa5vT2U9xcXHf5XSuKjjYtz7pyOn0rXm746s18LW18Cesgf1YA/uxBr2P5YF14cIFhYaGdtnucDjU0tJi5BgdHR2SpPnz53fG1KJFi/TII48oJSWl21//XZWWlqqkpMRrW25urvLy8nr0vL4uJibc7hGumaiovnaPEPBYA/uxBvZjDXoPywNrzJgxWrt2rfLz8zuvFH366ad69tlnu/w24Xd1/fXXS5KioqI6tw0aNEgdHR26fPlyl3u93G63YmNjJUkxMTFd9ns8Hg0dOrTbx8/MzFRaWprXNqezn9zu5m91Ht/E135SMX3%2BvUFwsENRUX3V1NSitrZ2u8cJSKyB/VgD%2B7EGV2fXD/eWB9by5cv14x//WN///vcVERGh9vZ2NTc364YbbtBvfvMbI8cYOHCgIiIiVFtbqxEjRkiS6urq1KdPH6Wmpmrnzp1ej6%2BurtaoUaMkSUlJSaqpqdGsWbMkSW1tbTpx4oQyMjK6ffy4uLguLwe6XOfU2hrY3/T%2BfP5tbe1%2BfX6%2BgDWwH2tgP9ag97A8sG644Qbt2bNHb775pv785z/L4XBoyJAhGj9%2BvIKDg40cw%2Bl0KiMjQ7/85S81ZswYRUREaMOGDZoxY4ZmzZqlf/u3f1NZWZlmzpypt99%2BW0eOHFFpaakkKSsrS0uXLtW9996rYcOG6YUXXlBISIgmTpxoZDYAAOD/LA8sSQoJCdFdd911TY%2BRn5%2BvS5cu6cEHH9Tly5c1ZcoUrVy5UuHh4dq0aZOeeeYZPf300xo0aJDWrVunW265RZI0YcIELV26VIsXL9bZs2c1cuRIbd68WWFhYdd0XgAA4D%2BCOnp6R/e39Je//EXr16/XRx99pAsXLnTZ/8Ybb1g5jmVcrnPGn9PpdOjuoreMP%2B%2B1sm/xOLtHMM7pdCgmJlxudzOX5W3CGtiPNbAfa3B1AwZE2nJcW%2B7Bamho0Pjx49WvXz%2BrDw8AAHDNWR5Y1dXVeuONNzp/aw8AAMDfWP57/tdddx1XrgAAgF%2BzPLAeffRRlZSU9PjNPAEAAHory18ifPPNN/Xuu%2B/q1Vdf1d/93d/J4fBuvFdeecXqkQAAAIyyPLAiIiI0YcIEqw8LAABgGcsDq7Cw0OpDAgAAWMqWD7P705/%2BpOLiYj355JOd29577z07RgEAADDO8sCqrKzUzJkzdeDAAe3evVvSl28%2BOnfuXL99k1EAABBYLA%2Bs5557Tj/5yU%2B0a9cuBQUFSfry8wnXrl2rDRs2WD0OAACAcZYH1v/8z/8oKytLkjoDS5KmTp2qkydPWj0OAACAcZYHVmRk5BU/g7ChoUEhISFWjwMAAGCc5YE1evRorVmzRufPn%2B/cdurUKS1btkzf//73rR4HAADAOMvfpuHJJ5/Uww8/rOTkZLW1tWn06NFqaWnR0KFDtXbtWqvHAQAAMM7ywBo4cKB2796tI0eO6NSpUwoLC9OQIUM0btw4r3uyAAAAfJXlgSVJffr00V133WXHoQEAAK45ywMrLS3t/7xSxXthAQAAX2d5YE2bNs0rsNra2nTq1ClVVVXp4YcftnocAAAA4ywPrIKCgituf/311/WHP/zB4mkAAADMs%2BWzCK/krrvu0p49e%2BweAwAAoMd6TWCdOHFCHR0ddo8BAADQY5a/RDhnzpwu21paWnTy5Endc889Vo8DAABgnOWBNXjw4C6/RRgaGqqMjAw9%2BOCDVo8DAABgnOWBxbu1AwAAf2d5YL322mvdfuz9999/DScBAAC4NiwPrBUrVqi9vb3LDe1BQUFe24KCgggsAADgkywPrF//%2Btd68cUXlZOTo2HDhqmjo0MffvihfvWrX%2BlHP/qRkpOTrR4JAADAKFvuwdq8ebPi4%2BM7t91xxx264YYbNG/ePO3evdvqkQAAAIyy/H2wPvnkE0VHR3fZHhUVpbq6OqvHAQAAMM7ywBo0aJDWrl0rt9vdua2pqUnr16/XjTfeaPU4AAAAxln%2BEuHy5cuVn5%2Bv0tJShYeHy%2BFw6Pz58woLC9OGDRusHgcAAMA4ywNr/PjxOnz4sI4cOaL6%2Bnp1dHQoPj5ed955pyIjI60eBwAAwDjLA0uS%2Bvbtq8mTJ6u%2Bvl433HCDHSMAAABcM5bfg3XhwgUtW7ZMt912m37wgx9I%2BvIerPnz56upqcnqcQAAAIyzPLDWrVun2tpaFRUVyeH438O3tbWpqKjI6nEAAACMszywXn/9df3rv/6rpk6d2vmhz1FRUSosLNSBAwesHgcAAMA4ywOrublZgwcP7rI9NjZWX3zxhdXjAAAAGGd5YN144436wx/%2BIElenz24f/9%2B/e3f/q3V4wAAABhn%2BW8R/vCHP9SiRYv0wAMPqL29XVu2bFF1dbVef/11rVixwupxAAAAjLM8sDIzM%2BV0OrV9%2B3YFBwfrl7/8pYYMGaKioiJNnTrV6nEAAACMszywGhsb9cADD%2BiBBx6w%2BtAAAACWsPwerMmTJ3vdewUAAOBvLA%2Bs5ORk7du3z%2BrDAgAAWMbylwj/5m/%2BRj/72c%2B0efNm3XjjjerTp4/X/vXr11s9EgAAgFGWB9bHH3%2Bsm266SZLkdrutPjwAAMA1Z1lgLVmyRM8995x%2B85vfdG7bsGGDcnNzrRoBAADAEpbdg1VRUdFl2%2BbNm606PAAAgGUsC6wr/eYgv00IAAD8kWWB9dUHO3/TNgAAAF9n%2Bds0AAAA%2BLuACKw1a9Zo2LBhnX%2BurKxURkaGRo8erenTp6u8vNzr8du2bdOUKVM0evRoZWVlqbq62uqRAQCAD7PstwgvX76s/Pz8b9xm%2Bn2wamtrtXPnzs4/NzQ0aOHChVqxYoVmzJihd955R4899piGDBmikSNHqqKiQsXFxfr1r3%2BtYcOGadu2bcrJydGBAwfUr18/o7MBAAD/ZNkVrNtvv10NDQ1e/1xpm0nt7e1avXq1srOzO7ft2rVLgwcPVkZGhkJDQ5WSkqK0tDSVlZVJkkpLS5Wenq5Ro0YpLCxM8%2BfPlyQdOnTI6GwAAMB/WXYF66/f/8oqr7zyikJDQzVjxgw9//zzkqSamholJiZ6PS4xMbHz43tqamo0bdq0zn0Oh0PDhw9XVVWVpk%2Bf3q3jNjQ0yOVyeW1zOvspLi6uJ6fTRXCwb73C63T61rzd8dUa%2BNpa%2BBPWwH6sgf1Yg97H8ndyt8pnn32m4uLiLmHn8XgUHx/vta1///6d7yrv8XgUHR3ttT86Ovpbvet8aWmpSkpKvLbl5uYqLy/v25yC34mJCbd7hGsmKqqv3SMEPNbAfqyB/ViD3sNvA6uwsFDp6elKSEjQ6dOnv9XX9vT9uTIzM5WWlua1zensJ7e7uUfP%2B3W%2B9pOK6fPvDYKDHYqK6qumpha1tbXbPU5AYg3sxxrYjzW4Ort%2BuPfLwKqsrNR7772n3bt3d9kXExMjj8fjtc3tdis2Nvaq%2Bz0ej4YOHdrt48fFxXV5OdDlOqfW1sD%2Bpvfn829ra/fr8/MFrIH9WAP7sQa9h29dAumm8vJynT17VpMmTVJycrLS09MlScnJybr55pu7vO1CdXW1Ro0aJUlKSkpSTU1N5762tjadOHGicz8AAMA38cvAeuKJJ/T6669r586d2rlzZ%2BdnHu7cuVMzZsxQXV2dysrKdPHiRR05ckRHjhzR7NmzJUlZWVl67bXX9Mc//lEtLS3auHGjQkJCNHHiRBvPCAAA%2BBK/fIkwOjra60b11tZWSdLAgQMlSZs2bdIzzzyjp59%2BWoMGDdK6det0yy23SJImTJigpUuXavHixTp79qxGjhypzZs3KywszPoTAQAAPimog09ctoTLdc74czqdDt1d9Jbx571W9i0eZ/cIxjmdDsXEhMvtbua%2BB5uwBvZjDezHGlzdgAGRthzXL18iBAAAsBOBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYBiBBQAAYJjfBlZdXZ1yc3OVnJyslJQUPfHEE2pqapIk1dbW6kc/%2BpFuv/123XPPPXrxxRe9vnbv3r2aMWOGbrvtNqWnp%2Bvo0aN2nAIAAPBRfhtYOTk5ioqKUkVFhV599VV99NFHevbZZ3XhwgU9%2Buij%2Bsd//Ee99dZbeu6557Rp0yYdOHBA0pfxtWzZMhUUFOjtt99Wdna2Hn/8cdXX19t8RgAAwFf4ZWA1NTUpKSlJ%2Bfn5Cg8P18CBAzVr1iwdP35chw8f1uXLl/XYY4%2BpX79%2BGjFihB588EGVlpZKksrKypSamqrU1FSFhoZq5syZuvnmm1VeXm7zWQEAAF/htHuAayEqKkqFhYVe286cOaO4uDjV1NRo2LBhCg4O7tyXmJiosrIySVJNTY1SU1O9vjYxMVFVVVXdPn5DQ4NcLpfXNqezn%2BLi4r7tqfyfgoN9q4%2BdTt%2Batzu%2BWgNfWwt/whrYjzWwH2vQ%2B/hlYH1dVVWVtm/fro0bN2rfvn2Kiory2t%2B/f395PB61t7fL4/EoOjraa390dLQ%2B/vjjbh%2BvtLRUJSUlXttyc3OVl5f33U/CD8TEhNs9wjUTFdXX7hECHmtgP9bAfqxB7%2BH3gfXOO%2B/oscceU35%2BvlJSUrRv374rPi4oKKjzvzs6Onp0zMzMTKWlpXltczr7ye1u7tHzfp2v/aRi%2Bvx7g%2BBgh6Ki%2BqqpqUVtbe12jxOQWAP7sQb2Yw2uzq4f7v06sCoqKvSTn/xEq1at0v333y9Jio2N1SeffOL1OI/Ho/79%2B8vhcCgmJkYej6fL/tjY2G4fNy4ursvLgS7XObW2BvY3vT%2Bff1tbu1%2Bfny9gDezHGtiPNeg9fOsSyLfw7rvvatmyZfrFL37RGVeSlJSUpA8//FCtra2d26qqqjRq1KjO/dXV1V7P9df7AQAAvolfBlZra6tWrlypgoICjR8/3mtfamqqIiIitHHjRrW0tOj999/Xjh07lJWVJUmaPXu2jh07psOHD%2BvixYvasWOHPvnkE82cOdOOUwEAAD4oqKOnNxz1QsePH9dDDz2kkJCQLvv279%2Bv5uZmrV69WtXV1br%2B%2Buv1yCOP6Ic//GHnYw4cOKD169errq5OCQkJWrFihcaMGdOjmVyucz36%2BitxOh26u%2Bgt4897rexbPM7uEYxzOh2KiQmX293MZXmbsAb2Yw3sxxpc3YABkbYc1y/vwbrjjjv04Ycf/p%2BP%2Bfd///er7rvnnnt0zz33mB4LAAAECL98iRAAAMBOBBYAAIBhfvkSIXqnHzz/n3aP8K344z1jAABrcAULAADAMAILAADAMAILAADAMAILAADAMAILAADAMAILAADAMAILAADAMAILAADAMAILAADAMAILAADAMAILAADAMAILAADAMAILAADAMAILAADAMAILAADAMAILAADAMAILAADAMKfdAwC91Q%2Be/0%2B7R/hW9i0eZ/cIAID/jytYAAAAhhFYAAAAhhFYAAAAhhFYAAAAhhFYAAAAhhFYAAAAhhFYAAAAhhFYAAAAhhFYAAAAhhFYAAAAhhFYAAAAhhFYAAAAhhFYAAAAhhFYAAAAhhFYAAAAhhFYAAAAhhFYAAAAhhFYAAAAhhFYAAAAhhFYAAAAhhFYAAAAhhFYAAAAhhFYAAAAhhFYAAAAhhFYAAAAhhFYAAAAhhFYAAAAhhFYAABJvBbCAAAHQklEQVQAhhFYAAAAhhFYV1BXV6cFCxYoOTlZkyZN0rp169Te3m73WAAAwEc47R6gN1q0aJFGjBihgwcP6uzZs3r00Ud1/fXX65//%2BZ/tHg0AAPgAAutrqqqq9MEHH2jLli2KjIxUZGSksrOztXXrVgILvdoPnv9Pu0dAL7Fv8Ti7RwACHoH1NTU1NRo0aJCio6M7t40YMUKnTp3S%2BfPnFRER8Y3P0dDQIJfL5bXN6eynuLg4o7MGB/MKL4CufCm2f19wp90j%2BIWv/j74%2Bt8Ldxe9Zcc434m/fS8QWF/j8XgUFRXlte2r2HK73d0KrNLSUpWUlHhte/zxx7Vo0SJzg%2BrLkHt44EfKzMw0Hm/onoaGBpWWlrIGNmIN7Mca2K%2BhoUFbt/66yxoc/9lUG6cKbFwCuYKOjo4efX1mZqZeffVVr38yMzMNTfe/XC6XSkpKulwtg3VYA/uxBvZjDezHGvQ%2BXMH6mtjYWHk8Hq9tHo9HQUFBio2N7dZzxMXF8VMcAAABjCtYX5OUlKQzZ86osbGxc1tVVZUSEhIUHh5u42QAAMBXEFhfk5iYqJEjR2r9%2BvU6f/68Tp48qS1btigrK8vu0QAAgI8Ifuqpp56ye4je5s4779Tu3bv1L//yL9qzZ48yMjI0b948BQUF2T1aF%2BHh4Ro7dixX12zEGtiPNbAfa2A/1qB3Cero6R3dAAAA8MJLhAAAAIYRWAAAAIYRWAAAAIYRWAAAAIYRWAAAAIYRWAAAAIYRWAAAAIYRWAAAAIYRWAAAAIYRWD6orq5OCxYsUHJysiZNmqR169apvb3d7rECTl1dnXJzc5WcnKyUlBQ98cQTampqsnusgLRmzRoNGzbM7jEC1saNGzV%2B/Hjdeuutys7O1unTp%2B0eKWCcOHFCc%2BfO1R133KFx48apoKBAjY2Ndo8FEVg%2BadGiRYqPj9fBgwe1ZcsWHTx4UFu3brV7rICTk5OjqKgoVVRU6NVXX9VHH32kZ5991u6xAk5tba127txp9xgB6%2BWXX1Z5ebm2bdumo0ePKiEhQS%2B99JLdYwWE1tZWLViwQLfeequOHTum3bt3q7GxUXzEcO9AYPmYqqoqffDBByooKFBkZKQGDx6s7OxslZaW2j1aQGlqalJSUpLy8/MVHh6ugQMHatasWTp%2B/LjdowWU9vZ2rV69WtnZ2XaPErBefPFFLVmyRDfddJMiIiK0cuVKrVy50u6xAoLL5ZLL5dJ9992nkJAQxcTE6O6771Ztba3do0EEls%2BpqanRoEGDFB0d3bltxIgROnXqlM6fP2/jZIElKipKhYWFuv766zu3nTlzRnFxcTZOFXheeeUVhYaGasaMGXaPEpA%2B/fRTnT59Wp9//rmmTZum5ORk5eXl8RKVReLj4zV8%2BHCVlpaqublZZ8%2Be1YEDBzRx4kS7R4MILJ/j8XgUFRXlte2r2HK73XaMBH15ZXH79u167LHH7B4lYHz22WcqLi7W6tWr7R4lYNXX10uS9u/fry1btmjnzp2qr6/nCpZFHA6HiouL9cYbb2j06NFKSUlRa2ur8vPz7R4NIrB8UkdHh90j4K%2B88847mjdvnvLz85WSkmL3OAGjsLBQ6enpSkhIsHuUgPXV/4vmz5%2Bv%2BPh4DRw4UIsWLVJFRYUuXrxo83T%2B79KlS8rJydHUqVN1/Phxvfnmm4qMjFRBQYHdo0EEls%2BJjY2Vx%2BPx2ubxeBQUFKTY2FibpgpcFRUVWrBggZYvX665c%2BfaPU7AqKys1Hvvvafc3Fy7RwloX71E/tdX1QcNGqSOjg6dPXvWrrECRmVlpU6fPq2lS5cqMjJS8fHxysvL0%2B9///suf0/AegSWj0lKStKZM2e87nGoqqpSQkKCwsPDbZws8Lz77rtatmyZfvGLX%2Bj%2B%2B%2B%2B3e5yAUl5errNnz2rSpElKTk5Wenq6JCk5OVl79uyxebrAMXDgQEVERHjdVF1XV6c%2BffpwP6IF2tra1N7e7vWqxqVLl2ycCH8tqIPXm3zO7NmzNXToUD355JP69NNPtWDBAv34xz/WQw89ZPdoAaO1tVUzZ87Uww8/rMzMTLvHCTiff/65WlpaOv9cX1%2BvzMxMHTlyRNHR0erbt6%2BN0wWWwsJCvfHGG3rhhRcUERGh3NxcDRkyRIWFhXaP5vfcbremTp2qOXPmKCcnRxcuXNDy5ct17tw5bd%2B%2B3e7xAh6B5YPq6%2Bu1atUq/dd//ZciIiI0Z84cPf744woKCrJ7tIBx/PhxPfTQQwoJCemyb//%2B/Ro0aJANUwWu06dPa/Lkyfrwww/tHiXgXLp0SYWFhdqzZ48uX76sKVOmaNWqVVxRt0h1dbWeffZZffDBBwoJCdHYsWP1xBNPKD4%2B3u7RAh6BBQAAYBj3YAEAABhGYAEAABhGYAEAABhGYAEAABhGYAEAABhGYAEAABhGYAEAABhGYAEAABhGYAEAABhGYAEAABhGYAEAABhGYAEAABhGYAEAABj2/wB%2BkoQn6iEOHwAAAABJRU5ErkJggg%3D%3D&quot;/&gt; &lt;/div&gt; &lt;div role=&quot;tabpanel&quot; class=&quot;tab-pane col-md-12&quot; id=&quot;common8522789940792607072&quot;&gt; Value Count Frequency (%) 0 1002 76.5% 1 170 13.0% 2 113 8.6% 3 8 0.6% 5 6 0.5% 4 6 0.5% 9 2 0.2% 6 2 0.2% &lt;/div&gt; &lt;div role=&quot;tabpanel&quot; class=&quot;tab-pane col-md-12&quot; id=&quot;extreme8522789940792607072&quot;&gt; &lt;p class=&quot;h4&quot;&gt;Minimum 5 values&lt;/p&gt; Value Count Frequency (%) 0 1002 76.5% 1 170 13.0% 2 113 8.6% 3 8 0.6% 4 6 0.5% &lt;p class=&quot;h4&quot;&gt;Maximum 5 values&lt;/p&gt; Value Count Frequency (%) 3 8 0.6% 4 6 0.5% 5 6 0.5% 6 2 0.2% 9 2 0.2% &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;col-md-3 namecol&quot;&gt; &lt;p class=&quot;h4 pp-anchor&quot; id=&quot;pp_var_pclass&quot;&gt;pclass&lt;br/&gt; &lt;small&gt;Numeric&lt;/small&gt; &lt;/p&gt; &lt;/div&gt;&lt;div class=&quot;col-md-6&quot;&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm-6&quot;&gt; &lt;table class=&quot;stats &quot;&gt; &lt;tr&gt; &lt;th&gt;Distinct count&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Unique (%)&lt;/th&gt; &lt;td&gt;0.2%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Missing (%)&lt;/th&gt; &lt;td&gt;0.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Missing (n)&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Infinite (%)&lt;/th&gt; &lt;td&gt;0.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Infinite (n)&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-6&quot;&gt; &lt;table class=&quot;stats &quot;&gt; &lt;tr&gt; &lt;th&gt;Mean&lt;/th&gt; &lt;td&gt;2.2949&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Minimum&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Maximum&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Zeros (%)&lt;/th&gt; &lt;td&gt;0.0%&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMgAAABLCAYAAAA1fMjoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD%2BnaQAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAAQtJREFUeJzt3MEJAkEQAEEVQzIIc/JtTgZhTmMC0qAgt5xV/4X5NDOvPc7MHIC3TlsPACs7bz0A%2B3W5PT5%2B87xffzDJ92wQCAKBIBAIAoEgEAgCgSAQCAKBIBAIAoEgEAgCgSAQCAKBIBAIAoEgEAgCgSAQCAKBIBAIAoEgEAgCgSAQCMv9rLiH3/jYDxsEgkAgLHdi/Tsn5lpsEAgCgSAQCAKBIBAIAoEgEAgCgSAQCAKBIBAIAoEgEAgCgSAQCAKBIBAIx5mZrYeAVdkgEAQCQSAQBAJBIBAEAkEgEAQCQSAQBAJBIBAEAkEgEAQCQSAQBAJBIBAEAkEgEAQCQSAQBAJBIBAEAkEgEF7/3RGT0dEIxAAAAABJRU5ErkJggg%3D%3D&quot;&gt; &lt;a role=&quot;button&quot; data-toggle=&quot;collapse&quot; data-target=&quot;#descriptives-8665591789917202290,#minihistogram-8665591789917202290&quot; aria-expanded=&quot;false&quot; aria-controls=&quot;collapseExample&quot;&gt; Toggle details &lt;/a&gt; &lt;ul class=&quot;nav nav-tabs&quot; role=&quot;tablist&quot;&gt; &lt;li role=&quot;presentation&quot; class=&quot;active&quot;&gt;&lt;a href=&quot;#quantiles-8665591789917202290&quot; aria-controls=&quot;quantiles-8665591789917202290&quot; role=&quot;tab&quot; data-toggle=&quot;tab&quot;&gt;Statistics&lt;/a&gt;&lt;/li&gt; &lt;li role=&quot;presentation&quot;&gt;&lt;a href=&quot;#histogram-8665591789917202290&quot; aria-controls=&quot;histogram-8665591789917202290&quot; role=&quot;tab&quot; data-toggle=&quot;tab&quot;&gt;Histogram&lt;/a&gt;&lt;/li&gt; &lt;li role=&quot;presentation&quot;&gt;&lt;a href=&quot;#common-8665591789917202290&quot; aria-controls=&quot;common-8665591789917202290&quot; role=&quot;tab&quot; data-toggle=&quot;tab&quot;&gt;Common Values&lt;/a&gt;&lt;/li&gt; &lt;li role=&quot;presentation&quot;&gt;&lt;a href=&quot;#extreme-8665591789917202290&quot; aria-controls=&quot;extreme-8665591789917202290&quot; role=&quot;tab&quot; data-toggle=&quot;tab&quot;&gt;Extreme Values&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;div class=&quot;tab-content&quot;&gt; &lt;div role=&quot;tabpanel&quot; class=&quot;tab-pane active row&quot; id=&quot;quantiles-8665591789917202290&quot;&gt; &lt;div class=&quot;col-md-4 col-md-offset-1&quot;&gt; &lt;p class=&quot;h4&quot;&gt;Quantile statistics&lt;/p&gt; &lt;table class=&quot;stats indent&quot;&gt; &lt;tr&gt; &lt;th&gt;Minimum&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;5-th percentile&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Q1&lt;/th&gt; &lt;td&gt;2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Median&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Q3&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;95-th percentile&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Maximum&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Range&lt;/th&gt; &lt;td&gt;2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Interquartile range&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;div class=&quot;col-md-4 col-md-offset-2&quot;&gt; &lt;p class=&quot;h4&quot;&gt;Descriptive statistics&lt;/p&gt; &lt;table class=&quot;stats indent&quot;&gt; &lt;tr&gt; &lt;th&gt;Standard deviation&lt;/th&gt; &lt;td&gt;0.83784&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Coef of variation&lt;/th&gt; &lt;td&gt;0.36509&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Kurtosis&lt;/th&gt; &lt;td&gt;-1.3151&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Mean&lt;/th&gt; &lt;td&gt;2.2949&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;MAD&lt;/th&gt; &lt;td&gt;0.76383&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;&quot;&gt; &lt;th&gt;Skewness&lt;/th&gt; &lt;td&gt;-0.59865&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Sum&lt;/th&gt; &lt;td&gt;3004&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Variance&lt;/th&gt; &lt;td&gt;0.70197&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Memory size&lt;/th&gt; &lt;td&gt;10.3 KiB&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;div role=&quot;tabpanel&quot; class=&quot;tab-pane col-md-8 col-md-offset-2&quot; id=&quot;histogram-8665591789917202290&quot;&gt; &lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAlgAAAGQCAYAAAByNR6YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD%2BnaQAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3X1clfXBx/EvDwIq8mSizly4VBQhpmkUOVE0LU1WpCK%2BWrmy%2BUAxUUsTt7QoberdSrxV1myxvBvDmk9T8y4fVov2am62A9qDD81iGoQcEUMUuO4/fHnujqgc9IJzztXn/Xr5Mn6/61zn9/XqOn69rsPBxzAMQwAAADCNr7sXAAAAYDUULAAAAJNRsAAAAExGwQIAADAZBQsAAMBkFCwAAACTUbAAAABMRsECAAAwGQULAADAZBQsAAAAk1GwAAAATEbBAgAAMBkFCwAAwGQULAAAAJNRsAAAAExGwQIAADAZBQsAAMBkFCwAAACTUbAAAABMRsECAAAwGQULAADAZBQsAAAAk1GwAAAATEbBAgAAMBkFCwAAwGQULAAAAJNRsAAAAExGwQIAADAZBQsAAMBkFCwAAACTUbAAAABMRsECAAAwGQULAADAZBQsAAAAk1GwAAAATEbBAgAAMBkFCwAAwGQULAAAAJNRsAAAAEzm7%2B4FfFeUl58yfZ%2B%2Bvj6KiGivEydOq6HBMH3/7mLVXJJ1s1k1l0Q2b2TVXJJ1s7Vkrk6dOpi6P1dxBcuL%2Bfr6yMfHR76%2BPu5eiqmsmkuybjar5pLI5o2smkuybjYr5qJgAQAAmIyCBQAAYDIKFgAAgMkoWAAAACajYAEAAJiMggUAAGAyChYAAIDJKFgAAAAmo2ABAACYjIIFAABgMgoWAACAyShYAAAAJqNgAQAAmMzf3QtoCR9%2B%2BKEeeughpzHDMHTu3Dl98sknKioq0vLly3X48GF17dpVU6dOVUpKimPb/Px8rVu3TuXl5YqOjlZ2drZiY2NbOwYAANfkrl//1d1LcNnfn73T3UswlSUL1qBBg2Sz2ZzGVq9erY8//lhlZWWaMWOGsrOzNXbsWO3du1fTp09Xjx49FBcXp507d2rFihV6%2BeWXFR0drfz8fE2bNk07duxQu3bt3JQIAAB4k%2B/ELcL//Oc/euWVV/TEE09o8%2BbNioqK0rhx4xQYGKjExEQlJyersLBQklRQUKDU1FTFx8crKChIU6ZMkSTt2rXLnREAAIAXseQVrIu9%2BOKLuu%2B%2B%2B/S9731PJSUliomJcZqPiYnRtm3bJEklJSUaPXq0Y87X11d9%2B/aVzWbTmDFjXHq%2BsrIylZeXO435%2B7dTZGTkNSZx5ufn6/S7VVg1l2TdbFbNJZHNG1k1l2TtbJK1clm%2BYH355ZfasWOHduzYIUmy2%2B3q3Lmz0zZhYWGqrKx0zIeGhjrNh4aGOuZdUVBQoNzcXKexjIwMZWZmXk2EJoWEtG2R/bqbVXNJ1s1m1VwS2byRVXNJ1s1mpVyWL1jr1q3TyJEj1alTJ5cfYxjGNT1nWlqakpOTncb8/dupsvL0Ne33Yn5%2BvgoJaauqqhrV1zeYum93smouybrZrJpLIps3smouydrZJLVIrvDw9qbuz1WWL1hvvfWW5s6d6/g6PDxcdrvdaZvKykpFRERcdt5ut6tXr14uP2dkZGSj24Hl5adUV9cyJ0N9fUOL7dudrJpLsm42q%2BaSyOaNrJpLsm42K%2BWyzs3OSzhw4IBKS0t1%2B%2B23O8bi4uJUXFzstF1xcbHi4%2BMlSbGxsSopKXHM1dfXa//%2B/Y55AACApli6YO3fv19hYWEKDg52jI0dO1alpaUqLCxUbW2t9uzZoz179mjChAmSpPT0dG3YsEH79u1TTU2NVq1apYCAAA0dOtRNKQAAgLex9C3Cr7/%2ButF7rzp27Kg1a9YoJydHixYtUrdu3bR06VL16dNHkjRkyBDNmjVLM2fOVEVFheLi4pSXl6egoCB3RAAAAF7Ix7jWd3TDJeXlp0zfp7%2B/r8LD26uy8rRl7llL1s0lWTebVXNJZPNGVs0lNT%2Bbt32Se0scs06dOpi6P1dZ%2BhYhAACAO1CwAAAATEbBAgAAMBkFCwAAwGQULAAAAJNRsAAAAExGwQIAADAZBQsAAMBkFCwAAACTUbAAAABMRsECAAAwGQULAADAZBQsAAAAk1GwAAAATEbBAgAAMBkFCwAAwGQULAAAAJNRsAAAAExGwQIAADAZBQsAAMBkFCwAAACTUbAAAABMRsECAAAwGQULAADAZBQsAAAAk1GwAAAATEbBAgAAMBkFCwAAwGQULAAAAJNRsAAAAExGwQIAADAZBQsAAMBkli5Yq1at0uDBg/XDH/5QkydP1pdffilJKioq0rhx4zRgwACNGTNGmzZtcnpcfn6%2BRo0apQEDBig9PV3FxcXuWD4AAPBSli1Y69at06ZNm5Sfn6/33ntPPXv21O9%2B9zuVlZVpxowZmjhxooqKipSdna1f/OIXstlskqSdO3dqxYoV%2BtWvfqX3339fw4YN07Rp0/TNN9%2B4OREAAPAWli1Ya9euVVZWln7wgx8oODhYCxYs0IIFC7R582ZFRUVp3LhxCgwMVGJiopKTk1VYWChJKigoUGpqquLj4xUUFKQpU6ZIknbt2uXOOAAAwIv4u3sBLeGrr77Sl19%2BqZMnT2r06NGqqKhQQkKCFi5cqJKSEsXExDhtHxMTo23btkmSSkpKNHr0aMecr6%2Bv%2BvbtK5vNpjFjxrj0/GVlZSovL3ca8/dvp8jIyGtM5szPz9fpd6uwai7Jutmsmksimzeyai7J2tkka%2BWyZME6fvy4JGn79u165ZVXZBiGMjMztWDBAp05c0adO3d22j4sLEyVlZWSJLvdrtDQUKf50NBQx7wrCgoKlJub6zSWkZGhzMzMq4nTpJCQti2yX3ezai7Jutmsmksimzeyai7JutmslMuSBcswDEnSlClTHGXqscce0yOPPKLExESXH3%2B10tLSlJyc7DTm799OlZWnr2m/F/Pz81VISFtVVdWovr7B1H27k1VzSdbNZtVcEtm8kVVzSdbOJqlFcoWHtzd1f66yZMG67rrrJEkhISGOsW7duskwDJ07d052u91p%2B8rKSkVEREiSwsPDG83b7Xb16tXL5eePjIxsdDuwvPyU6upa5mSor29osX27k1VzSdbNZtVcEtm8kVVzSdbNZqVc1rnZ%2BS1dunRRcHCwDhw44BgrLS1VmzZtlJSU1OhjF4qLixUfHy9Jio2NVUlJiWOuvr5e%2B/fvd8wDAAA0xZIFy9/fX%2BPGjdPq1av173//WxUVFVq5cqXGjh2re%2B%2B9V6WlpSosLFRtba327NmjPXv2aMKECZKk9PR0bdiwQfv27VNNTY1WrVqlgIAADR061L2hAACA17DkLUJJmj17ts6ePavx48fr3LlzGjVqlBYsWKD27dtrzZo1ysnJ0aJFi9StWzctXbpUffr0kSQNGTJEs2bN0syZM1VRUaG4uDjl5eUpKCjIzYkAAIC38DGu9R3dcEl5%2BSnT9%2Bnv76vw8PaqrDxtmXvWknVzSdbNZtVcEtm8kVVzSc3Pdtev/9oKqzLH35%2B9s0WOWadOHUzdn6sseYsQAADAnShYAAAAJqNgAQAAmIyCBQAAYDIKFgAAgMkoWAAAACajYAEAAJiMggUAAGAyChYAAIDJKFgAAAAmo2ABAACYjIIFAABgMgoWAACAyShYAAAAJqNgAQAAmIyCBQAAYDIKFgAAgMkoWAAAACajYAEAAJiMggUAAGAyChYAAIDJKFgAAAAmo2ABAACYjIIFAABgMgoWAACAyShYAAAAJqNgAQAAmIyCBQAAYDIKFgAAgMkoWAAAACajYAEAAJiMggUAAGAyyxas6OhoxcbGKi4uzvHrmWeekSQVFRVp3LhxGjBggMaMGaNNmzY5PTY/P1%2BjRo3SgAEDlJ6eruLiYndEAAAAXsrf3QtoSdu3b9f111/vNFZWVqYZM2YoOztbY8eO1d69ezV9%2BnT16NFDcXFx2rlzp1asWKGXX35Z0dHRys/P17Rp07Rjxw61a9fOTUkAAIA3sewVrMvZvHmzoqKiNG7cOAUGBioxMVHJyckqLCyUJBUUFCg1NVXx8fEKCgrSlClTJEm7du1y57IBAIAXsfQVrOXLl%2Buf//ynqqurddddd2nevHkqKSlRTEyM03YxMTHatm2bJKmkpESjR492zPn6%2Bqpv376y2WwaM2aMS89bVlam8vJypzF//3aKjIy8xkTO/Px8nX63Cqvmkqybzaq5JLJ5I6vmkqydTbJWLssWrB/%2B8IdKTEzU888/ry%2B%2B%2BEIzZ87UokWLZLfb1blzZ6dtw8LCVFlZKUmy2%2B0KDQ11mg8NDXXMu6KgoEC5ublOYxkZGcrMzLzKNFcWEtK2RfbrblbNJVk3m1VzSWTzRlbNJVk3m5VyWbZgFRQUOP77xhtv1Jw5czR9%2BnTdfPPNTT7WMIxreu60tDQlJyc7jfn7t1Nl5elr2u/F/Px8FRLSVlVVNaqvbzB13%2B5k1VySdbNZNZdENm9k1VyStbNJapFc4eHtTd2fqyxbsC52/fXXq76%2BXr6%2BvrLb7U5zlZWVioiIkCSFh4c3mrfb7erVq5fLzxUZGdnodmB5%2BSnV1bXMyVBf39Bi%2B3Ynq%2BaSrJvNqrkksnkjq%2BaSrJvNSrmsc7PzW/bv368lS5Y4jR06dEgBAQFKSkpq9LELxcXFio%2BPlyTFxsaqpKTEMVdfX6/9%2B/c75gEAAJpiyYLVsWNHFRQUKC8vT2fPntWRI0f04osvKi0tTT/%2B8Y9VWlqqwsJC1dbWas%2BePdqzZ48mTJggSUpPT9eGDRu0b98%2B1dTUaNWqVQoICNDQoUPdGwoAAHgNS94i7Ny5s/Ly8rR8%2BXJHQbr33nuVlZWlwMBArVmzRjk5OVq0aJG6deumpUuXqk%2BfPpKkIUOGaNasWZo5c6YqKioUFxenvLw8BQUFuTkVAADwFpYsWJI0aNAg/eEPf7js3MaNGy/72EmTJmnSpEkttTQAAGBxlrxFCAAA4E4ULAAAAJNRsAAAAExGwQIAADAZBQsAAMBkFCwAAACTUbAAAABMRsECAAAwGQULAADAZBQsAAAAk1GwAAAATEbBAgAAMBkFCwAAwGQULAAAAJNRsAAAAEzmcQUrOTlZubm5OnbsmLuXAgAAcFU8rmDdd9992rp1q0aMGKEpU6Zox44dqqurc/eyAAAAXOZxBSsjI0Nbt27VH//4R/Xq1UvPPfeckpKStHTpUh05csTdywMAAGiSxxWsC/r166e5c%2Bdq165dmj9/vv74xz9q9OjRevjhh/Wvf/3L3csDAAC4LI8tWOfOndPWrVv1yCOPaO7cuercubOefPJJ9e3bV5MnT9bmzZvdvUQAAIBL8nf3Ai526NAhrV%2B/Xhs2bNDp06c1atQovfrqq7r55psd2wwaNEgLFy7U2LFj3bhSAACAS/O4gjVmzBj16NFDU6dO1T333KOwsLBG2yQlJenEiRNuWB0AAEDTPK5g5efn65Zbbmlyu48%2B%2BqgVVgMAANB8HvcerOjoaE2bNk1vv/22Y%2Bx3v/udHnnkEdntdjeuDAAAwDUeV7AWL16sU6dOqWfPno6xoUOHqqGhQUuWLHHjygAAAFzjcbcI33vvPW3evFnh4eGOsaioKC1btkx33323G1cGAADgGo%2B7gnXmzBkFBgY2Gvf19VVNTY0bVgQAANA8HlewBg0apCVLlujkyZOOsa%2B%2B%2BkqLFi1y%2BqgGAAAAT%2BVxtwjnz5%2Bvhx56SLfddpuCg4PV0NCg06dPq3v37vr973/v7uUBAAA0yeMKVvfu3fXnP/9Zf/nLX3T06FH5%2BvqqR48eGjx4sPz8/Ny9PAAAgCZ5XMGSpICAAI0YMcLdywAAALgqHlewvvjiCy1fvlyfffaZzpw502j%2BnXfeccOqAAAAXOdxBWv%2B/PkqKyvT4MGD1a5dO1P2%2Bdxzz%2BnVV1/VJ598IkkqKirS8uXLdfjwYXXt2lVTp05VSkqKY/v8/HytW7dO5eXlio6OVnZ2tmJjY01ZCwAAsD6PK1jFxcV65513FBERYcr%2BDhw4oI0bNzq%2BLisr04wZM5Sdna2xY8dq7969mj59unr06KG4uDjt3LlTK1as0Msvv6zo6Gjl5%2Bdr2rRp2rFjh2mFDwAAWJvHfUxDx44dTSsyDQ0NeuqppzR58mTH2ObNmxUVFaVx48YpMDBQiYmJSk5OVmFhoSSpoKBAqampio%2BPV1BQkKZMmSJJ2rVrlylrAgAA1udxV7CmTp2q3NxczZ49Wz4%2BPte0rz/84Q8KDAzU2LFj9etf/1qSVFJSopiYGKftYmJitG3bNsf86NGjHXO%2Bvr7q27evbDabxowZ49LzlpWVqby83GnM37%2BdIiMjryVOI35%2Bvk6/W4VVc0nWzWbVXBLZvJFVc0nWziZZK5fHFay//OUv%2Bsc//qE333xT119/vXx9nf%2Bw//CHP7i0n6%2B//lorVqxo9NlZdrtdnTt3dhoLCwtTZWWlYz40NNRpPjQ01DHvioKCAuXm5jqNZWRkKDMz0%2BV9NEdISNsW2a%2B7WTWXZN1sVs0lkc0bWTWXZN1sVsrlcQUrODhYQ4YMueb9LF68WKmpqerZs6e%2B/PLLZj3WMIxreu60tDQlJyc7jfn7t1Nl5elr2u/F/Px8FRLSVlVVNaqvbzB13%2B5k1VySdbNZNZdENm9k1VyStbNJapFc4eHtTd2fqzyuYC1evPia91FUVKR//vOf2rJlS6O58PBw2e12p7HKykrHm%2BovNW%2B329WrVy%2BXnz8yMrLR7cDy8lOqq2uZk6G%2BvqHF9u1OVs0lWTebVXNJZPNGVs0lWTeblXJ55M3Ow4cPa8WKFXryyScdY//85z9dfvymTZtUUVGhYcOGKSEhQampqZKkhIQE9e7dW8XFxU7bFxcXKz4%2BXpIUGxurkpISx1x9fb3279/vmAcAAGiKxxWsoqIipaSkaMeOHY4rUF988YUeeOABlz9kdN68eXrrrbe0ceNGbdy4UXl5eZKkjRs3auzYsSotLVVhYaFqa2u1Z88e7dmzRxMmTJAkpaena8OGDdq3b59qamq0atUqBQQEaOjQoS2SFwAAWI/H3SJ84YUX9Pjjj%2BvBBx/UTTfdJOn8zydcsmSJVq5cqeHDhze5j9DQUKc3qtfV1UmSunTpIklas2aNcnJytGjRInXr1k1Lly5Vnz59JElDhgzRrFmzNHPmTFVUVCguLk55eXkKCgoyOyoAALAojytYn376qV577TVJcvqYhjvvvFPz58%2B/qn1ef/31jk9xl6RBgwY5ffjoxSZNmqRJkyZd1XMBAAB43C3CDh06XPJnEJaVlSkgIMANKwIAAGgejytYAwYM0HPPPafq6mrH2JEjRzR37lzddtttblwZAACAazzuFuGTTz6pBx98UAkJCaqvr9eAAQNUU1OjXr16acmSJe5eHgAAQJM8rmB16dJFW7Zs0Z49e3TkyBEFBQWpR48euv3226/5R%2BcAAAC0Bo8rWJLUpk0bjRgxwt3LAAAAuCoeV7CSk5OveKXK1c/CAgAAcBePK1ijR492Klj19fU6cuSIbDabHnzwQTeuDAAAwDUeV7DmzJlzyfG33npLf/vb31p5NQAAAM3ncR/TcDkjRozQn//8Z3cvAwAAoEleU7D2798vwzDcvQwAAIAmedwtwokTJzYaq6mp0aFDhzRy5Eg3rAgAAKB5PK5gRUVFNfouwsDAQI0bN07jx49306oAAABc53EFi09rBwAA3s7jCtaGDRtc3vaee%2B5pwZUAAABcHY8rWNnZ2WpoaGj0hnYfHx%2BnMR8fHwoWAADwSB5XsF5%2B%2BWWtXbtW06ZNU3R0tAzD0CeffKLf/OY3uv/%2B%2B5WQkODuJQIAAFyRxxWsJUuWKC8vT507d3aMDRw4UN27d9fDDz%2BsLVu2uHF1AAAATfO4z8H6/PPPFRoa2mg8JCREpaWlblgRAABA83hcwerWrZuWLFmiyspKx1hVVZWWL1%2Bu73//%2B25cGQAAgGs87hbh/PnzNXv2bBUUFKh9%2B/by9fVVdXW1goKCtHLlSncvDwAAoEkeV7AGDx6s3bt3a8%2BePTp%2B/LgMw1Dnzp31ox/9SB06dHD38gAAAJrkcQVLktq2bavhw4fr%2BPHj6t69u7uXAwAA0Cwe9x6sM2fOaO7cuerfv7/uuusuSeffgzVlyhRVVVW5eXUAAABN87iCtXTpUh04cEDLli2Tr%2B//L6%2B%2Bvl7Lli1z48oAAABc43EF66233tJLL72kO%2B%2B80/FDn0NCQrR48WLt2LHDzasDAABomscVrNOnTysqKqrReEREhL755pvWXxAAAEAzedyb3L///e/rb3/7mxISEpx%2B9uD27dv1ve99z40r80wDs7e7ewku2zbzdncvAQCAVuFxBWvSpEl67LHHdN9996mhoUGvvPKKiouL9dZbbyk7O9vdywMAAGiSxxWstLQ0%2Bfv767XXXpOfn59Wr16tHj16aNmyZbrzzjvdvTwAAIAmeVzBOnHihO677z7dd9997l4KAADAVfG4N7kPHz7c6b1XAAAA3sbjClZCQoK2bdvm7mUAAABcNY%2B7Rdi1a1c9%2B%2ByzysvL0/e//321adPGaX758uUu7efjjz/W4sWLVVxcrMDAQN1yyy3Kzs5Wp06dVFRUpOXLl%2Bvw4cPq2rWrpk6dqpSUFMdj8/PztW7dOpWXlys6OlrZ2dmKjY01NScAALAuj7uCdfDgQf3gBz9Qhw4dVFlZqbKyMqdfrjh79qweeugh3XLLLSoqKtKWLVtUUVGhhQsXqqysTDNmzNDEiRNVVFSk7Oxs/eIXv5DNZpMk7dy5UytWrNCvfvUrvf/%2B%2Bxo2bJimTZvGZ3ABAACXecwVrKysLL3wwgv6/e9/7xhbuXKlMjIymr2vmpoaZWVl6d5775W/v78iIiJ0xx136LXXXtPmzZsVFRWlcePGSZISExOVnJyswsJCxcXFqaCgQKmpqYqPj5ckTZkyRfn5%2Bdq1a5fGjBljTlgAAGBpHlOwdu7c2WgsLy/vqgpWaGioxo8f7/j68OHD%2BtOf/qS77rpLJSUliomJcdo%2BJibG8b6vkpISjR492jHn6%2Burvn37ymazuVywysrKVF5e7jTm799OkZGRzc5yJX5%2BHncB8or8/V1b74Vc3pbPFVbNZtVcEtm8kVVzSdbOJlkrl8cUrEt95%2BC1fjdhaWmpRo0apbq6Ok2YMEGZmZl65JFH1LlzZ6ftwsLCVFlZKUmy2%2B0KDQ11mg8NDXXMu6KgoEC5ublOYxkZGcrMzLzKJNYQHt6%2BWduHhLRtoZW4n1WzWTWXRDZvZNVcknWzWSmXxxSsCz/Yuamx5ujWrZtsNpv%2B/e9/65e//KWeeOIJlx53rcUuLS1NycnJTmP%2B/u1UWXn6mvZ7MW9r%2Bq7m9/PzVUhIW1VV1ai%2BvqGFV9W6rJrNqrkksnkjq%2BaSrJ1NUovkau4/7s3iMQWrpfj4%2BCgqKkpZWVmaOHGikpKSZLfbnbaprKxURESEJCk8PLzRvN1uV69evVx%2BzsjIyEa3A8vLT6muznonQ3M0N399fYNl/8ysms2quSSyeSOr5pKsm81KubzrEoiLioqKNGrUKDU0/P9B8vU9H/Wmm25ScXGx0/bFxcWON7XHxsaqpKTEMVdfX6/9%2B/c75gEAAJriMVewzp07p9mzZzc55srnYMXGxqq6ulpLly5VZmamampqtGLFCg0cOFDp6elau3atCgsLlZKSog8%2B%2BEB79uxRQUGBJCk9PV2zZs3S3XffrejoaP32t79VQECAhg4dalpWAABgbR5TsG6%2B%2BeZGn3N1qTFXdOjQQWvXrlVOTo5uvfVWtWvXTrfeequeffZZdezYUWvWrFFOTo4WLVqkbt26aenSperTp48kaciQIZo1a5ZmzpypiooKxcXFKS8vT0FBQabkBAAA1ucxBevbn39lhujo6Mvuc9CgQdq4ceNlHztp0iRNmjTJ1PUAAIDvDku%2BBwsAAMCdKFgAAAAmo2ABAACYzGPegwUAnmpg9nZ3L6FZts283d1LAL7zuIIFAABgMgoWAACAyShYAAAAJqNgAQAAmIyCBQAAYDIKFgAAgMkoWAAAACajYAEAAJiMggUAAGAyChYAAIDJKFgAAAAmo2ABAACYjIIFAABgMgoWAACAyShYAAAAJqNgAQAAmIyCBQAAYDIKFgAAgMkoWAAAACajYAEAAJiMggUAAGAyChYAAIDJKFgAAAAmo2ABAACYjIIFAABgMgoWAACAyShYAAAAJqNgAQAAmMyyBau0tFQZGRlKSEhQYmKi5s2bp6qqKknSgQMHdP/99%2Bvmm2/WyJEjtXbtWqfHbt26VWPHjlX//v2Vmpqq9957zx0RAACAl7JswZo2bZpCQkK0c%2BdOvfnmm/rss8/0/PPP68yZM5o6dapuvfVWvfvuu3rhhRe0Zs0a7dixQ9L58jV37lzNmTNHH3zwgSZPnqxHH31Ux48fd3MiAADgLSxZsKqqqhQbG6vZs2erffv26tKli%2B699179/e9/1%2B7du3Xu3DlNnz5d7dq1U79%2B/TR%2B/HgVFBRIkgoLC5WUlKSkpCQFBgYqJSVFvXv31qZNm9ycCgAAeAt/dy%2BgJYSEhGjx4sVOY8eOHVNkZKRKSkoUHR0tPz8/x1xMTIwKCwslSSUlJUpKSnJ6bExMjGw2m8vPX1ZWpvLycqcxf/92ioyMbG6UK/Lz865%2B7O/v2nov5PK2fK6wajar5pK8M9N3/Vyzai7J2tkka%2BWyZMG6mM1m02uvvaZVq1Zp27ZtCgkJcZoPCwuT3W5XQ0OD7Ha7QkNDneZDQ0N18OBBl5%2BvoKBAubm5TmMZGRnKzMy8%2BhAWEB7evlnbh4S0baGVuJ9Vs1k1l7fhXDvPqrkk62azUi7LF6y9e/dq%2BvTpmj17thITE7Vt27ZLbufj4%2BP4b8Mwruk509LSlJyc7DTm799OlZWnr2m/F/O2pu9qfj8/X4WEtFVVVY3q6xtaeFWty6rZrJpL8r7zTOJcs2ouydrZJLVIrub%2Bg8Msli5YO3fu1OOPP65f/OIXuueeeyRJERER%2Bvzzz522s9vtCgsLk6%2Bvr8LDw2W32xvNR0REuPxpFXPyAAAV8UlEQVS8kZGRjW4HlpefUl2d9U6G5mhu/vr6Bsv%2BmVk1m1VzeRvOtfOsmkuybjYr5fK%2Bf5q56B//%2BIfmzp2rF1980VGuJCk2NlaffPKJ6urqHGM2m03x8fGO%2BeLiYqd9fXseAACgKZYsWHV1dVqwYIHmzJmjwYMHO80lJSUpODhYq1atUk1NjT766COtX79e6enpkqQJEybo/fff1%2B7du1VbW6v169fr888/V0pKijuiAAAAL2TJW4T79u3ToUOHlJOTo5ycHKe57du3a/Xq1XrqqaeUl5en6667TllZWRo6dKgkqXfv3lq2bJkWL16s0tJS9ezZU2vWrFGnTp3ckAQAAHgjSxasgQMH6pNPPrniNq%2B//vpl50aOHKmRI0eavSwAAPAdYclbhAAAAO5EwQIAADAZBQsAAMBkFCwAAACTUbAAAABMRsECAAAwGQULAADAZBQsAAAAk1GwAAAATEbBAgAAMBkFCwAAwGQULAAAAJNRsAAAAExGwQIAADAZBQsAAMBkFCwAAACTUbAAAABMRsECAAAwGQULAADAZBQsAAAAk1GwAAAATEbBAgAAMBkFCwAAwGQULAAAAJNRsAAAAExGwQIAADAZBQsAAMBkFCwAAACTUbAAAABMRsECAAAwGQULAADAZBQsAAAAk1m2YL377rtKTExUVlZWo7mtW7dq7Nix6t%2B/v1JTU/Xee%2B855hoaGvTCCy9o%2BPDhGjRokB5%2B%2BGF98cUXrbl0AADg5SxZsH7zm98oJydHN9xwQ6O5AwcOaO7cuZozZ44%2B%2BOADTZ48WY8%2B%2BqiOHz8uSVq3bp02b96svLw87dq1S1FRUcrIyJBhGK0dAwAAeClLFqzAwECtX7/%2BkgWrsLBQSUlJSkpKUmBgoFJSUtS7d29t2rRJklRQUKDJkyfrxhtvVHBwsLKysnTo0CF99NFHrR0DAAB4KX93L6AlPPDAA5edKykpUVJSktNYTEyMbDabzpw5o4MHDyomJsYxFxwcrBtuuEE2m00//OEPXXr%2BsrIylZeXO435%2B7dTZGRkM1I0zc/Pu/qxv79r672Qy9vyucKq2ayaS/LOTN/1c82quSRrZ5OslcuSBetK7Ha7QkNDncZCQ0N18OBBnTx5UoZhXHK%2BsrLS5ecoKChQbm6u01hGRoYyMzOvfuEWEB7evlnbh4S0baGVuJ9Vs1k1l7fhXDvPqrkk62azUq7vXMGS1OT7qa71/VZpaWlKTk52GvP3b6fKytPXtN%2BLeVvTdzW/n5%2BvQkLaqqqqRvX1DS28qtZl1WxWzSV533kmca5ZNZdk7WySWiRXc//BYZbvXMEKDw%2BX3W53GrPb7YqIiFBYWJh8fX0vOd%2BxY0eXnyMyMrLR7cDy8lOqq7PeydAczc1fX99g2T8zq2azai5vw7l2nlVzSdbNZqVc3vdPs2sUGxur4uJipzGbzab4%2BHgFBgaqV69eKikpccxVVVXp6NGjuummm1p7qQAAwEt95wrWhAkT9P7772v37t2qra3V%2BvXr9fnnnyslJUWSlJ6ervz8fB06dEjV1dVatmyZ%2Bvbtq7i4ODevHAAAeAtL3iK8UIbq6uokSW%2B//bak81eqevfurWXLlmnx4sUqLS1Vz549tWbNGnXq1EmSNHHiRJWXl%2BsnP/mJTp8%2BrYSEhEZvWAcAALgSSxYsm812xfmRI0dq5MiRl5zz8fFRZmbmd/47/gAAwNX7zt0iBAAAaGkULAAAAJNRsAAAAExGwQIAADAZBQsAAMBkFCwAAACTUbAAAABMRsECAAAwGQULAADAZBQsAAAAk1GwAAAATEbBAgAAMBkFCwAAwGQULAAAAJNRsAAAAExGwQIAADAZBQsAAMBkFCwAAACTUbAAAABMRsECAAAwGQULAADAZBQsAAAAk1GwAAAATEbBAgAAMBkFCwAAwGQULAAAAJNRsAAAAExGwQIAADAZBQsAAMBkFCwAAACTUbAAAABMRsECAAAwGQXrEkpLS/Wzn/1MCQkJGjZsmJYuXaqGhgZ3LwsAAHgJf3cvwBM99thj6tevn95%2B%2B21VVFRo6tSpuu666/TTn/7U3UsDAABegCtYF7HZbPr44481Z84cdejQQVFRUZo8ebIKCgrcvTQAAOAluIJ1kZKSEnXr1k2hoaGOsX79%2BunIkSOqrq5WcHBwk/soKytTeXm505i/fztFRkaaulY/P%2B/qx/7%2Brq33Qi5vy%2BcKq2azai7JOzN91881q%2BaSrJ1NslYuCtZF7Ha7QkJCnMYulK3KykqXClZBQYFyc3Odxh599FE99thj5i1U54vcg10%2BU1pamunlzZ3Kysr06qsvWy6XZN1sVs0lWfc8k6x73KyaS2p%2Btr8/e2crrOralZWVacWKFZY6ZtapiiYyDOOaHp%2BWlqY333zT6VdaWppJq/t/5eXlys3NbXS1zNtZNZdk3WxWzSWRzRtZNZdk3WxWzMUVrItERETIbrc7jdntdvn4%2BCgiIsKlfURGRlqmgQMAgObjCtZFYmNjdezYMZ04ccIxZrPZ1LNnT7Vv396NKwMAAN6CgnWRmJgYxcXFafny5aqurtahQ4f0yiuvKD093d1LAwAAXsJv4cKFC929CE/zox/9SFu2bNEzzzyjP//5zxo3bpwefvhh%2Bfj4uHtpjbRv31633HKL5a6uWTWXZN1sVs0lkc0bWTWXZN1sVsvlY1zrO7oBAADghFuEAAAAJqNgAQAAmIyCBQAAYDIKFgAAgMkoWAAAACajYAEAAJiMggUAAGAyChYAAIDJKFgAAAAmo2B5mHfffVeJiYnKysq64nYNDQ164YUXNHz4cA0aNEgPP/ywvvjiC8e83W7XzJkzlZiYqMGDBys7O1tnzpxp6eVfUXOy5ebmKjk5Wf3791daWpr%2B/ve/O%2BZ/8pOfqF%2B/foqLi3P8SklJaenlX5aruebNm%2Bf4WZcXfg0cONAx783HbNSoUU654uLi1KdPH/3pT3%2BSJCUnJys2NtZpftq0aa0R4ZJKS0uVkZGhhIQEJSYmat68eaqqqrrktlu3btXYsWPVv39/paam6r333nPMNXUeukNzsu3YsUMpKSnq37%2B/Ro0apT/%2B8Y%2BOuRUrVqhv376NjuvXX3/dWlGcuJrrzTffVJ8%2BfRqt%2B1//%2Bpck7z5mCxYsaJQrJiZGTz75pKSmX2Na28cff6wHH3xQN998sxITEzVz5kyVl5dfctv8/HyNGjVKAwYMUHp6uoqLix1ztbW1%2BuUvf6khQ4YoISFBmZmZqqysbK0YV8%2BAx8jLyzNGjhxpTJw40Zg5c%2BYVt83PzzeGDRtmHDx40Dh16pTx9NNPG2PHjjUaGhoMwzCMRx991PjZz35mVFRUGMePHzfS0tKMZ555pjViXFJzsv32t781hg4danz66adGbW2t8dJLLxm33HKLcerUKcMwDOP%2B%2B%2B833njjjdZYdpOak2vu3LnGSy%2B9dNl5bz5mFzt69Khx2223GeXl5YZhGMawYcOMDz74oCWWeVXuvvtuY968eUZ1dbVx7NgxIzU11Zg/f36j7fbv32/ExsYau3fvNs6cOWNs3LjRiI%2BPN44dO2YYRtPnoTu4mu2jjz4y4uLijP/93/81zp07Z%2Bzevdvo16%2Bf8eGHHxqGYRgvvfSSMXfu3NZe/mW5muuNN94w7r///svux5uP2cXOnTtnjBkzxti9e7dhGE2/xrSm2tpa47bbbjNyc3ON2tpao6Kiwrj//vuNGTNmNNr2nXfeMQYOHGjs27fPqKmpMdasWWPcfvvtxunTpw3DMIzFixcbqampxn/%2B8x%2BjsrLSePTRR42pU6e2dqRm4wqWBwkMDNT69et1ww03NLltQUGBJk%2BerBtvvFHBwcHKysrSoUOH9NFHH%2Bnrr7/W22%2B/raysLEVERKhz586aMWOG3njjDZ07d64VkjTWnGy%2Bvr564okn1KtXLwUEBOihhx6S3W7Xp59%2B2gorbZ7m5LoSbz9mF3v22Wf10EMP6brrrmuBlV2bqqoqxcbGavbs2Wrfvr26dOmie%2B%2B91%2Bkq6QWFhYVKSkpSUlKSAgMDlZKSot69e2vTpk2SrnweukNzstntdk2dOlUjRoyQv7%2B/kpKS1Lt370tu627NydUUbz5mF3v11Vf1ve99T0lJSa2w0uapqalRVlaWpk6dqoCAAEVEROiOO%2B7QZ5991mjbgoICpaamKj4%2BXkFBQZoyZYokadeuXaqrq9P69es1Y8YMde3aVWFhYZo5c6Z2796tr776qrVjNQsFy4M88MAD6tChQ5PbnTlzRgcPHlRMTIxjLDg4WDfccINsNpsOHDggPz8/RUdHO%2Bb79eunb775RocPH26RtTfF1WySNHnyZN11112Or48fPy5JioyMdIxt3bpVo0ePVv/%2B/TV58mQdPXrU3AW7qDm5JOmDDz7QPffco/79%2B2vcuHGOy%2BDefsy%2B7YMPPtCBAwf0wAMPOI3n5%2BdrxIgR6t%2B/vzIzM1VRUWHWUpslJCREixcvdip/x44dc/r/64KSkhKn80ySYmJiZLPZmjwP3aE52YYMGaKMjAzH13V1dSovL1fnzp0dY5988okmTpyoAQMGaMyYMU63R1tTc3JdmPvpT3%2BqQYMGafjw4dq4caOkpl873aG52S6oqqrS6tWr9fjjjzuNX%2B41prWFhoZq/Pjx8vf3lyQdPnxYf/rTn5xe2y%2B4%2BDzz9fVV3759ZbPZdPToUZ06dUr9%2BvVzzN94440KCgpSSUlJywe5BhQsL3Ty5EkZhqHQ0FCn8dDQUFVWVsputys4OFg%2BPj5Oc5K84771t5w9e1bZ2dlKSUnR9ddfL%2Bn8ydWrVy/9z//8j9555x1FRERoypQpOnv2rJtXe2Xdu3fXDTfcoDVr1ujdd9/VwIED9dBDD1numK1evVo//elPFRAQ4Bjr27evbrrpJm3cuFFbt26V3W7Xz3/%2Bczeu8v/ZbDa99tprmj59eqM5u91%2B2fOsqfPQE1wp28WWLVumdu3aafTo0ZKkLl26qHv37nr%2B%2Bef117/%2BVePHj9e0adPcVvi/7Uq5IiIiFBUVpccff1x//etfNWvWLM2fP19FRUWWOmavvfaaBg0apF69ejnGrvQa4y6lpaWKjY3V6NGjFRcXp8zMzEbbXOk8s9vtks4X0W8LCQnxmGN2ORQsL2YYxlXNeYvq6mo98sgj8vPz06JFixzjCxcu1Ny5cxUWFqaIiAg9/fTTKi0t1d69e9242qZlZGToueeeU%2BfOnRUcHKzHH39cAQEBevvttyVZ45h9%2Bumn2rdvnyZMmOA0vnLlSk2dOlXt27dX165d9dRTT%2BnDDz9025XHC/bu3auHH35Ys2fPVmJi4iW3aeq4eOpxcyWbdH79S5cu1ZYtW7Rq1SoFBgZKksaPH6%2BXXnpJN9xwg9q2bavJkyerb9%2B%2Bjtuj7tJUrqFDh%2Brll19WTEyMAgICNGbMGN1xxx168803Hdt4%2BzGrr6/XunXrGl0lbuo1xh26desmm82m7du36/PPP9cTTzxxye289Ty7EgqWFwoLC5Ovr6%2Bj2V9gt9vVsWNHRUREqLq6WvX19U5zktSxY8dWXevVOnHihO6//3516NBBv/3tb9WuXbvLbhscHKzQ0FCPvx9/MT8/P3Xt2lVlZWWWOGaStH37dt16661XPF7S%2BRddSSorK2uNZV3Szp079bOf/Uzz589v9BfVBeHh4Zc8zyIiIpo8D93JlWzS%2Be%2Bomzdvnnbu3KnXX39dP/jBD664327dunn8MbuUC%2Bu2wjGTpA8//FBnz55t8jsEv/0a404%2BPj6KiopSVlaWtmzZohMnTjjNX%2Bk8i4iIcHz9bSdPnnT7MWsKBcsLBQYGqlevXk73n6uqqnT06FHddNNN6tu3rwzD0Mcff%2ByYt9lsCgkJUY8ePdyx5Gapra3V1KlT1a9fP7300ksKCgpyzFVXV2vhwoVOZerEiRM6ceKEunfv7o7lusQwDC1evNjpmJw9e1ZHjx5V9%2B7dvf6YXfDOO%2B/o9ttvdxorLS3VU0895XQL99ChQ5LktmP2j3/8Q3PnztWLL76oe%2B6557LbxcbGNnoPi81mU3x8fJPnobu4mk2SnnvuOX322Wd6/fXXGx2L//7v/1ZRUZHT2KFDhzz%2BmL3%2B%2BuvaunWr09iFdVvhmEnnz7Nbb73V8f4mqenXmNZWVFSkUaNGqaGhwTHm63u%2BcrRp08Zp29jYWKdjUl9fr/379ys%2BPl7du3dXaGio0/ynn36qs2fPKjY2toVTXBsKlpf46quvdOeddzo%2BryU9PV35%2Bfk6dOiQqqurtWzZMsdn1kRERGjUqFH69a9/rRMnTuj48eNauXKlxo0b53RCeoqLs61du1Zt2rTRM8884zghLwgODtZHH32knJwc2e12nTx5UosWLVJ0dLT69%2B/vjuVf1rdz%2Bfj46Msvv9SiRYv01Vdf6fTp01q2bJnatGmjESNGeP0xk86/mB88eNDxXrkLOnbsqJ07d2rJkiX65ptv9NVXX2nx4sUaNmyY0xuqW0tdXZ0WLFigOXPmaPDgwY3mH3zwQcdf0BMmTND777%2Bv3bt3q7a2VuvXr9fnn3/u%2BNy1K52H7tCcbHv37tWmTZuUl5ensLCwRtva7XYtWrRIhw8fVm1trdauXaujR4/q3nvvbfEcF2tOrrNnz%2BqZZ56RzWbTuXPntGXLFv3lL3/RxIkTJXn3MbvgwIEDjc6zpl5jWltsbKyqq6u1dOlS1dTU6MSJE1qxYoUGDhyoDh066M4773R8p2R6ero2bNigffv2qaamRqtWrVJAQICGDh0qPz8/TZgwQatXr9axY8dUWVmp//qv/9Idd9zhkd%2Bl/G2e98r9HXbhBK%2Brq5Mkx33zCy8UR44ccVwFmDhxosrLy/WTn/xEp0%2BfVkJCgnJzcx37evrpp/XUU09p%2BPDhatOmje6%2B%2B%2B4mPyyyJTUn2xtvvKFjx44pPj7eaR/Tp0/XjBkztHLlSj333HMaNWqUzp49q9tuu015eXmNylhraE6uZ599Vs8//7xSU1NVXV2tm266Sa%2B%2B%2Bqrjdpo3HzPp/F/IdXV1jV70goKC9PLLL2vJkiUaMmSIJOmOO%2B5wfDhia9u3b58OHTqknJwc5eTkOM1t375dX3zxhU6ePClJ6t27t5YtW6bFixertLRUPXv21Jo1a9SpUydJTZ%2BHra052d544w2dOnVKw4YNc9pu0KBBWrt2rWbPni3p/Hf12u129ezZU7/73e/UpUuX1gnzLc3J9cADD%2Bj06dP6%2Bc9/rvLycl1//fVauXKl42qHNx%2BzC8rLyy9ZLpp6jWlNHTp00Nq1a5WTk%2BN428Ctt96qZ599VpJ05MgRffPNN5LOf0frrFmzNHPmTFVUVCguLk55eXmOuxeZmZk6ffq0fvzjH6uurk7Dhg3TwoULWz1Tc/kY3vjOMQAAAA/GLUIAAACTUbAAAABMRsECAAAwGQULAADAZBQsAAAAk1GwAAAATEbBAgAAMBkFCwAAwGQULAAAAJNRsAAAAExGwQIAADAZBQsAAMBkFCwAAACT/R8JC4Y8CWYLUgAAAABJRU5ErkJggg%3D%3D&quot;/&gt; &lt;/div&gt; &lt;div role=&quot;tabpanel&quot; class=&quot;tab-pane col-md-12&quot; id=&quot;common-8665591789917202290&quot;&gt; Value Count Frequency (%) 3 709 54.2% 1 323 24.7% 2 277 21.2% &lt;/div&gt; &lt;div role=&quot;tabpanel&quot; class=&quot;tab-pane col-md-12&quot; id=&quot;extreme-8665591789917202290&quot;&gt; &lt;p class=&quot;h4&quot;&gt;Minimum 5 values&lt;/p&gt; Value Count Frequency (%) 1 323 24.7% 2 277 21.2% 3 709 54.2% &lt;p class=&quot;h4&quot;&gt;Maximum 5 values&lt;/p&gt; Value Count Frequency (%) 1 323 24.7% 2 277 21.2% 3 709 54.2% &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;col-md-3 namecol&quot;&gt; &lt;p class=&quot;h4 pp-anchor&quot; id=&quot;pp_var_sex&quot;&gt;sex&lt;br/&gt; &lt;small&gt;Categorical&lt;/small&gt; &lt;/p&gt; &lt;/div&gt;&lt;div class=&quot;col-md-3&quot;&gt; &lt;table class=&quot;stats &quot;&gt; &lt;tr class=&quot;&quot;&gt; &lt;th&gt;Distinct count&lt;/th&gt; &lt;td&gt;2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Unique (%)&lt;/th&gt; &lt;td&gt;0.2%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Missing (%)&lt;/th&gt; &lt;td&gt;0.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Missing (n)&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;table class=&quot;mini freq&quot;&gt; &lt;tr class=&quot;&quot;&gt; &lt;th&gt;male&lt;/th&gt; &lt;td&gt; &lt;div class=&quot;bar&quot; style=&quot;width:100%&quot; data-toggle=&quot;tooltip&quot; data-placement=&quot;right&quot; data-html=&quot;true&quot; data-delay=500 title=&quot;Percentage: 64.4%&quot;&gt; 843 &lt;/div&gt; &lt;/td&gt; female 466 &lt;/div&gt; &lt;/td&gt; &lt;/table&gt; &lt;a role=&quot;button&quot; data-toggle=&quot;collapse&quot; data-target=&quot;#freqtable3282040025542259819, #minifreqtable3282040025542259819&quot; aria-expanded=&quot;true&quot; aria-controls=&quot;collapseExample&quot;&gt; Toggle details &lt;/a&gt; Value Count Frequency (%) male 843 64.4% female 466 35.6% &lt;div class=&quot;col-md-3 namecol&quot;&gt; &lt;p class=&quot;h4 pp-anchor&quot; id=&quot;pp_var_sibsp&quot;&gt;sibsp&lt;br/&gt; &lt;small&gt;Numeric&lt;/small&gt; &lt;/p&gt; &lt;/div&gt;&lt;div class=&quot;col-md-6&quot;&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm-6&quot;&gt; &lt;table class=&quot;stats &quot;&gt; &lt;tr&gt; &lt;th&gt;Distinct count&lt;/th&gt; &lt;td&gt;7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Unique (%)&lt;/th&gt; &lt;td&gt;0.5%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Missing (%)&lt;/th&gt; &lt;td&gt;0.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Missing (n)&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Infinite (%)&lt;/th&gt; &lt;td&gt;0.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Infinite (n)&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-6&quot;&gt; &lt;table class=&quot;stats &quot;&gt; &lt;tr&gt; &lt;th&gt;Mean&lt;/th&gt; &lt;td&gt;0.49885&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Minimum&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Maximum&lt;/th&gt; &lt;td&gt;8&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;alert&quot;&gt; &lt;th&gt;Zeros (%)&lt;/th&gt; &lt;td&gt;68.1%&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMgAAABLCAYAAAA1fMjoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD%2BnaQAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAARxJREFUeJzt3MEJAjEUQEEVS7IIe/JsTxZhT/Eu8nADuovO3AP/8hICIfsxxtgBLx3WHgC27Lj2AM9Ol9viNffr%2BQOTgBMEkkAgCASCQCAIBIJAIAgEgkAgCASCQCAIBIJAIAgEgkAgCASCQCAIBIJAIAgEgkAgCASCQCAIBIJAIAgEgkAgCASCQCAIBMLmfnef4Ud4PsUJAkEgEAQCQSAQBAJBIBAEAkEgEAQCQSAQBAJBIBAEAkEgEH7iufsMT%2BR5x98GMmNpVDNBzYS71LdC/4VNaD/GGGsPAVvlDgJBIBAEAkEgEAQCQSAQBAJBIBAEAkEgEAQCQSAQBAJBIBAEAkEgEAQCQSAQBAJBIBAEAkEgEAQCQSAQBALhAdogGpWY44YEAAAAAElFTkSuQmCC&quot;&gt; &lt;a role=&quot;button&quot; data-toggle=&quot;collapse&quot; data-target=&quot;#descriptives-6934951144010511826,#minihistogram-6934951144010511826&quot; aria-expanded=&quot;false&quot; aria-controls=&quot;collapseExample&quot;&gt; Toggle details &lt;/a&gt; &lt;ul class=&quot;nav nav-tabs&quot; role=&quot;tablist&quot;&gt; &lt;li role=&quot;presentation&quot; class=&quot;active&quot;&gt;&lt;a href=&quot;#quantiles-6934951144010511826&quot; aria-controls=&quot;quantiles-6934951144010511826&quot; role=&quot;tab&quot; data-toggle=&quot;tab&quot;&gt;Statistics&lt;/a&gt;&lt;/li&gt; &lt;li role=&quot;presentation&quot;&gt;&lt;a href=&quot;#histogram-6934951144010511826&quot; aria-controls=&quot;histogram-6934951144010511826&quot; role=&quot;tab&quot; data-toggle=&quot;tab&quot;&gt;Histogram&lt;/a&gt;&lt;/li&gt; &lt;li role=&quot;presentation&quot;&gt;&lt;a href=&quot;#common-6934951144010511826&quot; aria-controls=&quot;common-6934951144010511826&quot; role=&quot;tab&quot; data-toggle=&quot;tab&quot;&gt;Common Values&lt;/a&gt;&lt;/li&gt; &lt;li role=&quot;presentation&quot;&gt;&lt;a href=&quot;#extreme-6934951144010511826&quot; aria-controls=&quot;extreme-6934951144010511826&quot; role=&quot;tab&quot; data-toggle=&quot;tab&quot;&gt;Extreme Values&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;div class=&quot;tab-content&quot;&gt; &lt;div role=&quot;tabpanel&quot; class=&quot;tab-pane active row&quot; id=&quot;quantiles-6934951144010511826&quot;&gt; &lt;div class=&quot;col-md-4 col-md-offset-1&quot;&gt; &lt;p class=&quot;h4&quot;&gt;Quantile statistics&lt;/p&gt; &lt;table class=&quot;stats indent&quot;&gt; &lt;tr&gt; &lt;th&gt;Minimum&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;5-th percentile&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Q1&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Median&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Q3&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;95-th percentile&lt;/th&gt; &lt;td&gt;2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Maximum&lt;/th&gt; &lt;td&gt;8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Range&lt;/th&gt; &lt;td&gt;8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Interquartile range&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;div class=&quot;col-md-4 col-md-offset-2&quot;&gt; &lt;p class=&quot;h4&quot;&gt;Descriptive statistics&lt;/p&gt; &lt;table class=&quot;stats indent&quot;&gt; &lt;tr&gt; &lt;th&gt;Standard deviation&lt;/th&gt; &lt;td&gt;1.0417&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Coef of variation&lt;/th&gt; &lt;td&gt;2.0881&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Kurtosis&lt;/th&gt; &lt;td&gt;20.043&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Mean&lt;/th&gt; &lt;td&gt;0.49885&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;MAD&lt;/th&gt; &lt;td&gt;0.67911&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;&quot;&gt; &lt;th&gt;Skewness&lt;/th&gt; &lt;td&gt;3.8442&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Sum&lt;/th&gt; &lt;td&gt;653&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Variance&lt;/th&gt; &lt;td&gt;1.0851&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Memory size&lt;/th&gt; &lt;td&gt;10.3 KiB&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;div role=&quot;tabpanel&quot; class=&quot;tab-pane col-md-8 col-md-offset-2&quot; id=&quot;histogram-6934951144010511826&quot;&gt; &lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAlgAAAGQCAYAAAByNR6YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD%2BnaQAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3X1UlHX%2B//EXMALJnZCBu1RrrWYiHg1v2EXzhkpN07zBEHePsZspRbIq7Ffz5pRnXbHU07roUWnTtDo16WlDMW%2B2KKuv7I1Z7YDmumbtwmqwwEQaptz8/ugn352sZZCPXTNXz8c5nZPXDNe83%2BLN02uGIaClpaVFAAAAMCbQ6gEAAADshsACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwjMACAAAwzGH1AN8V1dWfGT9nYGCAYmLCVFt7Vs3NLcbPbxW77iXZdze77iWxmz%2By616SfXe7kntdc02E0fN5iytYfiwwMEABAQEKDAywehSj7LqXZN/d7LqXxG7%2ByK57SfbdzY57EVgAAACGEVgAAACGEVgAAACGEVgAAACGEVgAAACGEVgAAACGEVgAAACGEVgAAACGEVgAAACGEVgAAACGEVgAAACGEVgAAACGEVgAAACGOaweAB0zcPFeq0fw2p65Q6weAQCAbwVXsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAwjsAAAAAyzbWAdOXJEM2bM0MCBAzVkyBDl5eWptrZWklRaWqq0tDQlJSVp3Lhx2rlzp8fHbtu2TaNHj1ZSUpIyMjJUVlZmxQoAAMBP2TKwGhsbNWvWLPXv318HDx5UcXGxamtr9eijj6qqqkoPPvigpk2bptLSUi1evFhLly6Vy%2BWSJJWUlKigoECPP/64Dh48qJEjRyorK0uff/65xVsBAAB/YcvAqq6uVnV1te6%2B%2B24FBwcrOjpad9xxh44ePapdu3ape/fuSktLU0hIiFJSUpSamqrt27dLkpxOpyZPnqx%2B/fopNDRUM2fOlCS9/vrrVq4EAAD8iMPqAa6EuLg49e7dW06nU7/4xS907tw57d%2B/XyNGjFB5ebkSEhI87p%2BQkKA9e/ZIksrLyzV27NjW2wIDA9W7d2%2B5XC6NGzfOq8evqqpSdXW1xzGHo7NiY2M7uJmnoCD/6mOHw7t5L%2B7lb/t5w6672XUvid38kV33kuy7mx33smVgBQYGqqCgQJmZmdq6daskafDgwcrNzdWDDz6ouLg4j/t36dJFdXV1kiS3262oqCiP26Oiolpv94bT6dS6des8jmVnZysnJ%2Bdy1rGN6Oiwdt0/MvKqKzSJ9ey6m133ktjNH9l1L8m%2Bu9lpL1sG1vnz55WVlaUxY8a0vn5q2bJlysvL8%2BrjW1paOvT46enpSk1N9TjmcHRWXd3ZDp33q/yt9L3dPygoUJGRV6m%2BvkFNTc1XeKpvl113s%2BteErv5I7vuJdl3tyu5V3v/cW%2BKLQOrtLRUFRUVmj9/voKCghQREaGcnBzdfffduvXWW%2BV2uz3uX1dXp5iYGElSdHT0Jbe73W717NnT68ePjY295OnA6urP1Nhon98Ml6O9%2Bzc1Ndv258yuu9l1L4nd/JFd95Lsu5ud9vKvSyBeampqUnNzs8eVqPPnz0uSUlJSLnnbhbKyMvXr10%2BSlJiYqPLyco9zHTlypPV2AACAttgysG655RZ17txZBQUFamhoUF1dnTZs2KBBgwbp7rvvVmVlpbZv364vvvhCBw4c0IEDB3TPPfdIkjIyMvTyyy/rvffeU0NDgzZs2KDg4GCNGDHC2qUAAIDfsGVgRUdH66mnntLhw4c1bNgw3XXXXQoNDdWaNWt09dVXa9OmTXr22Wc1YMAArVixQqtWrdLNN98sSRo2bJjmz5%2BvuXPnavDgwTp48KAKCwsVGhpq8VYAAMBf2PI1WNKXT/U988wzX3vboEGDVFRU9I0fO336dE2fPv1KjQYAAGzOllewAAAArERgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGEZgAQAAGGbrwNqwYYOGDh2q/v37KzMzUxUVFZKk0tJSpaWlKSkpSePGjdPOnTs9Pm7btm0aPXq0kpKSlJGRobKyMivGBwAAfsq2gfXcc89p586d2rZtm95%2B%2B2316NFDTz/9tKqqqvTggw9q2rRpKi0t1eLFi7V06VK5XC5JUklJiQoKCvT444/r4MGDGjlypLKysvT5559bvBEAAPAXtg2szZs3a968ebrxxhsVHh6uJUuWaMmSJdq1a5e6d%2B%2ButLQ0hYSEKCUlRampqdq%2Bfbskyel0avLkyerXr59CQ0M1c%2BZMSdLrr79u5ToAAMCP2DKwPvnkE1VUVOjTTz/V2LFjlZycrJycHNXW1qq8vFwJCQke909ISGh9GvCrtwcGBqp3796tV7gAAADa4rB6gCvh9OnTkqS9e/dqy5YtamlpUU5OjpYsWaJz584pLi7O4/5dunRRXV2dJMntdisqKsrj9qioqNbbvVFVVaXq6mqPYw5HZ8XGxl7OOt8oKMi/%2Btjh8G7ei3v5237esOtudt1LYjd/ZNe9JPvuZse9bBlYLS0tkqSZM2e2xtScOXN0//33KyUlxeuPv1xOp1Pr1q3zOJadna2cnJwOndffRUeHtev%2BkZFXXaFJrGfX3ey6l8Ru/siue0n23c1Oe9kysLp27SpJioyMbD0WHx%2BvlpYWXbhwQW632%2BP%2BdXV1iomJkSRFR0dfcrvb7VbPnj29fvz09HSlpqZ6HHM4Oquu7my79miLv5W%2Bt/sHBQUqMvIq1dc3qKmp%2BQpP9e2y62523UtiN39k170k%2B%2B52Jfdq7z/uTbFlYHXr1k3h4eE6evSo%2BvTpI0mqrKxUp06dNHz4cBUVFXncv6ysTP369ZMkJSYmqry8XJMmTZIkNTU16ciRI0pLS/P68WNjYy95OrC6%2BjM1NtrnN8PlaO/%2BTU3Ntv05s%2Btudt1LYjd/ZNe9JPvuZqe9/OsSiJccDofS0tK0ceNGffzxx6qpqdH69es1fvx4TZo0SZWVldq%2Bfbu%2B%2BOILHThwQAcOHNA999wjScrIyNDLL7%2Bs9957Tw0NDdqwYYOCg4M1YsQIa5cCAAB%2Bw5ZXsCQpNzdX58%2Bf19SpU3XhwgWNHj1aS5YsUVhYmDZt2qTly5dr2bJlio%2BP16pVq3TzzTdLkoYNG6b58%2Bdr7ty5qqmpUd%2B%2BfVVYWKjQ0FCLNwIAAP4ioKWjr%2BiGV6qrPzN%2BTocjUHesfsv4ea%2BUPXOHeHU/hyNQ0dFhqqs7a5tLxRfZdTe77iWxmz%2By616SfXe7kntdc02E0fN5y5ZPEQIAAFiJwAIAADCMwAIAADCMwAIAADCMwAIAADCMwAIAADDM5wIrNTVV69at06lTp6weBQAA4LL4XGBNmTJFr7zyim6//XbNnDlT%2B/fvV2Njo9VjAQAAeM3nAis7O1uvvPKKXnzxRfXs2VMrVqzQ8OHDtWrVKp08edLq8QAAANrkc4F1UZ8%2BfbRgwQK9/vrrWrRokV588UWNHTtW9913n/76179aPR4AAMA38tnAunDhgl555RXdf//9WrBggeLi4vTwww%2Brd%2B/eyszM1K5du6weEQAA4Gv53Dd7PnHihHbs2KGXX35ZZ8%2Be1ejRo7V161YNGDCg9T6DBg3So48%2BqvHjx1s4KQAAwNfzucAaN26cbrjhBs2ePVsTJ05Uly5dLrnP8OHDVVtba8F0AAAAbfO5wNq2bZsGDx7c5v3ef//9b2EaAACA9vO512D16tVLWVlZevXVV1uPPf3007r//vvldrstnAwAAMA7PhdY%2Bfn5%2Buyzz9SjR4/WYyNGjFBzc7NWrlxp4WQAAADe8bmnCN9%2B%2B23t2rVL0dHRrce6d%2B%2Bu1atX66677rJwMgAAAO/43BWsc%2BfOKSQk5JLjgYGBamhosGAiAACA9vG5wBo0aJBWrlypTz/9tPXYJ598omXLlnm8VQMAAICv8rmnCBctWqSf//zn%2BvGPf6zw8HA1Nzfr7Nmzuu666/TMM89YPR4AAECbfC6wrrvuOu3evVtvvvmm/vGPfygwMFA33HCDhg4dqqCgIKvHAwAAaJPPBZYkBQcH6/bbb7d6DAAAgMvic4H1z3/%2BU2vWrNHx48d17ty5S25/7bXXLJgKAADAez4XWIsWLVJVVZWGDh2qzp07Wz0OAABAu/lcYJWVlem1115TTEyM1aMAAABcFp97m4arr76aK1cAAMCv%2BVxgzZ49W%2BvWrVNLS4vVowAAAFwWn3uK8M0339Thw4f10ksv6dprr1VgoGcDvvDCCxZNBgAA4B2fC6zw8HANGzbM6jEAAAAum88FVn5%2BvtUjAAAAdIjPvQZLkj788EMVFBTo4Ycfbj327rvvWjgRAACA93wusEpLSzVhwgTt379fxcXFkr5889EZM2bwJqMAAMAv%2BFxgPfHEE/rlL3%2BpXbt2KSAgQNKX359w5cqVWr9%2BvcXTAQAAtM3nAutvf/ubMjIyJKk1sCRpzJgxOnHihFVjAQAAeM3nAisiIuJrvwdhVVWVgoODLZgIAACgfXwusJKSkrRixQqdOXOm9djJkye1YMEC/fjHP7ZwMgAAAO/43Ns0PPzww7r33nuVnJyspqYmJSUlqaGhQT179tTKlSutHg8AAKBNPhdY3bp1U3FxsQ4cOKCTJ08qNDRUN9xwg4YMGeLxmiwAAABf5XOBJUmdOnXS7bffbvUYAAAAl8XnAis1NfW/XqnivbAAAICv87nAGjt2rEdgNTU16eTJk3K5XLr33nstnAwAAMA7PhdYeXl5X3t83759%2BtOf/vQtTwMAANB%2BPvc2Dd/k9ttv1%2B7du60eAwAAoE1%2BE1hHjhxRS0uL1WMAAAC0yeeeIpw2bdolxxoaGnTixAmNGjXKgokAAADax%2BcCq3v37pd8FWFISIjS0tI0depUi6YCAADwns8FFu/WDgAA/J3PBdbLL7/s9X0nTpx4BScBAAC4PD4XWIsXL1Zzc/MlL2gPCAjwOBYQEEBgAQAAn%2BRzgfW73/1OmzdvVlZWlnr16qWWlhYdO3ZMTz75pH76058qOTnZ6hEBAAD%2BK58LrJUrV6qwsFBxcXGtxwYOHKjrrrtO9913n4qLiy2cDgAAoG0%2B9z5YH330kaKioi45HhkZqcrKSgsmAgAAaB%2BfC6z4%2BHitXLlSdXV1rcfq6%2Bu1Zs0aXX/99RZOBgAA4B2fe4pw0aJFys3NldPpVFhYmAIDA3XmzBmFhoZq/fr1Vo8HAADQJp8LrKFDh%2BqNN97QgQMHdPr0abW0tCguLk633nqrIiIirB4PAACgTT4XWJJ01VVX6bbbbtPp06d13XXXWT0OAABAu/jca7DOnTunBQsW6JZbbtGdd94p6cvXYM2cOVP19fUWTwcAANA2nwusVatW6ejRo1q9erUCA/9vvKamJq1evdrCyQAAALzjc4G1b98%2B/fa3v9WYMWNav%2BlzZGSk8vPztX///ss654oVK9SrV6/WH5eWliotLU1JSUkaN26cdu7c6XH/bdu2afTo0UpKSlJGRobKysoufyEAAPCd43OBdfbsWXXv3v2S4zExMfr888/bfb6jR4%2BqqKio9cdVVVV68MEHNW3aNJWWlmrx4sVaunSpXC6XJKmkpEQFBQV6/PHHdfDgQY0cOVJZWVmX9dgAAOC7yecC6/rrr9ef/vQnSfL43oN79%2B7V97///Xadq7m5WY888ogyMzNbj%2B3atUvdu3dXWlqaQkJClJKSotTUVG3fvl2S5HQ6NXnyZPXr10%2BhoaGaOXOmJOn111/v4GYAAOC7wue%2BinD69OmaM2eOpkyZoubmZm3ZskVlZWXat2%2BfFi9e3K5zvfDCCwoJCdH48eP1m9/8RpJUXl6uhIQEj/slJCRoz549rbePHTu29bbAwED17t1bLpdL48aN8%2Bpxq6qqVF1d7XHM4eis2NjYds3flqAgn%2Bvj/8rh8G7ei3v5237esOtudt1LYjd/ZNe9JPvuZse9fC6w0tPT5XA49OyzzyooKEgbN27UDTfcoNWrV2vMmDFen%2Bff//63CgoK9Mwzz3gcd7vdHt/nUJK6dOnS%2Bs7xbrf7km/VExUV5fHO8m1xOp1at26dx7Hs7Gzl5OR4fQ47io4Oa9f9IyOvukKTWM%2Buu9l1L4nd/JFd95Lsu5ud9vK5wKqtrdWUKVM0ZcqUDp0nPz9fkydPVo8ePVRRUdGuj/3PpyYvR3p6ulJTUz2OORydVVd3tkPn/Sp/K31v9w8KClRk5FWqr29QU1PzFZ7q22XX3ey6l8Ru/siue0n23e1K7tXef9yb4nOBddttt%2Bnw4cOtX0F4OUpLS/Xuu%2B%2BquLj4ktuio6Pldrs9jtXV1SkmJuYbb3e73erZs6fXjx8bG3vJ04HV1Z%2BpsdE%2BvxkuR3v3b2pqtu3PmV13s%2BteErv5I7vuJdl3Nzvt5XOXQJKTk1tfD3W5du7cqZqaGo0cOVLJycmaPHly67lvuummS952oaysTP369ZMkJSYmqry8vPW2pqYmHTlypPV2AACAtvjcFazvfe97%2BvWvf63CwkJdf/316tSpk8fta9asafMcCxcu1C9%2B8YvWH58%2BfVrp6ekqKipSc3OzNm3apO3bt2vChAn64x//qAMHDsjpdEqSMjIyNH/%2BfN11113q1auXnnrqKQUHB2vEiBFG9wQAAPblc4H197//XTfeeKMkteuF5f8pKirK44XqjY2NkqRu3bpJkjZt2qTly5dr2bJlio%2BP16pVq3TzzTdLkoYNG6b58%2Bdr7ty5qqmpUd%2B%2BfVVYWKjQ0NCOrAUAAL5DfCaw5s2bpyeeeMLjq/7Wr1%2Bv7OzsDp/72muv1bFjx1p/PGjQII83H/2q6dOna/r06R1%2BXAAA8N3kM6/BKikpueRYYWGhBZMAAAB0jM8E1te9NUJH3y4BAADACj4TWF/3tgwdeasGAAAAq/hMYAEAANgFgQUAAGCYz3wV4YULF5Sbm9vmMW/eBwsAAMBKPhNYAwYMUFVVVZvHAAAAfJ3PBNZ/vv8VAACAP%2BM1WAAAAIYRWAAAAIYRWAAAAIYRWAAAAIYRWAAAAIYRWAAAAIYRWAAAAIYRWAAAAIYRWAAAAIYRWAAAAIYRWAAAAIYRWAAAAIYRWAAAAIY5rB4A3x13/uZ/rR6hXfbMHWL1CAAAP8UVLAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMMILAAAAMNsG1iVlZXKzs5WcnKyUlJStHDhQtXX10uSjh49qp/%2B9KcaMGCARo0apc2bN3t87CuvvKLx48frlltu0eTJk/X2229bsQIAAPBTtg2srKwsRUZGqqSkRC%2B99JKOHz%2Buxx57TOfOndPs2bP1ox/9SG%2B99ZaeeOIJbdq0Sfv375f0ZXwtWLBAeXl5%2BuMf/6jMzEw99NBDOn36tMUbAQAAf2HLwKqvr1diYqJyc3MVFhambt26adKkSTp06JDeeOMNXbhwQQ888IA6d%2B6sPn36aOrUqXI6nZKk7du3a/jw4Ro%2BfLhCQkI0YcIE3XTTTdq5c6fFWwEAAH9hy8CKjIxUfn6%2Bunbt2nrs1KlTio2NVXl5uXr16qWgoKDW2xISElRWViZJKi8vV0JCgsf5EhIS5HK5vp3hAQCA33NYPcC3weVy6dlnn9WGDRu0Z88eRUZGetzepUsXud1uNTc3y%2B12KyoqyuP2qKgo/f3vf/f68aqqqlRdXe1xzOHorNjY2Mtf4msEBdmyj32Gw2H%2B5/fi58xunzu77iWxmz%2By616SfXez4162D6x33nlHDzzwgHJzc5WSkqI9e/Z87f0CAgJa/7%2BlpaVDj%2Bl0OrVu3TqPY9nZ2crJyenQefHtio4Ou2Lnjoy86oqd20p23UtiN39k170k%2B%2B5mp71sHVglJSX65S9/qaVLl2rixImSpJiYGH300Uce93O73erSpYsCAwMVHR0tt9t9ye0xMTFeP256erpSU1M9jjkcnVVXd/byFvkGdip9X2T68yV9%2BTmLjLxK9fUNampqNn5%2Bq9h1L4nd/JFd95Lsu9uV3OtK/mP5v7FtYB0%2BfFgLFizQ2rVrNXTo0NbjiYmJev7559XY2CiH48v1XS6X%2BvXr13r7xddjXeRyuTRu3DivHzs2NvaSpwOrqz9TY6N9fjN8F1zJz1dTU7Mtfz3YdS%2BJ3fyRXfeS7Lubnfay5SWQxsZGLVmyRHl5eR5xJUnDhw9XeHi4NmzYoIaGBr3//vvasWOHMjIyJEn33HOPDh48qDfeeENffPGFduzYoY8%2B%2BkgTJkywYhUAAOCHbHkF67333tOJEye0fPlyLV%2B%2B3OO2vXv3auPGjXrkkUdUWFiorl27at68eRoxYoQk6aabbtLq1auVn5%2BvyspK9ejRQ5s2bdI111xjwSYAAMAf2TKwBg4cqGPHjv3X%2Bzz//PPfeNuoUaM0atQo02MBAIDvCFs%2BRQgAAGAlAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwh9UDAL7qzt/8r9UjtMueuUOsHgEA8P9xBQsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwAgsAAMAwvooQANowcPFeq0doF76iFLAegQXYhD%2B9rQQBAMDueIoQAADAMAILAADAMALra1RWVmrWrFlKTk7WyJEjtWrVKjU3N1s9FgAA8BO8ButrzJkzR3369NGrr76qmpoazZ49W127dtXPfvYzq0cDAMBr/vTazEO/HmP1CEYRWF/hcrn0wQcfaMuWLYqIiFBERIQyMzO1detWAguAX/Cnv1T5ggfYFYH1FeXl5YqPj1dUVFTrsT59%2BujkyZM6c%2BaMwsPD2zxHVVWVqqurPY45HJ0VGxtrdNagIJ7hhX9yOPzn1y6/z66sK/Fr4eLnzI6fOzvvJtlrLwLrK9xutyIjIz2OXYyturo6rwLL6XRq3bp1HsceeughzZkzx9yg%2BjLk7u12XOnp6cbjzUpVVVVyOp2220uy72523Uuy7%2B8zyb6ft6qqKm3d%2Bjvb7SW1fzd/edqtqqpKBQUFtvqc2ScVDWppaenQx6enp%2Bull17y%2BC89Pd3QdP%2Bnurpa69atu%2BRqmb%2Bz616SfXez614Su/kju%2B4l2Xc3O%2B7FFayviImJkdvt9jjmdrsVEBCgmJgYr84RGxtrmwIHAADtxxWsr0hMTNSpU6dUW1vbeszlcqlHjx4KCwuzcDIAAOAvCKyvSEhIUN%2B%2BfbVmzRqdOXNGJ06c0JYtW5SRkWH1aAAAwE8EPfroo49aPYSvufXWW1VcXKxf/epX2r17t9LS0nTfffcpICDA6tEuERYWpsGDB9vu6ppd95Lsu5td95LYzR/ZdS/JvrvZba%2BAlo6%2BohsAAAAeeIoQAADAMAILAADAMAILAADAMAILAADAMAILAADAMAILAADAMAILAADAMAILAADAMAILAADAMALLD1VWVmrWrFlKTk7WyJEjtWrVKjU3N1s9ljFvvfWWUlJSNG/ePKtHMaqyslLZ2dlKTk5WSkqKFi5cqPr6eqvH6rAPPvhA9957rwYMGKCUlBTNnTtX1dXVVo9l1IoVK9SrVy%2BrxzCmV69eSkxMVN%2B%2BfVv/%2B9WvfmX1WMZs2LBBQ4cOVf/%2B/ZWZmamKigqrR%2Bqwv/zlLx6fr759%2ByoxMdEWvy6PHDmiGTNmaODAgRoyZIjy8vJUW1tr9VgdRmD5oTlz5iguLk6vvvqqtmzZoldffVVbt261eiwjnnzySS1fvlw/%2BMEPrB7FuKyhmNzSAAAGnklEQVSsLEVGRqqkpEQvvfSSjh8/rscee8zqsTrk/Pnz%2BvnPf67BgwertLRUxcXFqqmpkZ2%2BxenRo0dVVFRk9RjG7d27Vy6Xq/W/pUuXWj2SEc8995x27typbdu26e2331aPHj309NNPWz1Whw0aNMjj8%2BVyufTQQw/pzjvvtHq0DmlsbNSsWbPUv39/HTx4UMXFxaqtrbXFnyEElp9xuVz64IMPlJeXp4iICHXv3l2ZmZlyOp1Wj2ZESEiIduzYYbvAqq%2BvV2JionJzcxUWFqZu3bpp0qRJOnTokNWjdUhDQ4PmzZun2bNnKzg4WDExMbrjjjt0/Phxq0czorm5WY888ogyMzOtHgVe2rx5s%2BbNm6cbb7xR4eHhWrJkiZYsWWL1WMb961//0pYtW/Q///M/Vo/SIdXV1aqurtbdd9%2Bt4OBgRUdH64477tDRo0etHq3DCCw/U15ervj4eEVFRbUe69Onj06ePKkzZ85YOJkZM2bMUEREhNVjGBcZGan8/Hx17dq19dipU6cUGxtr4VQdFxUVpalTp8rhcEiSPvzwQ/3%2B97/3%2B39VX/TCCy8oJCRE48ePt3oU49asWaMRI0Zo4MCBWrp0qc6ePWv1SB32ySefqKKiQp9%2B%2BqnGjh2r5ORk5eTk2OLppq9au3atpkyZou9///tWj9IhcXFx6t27t5xOp86ePauamhrt379fI0aMsHq0DiOw/Izb7VZkZKTHsYuxVVdXZ8VIuAwul0vPPvusHnjgAatHMaKyslKJiYkaO3as%2Bvbtq5ycHKtH6rB///vfKigo0COPPGL1KMb1799fKSkp2r9/v5xOp9577z0tW7bM6rE67PTp05K%2BfPpzy5YtKioq0unTp213BauiokL79%2B/Xz372M6tH6bDAwEAVFBTotddeU1JSklJSUtTY2Kjc3FyrR%2BswAssPtbS0WD0COuCdd97Rfffdp9zcXKWkpFg9jhHx8fFyuVzau3evPvroI79/2kKS8vPzNXnyZPXo0cPqUYxzOp2aOnWqgoOD9cMf/lB5eXkqLi7W%2BfPnrR6tQy7%2B2Thz5kzFxcWpW7dumjNnjkpKSvTFF19YPJ05zz33nEaNGqVrrrnG6lE67Pz588rKytKYMWN06NAhvfnmm4qIiFBeXp7Vo3UYgeVnYmJi5Ha7PY653W4FBAQoJibGoqngrZKSEs2aNUuLFi3SjBkzrB7HqICAAHXv3l3z5s1rfaGqvyotLdW7776r7Oxsq0f5Vlx77bVqampSTU2N1aN0yMWn4P/zKn98fLxaWlr8frf/tG/fPqWmplo9hhGlpaWqqKjQ/PnzFRERobi4OOXk5OgPf/jDJX/X%2BRsCy88kJibq1KlTHn95uVwu9ejRQ2FhYRZOhrYcPnxYCxYs0Nq1azVx4kSrxzGitLRUo0eP9nibkMDAL/9Y6dSpk1VjddjOnTtVU1OjkSNHKjk5WZMnT5YkJScna/fu3RZP1zFHjhzRypUrPY6dOHFCwcHBfv%2BawG7duik8PNzjBdKVlZXq1KmT3%2B920dGjR1VZWakhQ4ZYPYoRTU1Nam5u9nhmxt%2BvpF5EYPmZhIQE9e3bV2vWrNGZM2d04sQJbdmyRRkZGVaPhv%2BisbFRS5YsUV5enoYOHWr1OMYkJibqzJkzWrVqlRoaGlRbW6uCggINHDjQr79YYeHChdq3b5%2BKiopUVFSkwsJCSVJRUZHfXzm4%2Buqr5XQ6VVhYqPPnz%2BvkyZNau3at0tPTFRQUZPV4HeJwOJSWlqaNGzfq448/Vk1NjdavX6/x48e3fiGGvzty5Ii6dOmi8PBwq0cx4pZbblHnzp1VUFCghoYG1dXVacOGDRo0aJC6dOli9XgdEtDCC3r8zunTp7V06VL9%2Bc9/Vnh4uKZNm6aHHnpIAQEBVo/WYX379pX0ZZBIav1D0eVyWTaTCYcOHdJPfvITBQcHX3Lb3r17FR8fb8FUZhw7dkzLly/XX//6V3Xu3Fk/%2BtGPtHDhQsXFxVk9mjEVFRW67bbbdOzYMatHMeIvf/mL1qxZo2PHjik4OFiTJk3SvHnzFBISYvVoHXb%2B/Hnl5%2Bdr9%2B7dunDhgkaPHq2lS5fa5gr/pk2btGvXLhUXF1s9ijFlZWV67LHH9MEHHyg4OFiDBw%2B2xZ8hBBYAAIBhPEUIAABgGIEFAABgGIEFAABgGIEFAABgGIEFAABgGIEFAABgGIEFAABgGIEFAABgGIEFAABgGIEFAABgGIEFAABgGIEFAABgGIEFAABg2P8DjiCk3ZLeIcwAAAAASUVORK5CYII%3D&quot;/&gt; &lt;/div&gt; &lt;div role=&quot;tabpanel&quot; class=&quot;tab-pane col-md-12&quot; id=&quot;common-6934951144010511826&quot;&gt; Value Count Frequency (%) 0 891 68.1% 1 319 24.4% 2 42 3.2% 4 22 1.7% 3 20 1.5% 8 9 0.7% 5 6 0.5% &lt;/div&gt; &lt;div role=&quot;tabpanel&quot; class=&quot;tab-pane col-md-12&quot; id=&quot;extreme-6934951144010511826&quot;&gt; &lt;p class=&quot;h4&quot;&gt;Minimum 5 values&lt;/p&gt; Value Count Frequency (%) 0 891 68.1% 1 319 24.4% 2 42 3.2% 3 20 1.5% 4 22 1.7% &lt;p class=&quot;h4&quot;&gt;Maximum 5 values&lt;/p&gt; Value Count Frequency (%) 2 42 3.2% 3 20 1.5% 4 22 1.7% 5 6 0.5% 8 9 0.7% &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;col-md-3 namecol&quot;&gt; &lt;p class=&quot;h4 pp-anchor&quot; id=&quot;pp_var_survived&quot;&gt;survived&lt;br/&gt; &lt;small&gt;Boolean&lt;/small&gt; &lt;/p&gt; &lt;/div&gt;&lt;div class=&quot;col-md-6&quot;&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm-6&quot;&gt; &lt;table class=&quot;stats &quot;&gt; &lt;tr class=&quot;&quot;&gt; &lt;th&gt;Distinct count&lt;/th&gt; &lt;td&gt;2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Unique (%)&lt;/th&gt; &lt;td&gt;0.2%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Missing (%)&lt;/th&gt; &lt;td&gt;0.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Missing (n)&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-6&quot;&gt; &lt;table class=&quot;stats &quot;&gt; &lt;tr&gt; &lt;th&gt;Mean&lt;/th&gt; &lt;td&gt;0.38197&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;table class=&quot;mini freq&quot;&gt; &lt;tr class=&quot;&quot;&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt; &lt;div class=&quot;bar&quot; style=&quot;width:100%&quot; data-toggle=&quot;tooltip&quot; data-placement=&quot;right&quot; data-html=&quot;true&quot; data-delay=500 title=&quot;Percentage: 61.8%&quot;&gt; 809 &lt;/div&gt; &lt;/td&gt; 1 500 &lt;/div&gt; &lt;/td&gt; &lt;/table&gt; &lt;a role=&quot;button&quot; data-toggle=&quot;collapse&quot; data-target=&quot;#freqtable-6456693858158561434, #minifreqtable-6456693858158561434&quot; aria-expanded=&quot;true&quot; aria-controls=&quot;collapseExample&quot;&gt; Toggle details &lt;/a&gt; Value Count Frequency (%) 0 809 61.8% 1 500 38.2% &lt;div class=&quot;col-md-3 namecol&quot;&gt; &lt;p class=&quot;h4 pp-anchor&quot; id=&quot;pp_var_ticket&quot;&gt;ticket&lt;br/&gt; &lt;small&gt;Categorical&lt;/small&gt; &lt;/p&gt; &lt;/div&gt;&lt;div class=&quot;col-md-3&quot;&gt; &lt;table class=&quot;stats &quot;&gt; &lt;tr class=&quot;alert&quot;&gt; &lt;th&gt;Distinct count&lt;/th&gt; &lt;td&gt;939&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Unique (%)&lt;/th&gt; &lt;td&gt;71.7%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Missing (%)&lt;/th&gt; &lt;td&gt;0.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;ignore&quot;&gt; &lt;th&gt;Missing (n)&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;table class=&quot;mini freq&quot;&gt; &lt;tr class=&quot;&quot;&gt; &lt;th&gt;CA. 2343&lt;/th&gt; &lt;td&gt; &lt;div class=&quot;bar&quot; style=&quot;width:1%&quot; data-toggle=&quot;tooltip&quot; data-placement=&quot;right&quot; data-html=&quot;true&quot; data-delay=500 title=&quot;Percentage: 0.8%&quot;&gt; &amp;nbsp; &lt;/div&gt; 11 &lt;/td&gt; CA 2144 &amp;nbsp; &lt;/div&gt; 8 &lt;/td&gt; 1601 &amp;nbsp; &lt;/div&gt; 8 &lt;/td&gt; Other values (936) 1282 &lt;/div&gt; &lt;/td&gt; &lt;/table&gt; &lt;a role=&quot;button&quot; data-toggle=&quot;collapse&quot; data-target=&quot;#freqtable6114187472754448460, #minifreqtable6114187472754448460&quot; aria-expanded=&quot;true&quot; aria-controls=&quot;collapseExample&quot;&gt; Toggle details &lt;/a&gt; Value Count Frequency (%) CA. 2343 11 0.8% CA 2144 8 0.6% 1601 8 0.6% 347082 7 0.5% 3101295 7 0.5% S.O.C. 14879 7 0.5% 347077 7 0.5% PC 17608 7 0.5% 19950 6 0.5% 113781 6 0.5% Other values (929) 1235 94.3% &lt;h1&gt;Correlations&lt;/h1&gt; &lt;/div&gt; &lt;div class=&quot;row variablerow&quot;&gt; &lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAmQAAAIZCAYAAAAIrSOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD%2BnaQAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3XlcVdX%2B//HXYTYREAU0hzQnxBHUHDNFTTPLrrdy%2BNqVrKxEDNKyLMvMhnvTNIcs6zrdrItXS027PxXLRhscUkRRQ8oxgeQgqMzn9wdxrkex0I17o76fj8d5HFh7%2BKy9FPicz15nHZvD4XAgIiIiIpZxs7oDIiIiItc6JWQiIiIiFlNCJiIiImIxJWQiIiIiFlNCJiIiImIxJWQiIiIiFlNCJiIiImIxJWQiIiIiFlNCJiIiImIxJWQiIiIiFvOwugMiUnGaNWt2wW2enp4EBATQsmVL7rrrLvr27YvNZjOxdyIiciE2fZalyNWjNCFr3bo1QUFBLtvy8vI4cOAAR48eBaBnz57MmjULLy8v0/spIiKulJCJXEVKE7K5c%2BfSu3fvMvdJSEjgiSee4PTp04wcOZIJEyaY2UURESmD5pCJXGN69%2B7NuHHjAPjggw/Iy8uzuEciIqI5ZCLXoFtvvZUXX3yRM2fOsHv3bsLDw122b968maVLl/Ljjz9it9vx9fUlNDSUv/71r9xxxx1lntNut7Nw4UI%2B//xzfvnlF/Lz8/H396d169bcf//9dOzY8bxjIiMjOXLkCO%2B88w55eXnMmDGDQ4cOsWjRItq1awfAvn37WLBgAVu2bOH48eN4eHgQHBzMTTfdxPDhw8ucN5ednc3ixYvZuHEjv/zyCwUFBdSsWZOIiAiioqJo1aqVy/6HDx%2BmV69eAOzcuZOUlBTmzZvH9u3bsdvtBAcH06tXL%2BLi4rjuuusuacxFRP6IKmQi16CAgADn1ydPnnTZNmPGDKKiotiwYQO%2Bvr507NiRwMBANm/ezPjx44mNjaWoqMjlmPT0dAYNGsRbb73FgQMHCAsLo1OnTnh5efHZZ58xYsQIVqxYccH%2BHDhwgLi4ODw9PenUqRM%2BPj4AfP/99/z1r3/lo48%2BIi8vj3bt2hEeHk5OTg7Lli3j3nvvZfPmzS7nOnLkCH/5y1%2BYPXs2P/30E6GhoXTq1AmHw8GaNWu49957Wb58%2BQX7sm3bNoYNG0ZSUhKhoaE0bNiQI0eOsGTJEmJiYso9xiIiF8UhIleNpk2bOpo2berYsGHDH%2B6XkpLi3Hfnzp3O9vXr1zuaNm3q6NChg%2BO7775zOeb77793dOnSxdG0aVPHwoULXbZNnTrV0bRpU0ffvn0dGRkZzvbCwkLHSy%2B95GjatKmjXbt2jpycHJfjevbs6WjatKkjMjLSMW/evPP6ec899ziaNm3qePXVVx1FRUUu550xY4ajadOmjgEDBrgcM2zYMEfTpk0dgwYNchw/ftzZXlRU5Jg9e7ajadOmjpYtWzoOHDjg3Hbo0CHnePTo0cMxf/58R3FxsXP7Rx995Ny%2BZ8%2BePxpaEZFLogqZyDVo/fr1APj7%2B9O8eXNn%2B7x58wCYMGECN910k8sxHTp04MknnwRg8eLFLtuCg4O5/fbbGTNmDDVq1HC2u7u7ExcXh5ubG9nZ2fz4449l9qe4uJhRo0ad175nzx4ABg0ahJvb/35dubu7M3bsWMaOHct9991Hfn4%2BADt27GDLli0A/OMf/yA4ONh5jJubG2PGjKFFixbk5%2Bfz73//u8y%2BNGnShIceeshlSZCBAwcSGBjojCEiUtE0h0zkGvP555/z9ttvA/Dggw/i4VHya%2BD48eMkJSUB0KdPnzKP7d27NzabjaNHj5KamkrDhg0BeOihhy4Yr0qVKtSoUYP09HTS09PL3KdTp04uCVcpPz8/MjIy%2BOSTT3jsscdctrm5uREdHe3S9uWXXwIl7zZt1KjRBa8hKSnpvFudpcqaI2ez2ahXrx4nTpwgMzOzzONERIxQQiZyFXr77bf58MMPXdry8/P5%2BeefOXToEFBSdXrwwQed2/ft2%2Bf8%2BqmnnrrguT08PCgoKODnn392JmSl5//888/ZtWsXaWlpnDx5Esfvq%2BpkZ2cDJZWwsoSEhJTZPmLECKZPn86bb77Jl19%2ByYABA%2BjcufMFF8D96aefgJIq14XceOONQMm8tbLUr1%2B/zHZvb28ACgoKLnhuEZFLpYRM5Cq0c%2BfO89rc3d2pXr06vXr1YvDgwdxyyy0u27Oyspxfb9y48U9jlCZZAMnJyURHR3P48OFL6q%2B/v3%2BZ7aW3Md9%2B%2B20SExNJTEwEICgoiH79%2BjFixAjq1at3Xp/8/PwuGKtatWpASWKVm5vrfANBKS2UKyJWUEImchX6o4VhL6R0zpSnpyeJiYnl/lil3NxcHn30UY4ePUqDBg0YPXo0nTt3pnr16nh6egL/W97iz2KXZdSoUQwZMoSEhAQ%2B//xzvvnmG9LT0/nXv/5FfHw8//jHP7jtttvKfZ2Os9bCLus2qYiIFZSQiQjwv6UwCgoKyMzMdE5i/zNffPEFR48exWazMX/%2BfG644Ybz9snNzTXUNz8/PwYNGsSgQYMoKirim2%2B%2BYe7cuWzfvp1nnnmGTp06Ub16dWel7dylPM5Wus3Hx0fVMBGpNPTyUEQAaNq0qfPr/fv3l/u4n3/%2BGSiZe1VWMnbw4EF%2B%2B%2B03w/0r5e7uzs0338ySJUsICQnh1KlTzlu0pXPHzp4Pd67SbX80z0xExGxKyEQEKJmX1aJFCwCWLVtW5j6pqakMHDjQuTwG/G9O1oU%2Bgmnu3LnOr89dUPaP7Nixg4kTJ7JgwYIyt3t5eVGzZk3gfxW40nlx%2B/btKzMpKy4udi75cfPNN5e7LyIil5sSMhFxeuSRRwBYs2YNixYtcplv9csvvzB27FiSk5M5duyYsz00NBSAX3/9lYSEBGf7mTNnmDp1KomJic6PZrrQOxvL4u7uzooVK5g1axabNm06b/uGDRvYs2cPnp6eREREABAWFka3bt0AmDhxIidOnHDuX1hYyGuvvUZKSgrVqlVjyJAh5e6LiMjlpjlkIuJ06623MmrUKObPn88rr7zCkiVLuPHGG8nMzGTPnj0UFRURFhbG%2BPHjnceEh4fTrVs3vvrqK2JiYmjTpg3e3t7s2rULDw8PFi5cyNq1a9m%2BfTtLlixh7969PProo3To0OEP%2B9KyZUtGjhzJggULePjhh6lTpw433HADbm5uHD582Hmr9JlnniEoKMh53Msvv8yIESNITEwkMjKSpk2b4u3tzf79%2B8nMzMTHx4fp06dfcKkNERErKCETERfjxo2jS5cuvPfee/z4449s3rwZb29vWrZsSf/%2B/Rk2bNh5k%2BFnzpzJtGnT2LhxI7t27SI4OJi%2BffvyyCOPUL9%2BfWrVqsXu3bvZtm0b%2B/fvL/c7OCdMmECHDh1YuXIlu3btYvv27RQWFhIUFMSAAQMYPnz4eR%2BMHhISwvLly1m0aBEbNmxg//79FBYWEhISwq233soDDzxQ5lw3EREr2Rxn35MQEREREdNpDpmIiIiIxZSQiYiIiFhMCZmIiIiIxZSQiYiIyFXpyy%2B/pEuXLsTFxf3hfsXFxcyYMYNevXrRoUMHHnjgAQ4dOuTcbrfbiY2NpUuXLnTr1o1nnnnG8CeQnEsJmYiIiFx13nnnHaZOnVqud1UvXbqUjz/%2BmPnz5/PZZ5/RoEEDoqOjnWsxTpo0iTNnzrBmzRpWrFhBSkoK06ZNq9D%2BKiETERGRq463tzfLly8vV0IWHx9PVFQUjRo1wtfXl7i4OFJSUtixYwcZGRkkJCQQFxdHYGAgISEhjB49mhUrVlBQUFBh/dU6ZCIiIlLppKWlkZ6e7tIWFBREcHBwuY7/29/%2BVq79cnNz%2BemnnwgLC3O2%2Bfr6csMNN5CYmEh2djbu7u40a9bMub1FixacPn2aAwcOuLQboYTsUpRzUcsK1bAh7N8PTZpAaqrp4VetNHe5uuuug969ISEBTp82NTQAR4%2BaH7NaNRg2DN5/H7KzzY//6LAscwPabCUXnZ0NViyHeN115scE8PCAwkJLQsc96WlqvBo1YOJEePllqMDPly%2B3l182P6bNBt7ekJdnzX/rKlXMjwlclr%2BL8bNmMWfOHJe2MWPGEBMTU6FxsrKycDgc%2BPv7u7T7%2B/uTmZlJQEAAvr6%2BLgtal%2B6bmZlZYf1QQnalCAgAd/eS52uAp2fJz7enuX8/LOXtDW5uJc9WJGSms9n%2B97iW1qe24gWdRapUKfk/bVmSYJFr6J/4sho8eDCRkZEubWd/TFpF%2B6N18s1YQ18JmYiIiBjjVvFT0oODg8t9e9KIgIAA3NzcsNvtLu12u50aNWoQGBhITk4ORUVFuLu7O7cB1KhRo8L6oUn9IiIics3y9vamSZMmJCUlOdtOnjzJwYMHad26Nc2bN8fhcJCcnOzcnpiYiJ%2BfHw0bNqywfighExEREWPc3Cr%2BcRkdP36cfv36OdcaGzp0KEuWLCElJYWcnBymTZtG8%2BbNadWqFYGBgfTt25eZM2dy4sQJfv31V%2BbOncvdd9%2BNh0fF3WjULUsREREx5jInUJeiVatWABT%2B/iaahIQEoKS6VVBQQGpqKvn5%2BQAMGTKE9PR07rvvPk6dOkXHjh1d3lAwZcoUnn/%2BeXr16oWnpycDBgz408VmL5YSMhEREbnqJCYmXnBb3bp12bt3r/N7m83G2LFjGTt2bJn7V6tWjddff73C%2B3g2JWQiIiJiTCWskF1pNIIiIiIiFlOFTERERIxRhcwwJWQiIiJijBIywzSCIiIiIhZThUxERESMUYXMMI2giIiIiMVUIRMRERFjVCEzTAmZiIiIGKOEzDCNoIiIiIjFVCETERERY1QhM0wjKCIiImIxVchERETEGFXIDFNCJiIiIsYoITNMIygiIiJiMVXIRERExBhVyAzTCIqIiIhYTBUyERERMUYVMsMq5Qh%2B%2BOGHdO3a1epuiIiISHm4uVX84xpz7V2xiIiISCWjW5YiIiJizDVY0apol3UEDx8%2BTLNmzVi3bh233347rVu3Zvjw4aSnpwPw1Vdfceedd9K2bVsGDhzI5s2byzzPV199xaBBgwgPD%2Bfmm29m1qxZzm0ZGRlER0fTsWNHIiIiiIqK4tChQwCkpqYSFRVF%2B/bt6dChA2PGjCEzM/NyXrKIiIjIRTOlQvbee%2B%2BxYMECfHx8GDNmDJMnT%2Ba5554jJiaGl156iT59%2BvDxxx8THR3Np59%2B6nLs6dOniYmJYeLEidx9993s27ePIUOG0LJlSyIjI3njjTfw9/fniy%2B%2BoKioiFdffZW///3vzJkzhxdffJGIiAjeffddTp06xYQJE5g3bx4TJ04sd9/T0tKcCWSpoIYNCQ4IqJCxKbfQUNdnk/n7mxvP19f12Wz5%2BebHLP0vZfZ/LSezX%2BGWxtMra9PUrWtuvOBg12ez2WzWxbQitsNhfkwn/RwbZkpCNmzYMEJCQgCIiooiNjaWtWvXUq9ePfr37w/AoEGD8Pb2pri42OXY6667ji%2B%2B%2BIKqVatis9lo1qwZzZo1Y9euXURGRnLy5EkCAgLw8vLCZrMxefJk3H7/j3Hy5El8fHzw8PDA39%2BfN99807mtvOLj45kzZ45L25jHHiPmsccudTiMef99S8L2sCQqtG9vUWAL9eljVeRq1oStWtWauFby9LQk7LhxloTlvvusiWslb2/zY545Y35MJyVkhpmSkDVs2ND5dZ06dcjPz%2BfQoUPUPefl2u23317m8f/9739ZtGgRR44cobi4mIKCAtr//pf6wQcf5NFHH%2BXLL7%2BkW7du3HbbbXTu3BmAMWPG8MQTT7By5Uq6devGgAEDaN269UX1ffDgwURGRrq0Bd1xByxefFHnMSw0tCQZGzYMkpPNjQ1sen2bqfF8fUuSsS1bICfH1NAAnFMUNUVAQEkytmED2O3mx7%2BnX7a5Ad3cSpKxU6fgnBdipvDxMT8mlCRjBQWWhJ4%2By9xEMDi4JBn7178gLc3U0ABER5sf02YrScby8iyuWMkVx5SE7Oyql%2BP3/6EOh%2BO8alhZNm/ezOTJk5k2bRp9%2BvTB09OTYcOGObe3atWKTz/9lC%2B//JJNmzYxZswY7r33XiZMmECPHj3YtGkTn3/%2BORs3bmT48OE8%2BeSTDB8%2BvNx9Dw4OJvjcentqarmPr3DJybB9u%2Blhs7JMDwmUJGNWxM7IMD9mKbvdovhWJEWlca2KfY05fNiauGlp1sS2MiFyOK6xhEwVMsNMGcGDBw86vz5y5Ag%2BPj40aNCA1HMSm/fee885Ib/Uzp07adiwIf3798fT05O8vDxSUlKc2%2B12O56envTq1YsXX3yRefPm8e9//xuAzMxMqlatSv/%2B/Zk%2BfTovvPAC8fHxl/FKRURERC6eKQnZBx98QEZGBna7ncWLF3PLLbcwYMAAjh07xrJly8jPz2ft2rW8/vrrVD1nPkmdOnX49ddfOXbsGBkZGUyePJng4GCOHz8OwJAhQ3jnnXfIy8ujoKCAHTt2cMMNN5Cbm0vfvn1ZtWoVhYWF5ObmkpSURP369c24ZBERkWuHFoY1zJQrvvPOOxkxYgQ333wzAM8//zw1a9bkn//8J4sWLaJDhw7Mnz%2BfuXPnEhgY6HJs37596d69O/3792fw4MH06NGDRx99lISEBF577TVmzpzJZ599RqdOnejSpQubN29m2rRp%2BPj48MYbb7Bo0SLat29Pjx49%2BPXXX3nuuefMuGQREZFrhxIyw0yZQ9a2bVvWrl17XnuHDh345JNPzmsfNGgQgwYNAsDT05MZM2act0/puzOhpAJXls6dO/PRRx9dardFRERETKGV%2BkVERMSYa7CiVdGUkImIiIgxSsgMu6wJWd26ddm7d%2B/lDCEiIiJyxVOFTERERIxRhcwwjaCIiIiIxVQhExEREWNUITNMCZmIiIgYo4TMMI2giIiIiMVUIRMRERFjVCEzTCMoIiIiYjFVyERERMQYVcgMU0ImIiIixighM0wJmYiIiFx1jhw5wgsvvMCOHTu47rrr6N%2B/P%2BPGjcPtnORx5MiR/PDDDy5thYWFREdHM2bMGO677z62bdvmclzDhg1ZvXp1hfZXCZmIiIgYUwkrZDExMbRo0YKEhAR%2B%2B%2B03Hn74YWrWrMn999/vst%2BCBQtcvj958iT9%2B/enT58%2BzrYXX3yRQYMGXdb%2BVr4RFBERETEgMTGR5ORkxo8fT7Vq1WjQoAFRUVHEx8f/6bEzZ86kT58%2BNGvWzISe/o8qZCIiImLMZaiQpaWlkZ6e7tIWFBREcHDwnx6blJREnTp18Pf3d7a1aNGC1NRUcnJy8PX1LfO4X375hZUrV5KQkODS/sknn/Duu%2B9y7Ngx2rRpw5QpU6hfv/4lXNWFqUImIiIixri5VfgjPj6eQYMGuTzKU%2BECsNvt%2BPn5ubSVJmeZmZkXPG7%2B/Pn89a9/JTAw0NnWqFEjmjRpwvvvv8/GjRsJDAzkwQcfJD8//xIG6sJUIRMREZFKZ/DgwURGRrq0BQUFlft4h8NxUfHsdjurVq3iv//9r0v75MmTXb6fMmUKHTt2ZOvWrXTu3PmiYvwRJWQiIiJizGW4ZRkcHFyu25NlCQwMxG63u7TZ7XZsNptL9etsGzdupGHDhtSrV%2B8Pz%2B3r64u/vz/Hjx%2B/pL5diG5ZioiIyFWlZcuWHDt2jBMnTjjbEhMTady4MVWrVi3zmI0bN9K1a1eXtpycHCZPnuySfJ04cYITJ078aeJ2sZSQiYiIiDGXYQ6ZEWFhYbRq1Yrp06eTk5NDSkoKCxcuZOjQoQD069ePLVu2uByzZ88e6tat69Lm6%2BvLjh07mDp1Kna7naysLF544QWaNWtGeHi4oT6eSwmZiIiIGFPJEjKAWbNmkZaWRteuXfnb3/7GXXfdxbBhwwBITU3l9OnTLvunp6dTs2bN884zd%2B5cHA4Hffv2pUePHhQUFDB//vzzFpg1yua42FlvwqpV5sf094cePWDTJsjKMj/%2BwLts5gYMD4dt2yAiArZvNzc2kGU3/8fCzQ2qVYPsbCguNj08Pj7mxrPZwMsL8vPBit9C3ifT/3yniubhAdWrQ2YmFBaaH3/3bnPj%2BfpCu3awdSvk5JgbG0gPizA9poeHG9WrVyUz8xSFheb%2BIAcFVTM1nou%2BfSv%2BnOvWVfw5KzFN6hcRERFjKuFK/VcajaCIiIiIxVQhExEREWNUITNMCZmIiIgYo4TMMI2giIiIiMVUIRMRERFjVCEzTCMoIiIiYjFVyERERMQYVcgMU0ImIiIixighM0wjKCIiImIxVchERETEGFXIDNMIioiIiFhMFTIRERExRhUyw5SQiYiIiDFKyAzTCIqIiIhYTBUyERERMUYVMsM0giIiIiIWU4VMREREjFGFzDAlZCIiImKMEjLDNIIiIiIiFlOFTERERIxRhcwwJWQiIiJijBIywzSCIiIiIhZThUxERESMUYXMMI2giIiIiMVUIRMRERFjVCEzrNKN4MiRI5k5c2aFnzclJYVmzZpx%2BPDhCj%2B3iIjINc3NreIf15hKVyFbsGCB1V0QERERMVWlS8hERETkCnMNVrQq2kWN4Pz58%2BnZsydt2rShb9%2B%2BrFq1iu%2B%2B%2B45mzZqRl5fn3C8uLo6nnnoKgA8//JABAwbw6quv0rZtW%2BbNm0dkZKTLeXfv3k3z5s05fvw49913H9OmTePzzz%2Bnbdu25ObmOvc7ceIEYWFh/PjjjwC899573HbbbbRp04bbb7%2BdhIQE576//fYbDz74IOHh4dx%2B%2B%2B3s3Lnz4kdHRERExATlrpBt27aNJUuWsGzZMmrXrs3XX39NTEwML7300p8em5aWhre3Nz/88ANZWVnMnj2b5ORkQkNDAdiwYQPt27cnJCTEeUyXLl3w8vLiq6%2B%2Bonfv3gB8%2Bumn1KpVi7Zt27J%2B/XrmzJnDu%2B%2B%2BS2hoKJ9%2B%2BimxsbGsX7%2Be66%2B/npdffpm8vDw2bdpEbm4u48ePv9ixcfY9PT3dpa2gIIiaNYMv6XyXytfX9dl04eHmxvv9/4bz2WRWvNgrjWnVC02bzZp4Zsd18rDgBoG7u%2Buz2cz%2BBVKliuuzyTw8zP9hcnd3c3m%2BZqhCZli5fyNlZ2fj5uaGj48PNpuNbt26sXXrVn744YdyHfvQQw/h6elJzZo1ad%2B%2BPQkJCc6ELCEhgaFDh7oc4%2BnpSa9evdi4caMzIUtISOC2224DYPny5dx99920bNkSgFtvvZV27dqxZs0aRo0aRUJCAjNmzMDf3x9/f3%2BGDx/O999/X97LdYqPj2fOnDkubdHRY7j77piLPldFaN/ekrCwbZs1cd9/35Kw1SyJWqJqVQuDW8DT06LAXtUtCgz4%2BVkTt107a%2BKGhVkS1sJ/Yfz8rElCLaOEzLByJ2SdO3cmLCyMyMhIOnfuTPfu3Rk4cGC5jvXz88P3rFdm/fr1Y9myZYwZM4ZffvmFlJQU%2BvXrd95x/fr1Y8KECRQVFZGbm8s333zD2LFjATh48CBff/01ixcvdu7vcDho3LgxmZmZ5ObmUrduXee2Bg0alPdSXQwePPi8W6x79gSxadMlne6S%2BfqWJGNbtkBOjrmxAXo8HmFuwNDQkmRs2DBITjY3NpD9ufkJqJtbSTJ26hQUF5seHm9vc%2BPZbCXJWEEBOBzmxgbwOpVpflB395Jk7ORJKCoyP/6BA%2BbGq1KlJBnbvRvOnDE3NpB5o/kVdnd3N/z8qnDy5BmKisz9Qa5e/Rp7NXeVKXdC5uXlxVtvvUVycjIbN25k6dKlLFiwgAkTJpy3b9E5v2g8zrk10LdvX6ZOncqRI0dYv349nTp1IjAw8LzzdOnSheLiYrZu3UpGRga1a9cm7PdXWj4%2BPowbN46RI0eed9zx48fP64fjEn/jBwcHExzsenvyp58gK%2BuSTmdYTo5FsbdvtyAoJcmYBbGtSIjOjm1FfCuSotK4lsQuLLQg6O%2BKiqyJb8WrOShJxiyIXVho3Q9yUVGxpfFNpwqZYeUewYKCAnJycggNDSU6OpqVK1dis9nYv38/AGfOevVz6NChPzxXjRo1aN%2B%2BPZs2bWLDhg3079%2B/zP1Kb1t%2B9tln5%2B1Xv3599u7d67L/0aNHcTgcBAYG4unpybFjx5zbfvrpp/JeqoiIiIipyp2QLViwgIceeohff/0VKFloNSsriy5duuDu7s66desoLCzko48%2BckmELuS2225j7dq17Nmzhz59%2Bvzhfl999RVfffWVS0I2ePBgPvnkEzZt2kRhYSHffvstAwYMYMeOHXh6etKpUyeWLFlCdnY2R44cYenSpeW9VBEREbkYWhjWsHJf8f3330/Tpk256667aNu2LbGxsYwfP542bdowfvx4Zs6cSadOndizZ88FK15nu/XWW/nxxx/p2rUr/v7%2BF9yvc%2BfOpKWlUatWLZo0aeJs79q1KxMmTGDKlClEREQwZcoUJk%2BeTNu2bQGc7/7s3r07Dz30ECNGjCjvpYqIiMjFUEJmmM1xqZOrrmGrVpkf098fevSATZusmUM28C6T1yYIDy95Z2dEhCVzyLLs5v9YuLlBtWqQnW3NHDIfH3Pj2Wzg5QX5%2BdbMIfM%2Bmf7nO1U0Dw%2BoXh0yM62ZQ7Z7t7nxfH1L3tm5daslc8jSw0x%2BMxIlS21Ur16VzMxTps8hCwqy8P3hTzxR8ed87bWKP2clppX6RURExJhrsKJV0TSCIiIiIhZThUxERESMUYXMMCVkIiIiYowSMsM0giIiInLVOXLkCKNGjaJjx4707NmT1157jeIy3jE1e/ZsmjdvTqtWrVweGRkZAOTl5fHcc8/RvXt3OnbsyNixY8nMrPhP%2BlBCJiIiIsZUwmUvYmJiCAkJISEhgYULF5KQkODycYtnGzhwIImJiS6PmjVrAjBjxgySkpKIj49n3bp1OBwOnn76acP9O5cSMhEREbmqJCYmkpyczPjx46lWrRoNGjQgKiqK%2BPj4izpPYWEhy5cvZ/To0dSuXZuAgABiY2PZtGmT82MaK4rmkImIiIgxl2EOWVpaGunprusFBgUFnff50mVJSkqiTp06LgvPt2jRgtTUVHJycvD19XXZf%2B/evQwZMoR9%2B/ZRu3Ztnn76abp168bBgwdj9oQ%2BAAAgAElEQVTJzs6mRYsWzn0bNWqEj48PSUlJhISEGLzK/1FCJiIiIsZchoQsPj6eOXPmuLSNGTOGmJiYPz3Wbrfj5%2Bfn0laanGVmZrokZLVq1aJevXqMGzeO4OBg4uPjeeSRR1i9ejV2ux3gvHP5%2BflV%2BDwyJWQiIiJS6QwePJjIyEiXtqCgoHIfX94PIrrnnnu45557nN9HRUWxdu1aVq9eTffu3S/qXEYoIRMRERFjLkOFLDg4uFy3J8sSGBjorG6Vstvt2Gw2AgMD//T4OnXqkJaW5tzXbrdTtWpV5/asrCxq1KhxSX27EE3qFxERkatKy5YtOXbsGCdOnHC2JSYm0rhxY5fECuDNN99k8%2BbNLm0pKSnUq1ePevXq4e/vT1JSknPbvn37yM/Pp2XLlhXaZyVkIiIiYkwlW/YiLCyMVq1aMX36dHJyckhJSWHhwoUMHToUgH79%2BrFlyxagpPr1wgsvcODAAfLy8liwYAEHDx7kL3/5C%2B7u7tx777289dZbHDt2jMzMTF5//XX69OnjXBajouiWpYiIiBhTCVfqnzVrFpMmTaJr1674%2BvoyZMgQhg0bBkBqaiqnT58GYNy4cUDJ3DG73U7jxo1ZtGgRtWrVAmDs2LGcOnWKgQMHUlhYSM%2BePZk8eXKF91cJmYiIiFx1atWqxTvvvFPmtr179zq/9vb2ZuLEiUycOLHMfb28vHj%2B%2Bed5/vnnL0s/SykhExEREWMqYYXsSqMRFBEREbGYKmQiIiJijCpkhikhExEREWOUkBmmERQRERGxmCpkIiIiYowqZIZpBEVEREQspgrZJTh61PyY%2Bfklz%2BnpkJFhfvws%2B%2BX/YNWzublBNSD7820UF5saGgD/AJv5QcPDYds2qt0SAdu3mx7%2B803m/hv7%2BkK7dpCYCDk5poYGoFat8n9IcUXx9oYG1eHnrOrk5Zkenmbl%2BAy/CuXjU/Ls5wdeXubGBoK8LPjl8XuZo7pHMbiZHD8rC/z9zY1ZShUyw5SQiYiIiDFKyAzTCIqIiIhYTBUyERERMUYVMsOUkImIiIgxSsgM0wiKiIiIWEwVMhERETFGFTLDNIIiIiIiFlOFTERERIxRhcwwJWQiIiJijBIywzSCIiIiIhZThUxERESMUYXMMI2giIiIiMVUIRMRERFjVCEzTAmZiIiIGKOEzDCNoIiIiIjFVCETERERY1QhM0wjKCIiImIxVchERETEGFXIDFNCJiIiIsYoITNMIygiIiJiMVXIRERExBhVyAzTCIqIiIhYTBUyERERMUYVMsOUkImIiIgxSsgM0wiKiIiIWEwVMhERETFGFTLDNIIiIiIiFlOFTERERIxRhcywSp%2BQJSYm8sorr7Bv3z68vLzo06cPzz77LJ6envznP/9hxowZ5OfnM3jwYOx2O0VFRbz66qsAvPfeeyxdupSjR49St25d4uLi6N27t8VXJCIicpVRQmZYpU/I4uLiuPPOO/nXv/7F8ePHGTJkCI0bNyYiIoJJkybxxhtv0KNHD9555x3%2B85//EBkZCcD69euZM2cO7777LqGhoXz66afExsayfv16rr/%2B%2BnLHT0tLIz09/ZzWIGrWDK7Aq/xzAQGuz2Yz%2B2etNJ5lP%2BPh4ebHDA11fTaZr6%2B58apUcX02m7e3%2BTG9vFyfTefmY2680kG2YrDBml8gVv7yKi42P6ZUGJvD4XBY3Yk/kpOTg5eXF16//wYbN24cHh4eNGjQgP/3//4fq1atAqCoqIjIyEg6d%2B7Mq6%2B%2ByqhRo2jatCnjx493nmvEiBF07dqVUaNGlTv%2B7NmzmTNnjktbdPQYxo6NqYCrExERqSBZWeDvb03s//634s95220Vf85KrNJXyL799lvmzp3Lzz//TGFhIYWFhfTr14/09HTq1Knj3M/d3Z2wsDDn9wcPHuTrr79m8eLFzjaHw0Hjxo0vKv7gwYOdVbdSX3wRxH/%2Bc4kXdIkCAqBPH9iwAex2c2MD9Otnbjw3N6haFU6dsuZFX7VbIswPGhoK778Pw4ZBcrLp4be%2Bs83UeFWqQFgY7N4NZ86YGhqAGjXMj%2BnlBddfD0ePQn6%2B%2BfEbFOw3N6C3N9SvDwcPQl6eubEBatUyP6bVv7zkilWpE7KUlBQee%2BwxJkyYwL333ouPjw9PPPEEhYWFFBcX4%2BHh2n23s0rEPj4%2BjBs3jpEjRxrqQ3BwMMHBrrcnv/gCMjIMnfaS2e3WxLbq90pxsUWxt2%2B3IOjvkpMtiZ%2BTY3pIoCQZsyK22bdoz5afb01%2BQn6uBUEpudhcC2JbmRBZ9svLIppDZlilHsE9e/bg5eXF3/72N3x8fHA4HOzZsweAGjVqcPToUee%2BRUVF7N692/l9/fr12bt3r8v5jh49SiW/QysiInLlcXOr%2BMc1plJfcZ06dcjNzWXPnj1kZWXx2muv4eXlRVpaGh07dmTXrl1s2rSJ/Px85s2bR%2B5Zr8AGDx7MJ598wqZNmygsLOTbb79lwIAB7Nixw8IrEhERETMcOXKEUaNG0bFjR3r27Mlrr71G8QWqlh988AF9%2B/YlPDycgQMHkpCQ4Nz21FNPERYWRqtWrZyP9u3bV3h/K/Uty/DwcP7v//6P4cOHU6VKFR599FEmTpzIo48%2Byvvvv09sbCzjx4/H09OTESNG0LFjR2w2GwBdu3ZlwoQJTJkyhYyMDOrWrcvkyZNp27atxVclIiJylamEFa2YmBhatGhBQkICv/32Gw8//DA1a9bk/vvvd9lv3bp1TJ8%2BnbfffpvWrVuzcuVKYmNj%2Be9//0u9evUAePTRR4mJubxv5qvUCRnAs88%2By7PPPuvS9sMPPwCQn5/PI4884mwfPny4S9Y6fPhwhg8fbk5HRUREpFJITEwkOTmZhQsXUq1aNapVq0ZUVBSLFy8%2BLyHLzc3l8ccfp127dgDcc889TJs2jR9//NGZkJmh0idkF3Lo0CH69evH7Nmz6dGjB9988w3bt2/n8ccft7prIiIi15bLUCErax3QoKCg895oV5akpCTq1KmD/1nLgLRo0YLU1FRycnLwPetdPQMHDnQ59uTJk5w6dYqQkBBn27fffsvGjRv55ZdfaNSoEZMnT6Zly5aXemllumITsnr16vHqq6/y2muv8fjjjxMSEsLzzz9PRIQFyxWIiIhcyy5DQhYfH3/eOqBjxowp161Du92On5%2BfS1tpcpaZmemSkJ3N4XDw7LPP0qZNG2666SagJN9wc3Pjscceo2rVqsyZM4eRI0eybt06qlevfimXVqYrNiEDuOOOO7jjjjus7oaIiIhUsLLWAQ0KCir38Re7qkJBQQFPPfUUP/30E0uWLHG2R0dHu%2Bz3xBNPsGbNGhISErjnnnsuKsYfuaITMhEREakELkOFrKx1QMsrMDAQ%2BzmrqNvtdmw2G4GBgeftn5uby%2BjRozlz5gxLly79w8qXu7s7tWvXJi0t7ZL6diGV720RIiIiIga0bNmSY8eOceLECWdbYmIijRs3pmrVqi77OhwO4uLi8PDwYNGiRS7JmMPh4JVXXiH5rE9Pyc/P5%2BDBgxU%2B4V8JmYiIiBhTyRaGLV03bPr06eTk5JCSksLChQsZOnQoAP369WPLli0AfPzxx/z000%2B88cYbeHt7u5zHZrNx%2BPBhXnjhBY4fP86pU6eYNm0anp6e9O7d21Afz6VbliIiImJMJVyHbNasWUyaNImuXbvi6%2BvLkCFDGDZsGACpqamcPn0agBUrVnDkyBHnJP5SAwcOZOrUqbz00kv8/e9/Z9CgQeTk5NC6dWsWL17MddddV6H9VUImIiIiV51atWrxzjvvlLnt7I9WXLx48R%2BeJyAggFdeeaVC%2B1YWJWQiIiJiTCWskF1plJCJiIiIMUrIDNMIioiIiFhMFTIRERExRhUywzSCIiIiIhZThUxERESMUYXMMCVkIiIiYowSMsM0giIiIiIWU4VMREREjFGFzDCNoIiIiIjFVCETERERY1QhM0wJmYiIiBijhMwwjaCIiIiIxVQhExEREWNUITNMIygiIiJiMZvD4XBY3YkrTlaW%2BTHd3KBaNcjOhuJi08Pn%2BfibGs9mAy8vyM8HK/6Hfvut%2BTF9faFdO9i6FXJyzI9/Sw%2BbuQHDw2HbNoiIgO3bzY0NsGiR%2BTEDA%2BGOO%2BDjj%2BHECfPj33STufF8fKBhQ0hNhdxcc2MDxwKamx7TwwOCgiA9HQoLTQ9P7drmxwRg//6KP2eTJhV/zkpMtyxFRETEGN2yNEwjKCIiImIxVchERETEGFXIDNMIioiIiFhMFTIRERExRhUyw5SQiYiIiDFKyAzTCIqIiIhYTBUyERERMUYVMsM0giIiIiIWU4VMREREjFGFzDAlZCIiImKMEjLDNIIiIiIiFlOFTERERIxRhcwwjaCIiIiIxVQhExEREWNUITNMCZmIiIgYo4TMMI2giIiIiMVUIRMRERFjVCEzTCMoIiIiYjFVyERERMQYVcgMU0ImIiIixighM0wjKCIiImIxVchERETEGFXIDNMIioiIiFhMFTIRERExRhUywyrdCP7www%2B0atWK/Px8Dh8%2BTLNmzUhJSbG6WyIiInIhbm4V/7jGVLor7tChA4mJiXh5eVndFREREblCHTlyhFGjRtGxY0d69uzJa6%2B9RnFxcZn7LlmyhL59%2BxIREcHQoUPZtWuXc1teXh7PPfcc3bt3p2PHjowdO5bMzMwK72%2BlS8hERETkClMJK2QxMTGEhISQkJDAwoULSUhIYPHixeft9%2BmnnzJ79mz%2B8Y9/8M0339CzZ08eeeQRTp8%2BDcCMGTNISkoiPj6edevW4XA4ePrppw3371yWJmTz58%2BnZ8%2BetGnThr59%2B7Jq1Sq%2B%2B%2B47mjVrRl5ennO/xMREBgwYQHh4OCNGjOD48eMAnDlzhgkTJtC5c2fCw8MZMmSIM6udPXs2UVFRvPnmm3Ts2JF27drxxhtvWHKdIiIiYp7ExESSk5MZP3481apVo0GDBkRFRREfH3/evvHx8QwaNIg2bdrg4%2BPDgw8%2BCMBnn31GYWEhy5cvZ/To0dSuXZuAgABiY2PZtGmTMxepKJZN6t%2B2bRtLlixh2bJl1K5dm6%2B//pqYmBheeuml8/ZdtmwZ8%2BfPp2rVqkRHRzNp0iTmz5/P4sWLycjIYMOGDXh5efHOO%2B8wadIkPvroIwB27NhBeHg4X375JYmJiTzwwAO0aNGC3r17l7ufaWlppKenu7QFValCcFCQsQG4WKWvFiy6r26zWRPP7LilfH3Nj1mliuuz6cLDzY0XGur6bLbAQPNj%2Bvu7PpvNx8fceKVTTyyaguJhwV%2B40phWxC4sND%2Bm02X421Tm39%2BgIIKDg//02KSkJOrUqYP/WT9rLVq0IDU1lZycHHzP%2BiWflJRE//79nd%2B7ubnRvHlzEhMTad68OdnZ2bRo0cK5vVGjRvj4%2BJCUlERISIiRS3RhWUKWnZ2Nm5sbPj4%2B2Gw2unXrxtatW/nhhx/O2/f//u//uP766wGIiooiNjaWwsJCTp48iaenJz4%2BPnh4eDB69GhGjx7tPM7NzY3o6Gg8PDxo164d3bp1Y9OmTReVkMXHxzNnzhyXtjHR0cSMHXuJV25Q1aqWhLVqRp%2BnpzVx27WzJi5AWJhFgbdtsybu%2B%2B9bE9dK3btb3QNz1aljSViTXza7qF7d/JjHjpkfs5SDin/1XObf3zFjiImJ%2BdNj7XY7fn5%2BLm2lyVlmZqZLQma3210St9J9MzMzsdvtAOedy8/Pr8LnkVmWkHXu3JmwsDAiIyPp3Lkz3bt3Z%2BDAgWXu26hRI%2BfX9evXp6CggN9%2B%2B41hw4bxwAMPcMstt3DzzTfTu3dvevXq5bKvx1kvU66//np%2B/vnni%2Brn4MGDiYyMdGkLqlIFsrMv6jyGubmVJGOnTsEFJiVeTvne1UyNZ7OVJGMFBeBwmBoagMRE82NWqVKSjO3eDWfOmB%2B/3UMR5gYMDS1JxoYNg%2BRkc2MDvPCC%2BTH9/UuSsS%2B%2BgKws8%2BO3bGluPC%2BvkmTsyBHIzzc3NpDu29D0mB4eJclYZqbFFaurQJl/fy/i7pTjIv54/Nm%2BF3OuS2VZQubl5cVbb71FcnIyGzduZOnSpSxYsIAJEyact6/bWaXQ0kHx9vYmJCSETz75hO%2B%2B%2B45PP/2U5557jtWrVzNr1iwAioqKXM7jcDiwXeQ9sODg4PPLo1lZliRFQElcC2JbkRSVxrUidk6O%2BTFLnTljUfzt2y0ISkkyZkXsEyfMj1kqK8ua%2BLm55seEkmTMgtiFJt%2BhdYldeG0lZJfjz1KZf3/LKTAw0FndKmW327HZbASeM12hevXqZe7bpEkT5752u52qZ92hysrKokaNGpfUtwuxbFJ/QUEBOTk5hIaGEh0dzcqVK7HZbOclUQCpqanOrw8dOoSPjw8BAQGcOnWKoqIiunTpwrPPPst//vMf1q1b5ywjHjt2jMKzfiKOHj1aofd7RURE5H%2B1gop8GNGyZUuOHTvGibNe%2BCQmJtK4cWOXxKp036SkJOf3RUVF7N69mzZt2lCvXj38/f1dtu/bt4/8/HxaVnDF2bKEbMGCBTz00EP8%2BuuvAKSkpJCVlcWxMm6CL126lPT0dLKzs1m8eLFzDtjYsWP5%2B9//Tk5ODsXFxWzfvp2AgADnveDCwkLeffdd8vPz2bJlC19//fV55U8RERG5uoSFhdGqVSumT59OTk4OKSkpLFy4kKFDhwLQr18/tmzZAsDQoUNZuXIlP/74I2fOnGHevHl4eXnRo0cP3N3duffee3nrrbc4duwYmZmZvP766/Tp04eaNWtWaJ8tu2V5//33c/ToUe666y5yc3OpXbs248ePp379%2BuftO2TIEEaMGMGxY8eIiIhg4sSJALz44ovOxdpsNhtNmjRh7ty5zlucTZo0obCwkJtvvpnCwkIeeOABevToYeZlioiIXPWsmsXzR2bNmsWkSZPo2rUrvr6%2BDBkyhGHDhgEld95K1xnr3r07jz/%2BOLGxsfz222%2B0atWK%2BfPn4/P7u5LHjh3LqVOnGDhwIIWFhfTs2ZPJkydXeH9tDjNmqllg9uzZfPnllyxbtqziT27FZFw3N6hWreTNBBb8z8/zMfdt%2BjZbyXzg/Hxr5pB9%2B635MX19S97duXWrNXPIbulh8hoj4eEl7%2ByMiLBmDtmiRebHDAyEO%2B6Ajz%2B2Zg7ZTTeZG8/HBxo2hNRUS%2BaQHQtobnpMDw8ICoL0dGvmkNWubX5MgLOWDq0w3t4Vf87KTB8uLiIiIoZUxgrZlUYJmYiIiBiihMy4q/azLGNiYi7P7UoRERGRCqYKmYiIiBiiCplxV22FTERERORKoQqZiIiIGKIKmXFKyERERMQQJWTG6ZaliIiIiMVUIRMRERFDVCEzThUyEREREYupQiYiIiKGqEJmnBIyERERMUQJmXG6ZSkiIiJiMVXIRERExBBVyIxThUxERETEYqqQiYiIiCGqkBmnhExEREQMUUJmnG5ZioiIiFhMFTIRERExRBUy41QhExEREbGYKmQiIiJiiCpkxikhExEREUOUkBmnhOxSXHeddbF9fCwJ621PNzeghwd4VcfrVCYUFpobG6hVK8j0mN7eJc81aoCvr%2BnhYdEic%2BMFBpY8v/ACnDhhbmyAqCjzY4aHwx13wPPPw/bt5sf/5htz41WtWvKcnQ2nTpkbG6hdN9v0mLi5AVUJuu6URVlKNQtiSkVQQiYiIiKGqEJmnCb1i4iIiFhMFTIRERExRBUy45SQiYiIiCFKyIzTLUsRERERi6lCJiIiIoaoQmacKmQiIiIiFlOFTERERAxRhcw4JWQiIiJiiBIy43TLUkRERMRiqpCJiIiIIaqQGacKmYiIiIjFVCETERERQ1QhM04JmYiIiBiihMw43bIUERERsZgqZCIiImKIKmTGKSETERERQ5SQGadbliIiIiIWU4VMREREDFGFzDhVyEREROSaYrfbiY2NpUuXLnTr1o1nnnmG3NzcC%2B6/fv167rzzTsLDw%2Bnbty/Lli1zbps9ezbNmzenVatWLo%2BMjIyL6pMqZCIiImLIlVYhmzRpEvn5%2BaxZs4aCggIee%2Bwxpk2bxrPPPnvevjt37mT8%2BPG8/vrr9OjRg6%2B//pro6GhuvPFG2rdvD8DAgQN59dVXDfVJFTIRERExpLi44h%2BXS0ZGBgkJCcTFxREYGEhISAijR49mxYoVFBQUnLe/3W7n4Ycfpnfv3nh4eHDLLbfQtGlTtmzZUqH9UoVMREREKp20tDTS09Nd2oKCgggODjZ03j179uDu7k6zZs2cbS1atOD06dMcOHDApR2ge/fudO/e3fl9YWEh6enphISEONv27t3LkCFD2LdvH7Vr1%2Bbpp5%2BmW7duF9UvJWQiIiJiyOWoaMXHxzNnzhyXtjFjxhATE2PovHa7HV9fX2w2m7PN398fgMzMzD89ftq0aVx33XX0798fgFq1alGvXj3GjRtHcHAw8fHxPPLII6xevZobb7yx3P1SQiYiIiKVzuDBg4mMjHRpCwoKKtexq1at4sknnyxzW1xcHA6H46L743A4mDZtGmvWrGHJkiV4e3sDcM8993DPPfc494uKimLt2rWsXr2a2NjYcp//qk7I7rvvPtq0acP48eOt7oqIiMhV63JUyIKDgy/59uTAgQMZOHBgmdu%2B/vprcnJyKCoqwt3dHSipmgHUqFGjzGOKi4t5%2Bumn2blzJx988AH16tX7w/h16tQhLS3tovqsSf0iIiJiyJU0qb958%2BY4HA6Sk5OdbYmJifj5%2BdGwYcMyj3n55ZfZv39/mcnYm2%2B%2ByebNm13aUlJS/jRpO5cSMhEREblmBAYG0rdvX2bOnMmJEyf49ddfmTt3LnfffTceHiU3DkeMGMEnn3wCwNatW1m9ejXz588nICDgvPPZ7XZeeOEFDhw4QF5eHgsWLODgwYP85S9/uah%2BmZ6QHT58mGbNmrFu3Tpuv/12WrduzfDhw53vpFi9ejX9%2B/cnPDycyMhI3n//feexs2fP5uGHHyY2NpaIiAgAzpw5w6RJk%2BjYsSOdOnVyri1SqqioiOeee46IiAg6d%2B7sHGARERGpGFdShQxgypQpVKtWjV69enHnnXfSunVr4uLinNsPHTpEVlYWACtWrCA7O5uePXu6LPw6cuRIAMaNG0f37t2JioqiQ4cOrFmzhkWLFlGrVq2L6pPNcSkz2ww4fPgwvXr14qabbmLatGn4%2BPgwZswY/Pz8eOqpp7j11lv55z//SefOnfn2228ZOXIkH330EaGhocyePZv33nuPxx57jMGDB%2BPu7s5LL73Erl27mDt3LgAPPvggN998M3Fxcdx33338/PPPvPzyy3Ts2JE5c%2Bbw73//m2%2B%2B%2BcaZBf%2BZMt92W706weWcWFihPD2hjDVSTJGTY248d3fw84OTJ6GoyNzYwM9Z1U2P6eUF118PR4/CWa8pTNMg8WNzA/r7Q/fu8MUX8PsvPlM9/7z5MUND4f33YdgwOOt2iWkWLTI3XpUq0KQJ7N8PZ86YGxugUSPzY7q5lVz3mTPmr5ZaXAzVqpkb83fr1lX8Ofv2rfhzVmaWTeofNmyYcw2PqKgoYmNjmTVrFt9%2B%2B63z7aedO3emRo0aJCUlERoaCoC7uztDhw7FZrPhcDhYuXIlL7/8MoGBgUDJfd6TJ08640RERHDzzTcD0K9fP95%2B%2B21OnDhR7omCZb7tNjqamLFjjQ3ApfL0tCZudfMTFKAkKbNAA4suF0qSMks0uMOauGet72OqOyy6XihJyq4lTZpY3QPzValifszsbPNj/u5KW6m/MrIsITt74lydOnXIz88nKyuLZcuWsXz5ctLS0nA4HOTn57vcgqxVq5Zz7ZDMzExOnjxJ3bp1ndtLE7dSZ28rfYtq/kWUH8p822316tZUqlQhM40qZCZQhcz8%2BKqQXX5WVsgsdA1d6mVjWUJWfNa/Xuld0%2BXLlzN//nzefPNNOnTogLu7O7fccovLcWffanRzczvvXOc6e%2BG3S1Hm226tSoqsVFhoTdyiIkti5%2BWZHtIpP9%2Bi%2BCdOWBCUkmTMitjbt5sfs1RysjXxT50yPyaUJCdWxLYySzBjIpRcVSx7l%2BXBgwedXx85cgQfHx8OHz5M%2B/bt6dSpE%2B7u7qSnp//hOh4BAQH4%2BfmRmprqbEtKSmLVqlWXte8iIiLyP1fapP7KyLKE7IMPPiAjIwO73c7ixYu55ZZbqFOnDgcOHCArK4sjR44wdepUrr/%2Beo4fP37B8wwaNIh3332X48ePk5mZyYsvvsj%2B/ftNvBIRERERYyy7ZXnnnXcyYsQIDh48SNu2bXn%2B%2Befx9PTk%2B%2B%2B/dyZnkydPZteuXcycOfOCH5cwbtw4pk6dSv/%2B/fHy8qJ3796MGTPG5KsRERG5dl2LFa2KZllC1rZtW9auXXte%2Bz//%2BU%2BX7zt06MD999/v/P7cDxX18vJiypQpTJky5bxz/etf/3L5vlGjRuzdu9dIt0VEROQcSsiM00r9IiIiIha7qj9cXERERC4/VciMMz0hq1u3rm4bioiIiJxFFTIRERExRBUy45SQiYiIiCFKyIzTpH4RERERi6lCJiIiIoaoQmacKmQiIiIiFlOFTERERAxRhcw4JWQiIiJiiBIy43TLUkRERMRiqpCJiIiIIaqQGacKmYiIiIjFVCETERERQ1QhM04JmYiIiBiihMw43bIUERERsZgqZCIiImKIKmTGqUImIiIiYjFVyERERMQQVciMU0ImIiIihighM063LEVEREQspgqZiIiIGKIKmXFKyERERMQQJWTGKSG7BHFPepoes25dGDcOps/y5PBh08Mz467d5gb09YV27eDAAcjJMTc20Cww0PSYuPkATWhQsB/yc82Pf9NN5sbz8Sl5btkSci243m%2B%2BMT9m1aolz4sWwalT5sfv0sXceOHhsG0bREXB9u3mxgZysh2mx3SzwXXAaVtVim2mh8fX/JBSQZSQiYiIiCGqkBmnSf0iIqv8ZAUAACAASURBVCIiFlOFTERERAxRhcw4JWQiIiJiiBIy43TLUkRERMRiqpCJiIiIIaqQGacKmYiIiIjFVCETERERQ1QhM04JmYiIiBiihMw43bIUERERsZgSMhERETGkuLjiH5eT3W4nNjaWLl260K1bN5555hlyL/ARbh9%2B%2BCGhoaG0atXK5bFz587fr72YGTNm0KtXLzp06MADDzzAoUOHLrpPSshERETkmjJp0iTOnDnDmjVrWLFiBSkpKUybNu2C%2B3fo0IHExESXR%2BvWrQFYunQpH3/8MfPnz%2Bezzz6jQYMGREdH43Bc3GepKiETERERQ66kCllGRgYJCQnExcURGBhISEgIo0ePZsWKFRQUFFz0%2BeLj44mKiqJRo0b4%2BvoSFxdHSkoKO3bsuKjzKCETERERQ66khGzPnj24u7vTrFkzZ1uLFi04ffo0Bw4cKPOYY8eOcf/999OhQwd69erFqlWrAMjNzeWnn34iLCzMua%2Bvry833HADiYmJF9UvvctSREREKp20tDTS09Nd2oKCgggODjZ0Xrvdjq%2BvLzabzdnm7%2B8PQGZm5nn7BwYG0qBBAx5//HEaN27Mhg0bePLJJwkODubGG2/E4XA4jz/7fGWd648oIRMRkf/f3r3HRVnlfwD/zHAbBDFxQQXxki9DS0UINe%2BKGoQo6aKgRmpZP0MBodVdMwtvXbygKZpbromFLl5ITE0Cy2UxTVEMvBe6CQoMEshVLjPz%2B4Nl1gk1aC5nGD7v14sXzHmemfN9mOHhO99z5jxEWtFHRSs%2BPh4xMTEabQsWLEBoaOjv3jcxMRGLFy9%2B6LaIiIhmze8aPXo0Ro8erb49YcIEJCcnIyEhAX/5y18AoNnzxR6GCRkREREZncDAQHh5eWm0OTg4NOm%2B/v7%2B8Pf3f%2Bi2kydPory8HAqFAmZmZgDqq2YA0KFDhyY9vrOzMy5evIgnnngCUqlUff8GJSUlTX6sBkzIiIiISCv6qJA5OjpqPTz5MH369IFKpcLVq1fxzDPPAACysrJgZ2eHHj16NNp/z549aNeuHXx9fdVt2dnZcHFxgZWVFXr16oVLly5h0KBBAIDS0lLcunVL/SnMpuKkfiIiItJKS5rUb29vD29vb2zcuBG//vor8vPzsWXLFgQEBMDcvL5ONWvWLBw9ehQAUFNTg5UrVyIrKwu1tbU4fPgwUlNTERQUBACYPn06du3ahezsbJSXl2PdunXo06cP%2BvXr16y4WCEjIiKiVmXFihV49913MXbsWFhYWMDPzw8RERHq7Tk5Obh37x4A4OWXX0ZFRQXCw8NRWFiILl26YMuWLejbty8AICgoCIWFhQgODkZFRQUGDx7caO5bUzAhIyIiIq20tGtZtm3bFtHR0Y/c/u2336p/lkgkCAkJQUhIyEP3lUgkCAsLQ1hYmFYxcciSiIiISDBWyIiIiEgrLa1CZoyYkBEREZFWmJBpj0OWRERERIIZdUJ2%2BvRpjBw5UmPtDyIiIjIuLWnZC2Nl1AlZbGwsBgwYgMOHD4sOhYiIiEhvjHoOWXl5Odzc3CCVGnXeSERE1Kq1xoqWrhltpvPSSy/h7Nmz2LFjB7y9vZGWloYpU6bA3d0dI0aMwKZNm9T7JiQkwM/PDx988AEGDBiAgoICKJVKbNq0CePGjYObmxv%2B/Oc/49y5cwKPiIiIyDRxyFJ7Rlsh%2B%2BKLLxAcHAw3NzeEhIRg2LBheOuttxAQEIDr168jKCgIffv2VV94VC6Xw8rKCmfPnoWFhQU%2B%2B%2BwzHDlyBNu3b4eTkxPi4%2BPxxhtv4MSJE2jTpk2T45DL5SgsLNRos7Z2QIcOur%2B%2B1uM0XM5LD5f1ahpbW8P2Z22t%2Bd3QZDLD92llpfnd0MwNfDqwtNT8bmg2NobvU/Tr2t3dsP317q353cBEDK5IJP/7buj%2BW2MSY0qMNiF7UJs2bZCamgobGxtIJBK4urrC1dUVFy9eVCdkZWVleO2112BhYQEA2L9/P2bPno3u3bsDAIKDgxEbG4sTJ04060MC8fHxjS6BMH/%2BAoSFherm4JopOFhItwCeFdPt00%2BL6Vekrl1FR2BYzs6iIzC8Xr3E9Hv%2BvJh%2Bd%2B8W0m3T33rrnoicu7zc8H02YDKovRaRkAHA119/jZ07d%2BL27dtQKpWora2Fp6enerudnR1sH6ji3Lp1C6tXr8Z7772nblMqlcjLy2tWv4GBgeqkr0FcnAPWr/%2BDB/IHOTrWJ2Offw7I5YbtGwDeHG3g4V5r6/pk7PJloKrKsH0DgJ2d4fu0sqpPxm7dAqqrDd%2B/iAqZszNw%2BzZQU2PYvgGgrMzwfVpb1ydjP/0k5nU9e7Zh%2B%2Bvduz4ZmzEDuHrVsH0DqEwzfAIqkdQ/zVVVgEpl8O6pBWsRCdmpU6cQFRWFdevWYfz48bCwsMCMGTM09jH/zT8TmUyGVatWwdvbW6u%2BHR0d4fibccKqKiA3V6uH/cPkckF9i3rrVVUlpm9Rw2hAfTJ2/77h%2BzV0QtagpkbM8VZUGL7PBlVVYvrPyDB8n0B9MiagbxFVm4ZhSpWqdVWNWtOx6ovRTup/UGZmJnr06AFfX19YWFiguroa2dnZj72Pi4sLrl27ptGWKyqLIiIiMmGc1K%2B9FpGQOTs7Iz8/H3l5ebh79y6ioqLg6OiIgoKCR94nKCgIcXFxuHDhAhQKBY4ePQo/Pz/cuXPHgJETERER/b4WMWTp7e2N48ePw9fXF/b29li8eDFGjBiBpUuXYu3atejZs2ej%2BwQEBCAvLw8LFixAeXk5nnzyScTExMDJyUnAERAREZmu1ljR0jWjTsg%2B//xz9c8bNmxotP3BT0tOmTJFY5tUKkV4eDjCw8P1FyARERGRDhh1QkZERETGjxUy7TEhIyIiIq0wIdNei5jUT0RERGTKWCEjIiIirbBCpj1WyIiIiIgEY4WMiIiItMIKmfaYkBEREZFWmJBpj0OWRERERIKxQkZERERaYYVMe0zIiIiISCtMyLTHIUsiIiIiwVghIyIiIq2wQqY9VsiIiIiIBGOFjIiIiLTCCpn2mJARERGRVpiQaY9DlkRERESCsUJGREREWmGFTHuskBEREREJxgoZERERaYUVMu0xISMiIiKtMCHTHocsiYiIiARjhYyIiIi0wgqZ9iQqlUolOoiWpqrK8H1KJIBMBty/D4h4xsrLywzan7m5FO3b26C4uAJ1dYb/S3ewFHB2kUqBtm2BsjIhZ7e8ynYG7c/cHHBwAAoLgbo6g3YNAOhsa9jXNID659jGBqioEPIcl0vaGrQ/qRRo0waorBTzD9u2rcTwnbq7A%2BfPAx4eQEaG4fsX9C99wgTdP%2BaRI7p/TGPGChkRERFphRUy7TEhIyIiIq0wIdMeJ/UTERERCcYKGREREWmlpVXISkpKEBUVhTNnzkAqlWLUqFFYtmwZZDJZo33ffvttJCYmarQpFAr4%2B/vj/fffx9/%2B9jccOnQIZmZm6u1WVlZIT09vVkyskBEREVGrsmzZMlRVVeHw4cM4cOAAsrOzsW7duofuu2rVKmRlZam/MjIy8OSTT8LHx0e9zxtvvKGxT3OTMYAJGREREWlJqdT9l77cvXsXKSkpiIiIgL29PTp27IiQkBAcOHAAtbW1v3v/2NhYODk5YdSoUTqNi0OWREREpBV9JFByuRyFhYUabQ4ODnB0dNTqca9cuQIzMzO4urqq25555hlUVlbixo0bGu2/VVpaim3btmH37t0a7adPn8bx48fxyy%2B/oGfPnoiKikLfvn2bFRcTMiIiIjI68fHxiImJ0WhbsGABQkNDtXrckpIS2NraQiL53zp17drVr8NYXFz82Pt%2B8cUXGDhwIHr16qVuc3FxgVQqRXh4OGxsbBATE4NXXnkFSUlJaN%2B%2BfZPjYkJGREREWtFHhSwwMBBeXl4abQ4ODk26b2JiIhYvXvzQbREREfgja%2BIrFArExcVh/fr1Gu3z58/XuL1o0SIcPnwYKSkpmDp1apMfnwkZERERGR1HR8c/PDzp7%2B8Pf3//h247efIkysvLoVAo1J%2BMLCkpAQB06NDhkY959uxZ1NTUwNPT87F9m5mZoXPnzpDL5c2KmZP6iYiISCstaVJ/nz59oFKpcPXqVXVbVlYW7Ozs0KNHj0fe7/jx43juuedgbv6/WpZKpcL777%2Bv8Vg1NTW4desWXFxcmhUXEzIiIiLSSktKyOzt7eHt7Y2NGzfi119/RX5%2BPrZs2YKAgAB1sjVr1iwcPXpU435XrlxBly5dNNokEglyc3OxfPlyFBQUoKKiAuvWrYOFhQXGjRvXrLiYkBEREVGrsmLFCrRt2xZjx47FpEmT0L9/f0RERKi35%2BTk4N69exr3KSwsxJ/%2B9KdGj7V69Wp0794dU6ZMwdChQ3HlyhXExsaiTZs2zYqJc8iIiIhIKy1tpf62bdsiOjr6kdu//fbbRm1JSUkP3feJJ57A%2B%2B%2B/r3VMrJARERERCcYKGREREWmlpVXIjBETMiIiItIKEzLtcciSiIiISDBWyIiIiEgrrJBpjxUyIiIiIsFYISMiIiKtsEKmPSZkREREpBUmZNrjkCURERGRYKyQERERkVZYIdMeK2REREREghlFQpabmwtXV1dkZ2fr5PGGDRuGhIQEnTwWERERPZ5Sqfuv1oZDlkRERKSV1phA6ZpRVMiIiIiIWjOjSsiysrLg5%2BcHd3d3zJo1CwUFBQCA9PR0TJs2De7u7hg%2BfDg2bNgA5X/T8bq6OqxcuRKDBw/GiBEjsG/fPvXjbdmyBVOmTNHoIz09Hf3790d5ebnhDoyIiMiEcchSe0Y1ZLl371588sknsLGxwfz587Fs2TK89957ePXVV7F48WJMnToVP//8M1577TU4Ojpi5syZOHDgAI4dO4bdu3ejc%2BfO%2BPDDD3Hv3j0AgL%2B/PzZv3ozs7Gz07NkTAJCUlIQxY8bA1ta2STHJ5XIUFhZqtNnZOcDBwVG3B/87JBLN74Zmbm7Y3N3MTKrx3eBEdCuVan43MHMDnw0a%2BjN0v2oifs%2BCn2Opgc8fD563hByyu7vh%2B%2BzdW/O7IWVkGL7P/2qNCZSuGVVCNnPmTDg5OQEAZs%2BejYULFyIxMRFOTk6YOXMmAODpp5%2BGv78/vv76a8ycORPJycmYOHGiOuEKDw9HfHw8AKBLly7w9PTEV199hYULFwIAUlJSsHTp0ibHFB8fj5iYGI22%2BfMXICwsVOvj/SOsrIR0C5nMRki/dnbWQvoVykbM79qhrZBu0b69mH4BMb9nAIC1mNd1GyG9Cjtc4Px5QR0D2L3b8H2KesdOOmFUCVlDUgUAXbt2RW1tLW7evKnRDgDdunXD119/DQAoKCjA6NGj1dvs7e3Rrl079W1/f3/8/e9/x8KFC5GVlYWKigqMHDmyyTEFBgbCy8tLo83OzgH37zfnyLQnkdQnY9XVgEpl2L4BoKqqwqD9mZlJYWdnjdLSKigUhn/r1d5cwNs9qbQ%2BGauoEPJ2s/C%2BYTMyc/P6ZKy4GKirM2jXAACHNoZ9TQOof46trYGqKiHPcaXEsEmoRPK/wxVx3moz3MPwnfbuXZ%2BMzZgBXL1q%2BP4FYYVMe0aVkEkfqGmr/vvXK3lExt/QXlNTg7rfnM2VD7wyXnjhBaxatQoXLlzAd999Bx8fH1haWjY5JkdHRzg6ag5Pijq5APX9iui7rk7MX5tCoRTTt1Tg2UXQBAoRSVFDv0L6FvkfRNBzrDRwAaXhlK5SCfp1CxzCw9WrYvunFseoJvXfvHlT/XNOTg5kMhm6deuGGzduaOx348YNuLi4AKhPmPLz89Xb5HI5SktL1bdtbW0xduxYHDt2DF9//TUmTZqk56MgIiJqXTipX3tGlZDFxcWhsLAQZWVliI2Nxbhx4/DCCy8gJycH8fHxqKurQ2ZmJr788ktMnjwZADBixAgcPnwY//nPf1BeXo4NGzbA6jcTrfz9/bFv3z7U1tbi2WefFXFoREREJosJmfaMasgyKCgIs2bNQl5eHjw8PPDWW2%2BhQ4cOiImJwUcffYQPPvgAjo6OCA8Px4svvgigfvJ/Tk4Opk2bBktLS4SFheHcuXMajzt8%2BHBYW1vDz8/vkUOgRERERKJIVCpRs6EMp7y8HKNGjUJCQgK6deum9eNVVekgqGaSSACZDLh/X8wcsvLyMoP2Z24uRfv2NigurhAyh8zBUtCk/rZtgbIyIW8P8yrb/f5OOmRuDjg4AIWFYuaQdbY17GsagPAPbpRLDPvBDakUaNMGqKwUU/GwbSvgDbi7e/2nOz08xMwhE/Qv/b%2BziHQqJ0f3j2nMjGrIUh%2Bqq6uxYsUKDB8%2BXCfJGBEREZGumXRClp6ejoEDB6KoqAjvvvuu6HCIiIhMEueQac%2Bo5pDpmqenJzIzM0WHQUREZNJaYwKlayZdISMiIiJqCUy6QkZERET6xwqZ9lghIyIiIhKMFTIiIiLSCitk2mNCRkRERFphQqY9DlkSERERCcYKGREREWmFFTLtsUJGREREJBgrZERERKQVVsi0x4SMiIiItMKETHscsiQiIiISjBUyIiIi0gorZNpjhYyIiIhIMFbIiIiISCuskGmPCRkRERFphQmZ9jhkSURERCQYEzIiIiLSilKp%2By99y8rKwvjx4zFt2rTf3XfXrl3w9vaGh4cHpk%2BfjosXL6q3VVdX45133sHIkSMxePBghIWFobi4uNnxMCEjIiKiVuXQoUMIDQ1Ft27dfnffb7/9Fps3b8aaNWvw/fffY8yYMZg3bx4qKysBABs2bMClS5cQHx%2BPpKQkqFQqLFmypNkxMSEjIiIirbS0Cll1dTXi4%2BPh5ub2u/vGx8djypQpcHNzg0wmw9y5cwEA3333Herq6rB//36EhISgc%2BfOeOKJJ7Bw4UKcOHECBQUFzYqJk/qJiIhIK/pIoORyOQoLCzXaHBwc4OjoqPVjT506tcn7Xrp0Cb6%2BvurbUqkUffr0QVZWFvr06YOysjI888wz6u09e/aETCbDpUuX0LFjxyb3w4TsD7C2Nnyfcrkcn34aj8DAQJ28GJvL2rqtQfuTy%2BXYvHmHsOMVQS6XI37nTmHH3LmdYfurf47rX9OdO4t4jg37mgb%2B%2BxzvEPe6tjVwf3K5HP/4h7jzFlQqg3cpl8sRv3kzAo8dazXnLkA/v%2BrNm%2BMRExOj0bZgwQKEhobqvrPHKCkpQbt2mifIdu3aobi4GCUlJQAAOzs7je12dnbNnkfGIcsWorCwEDExMY3eLZiq1na8QOs75tZ2vEDrO%2BbWdrxA6zxmfQkMDERCQoLGV2BgYJPum5iYCFdX14d%2BJSQkNDsW1e9knL%2B3vSlYISMiIiKj4%2Bjo%2BIerjP7%2B/vD399dJHO3bt1dXwhqUlJSgV69esLe3V9%2B2sbFRb7937x46dOjQrH5YISMiIiJ6hL59%2B%2BLSpUvq2wqFApcvX4abmxtcXFzQrl07je3Xr19HTU0N%2Bvbt26x%2BmJARERERPcDHxwfp6ekAgOnTp%2BPgwYO4cOECqqqq8PHHH8PS0hKjR4%2BGmZkZpk2bhm3btiEvLw/FxcWIjo7G%2BPHj8ac//alZfXLIsoVwcHDAggUL4ODgIDoUg2htxwu0vmNubccLtL5jbm3HC7TOY26JvL29cefOHSgUCiiVSvTr1w8AcOzYMTg7O%2BPmzZvqdcZGjhyJyMhILFy4EEVFRejXrx8%2B%2BeQTyGQyAEBYWBgqKirg7%2B%2BPuro6jBkzBlFRUc2OSaLSxUw0IiIiIvrDOGRJREREJBgTMiIiIiLBmJARERERCcaEjIiIiEgwJmREREREgjEhIyIiIhKMCRkRERGRYEzIiIiIiARjQkZEREQkGBMyIiIiIsF4LUsS6s6dO03e18nJSY%2BREOmPUqnEr7/%2Bipqamkbb%2BLpu%2BSoqKmBjYyM6DGrheC1LI1ZSUoJt27bhb3/7GwAgLi4O8fHx6NatG5YtWwZHR0fBEWqvd%2B/ekEgkTdr3ypUreo7G8PLz83H48GHk5%2Bfj7bffBgBkZmaif//%2BgiPTn2%2B//RYnTpyAXC4HAHTq1AleXl4YOXKk4Mj04%2BjRo1i%2BfDlKS0s12lUqFSQSiUm%2Bru/fv4/vvvsO%2Bfn5mDNnDoD613qnTp0ER6Yfbm5u8PLywqRJkzBixAiYm7PWQc3HhMyIhYaGQqFQYOvWrcjKykJwcDCioqJw8eJFyOVybNq0SXSIWrtx44b658zMTBw4cADBwcHo3r07lEolfv75Z%2BzevRuzZ8/G888/LzBS3Tt%2B/DgiIiLg4eGBc%2BfOISsrC3l5efDz88OKFSswYcIE0SHq3KZNm/DZZ59h%2BPDhcHJygkqlwp07d3Dy5EnMnTsX8%2BfPFx2izg0bNgwBAQHw8fGBlZVVo%2B1PPvmkgKj05/z583jjjTdgZ2eHvLw8XLx4Ebdv34afnx%2B2bt2KIUOGiA5R59LT0/HNN98gJSUFFRUV8PHxwaRJk/Dss8%2BKDo1aEhUZrUGDBqlKS0tVKpVK9d5776nefPNNlUqlUlVVVamGDBkiMjS98PPzUxUUFDRqz83NVU2YMEFARPrl5%2BenSk5OVqlUKlW/fv3U7adOnTLJ41WpVCpPT0/VuXPnGrWfPXtW5enpKSAi/fPw8FDV1taKDsNgAgICVF988YVKpdJ8XR85ckQ1ZcoUUWEZTGZmpio6Olr1/PPPq8aMGaOKjo5W3bp1S3RY1AJwUr8RUyqVsLW1BQCcPHkSY8eOBQBYWFigqqpKZGh6cfv2bbRp06ZRe7t27XD79m0BEelXTk4OvLy8AEBj2HbgwIHIzc0VFZZemZmZoV%2B/fo3a3dzcYGZmJiAi/fPz88OZM2dEh2EwP/30EwIDAwFovq59fHw0KuKmql%2B/fhg7diy8vLxQWlqKAwcOYMqUKYiMjERxcbHo8MiIcaDbiPXt2xdbtmyBlZUV5HI5Ro8eDaB%2BTkqPHj3EBqcHHh4eCAkJwauvvgpnZ2fU1dUhPz8fu3btgru7u%2BjwdM7JyQnXrl1Dnz59NNrT0tLQoUMHQVHp16xZs/D3v/8dISEhkErr3w8qlUp89tlnCA4OFhyd7kRHR6t/btOmDZYsWQIPDw906dKl0ZzJyMhIQ4enVw4ODsjLy4OLi4tGe1ZWlvoNpim6efMmvvrqKxw%2BfBiFhYXw8vJCdHQ0hg8fjsrKSqxYsQKLFy/Gp59%2BKjpUMlJMyIzYu%2B%2B%2Bi5UrV6K0tBRr166FtbU1SkpKsGrVKpOYP/Zba9aswerVqxEeHo779%2B8DAMzNzTFkyBCsXr1acHS6N2PGDLz66qsICAiAQqHAzp07ce3aNRw9ehSLFy8WHZ5enDlzBj/%2B%2BCNiY2Ph4uICpVKJvLw81NbW4qmnnsK///1v9b7//Oc/BUaqnYyMDI3bXbt2xd27d3H37l2N9qZ%2BoKUlmThxIl577TXMmTMHSqUSKSkpuHr1KuLi4jBjxgzR4enFlClTcPXqVTz77LN4/fXX4ePjo5F82traYuXKlRg0aJDAKMnYcVJ/C1RdXf3QycGmpKSkBDU1NbC3tzfpTyx98803OHDgAG7dugWZTAYXFxcEBQVh6NChokPTi5iYmCbvu2DBAj1GQvqiUqmwc%2BfOh76uAwICTDIJ3bp1K/z9/eHs7PzY/c6dO8eJ/vRITMiMWE5ODtauXauuhq1Zs0a97MXatWvRs2dPwRHqXsMyEAUFBVi6dCkA018GorW7d%2B8e2rVrJzoMvamtrcWWLVswfPhweHp6AgAOHTqEn3/%2BGQsWLIClpaXgCHWrsLAQDg4OosPQu4MHDzZ53xdffFGPkZCpYEJmxF555RW4uLhg%2BfLlOH36NEJDQ7F161ZcuHABp0%2Bfxj/%2B8Q/RIepUa1sGYsmSJY/cJpVK0bFjR4wcORIDBgwwYFT6denSJSxbtgwJCQkAgPDwcCQlJaF9%2B/bYunWrSc4VfOedd3Dx4kV88MEHeOqppwAAly9fxooVK%2BDq6orly5cLjlC33N3dce7cOfUcQVM1fPhwjdulpaWora2FnZ0dVCoVSktLIZPJ0LFjRyQlJQmKkloSJmRGzNPTE2lpaZDJZHj33XehUqmwYsUK1NbWYvjw4fjhhx9Eh6hTEydORHh4OMaNG4f%2B/fsjMzMTAHD69GmsWrUKhw8fFhyhbr399ttITk6GtbU1nn76aUilUly%2BfBnV1dUYNGgQ7t69i4yMDERFRSEgIEB0uDoxffp0jBgxAiEhIUhJSUFUVBT27t2L8%2BfPY/fu3di9e7foEHVu6NChOHLkCNq3b6/RXlxcDD8/P5w8eVJQZPrx4YcfQiaTYe7cua1m9fp9%2B/bh0qVLCA8PVz/PcrkcGzduhLu7O6ZOnSo4QmoJTHdyjgkwMzNTLwWQlpamXsldpVKhtrZWZGh60dqWgXjiiScQHBzc6BOHH3/8MSwsLPD6668jLS0Nq1atMpmE7Pr16/j8888B1FdEfX194eTkhM6dO5tcpaiBQqF46Lyp2tpaVFdXC4hIv9LS0iCXy/HJJ5/Azs6u0XImaWlpgiLTn5iYGCQlJUEmk6nbHB0d8dZbb8HX15cJGTUJEzIjNnDgQCxfvhwWFhaoqalRl8h37tyJ3r17C45O91rbMhB79%2B5FWlqaxtCOVCrFa6%2B9hjFjxuD111/HsGHDUFBQIDBK3bKyskJtbS0kEgn%2B/e9/Y%2B3atQCAyspKKJVKwdHpx/PPP4/58%2BfjlVdegbOzM1QqFW7evInt27eb3DA8UD/VorW5f/8%2B8vLyGi1HVFRUZJJJN%2BkHEzIjtnz5cnz00UcoLi5WV03u3buHL7/8Ehs3bhQdns61tmUgLCwskJqainHjxmm0nzp1Sn0R6hMnTqBz584iwtOLkSNHIiwsDObm5rC1tcVzzz2H2tpabNiwAR4eHqLD04ulS5di/fr1WLJkifp6lnZ2dpgyZQrefPNNwdHp3uTJkx%2B5bf369QaMxHD8/PwQHByMiRMnokuXLlAoFMjLy8ORI0fg7e0tN47s9gAADLlJREFUOjxqITiHrIVav369SZ7MW9MyEPv27cM777yD3r17w9nZGebm5rhz5w6ysrIQERGB2bNnw8PDAx9%2B%2BKHJVFLu37%2BPnTt3oqysDDNmzICzszMqKysRGhqK1atXm%2BzFpxsUFxdDKpWa9KdKgfo3EhcvXlS/sQCAgoICJCcn4/z58wIj0w%2BFQoH9%2B/cjJSUF%2Bfn5qKmpgaOjI0aOHInZs2fDwsJCdIjUAjAhM3Kt6cTWGpe3uHz5MlJTU1FYWAilUokOHTpg8ODB2LNnD6Kjo5Gbm4suXbqIDlPnlEolioqKYGVlBTs7O9Hh6JWnpyfOnj1rkutvPczmzZuxY8cOuLq6IjMzE%2B7u7sjOzkbHjh0xb948vPDCC6JDJDJKHLI0Yo87sZniyvXTp09H586d4evrC19fX5OcJ/cghUKBjIwM/PLLL%2BqEu7S0FD/88AOuX78OACaXjBUWFmLZsmU4efIk6urqAAAymQzjxo3DkiVLYG9vLzhC3RsxYgT27t2rvr6jqdu/fz/27t2LXr16oX///oiLi0N1dTWWL19u0os879mzB0ePHsXt27chkUjQtWtXTJ48GZMmTRIdGrUQrJAZsVGjRmH79u3qE1tmZqb6xDZmzBiMHz9edIg6VVxcjOPHjyMlJQWnTp2Ck5OTOjkzxUVwo6Ki8N1338HT0xPHjh3DhAkTcOXKFVhaWmLp0qUmOadqzpw5qKurw%2BzZs9G1a1eoVCrcunULu3btgpWVlUle52/evHn48ccfYWZmhk6dOjVKSlryJaIexsPDQ129d3d3R3p6OszMzHD37l1Mnz4dycnJgiPUvY0bN2L//v3w9/dHt27dAADZ2dk4ePAgIiIiEBQUJDhCaglM9%2B2KCSgrK0OvXr0A1C%2BBoVAoYGVlhcjISEyfPt3kErL27dsjICAAAQEBqKioQGpqKpKTkzFjxgx06tQJiYmJokPUqZSUFOzfvx%2BdOnVCcnIy1qxZA5VKhXXr1uHatWsmmZBduHABqampaNu2rbrtqaeewuDBgzF69GhxgelR37590bdvX9FhGEz37t2RkJCAyZMnw8nJCSkpKfD29kZdXR2KiopEh6cXCQkJ%2BPTTTxt9QnzChAn461//yoSMmoQJmRFrjSe2Bm3atIGDgwM6deqEjh07Ij8/X3RIOlddXa2exG5mZoaamhpYWlri9ddfx8SJEzF9%2BnTBEepely5dUFlZqZGQAZq/C1PzuGtymuJCuG%2B%2B%2BSZCQ0Px/PPPY9asWYiMjMSTTz6J/Px8k026y8vL1W%2BeH/TMM89ALpcLiIhaIiZkRiwyMhJhYWEPPbGNGTNGdHg6V1NTg7S0NKSkpODEiRNQKBQYN24cFi1ahCFDhogOT%2BeeeuopxMTE4P/%2B7//Qo0cP7Nu3DzNnzkReXh4qKytFh6czN2/eVP88d%2B5cvPnmm5g5cyZ69uwJqVSKmzdvIi4uDqGhoQKj1K/r16/j0qVLjT6c89lnn2HGjBkCI9MNPz8/9ZU0Vq1ahe%2B//x4ymQzTpk2Di4sLsrKy4OzsDB8fH8GR6kevXr2wf//%2BRpWwhIQE9RAm0e/hHDIjV1VVBWtrawD161M1nNi8vb1NboKsu7s7LC0tMXbsWPj4%2BGDo0KEmd4wPysrKQmRkJBITE/H9999j4cKFsLS0RHV1NWbOnIm33npLdIg60bt3b0gkEvzeqUYikeDKlSsGispw9uzZg5UrV6JDhw64e/cuOnbsCLlcDmdnZ8ycOROzZ88WHaLWRo0aBXd3d3Tt2hU7duzAq6%2B%2B%2BsjnOzIy0sDR6V96ejrmzp0LZ2dn9XzX7Oxs5ObmYvPmzRg5cqTgCKklYEJGRiM1NdXkk7DHuXHjBq5cuQJnZ2eTuqD47du3m7yvs7OzHiMRY9y4cVi1ahWee%2B459YdzCgsLsXr1arz00kvw9PQUHaLWfvjhB8TGxqK8vBxnz5595DFJJBLs2rXLwNEZRlFRERITE/Gf//wHQP2UEx8fHzg5OYkNjFoMJmRGpuHySE1hCteE27RpE8LCwgAA0dHRj93XFN9Ztwa1tbXqhTEfHLJ7GEtLS0OEZFDu7u7IyMgAAAwYMAAZGRmQSCS4ffs25s2bh6%2B%2B%2BkpwhLoVHBysvl5pa9GwnEtaWpp6ORdra2uTXs6FdK91liKMmCmuvv84P/74o/rnhn9aD9NaFtU0RZ6enurnuX///o99Lk1xyNLJyQmnT5/Gc889BwcHB6Snp2PgwIFo27YtcnNzRYenc60tGQOARYsWQaFQ4KOPPmq0nMtf//pXk1zOhXSPFTIjV1paCoVCgfbt2wMAcnNzYWNjo75tSgoLC%2BHg4CA6DNKx9PR09RDWmTNnUFRUpL5YfFlZmfq6f3369MGgQYNEhqoXhw4dwpIlS3D69GnExcXh008/xcCBA3Hjxg107doV27dvFx0iacnd3b3Rci5A/et79OjROHfunKDIqCWRig6AHu306dPw8vLCqVOn1G3/%2Bte/MH78ePzwww8CI9OPUaNG4eWXX0Z8fDxKSkpEh0M68uB8ovz8fCxduhSDBg1Cv379sGbNGqxfvx4ff/wxcnJyBEapP5MmTUJSUhJsbW0RGBiIefPmoWPHjggICMCGDRtEh0c60LCcy2%2BZ8nIupHuskBmxyZMn4%2BWXX8bkyZM12o8cOYLt27fjyy%2B/FBSZfly%2BfBkpKSlITk7GzZs3MXjwYEyYMAHjx49v9M6TWqYXXngBb7/9NoYNG4Z//vOfiIuLw8GDB/Hzzz8jMjISR44cER2izhUVFSEqKgqpqamoqamBSqWCtbU1Ro0ahXfeeYfzi1qoB5dzyczMVC9b89vlXIKCgkx2uQ/SLSZkRuzBy448qLa2FoMGDXrsnKuW7pdffkFycjKSk5Nx9epVDBs2DFu3bhUdFmlpwIABuHDhAgAgJCQE/fr1wxtvvAFAc/K7KQkODoZEIsHs2bPVn7jLzc1FbGwspFIpYmNjBUdIf0RrX86FdI%2BT%2Bo1Y165dkZyc3Ojd1cGDB03%2Bo9TdunXDhAkTYG1tDXNzc6SmpooOiXTA3t4eBQUFsLS0xKlTpxAeHg6gfpFUmUwmODr9yMzMRFpamkaVt3fv3vD09MSoUaMERkbaOH78uOgQyMQwITNiixYtQmhoKLZt24YuXbpAqVTixo0byM/PN9mJwNeuXUNKSgqOHz%2BO69evw9PTExMnTsTmzZtFh0Y6EBQUhICAAJiZmWHw4MFwdXVFeXk5IiIiTHZYx8XFBffv32807K5QKODi4iIoKtKWKa6ZR2JxyNLIFRQU4OjRo8jNzYVKpUK3bt3g5%2Ben/pSaKRk7diwKCgrw7LPPwsfHB97e3pxfY4IyMjJQWlqKIUOGwNLSEnV1ddixYwfmzJmjXq/MlHzzzTf44osv8NJLL6F79%2B5QKpW4desW9uzZg0mTJmksAtyjRw%2BBkRKRSEzIjFjDZOB//etf6gU1ra2tMXr0aCxbtszkkpVt27Zh6tSpJplsUuvVu3fvx25vmIfEuUZErRsTMiPW2iYDu7u749y5c5BKuRoLmY7WfukoImoaJmRGzM3NrdFkYAAoKSnBqFGjNFa5NwUffvghZDIZ5s6dCxsbG9HhEBERGQwn9Rux1jYZOC0tDXK5HJ988gns7OwaLfdhCtfuJCIiehhWyIxYa5sM/HsL3f52gVwiIiJTwYTMiHEyMBERUevAhMyItbbJwEuWLHns9vfff99AkRARERkW55AZMVNIspqjurpa47ZCoUBOTg7y8vIwYcIEQVERERHpHxMyMhrR0dEPbf/yyy9x7do1A0dDRERkOByyJKOnUCgwZMgQnDlzRnQoREREesEKGRmNhqsRPOj%2B/ftISkoyyUvqEBERNWBCRkajf//%2BkEgkjdrNzMzwl7/8RUBEREREhsEhSzIaZ86cQVFRkfpalmVlZfjpp58wdOhQ9O/fX3B0RERE%2BsOEjIzGoUOHEBUVhfPnz6OqqgovvvgiAODevXtYtGgR/vznPwuOkIiISD94FWcyGh9//DE2b94MAEhMTISlpSWOHj2K2NhY7NixQ3B0RERE%2BsOEjIxGXl4ehg0bBgBITU2Fr68vzMzM4Orqijt37giOjoiISH%2BYkJHRsLe3R0FBAYqLi3Hq1Cl4eXkBAAoKCiCTyQRHR0REpD/8lCUZjaCgIAQEBMDMzAyDBw%2BGq6srysvLERERAR8fH9HhERER6Q0n9ZNRycjIQGlpKYYMGQJLS0vU1dVhx44dmDNnDtciIyIik8WEjIiIiEgwziEjIiIiEowJGREREZFgTMiIiIiIBGNCRkRERCQYEzIiIiIiwZiQEREREQnGhIyIiIhIMCZkRERERIL9Pyq4NsPNXzECAAAAAElFTkSuQmCC&quot; class=&quot;center-img&quot;&gt; &lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAmQAAAIZCAYAAAAIrSOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD%2BnaQAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3X98zfX///Hb2e8Ym7Gh4U3Y/FpMvGdILCJE%2BZAfkfVLYtNEb/JG6PebEPrxpoio91TvIvSVkXcpFZFmGZqVn9mwY4bZr/P9Y%2B3k2Gi85vWa3K%2BXyy5ne76er9fj%2BXpuO%2BdxHq8fx%2BZwOByIiIiIiGXcrB6AiIiIyPVOCZmIiIiIxZSQiYiIiFhMCZmIiIiIxZSQiYiIiFhMCZmIiIiIxZSQiYiIiFhMCZmIiIiIxZSQiYiIiFhMCZmIiIiIxTysHoCIlM6PP/7If//7X7Zv387Bgwc5e/Ys3t7eBAUF0axZM7p3705UVBQ2m83qoYqIyGWy6bMsRco3h8PBc889xzvvvAOAn58fISEh%2BPr6kpWVxa5du8jKygKgU6dOzJkzBy8vLyuHLCIil0kJmUg5t2zZMqZNm4aPjw9Tp07lrrvuwt3d3bk8Ozub//znP7z00ksUFBQwbNgwxowZY%2BGIRUTkcukcMpFy7r333gPg/vvv5%2B6773ZJxgB8fHyIjo7mkUceAWDp0qVkZ2ebPk4REblySshEyrn9%2B/cDEBoaesl%2B0dHRvPnmm6xbtw4fHx8Axo8fT2hoKC%2B99BKnT5/mhRdeICoqirCwMNq0aUNMTAx79uy56DY3b95MTEwM7du3p1mzZrRp04bo6Gg%2B%2BeSTi65jt9uZNWsWd999N%2BHh4TRt2pS2bdsyfPhwvv322xLXiYqKIjQ0lC%2B%2B%2BIJ169bRvXt3wsLC%2BP777wGYO3cuoaGhjB8/npycHObOnUvXrl25%2Beabue2223jmmWc4c%2BYMAHv37mXUqFG0a9eOsLAw7rrrLlauXFli3LNnz7JgwQLuvfdeWrVqRZMmTYiIiOCBBx5g3bp1Ja4zZMgQQkNDee%2B99zh9%2BjSzZs1yjqV169Y88sgj7Ny586LzIyJSEiVkIuVcQEAAAD/88MOf9rv11lupVq1asWWnT58mOjqa9957jypVqtC6dWvy8/NZt24d/fr1KzGBmDVrFtHR0axbtw5fX18iIiIICAhg8%2BbNjB07lri4OPLz813WSU9Pp0%2BfPrzxxhvs27ePJk2a0KZNG7y8vPj8888ZOnQoH3744UX3Yd%2B%2BfYwePRpPT0/atGnjTCzP9%2BSTT7Jw4UKCg4Np2LAhaWlpLF26lHHjxrFnzx4GDhxIcnIyjRo1IjAwkD179vDkk0%2ByceNGl%2B2cPXuWQYMGMWPGDJKSkqhfvz6RkZH4%2Bfnx9ddfExMTw9y5cy861pycHKKjo1m8eDE1a9akadOm5Ofn88UXXzBkyBBnIi0iUioOESnXxo0b5wgJCXGEhoY6XnrpJcfRo0cve92WLVs6evbs6bJuZmamo2/fvo6QkBBH3759Xdb77LPPHCEhIY7WrVs7vv32W5dl3333naNt27aOkJAQx6JFi1yWPfvss46QkBBH165dHceOHXO25%2BXlOZ577jlHSEiI45ZbbnFkZWW5rNepUydHSEiIIyoqyvH6668X2485c%2BY4QkJCHO3bt3f06tXLkZ6e7ly2bt065/x069bNMXfuXEdBQYHD4XA48vPzHSNHjnSEhIQ4HnroIZdtLlq0yBESEuJo06aNIzU1tcRljRs3duzfv99l2eDBgx0hISGOjh07OgYNGuQ4fvy4c1laWpqjXbt2jpCQEMfzzz9fbD9ERC5GFTKRci4uLo6aNWvicDh466236NixI4MHD2b27Nl8/fXXnD179k%2B3kZWVxZQpUwgKCnK2VapUifHjxwOFt9T45ZdfnMtef/11AMaNG8ff//53l221bt2af/zjHwAsXrzYZVlQUBA9evQgJiaGqlWrOtvd3d0ZPXo0bm5unDp16qLVvqKLEi4mLS2NqVOnulQBO3fuTGBgIA6HAw8PD0aOHOm89Yebmxv9%2B/cH4KeffnLZVsWKFbnrrrsYPnw4devWdVk2dOhQAgMDyc/PZ/PmzSWOJT09nZkzZzormACBgYHcddddAOzYseOi%2ByEiciHdh0yknKtRowbvv/8%2B06dPZ9WqVeTn57Nlyxa2bNnC66%2B/jqenJxEREfTq1YsePXrg4VH83zooKIhbbrmlWHt4eDgVK1bk9OnT/PDDD9StW5ejR4%2BSlJQEQJcuXUocU%2BfOnbHZbBw%2BfJjU1FTq1asH4LywoCQ33HADVatWJT09nfT09BL7tGnTBje3i79PDAwMpEWLFsXag4ODSU9Pp1OnTsXuwxYcHAzAyZMnXdr79etHv379Soxjs9moVavWJcf697//nerVqxdrr1OnDgAZGRkX3Q8RkQspIRO5BgQGBvKvf/2Lf/zjH6xbt47NmzezdetWjh8/Tm5uLps2bWLTpk0sWLCA1157zZkUFAkJCSlxu25ubgQHB7Nnzx4OHjwI4HKSf1EFrSQeHh7k5ubyyy%2B/OBMyKDy36n//%2Bx87d%2B4kLS2NzMxMHL/fXefUqVNAYSWsJCUlOOerVatWie1F910rSr5KWpaXl1dsWUFBAV999RXbt28nLS0Nu93uHFtqauolx1q7du0S2729vQHIzc291K6IiLhQQiZyDalWrRoDBw5k4MCBQOFJ8N9%2B%2By0rVqxg%2B/bt7N27l2HDhvHJJ5/g6enpXM/Pz%2B%2Bi26xUqRJQeOI/uFaS1q9f/6djKkqyAJKTkxk5cqQzubtclxon4LJPV7L8fIcPH2bkyJHFDmWWlm6%2BKyJlSQmZyDXspptu4qabbmLgwIEsXryY559/ntTUVNasWUPv3r2d/S51GLCoAlTUp%2BiQn6enJ4mJiaX%2BKKbs7Gwee%2BwxDh8%2BTN26dRkxYgSRkZFUqVLFmShFRUVx6NChi27DzI99GjVqFD/99BPVqlUjNjaWDh06UK1aNWeiNWTIEL777jvTxiMi1zclZCJ/EUOHDmXp0qXs37%2BfvXv3uiwr%2BmilkhQtK6qU%2Bfv7A4WH3DIyMlxOWr%2BUL774gsOHD2Oz2Zg/fz5/%2B9vfivUpLzesTUpKIjExEYAZM2YQGRlZrE95GauIXB90laVIObZu3TomT57M/PnzS9W/6JBf0XlMRX7%2B%2BecS%2B%2Bfn5zsrVkXnRJ1/vtmFid2lFF2lWadOnRKTsf3793P8%2BPFSb%2B9q%2BvXXXwGc9zu70OnTpy95w1wRkbKmhEykHEtMTCQ%2BPp758%2Bdz%2BPDhS/Y9evQoycnJAMWuRDxw4AC7du0qts7333/vvMN9eHg4UHgBQdOmTQFYvnx5ibFSU1Pp3bu38/YY8EeF7dy5cyWu8%2Bqrrzq/v/CGsmbz9fUFCg/XlnTy/VtvveWskJV0MYCISFlTQiZSjj3wwAMEBgZy6tQphg4dWuJHDzkcDr755hsefvhhcnNzadGiBe3atXPpU6lSJSZOnOhSobLb7bz44otA4e0mbrzxRuey4cOHA7Bq1Srefvtt51WSUFhdGjVqFMnJyRw5csTZ3qhRIwB%2B%2B%2B03EhISnO1nz57l2WefJTEx0Zn07du374rnpCyEhITg7u5Ofn4%2B7777rrM9Ly%2BPf//738THx9OpUyfgj6stRUSuJp1DJlKOValShYULFxIbG8svv/zC/fffT2BgIDfddBMVKlTAbrdz8OBB572ybrnlFubNm1fsJP6oqCgOHDjA7bffTlhYGJ6enuzYsYOsrCx8fX2ZPHmyS/877riDYcOGMX/%2BfF544QWWLFnCTTfdREZGBrt27SI/P58mTZowduxY5zrh4eG0b9%2BeTZs2ERsbS/PmzfH29mbnzp14eHiwaNEiVq9ezfbt21myZAm7d%2B/mscceo3Xr1ld/Ii9Qo0YN%2BvbtS3x8PC%2B88AIrV67E39%2BfXbt2cfr0aebNm8fBgwf5/PPPWbt2LUOGDGHQoEHceeedpo9VRK4PSshEyrmQkBA%2B%2BeQTVq9ezYYNG0hOTubHH38kJycHHx8fqlevTmRkJN27d6djx44lXqno5ubGwoULee211/jss884fPgwFStWpGvXrowePdrlPmJFxowZQ9u2bVm6dCk//PADmzdvxtvbm2bNmtG9e3cGDRpU7NYPs2fPZsaMGaxfv56dO3cSFBRE165dGT58OHXq1KFGjRr89NNPbNu2jb1795p6VeWFJk6ciJ%2BfH6tWrWLPnj1UrVqViIgIHn30URo3bkx2djbffvstX3zxBXv37nWpEoqIlDWbQ88yIn9Z48eP56OPPuKee%2B5xHp4UEZHyR%2BeQiYiIiFhMCZmIiIiIxZSQiYiIiFhMCZmIiIj8JX355Ze0bduW0aNHX7JfQUEBs2bN4vbbb6d169Y89NBDHDhwwLncbrcTFxdH27Ztad%2B%2BPf/85z/L/NM8dFK/iIiI/OUsWLCADz74gICAAGrUqMGsWbMu2vedd95h0aJFLFiwgOrVqzNr1iy2bNnCihUrsNlsxMbGkpOTwwsvvEBubi6PP/44zZo1Y%2BLEiWU2XlXIRERE5C/H29ubDz74oMSPcrtQfHw80dHR1K9fH19fX0aPHk1KSgo7duzg2LFjJCQkMHr0aAICAqhevTojRozgww8/LPGTPq6U7kMmIiIi5U5aWprzptdFAgMDCQoKKtX6999/f6n6ZWdn8/PPP9OkSRNnm6%2BvL3/7299ITEzk1KlTuLu7Exoa6lzetGlTzpw5w759%2B1zajVBCdiWsuJllvXqwdy80bAgWfJTLio/NPbJdoQJ07gwJCfD7Ry2aKi3N/JiVKsG998Ly5XDqlPnx%2B/Y1N56bG1SuDJmZUFBgbmwo/BuzgpcX5ORYE/uee8yNV6MGLFgAjzwCv/1mbmyADz80P6bNBt7ecO4cWHFC0A03mB8TuCqvi/Fz5jBv3jyXtpiYGGJjY8s0zsmTJ3E4HPj5%2Bbm0%2B/n5kZGRgb%2B/P76%2Bvi43si7qm5GRUWbjUEJ2rfD3B3f3wsfrgKdn4f%2B3p6fVIzGPl1dhknLBze//smy2P76uF%2Bfv8/Vw9m7FioVPWxUrWj0Sc11Pf9NXU//%2B/YmKinJpCwwMvGrxLnVKvRmn2yshExEREWPcyv6U9KCgoFIfnjTC398fNzc37Ha7S7vdbqdq1aoEBASQlZVFfn4%2B7u7uzmUAVatWLbNx6KR%2BERERuW55e3vTsGFDkpKSnG2ZmZns37%2Bfm2%2B%2BmcaNG%2BNwOEhOTnYuT0xMpHLlyiV%2BDvCVUkImIiIixri5lf3XVXT06FG6devmvNfYwIEDWbJkCSkpKWRlZTFjxgwaN25MWFgYAQEBdO3aldmzZ3PixAl%2B%2B%2B03Xn31Vfr27YuHR9kdaNQhSxERETHmKidQVyIsLAyAvLw8ABISEoDC6lZubi6pqank/H6FzYABA0hPT2fIkCGcPn2aiIgIlwsKpk2bxtNPP83tt9%2BOp6cnPXv2/NObzV4uJWQiIiLyl5OYmHjRZbVq1WL37t3On202G6NGjWLUqFEl9q9UqRIzZ84s8zGeTwmZiIiIGFMOK2TXGs2giIiIiMVUIRMRERFjVCEzTAmZiIiIGKOEzDDNoIiIiIjFVCETERERY1QhM0wzKCIiImIxVchERETEGFXIDFNCJiIiIsYoITNMMygiIiJiMVXIRERExBhVyAzTDIqIiIhYTBUyERERMUYVMsOUkImIiIgxSsgM0wyKiIiIWEwVMhERETFGFTLDNIMiIiIiFlOFTERERIxRhcywcjmD//3vf2nXrp3VwxAREZHScHMr%2B6/rzPW3xyIiIiLljA5ZioiIiDHXYUWrrF3VGTx48CChoaGsXbuWHj16cPPNNzN48GDS09MB2LRpE7169aJFixb07t2bzZs3l7idTZs20adPH8LDw7n11luZM2eOc9mxY8cYOXIkERERtGzZkujoaA4cOABAamoq0dHRtGrVitatWxMTE0NGRsbV3GURERGRy2ZKhWzp0qUsXLgQHx8fYmJimDJlCpMnTyY2NpbnnnuOLl268MknnzBy5Eg2bNjgsu6ZM2eIjY1lwoQJ9O3blz179jBgwACaNWtGVFQUr7zyCn5%2BfnzxxRfk5%2Bfz4osv8tJLLzFv3jyeeeYZWrZsyZtvvsnp06cZN24cr7/%2BOhMmTCj12NPS0pwJZJHAevUI8vcvk7kptUaNXB9N5udnbjxfX9dHs%2BXnmx%2Bz6E/K7D%2BtIu7u5sYrekNt1Rtrm826mFbEBqhf39x4tWq5PprtevsdOxzmx3RShcwwUxKyQYMGUb16dQCio6OJi4tj9erV1K5dm%2B7duwPQp08fvL29KSgocFm3QoUKfPHFF1SsWBGbzUZoaCihoaHs3LmTqKgoMjMz8ff3x8vLC5vNxpQpU3D7/Q8jMzMTHx8fPDw88PPz47XXXnMuK634%2BHjmzZvn0hbz%2BOPEPv74lU6HMe%2B%2Ba0nYjpZEhVatLApsoagoq0dgLquSbit5eloTd%2B5ca%2BKOG2dNXCt5e5sf8%2BxZ82M6KSEzzJSErF69es7vg4ODycnJ4cCBA9S64G1Tjx49Slz/008/5e233%2BbQoUMUFBSQm5tLq99fqR9%2B%2BGEee%2BwxvvzyS9q3b8%2Bdd95JZGQkADExMTz55JN8/PHHtG/fnp49e3LzzTdf1tj79%2B9P1AWvkIF33QWLF1/Wdgxr1KgwGRs0CJKTzY0NbJy5zdR4vr6FydjWrZCVZWpoAE6cMD%2Bmv39hMrZhA9jt5sfv3NnceG5uhb/nrCy44H2YKXx8zI9psxUmY7m51lQzxowxN16tWoXJ2EsvwcGD5sYGmD7d/Jg2W2Eydu6cxRUrueaYkpCdX/Vy/P4X6nA4ilXDSrJ582amTJnCjBkz6NKlC56engwaNMi5PCwsjA0bNvDll1%2ByceNGYmJiuPfeexk3bhwdO3Zk48aN/O9//2P9%2BvUMHjyYf/zjHwwePLjUYw8KCiIoKMi1MTW11OuXueRk2L7d9LAnT5oeEih8sbYi9vHj5scsYrdbE9%2BKw7RQmIxZEdvKF0uHw5r4KSnmx4TCZMyK2Nfj79gyqpAZZsoM7t%2B/3/n9oUOH8PHxoW7duqRekNgsXbrUeUJ%2BkR9//JF69erRvXt3PD09OXfuHCnn/Wfb7XY8PT25/fbbeeaZZ3j99df5z3/%2BA0BGRgYVK1ake/fuvPzyy0ydOpX4%2BPiruKciIiIil8%2BUhOy9997j2LFj2O12Fi9ezG233UbPnj05cuQIy5cvJycnh9WrVzNz5kwqVqzosm5wcDC//fYbR44c4dixY0yZMoWgoCCOHj0KwIABA1iwYAHnzp0jNzeXHTt28Le//Y3s7Gy6du3KihUryMvLIzs7m6SkJOrUqWPGLouIiFw/dGNYw0zZ4169ejF06FBuvfVWAJ5%2B%2BmmqVavGW2%2B9xdtvv03r1q2ZP38%2Br776KgEBAS7rdu3alQ4dOtC9e3f69%2B9Px44deeyxx0hISGD69OnMnj2bzz//nDZt2tC2bVs2b97MjBkz8PHx4ZVXXuHtt9%2BmVatWdOzYkd9%2B%2B43JkyebscsiIiLXDyVkhplyDlmLFi1YvXp1sfbWrVuzZs2aYu19%2BvShT58%2BAHh6ejJr1qxifYquzoTCClxJIiMj%2Beijj6502CIiIiKm0J36RURExJjrsKJV1pSQiYiIiDFKyAy7qglZrVq12L1799UMISIiInLNU4VMREREjFGFzDDNoIiIiIjFVCETERERY1QhM0wJmYiIiBijhMwwzaCIiIiIxVQhExEREWNUITNMMygiIiJiMVXIRERExBhVyAxTQiYiIiLGKCEzTAmZiIiI/OUcOnSIqVOnsmPHDipUqED37t0ZM2YMbhckjw8%2B%2BCBbtmxxacvLy2PkyJHExMQwZMgQtm3b5rJevXr1WLlyZZmOVwmZiIiIGFMOK2SxsbE0bdqUhIQEjh8/zqOPPkq1atV44IEHXPotXLjQ5efMzEy6d%2B9Oly5dnG3PPPMMffr0uarjLX8zKCIiImJAYmIiycnJjB07lkqVKlG3bl2io6OJj4//03Vnz55Nly5dCA0NNWGkf1CFTERERIy5ChWytLQ00tPTXdoCAwMJCgr603WTkpIIDg7Gz8/P2da0aVNSU1PJysrC19e3xPV%2B/fVXPv74YxISElza16xZw5tvvsmRI0do3rw506ZNo06dOlewVxenCpmIiIgY4%2BZW5l/x8fH06dPH5as0FS4Au91O5cqVXdqKkrOMjIyLrjd//nz%2B7//%2Bj4CAAGdb/fr1adiwIe%2B%2B%2By7r168nICCAhx9%2BmJycnCuYqItThUxERETKnf79%2BxMVFeXSFhgYWOr1HQ7HZcWz2%2B2sWLGCTz/91KV9ypQpLj9PmzaNiIgIvv/%2BeyIjIy8rxqUoIRMRERFjrsIhy6CgoFIdnixJQEAAdrvdpc1ut2Oz2VyqX%2Bdbv3499erVo3bt2pfctq%2BvL35%2Bfhw9evSKxnYxOmQpIiIifynNmjXjyJEjnDhxwtmWmJhIgwYNqFixYonrrF%2B/nnbt2rm0ZWVlMWXKFJfk68SJE5w4ceJPE7fLpYRMREREjLkK55AZ0aRJE8LCwnj55ZfJysoiJSWFRYsWMXDgQAC6devG1q1bXdbZtWsXtWrVcmnz9fVlx44dPPvss9jtdk6ePMnUqVMJDQ0lPDzc0BgvpIRMREREjClnCRnAnDlzSEtLo127dtx///3cfffdDBo0CIDU1FTOnDnj0j89PZ1q1aoV286rr76Kw%2BGga9eudOzYkdzcXObPn1/sBrNG2RyXe9absGKF%2BTH9/KBjR9i4EU6eND9%2B77tt5gYMD4dt26BlS9i%2B3dzYQNYp8/8t3NygQgU4cwYKCkwPj%2B/Z9D/vVJY8PKBKFcjIgLw8c2MDHDtmfkwfH6hXD1JTITvb/PgpKebGq1wZOnSAL76AzExzYwPpEbeZHtPDw40qVSqSkXGavDxz/5EDAyuZGs9F165lv821a8t%2Bm%2BWYTuoXERERY8rhnfqvNZpBEREREYupQiYiIiLGqEJmmBIyERERMUYJmWGaQRERERGLqUImIiIixqhCZphmUERERMRiqpCJiIiIMaqQGaaETERERIxRQmaYZlBERETEYqqQiYiIiDGqkBmmGRQRERGxmCpkIiIiYowqZIYpIRMRERFjlJAZphkUERERsZgqZCIiImKMKmSGaQZFRERELKYKmYiIiBijCplhSshERETEGCVkhmkGRURERCymCpmIiIgYowqZYUrIRERExBglZIZpBkVEREQspgqZiIiIGKMKmWGaQRERERGLqUImIiIixqhCZli5m8EHH3yQ2bNnl/l2U1JSCA0N5eDBg2W%2BbRERkeuam1vZf11nyl2FbOHChVYPQURERMRU5S4hExERkWvMdVjRKmuXNYPz58%2BnU6dONG/enK5du7JixQq%2B/fZbQkNDOXfunLPf6NGjGT9%2BPAD//e9/6dmzJy%2B%2B%2BCItWrTg9ddfJyoqymW7P/30E40bN%2Bbo0aMMGTKEGTNm8L///Y8WLVqQnZ3t7HfixAmaNGnCDz/8AMDSpUu58847ad68OT169CAhIcHZ9/jx4zz88MOEh4fTo0cPfvzxx8ufHRERERETlLpCtm3bNpYsWcLy5cupWbMmX331FbGxsTz33HN/um5aWhre3t5s2bKFkydPMnfuXJKTk2nUqBEA69ato1WrVlSvXt25Ttu2bfHy8mLTpk107twZgA0bNlCjRg1atGjBZ599xrx583jzzTdp1KgRGzZsIC4ujs8%2B%2B4wbb7yR559/nnPnzrFx40ays7MZO3bs5c6Nc%2Bzp6ekubbm5gVSrFnRF27tSvr6uj6YLDzc33u9/G85Hk1nxZs9m%2B%2BPRkjebHiYXzN3dXR/N5uNjfkwvL9dHs1WubG48i5%2B4PDzM/0dyd3dzebxuqEJmWKmfgU%2BdOoWbmxs%2BPj7YbDbat2/P999/z5YtW0q17iOPPIKnpyfVqlWjVatWJCQkOBOyhIQEBg4c6LKOp6cnt99%2BO%2BvXr3cmZAkJCdx5550AfPDBB/Tt25dmzZoBcMcdd3DLLbewatUqhg0bRkJCArNmzcLPzw8/Pz8GDx7Md999V9rddYqPj2fevHkubSNHxtC3b%2Bxlb6sstGplSVjYts2auO%2B%2Ba0nYCpZELXTDDRYFrlDFmrhmJwlFqli0vwDBwdbErVfPmrgtW1oS1sLfMJUrW/WPbBElZIaVOiGLjIykSZMmREVFERkZSYcOHejdu3ep1q1cuTK%2B571D6tatG8uXLycmJoZff/2VlJQUunXrVmy9bt26MW7cOPLz88nOzubrr79m1KhRAOzfv5%2BvvvqKxYsXO/s7HA4aNGhARkYG2dnZ1KpVy7msbt26pd1VF/379y92iHXXrkA2bryizV0xX9/CZGzrVsjKMjc2QMcnTH5CbdSoMBkbNAiSk82NDZzZZH4CarMVJmNnz4LDYXp4KpzLMDegu3thMpaZCfn55sYGsNvNj%2BnlVZiMHToEOTnmxz9wwNx4vr6Fydi2bZY8cWWE3WJ6THd3NypXvoHMzLPk5xeYGrtKlYqmxpOyVeqEzMvLizfeeIPk5GTWr1/PsmXLWLhwIePGjSvWN/%2BCJ1ePCw6FdO3alWeffZZDhw7x2Wef0aZNGwICAoptp23bthQUFPD9999z7NgxatasSZMmTQDw8fFhzJgxPPjgg8XWO3r0aLFxOK7wFS4oKIigINfDkz//DCdPXtHmDMvKsij29u0WBKUwGbMgdoG5z6PAH28wHQ5r4pOXZ0FQCpMxK2Kfd36q6XJyrImfmWl%2BTCh84rIgdl6eFf9IhfLzCyyNbzpVyAwr9Qzm5uaSlZVFo0aNGDlyJB9//DE2m429e/cCcPbsWWffA3/yLqxq1aq0atWKjRs3sm7dOrp3715iv6LDlp9//nmxfnXq1GH37t0u/Q8fPozD4SAgIABPT0%2BOHDniXPbzzz%2BXdldFRERETFXqhGzhwoU88sgj/Pbbb0DhjVZPnjxJ27ZtcXd3Z%2B3ateTl5fHRRx%2B5JEIXc%2Bedd7J69Wp27dpFly5dLtlv06ZNbNq0ySUh69%2B/P2vWrGHjxo3k5eXxzTff0LNnT3bs2IGnpyf6fHE%2BAAAgAElEQVRt2rRhyZIlnDp1ikOHDrFs2bLS7qqIiIhcDt0Y1rBS7/EDDzxASEgId999Ny1atCAuLo6xY8fSvHlzxo4dy%2BzZs2nTpg27du26aMXrfHfccQc//PAD7dq1w8/P76L9IiMjSUtLo0aNGjRs2NDZ3q5dO8aNG8e0adNo2bIl06ZNY8qUKbRo0QLAefVnhw4deOSRRxg6dGhpd1VEREQuhxIyw2yOKz256jq2YoX5Mf38oGNH2LjRmnPIet9tMzdgeHjhicAtW1pyDlnWKfP/LdzcoEIFOHPGmnPIfM%2Bm/3mnsuThUXilY0aGNeeQHTtmfkwfn8IrHVNTrTmHLCXF3HiVK0OHDvDFF5acQ5YecZvpMT083KhSpSIZGadNP4csMLCSqfFcPPlk2W9z%2BvSy32Y5pjv1i4iIiDHXYUWrrGkGRURERCymCpmIiIgYowqZYUrIRERExBglZIZpBkVEROQv59ChQwwbNoyIiAg6derE9OnTKSjhiqm5c%2BfSuHFjwsLCXL6O/X7hz7lz55g8eTIdOnQgIiKCUaNGkZFR9p9sooRMREREjCmHt72IjY2levXqJCQksGjRIhISElw%2BbvF8vXv3JjEx0eWrWrVqAMyaNYukpCTi4%2BNZu3YtDoeDp556yvD4LqSETERERP5SEhMTSU5OZuzYsVSqVIm6desSHR1NfHz8ZW0nLy%2BPDz74gBEjRlCzZk38/f2Ji4tj48aNzo9pLCs6h0xERESMuQrnkKWlpZGe7np/xMDAwGKfL12SpKQkgoODXW4837RpU1JTU8nKysLX19el/%2B7duxkwYAB79uyhZs2aPPXUU7Rv3579%2B/dz6tQpmjZt6uxbv359fHx8SEpKonr16gb38g9KyERERMSYq5CQxcfHM2/ePJe2mJgYYmNj/3Rdu91O5cqVXdqKkrOMjAyXhKxGjRrUrl2bMWPGEBQURHx8PMOHD2flypXY7XaAYtuqXLlymZ9HpoRMREREyp3%2B/fsTFRXl0hYYGFjq9Uv7QUT9%2BvWjX79%2Bzp%2Bjo6NZvXo1K1eupEOHDpe1LSOUkImIiIgxV6FCFhQUVKrDkyUJCAhwVreK2O12bDYbAQEBf7p%2BcHAwaWlpzr52u52KFSs6l588eZKqVate0dguRif1i4iIyF9Ks2bNOHLkCCdOnHC2JSYm0qBBA5fECuC1115j8%2BbNLm0pKSnUrl2b2rVr4%2BfnR1JSknPZnj17yMnJoVmzZmU6ZiVkIiIiYkw5u%2B1FkyZNCAsL4%2BWXXyYrK4uUlBQWLVrEwIEDAejWrRtbt24FCqtfU6dOZd%2B%2BfZw7d46FCxeyf/9%2B7rnnHtzd3bn33nt54403OHLkCBkZGcycOZMuXbo4b4tRVnTIUkRERIwph3fqnzNnDpMmTaJdu3b4%2BvoyYMAABg0aBEBqaipnzpwBYMyYMUDhuWN2u50GDRrw9ttvU6NGDQBGjRrF6dOn6d27N3l5eXTq1IkpU6aU%2BXiVkImIiMhfTo0aNViwYEGJy3bv3u383tvbmwkTJjBhwoQS%2B3p5efH000/z9NNPX5VxFlFCJiIiIsaUwwrZtUYzKCIiImIxVchERETEGFXIDFNCJiIiIsYoITNMMygiIiJiMVXIRERExBhVyAzTDIqIiIhYTBWyK5CWZn7M/PzCxxMn4Phx8%2BNnnbr6H6x6Pjc3qACc2bSNggJTQwPgW8lmftDwcNi2jQrtW8L27aaHT91n7u/YywuCq8ChM1XIyTE1NABpmaX/kOKyUiEPwoDErHr8fk9KU0UEZ5sb8IYbCh%2BDgqBSJXNjA4H%2BPqbHLFLF18v8oLm54OlpflxQhawMKCETERERY5SQGaYZFBEREbGYKmQiIiJijCpkhikhExEREWOUkBmmGRQRERGxmCpkIiIiYowqZIZpBkVEREQspgqZiIiIGKMKmWFKyERERMQYJWSGaQZFRERELKYKmYiIiBijCplhmkERERERi6lCJiIiIsaoQmaYEjIRERExRgmZYZpBEREREYupQiYiIiLGqEJmmGZQRERExGKqkImIiIgxqpAZpoRMREREjFFCZphmUERERMRiqpCJiIiIMaqQGaYZFBEREbGYKmQiIiJijCpkhikhExEREWOUkBmmGRQRERGxmCpkIiIiYowqZIZpBkVEREQspgqZiIiIGKMKmWHlPiFLTEzkhRdeYM%2BePXh5edGlSxcmTpyIp6cn77//PrNmzSInJ4f%2B/ftjt9vJz8/nxRdfBGDp0qUsW7aMw4cPU6tWLUaPHk3nzp0t3iMREZG/GCVkhpX7hGz06NH06tWLd955h6NHjzJgwAAaNGhAy5YtmTRpEq%2B88godO3ZkwYIFvP/%2B%2B0RFRQHw2WefMW/ePN58800aNWrEhg0biIuL47PPPuPGG28sdfy0tDTS09MvaA2katWgMtzLP%2Bfv7/poNrP/12y2Px4t%2BT8PDzc/ZqNGro8m8/IyN56np%2Buj2SpUMD%2Bmj4/ro%2Bk8bzA3nre366OIXJTN4XA4rB7EpWRlZeHl5YXX768WY8aMwcPDg7p16/L//t//Y8WKFQDk5%2BcTFRVFZGQkL774IsOGDSMkJISxY8c6tzV06FDatWvHsGHDSh1/7ty5zJs3z6Vt5MgYRo2KLYO9ExERKSO5uda9w/n007Lf5p13lv02y7FyXyH75ptvePXVV/nll1/Iy8sjLy%2BPbt26kZ6eTnBwsLOfu7s7TZo0cf68f/9%2BvvrqKxYvXuxsczgcNGjQ4LLi9%2B/f31l1K/L114H8979XuENXyN8foqJgwwaw282NDdCtm7nxbDa44QY4exaseMtQoX1L84M2agTvvguDBkFysunhD32yzdR4np4QFARpaYWvI2Y7ccL8mD4%2B0LAh7N0L2dnmxw/zNPnvytsb6tWD1FQ4d87c2AD165sfEwr/uK34o5ZrWrlOyFJSUnj88ccZN24c9957Lz4%2BPjz55JPk5eVRUFCAh4fr8N3OO7bl4%2BPDmDFjePDBBw2NISgoiKAg18OTX38Nx48b2uwVs9utiV1QYG68ol%2Blw2F%2BbAC2b7cg6O%2BSky2Jn5Njekig8HXLithnzpgfs0h2tkXxvc5aEJTCZOysRbHFHDqHzLByPYO7du3Cy8uL%2B%2B%2B/Hx8fHxwOB7t27QKgatWqHD582Nk3Pz%2Bfn376yflznTp12L17t8v2Dh8%2BTDk/QisiInLtcXMr%2B6/rTLne4%2BDgYLKzs9m1axcnT55k%2BvTpeHl5kZaWRkREBDt37mTjxo3k5OTw%2Buuvk33eMYD%2B/fuzZs0aNm7cSF5eHt988w09e/Zkx44dFu6RiIiImOHQoUMMGzaMiIgIOnXqxPTp0ym4yCGX9957j65duxIeHk7v3r1JSEhwLhs/fjxNmjQhLCzM%2BdWqVasyH2%2B5PmQZHh7Offfdx%2BDBg7nhhht47LHHmDBhAo899hjvvvsucXFxjB07Fk9PT4YOHUpERAS23y/Pa9euHePGjWPatGkcO3aMWrVqMWXKFFq0aGHxXomIiPzFlMOKVmxsLE2bNiUhIYHjx4/z6KOPUq1aNR544AGXfmvXruXll1/m3//%2BNzfffDMff/wxcXFxfPrpp9SuXRuAxx57jNjYq3sxX7lOyAAmTpzIxIkTXdq2bNkCQE5ODsOHD3e2Dx482CVrHTx4MIMHDzZnoCIiIlIuJCYmkpyczKJFi6hUqRKVKlUiOjqaxYsXF0vIsrOzeeKJJ7jlllsA6NevHzNmzOCHH35wJmRmKPcJ2cUcOHCAbt26MXfuXDp27MjXX3/N9u3beeKJJ6wemoiIyPXlKlTISroPaGBgYLEL7UqSlJREcHAwfn5%2BzramTZuSmppKVlYWvr6%2BzvbevXu7rJuZmcnp06epXr26s%2B2bb75h/fr1/Prrr9SvX58pU6bQrFmzK921El2zCVnt2rV58cUXmT59Ok888QTVq1fn6aefpmVLC25XICIicj27CglZfHx8sfuAxsTElOrQod1up3Llyi5tRclZRkaGS0J2PofDwcSJE2nevDl///vfgcJ8w83Njccff5yKFSsyb948HnzwQdauXUuVKlWuZNdKdM0mZAB33XUXd911l9XDEBERkTJW0n1AAwMDS73%2B5d5VITc3l/Hjx/Pzzz%2BzZMkSZ/vIkSNd%2Bj355JOsWrWKhIQE%2BvXrd1kxLuWaTshERESkHLgKFbKS7gNaWgEBAdgvuIu63W7HZrMREBBQrH92djYjRozg7NmzLFu27JKVL3d3d2rWrElaWtoVje1iyt9lESIiIiIGNGvWjCNHjnDivI/kSExMpEGDBlSsWNGlr8PhYPTo0Xh4ePD222%2B7JGMOh4MXXniB5PM%2BPSUnJ4f9%2B/eX%2BQn/SshERETEmHJ2Y9ii%2B4a9/PLLZGVlkZKSwqJFixg4cCAA3bp1Y%2BvWrQB88skn/Pzzz7zyyit4e3u7bMdms3Hw4EGmTp3K0aNHOX36NDNmzMDT05POnTsbGuOFdMhSREREjCmH9yGbM2cOkyZNol27dvj6%2BjJgwAAGDRoEQGpqKmd%2B//yyDz/8kEOHDjlP4i/Su3dvnn32WZ577jleeukl%2BvTpQ1ZWFjfffDOLFy%2BmQoUKZTpeJWQiIiLyl1OjRg0WLFhQ4rLzP1px8eLFl9yOv78/L7zwQpmOrSRKyERERMSYclghu9YoIRMRERFjlJAZphkUERERsZgqZCIiImKMKmSGaQZFRERELKYKmYiIiBijCplhSshERETEGCVkhmkGRURERCymCpmIiIgYowqZYZpBEREREYupQiYiIiLGqEJmmBIyERERMUYJmWGaQRERERGLqUImIiIixqhCZphmUERERMRiqpBdgb59zY/p7l742Lkz5OebH9/3bLq5AT08oEIVKpzLgLw8c2MDqfscpsf08oJg4NAn28jJMT089W6ymRswPBy2bSP4rpawfbu5sYF6M2eaHpOgIAi7j7Afl0FamvnxCwrMjRcUBI0awZYtluzvqQbhpsd0c4OKnnA6x9P06Qao5Gl%2BTEAVsjKghExERESMUUJmmGZQRERExGKqkImIiIgxqpAZphkUERERsZgqZCIiImKMKmSGKSETERERY5SQGaYZFBEREbGYKmQiIiJijCpkhmkGRURERCymCpmIiIgYowqZYUrIRERExBglZIZpBkVEREQspgqZiIiIGKMKmWGaQRERERGLqUImIiIixqhCZpgSMhERETFGCZlhmkERERERi6lCJiIiIsaoQmaYZlBERETEYqqQiYiIiDGqkBmmhExERESMUUJmmGZQRERExGKqkImIiIgxqpAZphkUERERsZgqZCIiImKMKmSGlbsZ3LJlC2FhYeTk5HDw4EFCQ0NJSUmxelgiIiJyMW5uZf91nSl3e9y6dWsSExPx8vKyeigiIiJyjTp06BDDhg0jIiKCTp06MX36dAoKCkrsu2TJErp27UrLli0ZOHAgO3fudC47d%2B4ckydPpkOHDkRERDBq1CgyMjLKfLzlLiETERGRa0w5rJDFxsZSvXp1EhISWLRoEQkJCSxevLhYvw0bNjB37lz%2B9a9/8fXXX9OpUyeGDx/OmTNnAJg1axZJSUnEx8ezdu1aHA4HTz31lOHxXcjShGz%2B/Pl06tSJ5s2b07VrV1asWMG3335LaGgo586dc/ZLTEykZ8%2BehIeHM3ToUI4ePQrA2bNnGTduHJGRkYSHhzNgwABnVjt37lyio6N57bXXiIiI4JZbbuGVV16xZD9FRETEPImJiSQnJzN27FgqVapE3bp1iY6OJj4%2Bvljf%2BPh4%2BvTpQ/PmzfHx8eHhhx8G4PPPPycvL48PPviAESNGULNmTfz9/YmLi2Pjxo3OXKSsWHZS/7Zt21iyZAnLly%2BnZs2afPXVV8TGxvLcc88V67t8%2BXLmz59PxYoVGTlyJJMmTWL%2B/PksXryYY8eOsW7dOry8vFiwYAGTJk3io48%2BAmDHjh2Eh4fz5ZdfkpiYyEMPPUTTpk3p3LlzqceZlpZGenq6S5uPTyCBgUHGJuAyFb1ZsO6wusl/Ku7uro8ms%2BKIuaen66PpwsPNjdeokeuj2YLM/R8GoEoV10ezXeRwzVUTEOD6aDIrni%2BtfK42%2B9fr4irscEmvv4GBgQSV4n83KSmJ4OBg/Pz8nG1NmzYlNTWVrKwsfH19Xfp2797d%2BbObmxuNGzcmMTGRxo0bc%2BrUKZo2bepcXr9%2BfXx8fEhKSqJ69epGdtGFZQnZqVOncHNzw8fHB5vNRvv27fn%2B%2B%2B/ZsmVLsb733XcfN954IwDR0dHExcWRl5dHZmYmnp6e%2BPj44OHhwYgRIxgxYoRzPTc3N0aOHImHhwe33HIL7du3Z%2BPGjZeVkMXHxzNv3jyXtpEjYxg1KvYK99yY8/6GTGbRC0jlypaEDbZod8GaPAGAbdusifvuu9bEtdJ5T/7XhR49LAlb0ZKohW64wfyYp06ZH7OIA1uZb7Ok19%2BYmBhiY//89ddut1P5gtePouQsIyPDJSGz2%2B0uiVtR34yMDOx2O0CxbVWuXLnMzyOzLCGLjIykSZMmREVFERkZSYcOHejdu3eJfevXr%2B/8vk6dOuTm5nL8%2BHEGDRrEQw89xG233catt95K586duf322136enj8sYs33ngjv/zyy2WNs3///kRFRbm0%2BfgEkpl5WZsxzM2tMBnLyrLmXVDl/LI/gfGS3N0Lk7HMTMjPNzc2cOiM%2BRmZp2dhMpaWBrm5pocn%2BK6W5gZs1KgwGRs0CJKTzY0NMGaM%2BTGrVClMxtasgatwUvCfsqJC1qMHrF4NJ06YGxs43WeI6THd3AqTsbNnLa5Y/QWU9PobGBhY6vUdDkeZ9b2cbV0pyxIyLy8v3njjDZKTk1m/fj3Lli1j4cKFjBs3rlhft/NKoUWT4u3tTfXq1VmzZg3ffvstGzZsYPLkyaxcuZI5c%2BYAkH/BC7nD4cBmu7wsPigoqFh5NCPDkhwBKPwHtyR2Xp4FQSncWQti5%2BSYHtIpN9ei%2BNu3WxCUwmTMithpaebHLJKRYU18qzKEEycs2V8rE6KCgusrIbsa%2B1rS629pBQQEOKtbRex2OzabjYALDqFXqVKlxL4NGzZ09rXb7VSs%2BEfN9eTJk1StWvWKxnYxlp2RlJubS1ZWFo0aNWLkyJF8/PHH2Gy2YkkUQGpqqvP7AwcO4OPjg7%2B/P6dPnyY/P5%2B2bdsyceJE3n//fdauXessIx45coS8817MDx8%2BXKbHe0VEROSPBLQsv4xo1qwZR44c4cR5ldnExEQaNGjgklgV9U1KSnL%2BnJ%2Bfz08//UTz5s2pXbs2fn5%2BLsv37NlDTk4OzZo1MzbIC1iWkC1cuJBHHnmE3377DYCUlBROnjzJkSNHivVdtmwZ6enpnDp1isWLFzvPARs1ahQvvfQSWVlZFBQUsH37dvz9/Z3HgvPy8njzzTfJyclh69atfPXVV8XKnyIiIvLX0qRJE8LCwnj55ZfJysoiJSWFRYsWMXDgQAC6devG1q1bARg4cCAff/wxP/zwA2fPnuX111/Hy8uLjh074u7uzr333ssbb7zBkSNHyMjIYObMmXTp0oVq1aqV6ZgtO2T5wAMPcPjwYe6%2B%2B26ys7OpWbMmY8eOpU6dOsX6DhgwgKFDh3LkyBFatmzJhAkTAHjmmWecN2uz2Ww0bNiQV1991XmIs2HDhuTl5XHrrbeSl5fHQw89RMeOHc3cTRERkb%2B88nh4ds6cOUyaNIl27drh6%2BvLgAEDGDRoEFB45K3oPmMdOnTgiSeeIC4ujuPHjxMWFsb8%2BfPx8fEBCos/p0%2Bfpnfv3uTl5dGpUyemTJlS5uO1Ocw4U80Cc%2BfO5csvv2T58uVlvm0rzsW1%2BBx3quSl/3mnsuThUXgCdEaGJeeQpWaV/sTRsuLlBcHBcOiQNeeQ1bup7K%2BSuqTw8MIrO1u2tOYcspkzzY8ZFAT33QfLll0f55AFBcGQIfDOO5bs76lh5l%2B44eYGFSvC6dPWJCmVKpkfE%2BC8W4eWGW/vst9meaYPFxcRERFDymOF7FqjhExEREQMUUJm3F/2syxjY2OvyuFKERERkbKmCpmIiIgYogqZcX/ZCpmIiIjItUIVMhERETFEFTLjlJCJiIiIIUrIjNMhSxERERGLqUImIiIihqhCZpwqZCIiIiIWU4VMREREDFGFzDglZCIiImKIEjLjdMhSRERExGKqkImIiIghqpAZpwqZiIiIiMVUIRMRERFDVCEzTgmZiIiIGKKEzDgdshQRERGxmCpkIiIiYogqZMapQiYiIiJiMVXIRERExBBVyIxTQiYiIiKGKCEzTgnZFahQwfyYNlvho48POBzmx2ffMXPj%2BfhAlSpgt0N2trmxgbTMQNNjVqgAwcFw4gScOWN6eOrNnGluwKCgwscxYyAtzdzYAE88YX7M8HC47z54%2BWXYvt38%2BG%2B8YW68oifLChXA19fc2EAlXyueLAFsVKxgXWy5NikhExEREUNUITNOJ/WLiIiIWEwVMhERETFEFTLjlJCJiIiIIUrIjNMhSxERERGLqUImIiIihqhCZpwqZCIiIiIWU4VMREREDFGFzDglZCIiImKIEjLjdMhSRERExGKqkImIiIghqpAZpwqZiIiIiMVUIRMRERFDVCEzTgmZiIiIGKKEzDgdshQRERGxmCpkIiIiYogqZMYpIRMRERFDlJAZp0OWIiIiIhZThUxEREQMUYXMOFXIRERE5Lpit9uJi4ujbdu2tG/fnn/%2B859kZ2dftP9nn31Gr169CA8Pp2vXrixfvty5bO7cuTRu3JiwsDCXr2PHjl3WmFQhExEREUOutQrZpEmTyMnJYdWqVeTm5vL4448zY8YMJk6cWKzvjz/%2ByNixY5k5cyYdO3bkq6%2B%2BYuTIkdx00020atUKgN69e/Piiy8aGpMqZCIiImJIQUHZf10tx44dIyEhgdGjRxMQEED16tUZMWIEH374Ibm5ucX62%2B12Hn30UTp37oyHhwe33XYbISEhbN26tUzHpQqZiIiIlDtpaWmkp6e7tAUGBhIUFGRou7t27cLd3Z3Q0FBnW9OmTTlz5gz79u1zaQfo0KEDHTp0cP6cl5dHeno61atXd7bt3r2bAQMGsGfPHmrWrMlTTz1F%2B/btL2tcSshERETEkKtR0YqPj2fevHkubTExMcTGxhrart1ux9fXF5vN5mzz8/MDICMj40/XnzFjBhUqVKB79%2B4A1KhRg9q1azNmzBiCgoKIj49n%2BPDhrFy5kptuuqnU41JCJiIiIuVO//79iYqKcmkLDAws1borVqzgH//4R4nLRo8ejcPhuOzxOBwOZsyYwapVq1iyZAne3t4A9OvXj379%2Bjn7RUdHs3r1alauXElcXFypt/%2BXTsiGDBlC8%2BbNGTt2rNVDERER%2Bcu6GhWyoKCgKz482bt3b3r37l3isq%2B%2B%2BoqsrCzy8/Nxd3cHCqtmAFWrVi1xnYKCAp566il%2B/PFH3nvvPWrXrn3J%2BMHBwaSlpV3WmHVSv4iIiBhyLZ3U37hxYxwOB8nJyc62xMREKleuTL169Upc5/nnn2fv3r0lJmOvvfYamzdvdmlLSUn506TtQkrIRERE5LoREBBA165dmT17NidOnOC3337j1VdfpW/fvnh4FB44HDp0KGvWrAHg%2B%2B%2B/Z%2BXKlcyfPx9/f/9i27Pb7UydOpV9%2B/Zx7tw5Fi5cyP79%2B7nnnnsua1ymJ2QHDx4kNDSUtWvX0qNHD26%2B%2BWYGDx7svJJi5cqVdO/enfDwcKKionj33Xed686dO5dHH32UuLg4WrZsCcDZs2eZNGkSERERtGnTxnlvkSL5%2BflMnjyZli1bEhkZ6ZxgERERKRvXUoUMYNq0aVSqVInbb7%2BdXr16cfPNNzN69Gjn8gMHDnDy5EkAPvzwQ06dOkWnTp1cbvz64IMPAjBmzBg6dOhAdHQ0rVu3ZtWqVbz99tvUqFHjssZk2TlkS5cuZeHChfj4%2BBATE8OUKVMYP34848aN46233iIyMpJvvvmGBx98kJYtW9KoUSMAfvjhBx5//HFefvllAGbOnMnPP//Mp59%2BCsDDDz/Mq6%2B%2B6pzYVatW8fzzzzNx4kTmzZvHlClTuOOOO5xZ8J8p6bJbPz/jl91erqKLQc67KMRcPj7mxvPycn00WYU882MWTbHZU%2B1k8t80Vaq4PpotPNz8mL8/jzkfzVatmrnxiqoJJVQVRKxUqVIlZs6cedHlGzZscH7//PPP8/zzz1%2B0r7e3NxMmTGDChAmGxmRZQjZo0CDnPTyio6OJi4tjzpw5fPPNN87LTyMjI6latSpJSUnOhMzd3Z2BAwdis9lwOBx8/PHHPP/88wQEBACFE5eZmemM07JlS2699VYAunXrxr///W9OnDhR6oSqpMtuR46MYdQoY5fdXilPT0vCwkWOq191wcGWhA2zJGqhhg0tChx2nzVxf7903HT3WbS/AOdV/q8Lt99u9QjMZ8W75yu4crCsXGt36i%2BPLEvIzj9xLjg4mJycHE6ePMny5cv54IMPSEtLw%2BFwkJOT43IIskaNGs57h2RkZJCZmUmtWrWcyxtd8M7z/GVFl6iev70/U9Jlt35%2BgVzGJsqEzVaYjOXmWvM/53Uo1eSAXoXJ2KFDmD7ZQGKW%2BQmoj09hMrZ3L1ziI9WumrAfl5kbsEqVwmRszRooxb1/ytzvVXZTNWpUmIwNGgTnnVBsmn/%2B09x4/v6Fydj69fD7VWym6tPH/JhQ%2BIRtYXJkBSVkxlmWkBWc99sruh/IBx98wPz583nttddo3bo17u7u3HbbbS7rnX%2Bo0c3Nrdi2LmQz%2BC6lpMtuz52z7n/N4bAothUZAhQmYxbEPnPG9JBO2dkWxb/MS7TLTEaGNbG3bzc/ZpHkZGviX%2BaHHZcZu9262CLXCMuusty/f7/z%2B0OHDuHj48PBgwdp1aoVbdq0wd3dnfT09Evex8Pf35/KlSuTmvpH9SYpKYkVK1Zc1bGLiKGngAAAACAASURBVIjIH661k/rLI8sSsvfee49jx45ht9tZvHgxt912G8HBwezbt4%2BTJ09y6NAhnn32WW688UaOHj160e306dOHN998k6NHj5KRkcEzzzzD3r17TdwTEREREWMsO2TZq1cvhg4dyv79%2B2nRogVPP/00np6efPfdd87kbMqUKezcuZPZs2df9OMSxowZw7PPPkv37t3x8vKic%2BfOxMTEmLw3IiIi16/rsaJV1ixLyFq0aMHq1auLtb/11lsuP7du3ZoHHnjA%2BfOFHyrq5eXFtGnTmDZtWrFtvfPOOy4/169fn927dxsZtoiIiFxACZlxulO/iIiIiMX%2B0h8uLiIiIlefKmTGmZ6Q1apVS4cNRURERM6jCpmIiIgYogqZcUrIRERExBAlZMbppH4RERERi6lCJiIiIoaoQmacKmQiIiIiFlOFTERERAxRhcw4JWQiIiJiiBIy43TIUkRERMRiqpCJiIiIIaqQGacKmYiIiIjFVCETERERQ1QhM04JmYiIiBiihMw4HbIUERERsZgqZCIiImKIKmTGqUImIiIiYjFVyERERMQQVciMU0ImIiIihighM06HLEVEREQspgqZiIiIGKIKmXFKyERERMQQJWTGKSG7AvfcY37M%2BvVh7lwYMwZSUsyPv2aEyUErV4Z69eDAAcjMNDc2EBGcbXpMPG8AGhHmmQxeZ82Pb/YzalG8ggJrns3feMP8mNWqFT7%2B859w7Nj/b%2B/e46Kq1v%2BBfwYEBkFUPKBy8fo1MK8Qat4VNEhQ0lBQIzWtr6GI0NGOmYW3yrspmsf8mnhCD15ITC0D08PBNEUx8H5ET4ICgwYiyHWY3x8c5%2Bcc1KA9M2uY%2BbxfL14wa%2B/Z69kybJ951pq19d//zJn67c/DA3j9dWD5ciA9Xb99A8Cbb%2Bq/T5kMkMuBigpApdJ//9bW%2Bu%2BTtIIJGREREUnCCpl0nNRPREREJBgrZERERCQJK2TSMSEjIiIiSZiQScchSyIiIiLBWCEjIiIiSVghk44VMiIiIiLBWCEjIiIiSVghk44JGREREUnChEw6DlkSERERCcaEjIiIiCR5fAc0bX7pUlFREebOnYsBAwZg0KBBWLhwIcrLn37LvISEBLi7u6NHjx4aXxkZGf859xqsW7cOPj4%2B6NOnD6ZPn47s7OwGx8SEjIiIiEzKokWLUFZWhkOHDmH//v3IysrC6tWrn7l/nz59kJmZqfHVs2dPAEBcXBy%2B/fZbbN26FcePH0eHDh0wa9YsqBp4L1MmZERERCRJY6qQ3bt3D8nJyYiMjIS9vT1at26NsLAw7N%2B/H1VVVQ0%2BXnx8PKZOnYrOnTvD1tYWkZGRyMrKwi%2B//NKg4zAhIyIiIkkaU0J25coVmJubw83NTd3WrVs3PHr0CDdv3nzqc3JzczFt2jT06dMHPj4%2BSExMBACUl5fjxo0bePHFF9X72traon379sjMzGxQXPyUJRERERkchUKBgoICjTYHBwc4OjpKOm5RURFsbW0hk8nUbc2bNwcAFBYW1tnf3t4eHTp0QFRUFP7nf/4HSUlJmD9/PhwdHdGpUyeoVCr185883tOO9TxMyIiIiEgSXVS04uPjERMTo9E2e/ZshIeH/%2B5zExMTMX/%2B/Kdui4yMbND8rmHDhmHYsGHqx/7%2B/khKSkJCQgL%2B/Oc/A0CD54s9DRMyIiIiMjjBwcHw9vbWaHNwcKjXcwMDAxEYGPjUbSdPnkRJSQmUSiXMzc0B1FbNAKBVq1b1Or6zszMuXryIFi1awMzMTP38x4qKiup9rMeYkBEREZEkuqiQOTo6Sh6efJquXbtCpVLh6tWr6NatGwAgMzMTdnZ26NixY539d%2B/ejebNm2PUqFHqtqysLLi6usLKygpdunTBpUuX0LdvXwBAcXExbt%2B%2Brf4UZn1xUj8RERFJ0pgm9dvb28PX1xfr16/Hb7/9hry8PGzatAlBQUFo0qS2TjVlyhQcOXIEAFBZWYmlS5ciMzMTVVVVOHToEFJSUhASEgIAmDhxInbu3ImsrCyUlJRg9erV6Nq1K3r06NGguFghIyIiIpOyZMkSfPzxx/Dx8YGFhQUCAgIQGRmp3p6dnY0HDx4AAN58802UlpYiIiICBQUFcHFxwaZNm9C9e3cAQEhICAoKChAaGorS0lL069evzty3%2BmBCRkRERJI0tntZNmvWDGvXrn3m9h9//FH9s0wmQ1hYGMLCwp66r0wmw5w5czBnzhxJMXHIkoiIiEgwVsiIiIhIksZWITNETMiIiIhIEiZk0nHIkoiIiEgwg07ITp8%2BjSFDhmis/UFERESGpTEte2GoDDohi42NRe/evXHo0CHRoRARERHpjEHPISspKUGvXr1gZmbQeSMREZFJM8WKlrYZbKbzxhtv4OzZs9i%2BfTt8fX2RmpqKcePGwcPDA4MHD8aGDRvU%2ByYkJCAgIACfffYZevfujfz8fNTU1GDDhg0YMWIEevXqhddffx3nzp0TeEZERETGiUOW0hlshezrr79GaGgoevXqhbCwMAwcOBAffPABgoKCcP36dYSEhKB79%2B7qG48qFApYWVnh7NmzsLCwwFdffYXDhw9j27ZtcHJyQnx8PN59912cOHECTZs2rXccCoUCBQUFGm329g5o2VL799d6HhcXze96Z2en3/5sbTW/65u1tf77tLLS/K5vOrhn3HPZ22t%2B17cGXAe0pkULze/65uGh3/7c3TW/65tMJq5PEX2rVPrvk7TGYBOyJzVt2hQpKSmwsbGBTCaDm5sb3NzccPHiRXVC9vDhQ7z99tuwsLAAAOzbtw9Tp05Fhw4dAAChoaGIjY3FiRMnGvQhgfj4%2BDq3QAgLm42IiHDtnFwDvf%2B%2BkG4BDBHTraenmH5FesrNbfVC1H%2Ba/v5i%2BhXJx0dMv6%2B/LqbfXbvE9CuSiDdWZWX67/M/TLGipW2NIiEDgO%2B%2B%2Bw47duzAnTt3UFNTg6qqKnh5eam329nZwfaJasrt27exfPlyfPLJJ%2Bq2mpoa5ObmNqjf4OBgddL32KefOiBcz/mYi0ttMrZiBZCTo9%2B%2BAWDj%2BBT9dmhrW5uMnT8PlJTot29A/9UioPYC3rEjcOsWUFGh//7PntVvf/b2tcnY4cPAb7/pt29AXIXMxwc4dgwoKtJ//8uX67c/d/faZGzSJODqVf32DQA//aT/PmWy2r/ligpWrKhBGkVCdurUKURHR2P16tUYOXIkLCwsMGnSJI19Ht%2Bh/TG5XI5ly5bB19dXUt%2BOjo5w/K//nH/7Tcz/H0BtMpaVJaDj4mIBnaI2GRPRd7Nm%2Bu/zsYoKMe90FQr99wnU/jGJ6FvUcDhQm4zdu6f/ftPT9d8nUJuMiehbZEKkUplUQsYKmXQGO6n/SRkZGejYsSNGjRoFCwsLVFRUIOt3shJXV1dcu3ZNoy1HRGmJiIjIyHFSv3SNIiFzdnZGXl4ecnNzce/ePURHR8PR0RH5%2BfnPfE5ISAji4uJw4cIFKJVKHDlyBAEBAbh7964eIyciIiL6fY1iyNLX1xfHjh3DqFGjYG9vj/nz52Pw4MFYuHAhVq1ahc6dO9d5TlBQEHJzczF79myUlJSgU6dOiImJgZOTk4AzICIiMl6mWNHSNoNOyP72t7%2Bpf163bl2d7U9%2BWnLcuHEa28zMzBAREYGIiAjdBUhERESkBQadkBEREZHhY4VMOiZkREREJAkTMukaxaR%2BIiIiImPGChkRERFJwgqZdKyQEREREQnGChkRERFJwgqZdEzIiIiISBImZNJxyJKIiIhIMFbIiIiISBJWyKRjQkZERESSMCGTjkOWRERERIKxQkZERESSsEImHStkRERERIKxQkZERESSsEImHRMyIiIikoQJmXQcsiQiIiISjBUyIiIikoQVMulYISMiIiISjBUyIiIikoQVMumYkBEREZEkTMik45AlERERkWCskBEREZEkrJBJJ1OpVCrRQTQ2ZWX671MmA%2BRyoLwcEPEbKyl5qNf%2BmjQxQ8uWNigsLEV1tf7/0h1ayPXeJwDAwgKoqhLS9cNyC732Z2YG2NgApaViLubNbAVd%2BmQyMX/EQO0FRJ9EX7iaNtV/nx4ewPnzgKcnkJ6u//4Fvbb8/bV/zMOHtX9MQ8YKGREREUnCCpl0TMiIiIhIEiZk0nFSPxEREZFgrJARERGRJI2tQlZUVITo6GicOXMGZmZmGDp0KBYtWgS5vO784Q8//BCJiYkabUqlEoGBgfj000/xl7/8BQcPHoS5ubl6u5WVFdLS0hoUEytkREREZFIWLVqEsrIyHDp0CPv370dWVhZWr1791H2XLVuGzMxM9Vd6ejo6deoEPz8/9T7vvvuuxj4NTcYAJmREREQkUU2N9r905d69e0hOTkZkZCTs7e3RunVrhIWFYf/%2B/aiqx6fcY2Nj4eTkhKFDh2o1Lg5ZEhERkSS6SKAUCgUKCgo02hwcHODo6CjpuFeuXIG5uTnc3NzUbd26dcOjR49w8%2BZNjfb/VlxcjC1btmDXrl0a7adPn8axY8fw66%2B/onPnzoiOjkb37t0bFBcTMiIiIjI48fHxiImJ0WibPXs2wsPDJR23qKgItra2kMlk6rbmzZsDAAoLC5/73K%2B//hp9%2BvRBly5d1G2urq4wMzNDREQEbGxsEBMTg7feegtHjx5Fy5Yt6x0XEzIiIiKSRBcVsuDgYHh7e2u0OTg41Ou5iYmJmD9//lO3RUZG4o%2Bsia9UKhEXF4c1a9ZotM%2BaNUvj8bx583Do0CEkJydj/Pjx9T4%2BEzIiIiIyOI6Ojn94eDIwMBCBgYFP3Xby5EmUlJRAqVSqPxlZVFQEAGjVqtUzj3n27FlUVlbCy8vruX2bm5ujbdu2UCgUDYqZk/qJiIhIksY0qb9r165QqVS4evWqui0zMxN2dnbo2LHjM5937NgxvPzyy2jS5P/XslQqFT799FONY1VWVuL27dtwdXVtUFxMyIiIiEiSxpSQ2dvbw9fXF%2BvXr8dvv/2GvLw8bNq0CUFBQepka8qUKThy5IjG865cuQIXFxeNNplMhpycHCxevBj5%2BfkoLS3F6tWrYWFhgREjRjQoLiZkREREZFKWLFmCZs2awcfHB2PGjEHPnj0RGRmp3p6dnY0HDx5oPKegoAB/%2BtOf6hxr%2BfLl6NChA8aNG4cBAwbgypUriI2NRdMG3txepvojM9tMXFmZ/vuUyQC5HCgvB0T8xkpKHuq1vyZNzNCypQ0KC0tRXa3/JaAdWtRdrVkvLCyAeqyDowsPyy302p%2BZGWBjA5SWilnlu5mtoEufTCbmjxiovYDok%2BgLVwP/Q9QKDw/g/HnA0xNIT9d//4JeW/37a/%2BYp05p/5iGjBUyIiIiIsH4KUsiIiKSpLHdy9IQMSEjIiIiSZiQScchSyIiIiLBWCEjIiIiSVghk44VMiIiIiLBWCEjIiIiSVghk44JGREREUnChEw6DlkSERERCcYKGREREUnCCpl0rJARERERCWYQCVlOTg7c3NyQlZWlleMNHDgQCQkJWjkWERERPV9Njfa/TA2HLImIiEgSU0ygtM0gKmREREREpsygErLMzEwEBATAw8MDU6ZMQX5%2BPgAgLS0NEyZMgIeHBwYNGoR169ah5j/peHV1NZYuXYp%2B/fph8ODB2Lt3r/p4mzZtwrhx4zT6SEtLQ8%2BePVFSUqK/EyMiIjJiHLKUzqCGLPfs2YOtW7fCxsYGs2bNwqJFi/DJJ59g%2BvTpmD9/PsaPH48bN27g7bffhqOjIyZPnoz9%2B/fj%2B%2B%2B/x65du9C2bVusWLECDx48AAAEBgZi48aNyMrKQufOnQEAR48exfDhw2Fra1uvmBQKBQoKCjTa7Owc4ODgqN2T/x0ymeZ3fWvSRL%2B5u7m5mcZ30j0zPf9TP%2B5P3/2aNH1fQERfuDw89N%2Bnu7vmd31KT9d/n/9higmUthlUQjZ58mQ4OTkBAKZOnYq5c%2BciMTERTk5OmDx5MgDgxRdfRGBgIL777jtMnjwZSUlJGD16tDrhioiIQHx8PADAxcUFXl5e%2BPbbbzF37lwAQHJyMhYuXFjvmOLj4xETE6PRNmvWbMyZEy75fP8IKysh3UIutxHSr52dtZB%2BhbKwENKtjZhuYS3sVywoSQDEJShyuZh%2BRV24zp8X0y8A7Nql/z5Fva5IKwwqIXucVAFAu3btUFVVhVu3bmm0A0D79u3x3XffAQDy8/MxbNgw9TZ7e3s0b95c/TgwMBB//etfMXfuXGRmZqK0tBRDhgypd0zBwcHw9vbWaLOzc0B5eUPOTDqZrPaaVlEBqFT67RsAyspK9dqfubkZ7OysUVxcBqVS/2%2B9Wtpa6r1PALXJWFWVkK5LK/WbkZmZ1SZjZWVi3l3bNBXwhwTU/jGL%2BCMGai8g%2BiT6wjVggP77dHevTcYmTQKuXtV//4KwQiadQSVkZk%2BMXaj%2B88cre0bG/7i9srIS1dXVGttqnnhlvPrqq1i2bBkuXLiA48ePw8/PD5aW9f/P1tHREY6OmsOTZWXirqcqlZi%2Bq6vF/LUplTXC%2BjY1oi6opjpfRAhTu3AJHMLD1ati%2B6dGx6Bmb9y6dUv9c3Z2NuRyOdq3b4%2BbN29q7Hfz5k24uroCqE2Y8vLy1NsUCgWKi4vVj21tbeHj44Pvv/8e3333HcaMGaPjsyAiIjItnNQvnUElZHFxcSgoKMDDhw8RGxuLESNG4NVXX0V2djbi4%2BNRXV2NjIwMfPPNNxg7diwAYPDgwTh06BD%2B/e9/o6SkBOvWrYPVf81XCAwMxN69e1FVVYWXXnpJxKkREREZLSZk0hnUkGVISAimTJmC3NxceHp64oMPPkCrVq0QExODzz//HJ999hkcHR0RERGB1157DUDt5P/s7GxMmDABlpaWmDNnDs6dO6dx3EGDBsHa2hoBAQHPHAIlIiIiEkWmUomaVKA/JSUlGDp0KBISEtC%2BfXvJxysr00JQDSST1X5AqrxczFSMkpKHeu2vSRMztGxpg8LCUiFzyBxaCPo0msBJ/Q/L9T%2Bp38YGKC0V8264ma0JTuoX8WkkkReupk3136eHR%2B2nOz09xcwhE/Ta%2Bs8sIq3Kztb%2BMQ2ZQQ1Z6kJFRQWWLFmCQYMGaSUZIyIiItI2o07I0tLS0KdPH9y/fx8ff/yx6HCIiIiMEueQSWdQc8i0zcvLCxkZGaLDICIiMmqmmEBpm1FXyIiIiIgaA6OukBEREZHusUImHStkRERERIKxQkZERESSsEImHRMyIiIikoQJmXQcsiQiIiISjBUyIiIikoQVMulYISMiIiISjBUyIiIikoQVMumYkBEREZEkTMik45AlERERkWCskBEREZEkrJBJxwoZERERkWCskBEREZEkrJBJx4SMiIiIJGFCJh2HLImIiIgEY0JGREREktTUaP9L1zIzMzFy5EhMmDDhd/fduXMnfH194enpiYkTJ%2BLixYvqbRUVFfjoo48wZMgQ9OvXD3PmzEFhYWGD42FCRkRERCbl4MGDCA8PR/v27X933x9//BEbN27EypUr8dNPP2H48OGYOXMmHj16BABYt24dLl26hPj4eBw9ehQqlQoLFixocExMyIiIiEiSxlYhq6ioQHx8PHr16vW7%2B8bHx2PcuHHo1asX5HI5ZsyYAQA4fvw4qqursW/fPoSFhaFt27Zo0aIF5s6dixMnTiA/P79BMXFSPxEREUmiiwRKoVCgoKBAo83BwQGOjo6Sjz1%2B/Ph673vp0iWMGjVK/djMzAxdu3ZFZmYmunbtiocPH6Jbt27q7Z07d4ZcLselS5fQunXrevfDhOwPsLbWf58KhQJffhmP4OBgrbwYG8rauple%2B1MoFNi4cbuw8xVBoVAgPl7c77iZhX77UygU2L5d3PkCMr33KPp3rO%2BLl0KhQPyXX4o7X5VK710qFArEb9yI4O%2B/N5lrF6Cbf%2BqNG%2BMRExOj0TZ79myEh4drv7PnKCoqQvPmzTXamjdvjsLCQhQVFQEA7OzsNLbb2dk1eB4ZhywbiYKCAsTExNR5t2CsTO18AdM7Z1M7X8D0ztnUzhcwzXPWleDgYCQkJGh8BQcH1%2Bu5iYmJcHNze%2BpXQkJCg2NR/U7G%2BXvb64MVMiIiIjI4jo6Of7jKGBgYiMDAQK3E0bJlS3Ul7LGioiJ06dIF9vb26sc2Njbq7Q8ePECrVq0a1A8rZERERETP0L17d1y6dEn9WKlU4vLly%2BjVqxdcXV3RvHlzje3Xr19HZWUlunfv3qB%2BmJARERERPcHPzw9paWkAgIkTJ%2BLAgQO4cOECysrK8MUXX8DS0hLDhg2Dubk5JkyYgC1btiA3NxeFhYVYu3YtRo4ciT/96U8N6pNDlo2Eg4MDZs%2BeDQcHB9Gh6IWpnS9geudsaucLmN45m9r5AqZ5zo2Rr68v7t69C6VSiZqaGvTo0QMA8P3338PZ2Rm3bt1SrzM2ZMgQREVFYe7cubh//z569OiBrVu3Qi6XAwDmzJmD0tJSBAYGorq6GsOHD0d0dHSDY5KptDETjYiIiIj%2BMA5ZEhEREQnGhIyIiIhIMCZkRERERIIxISMiIiISjAkZERERkWBMyIiIiIgEY0JGREREJBgTMiIiIiLBmJARERERCcaEjIiIiEgw3suShLp7926993VyctJhJES6U1NTg99%2B%2Bw2VlZV1tvF13fiVlpbCxsZGdBjUyPFelgasqKgIW7ZswV/%2B8hcAQFxcHOLj49G%2BfXssWrQIjo6OgiOUzt3dHTKZrF77XrlyRcfR6F9eXh4OHTqEvLw8fPjhhwCAjIwM9OzZU3BkuvPjjz/ixIkTUCgUAIA2bdrA29sbQ4YMERyZbhw5cgSLFy9GcXGxRrtKpYJMJjPK13V5eTmOHz%2BOvLw8TJs2DUDta71NmzaCI9ONXr16wdvbG2PGjMHgwYPRpAlrHdRwTMgMWHh4OJRKJTZv3ozMzEyEhoYiOjoaFy9ehEKhwIYNG0SHKNnNmzfVP2dkZGD//v0IDQ1Fhw4dUFNTgxs3bmDXrl2YOnUqXnnlFYGRat%2BxY8cQGRkJT09PnDt3DpmZmcjNzUVAQACWLFkCf39/0SFq3YYNG/DVV19h0KBBcHJygkqlwt27d3Hy5EnMmDEDs2bNEh2i1g0cOBBBQUHw8/ODlZVVne2dOnUSEJXunD9/Hu%2B%2B%2By7s7OyQm5uLixcv4s6dOwgICMDmzZvRv39/0SFqXVpaGn744QckJyejtLQUfn5%2BGDNmDF566SXRoVFjoiKD1bdvX1VxcbFKpVKpPvnkE9V7772nUqlUqrKyMlX//v1FhqYTAQEBqvz8/DrtOTk5Kn9/fwER6VZAQIAqKSlJpVKpVD169FC3nzp1yijPV6VSqby8vFTnzp2r03727FmVl5eXgIh0z9PTU1VVVSU6DL0JCgpSff311yqVSvN1ffjwYdW4ceNEhaU3GRkZqrVr16peeeUV1fDhw1Vr165V3b59W3RY1AhwUr8Bq6mpga2tLQDg5MmT8PHxAQBYWFigrKxMZGg6cefOHTRt2rROe/PmzXHnzh0BEelWdnY2vL29AUBj2LZPnz7IyckRFZZOmZubo0ePHnXae/XqBXNzcwER6V5AQADOnDkjOgy9%2Bde//oXg4GAAmq9rPz8/jYq4serRowd8fHzg7e2N4uJi7N%2B/H%2BPGjUNUVBQKCwtFh0cGjAPdBqx79%2B7YtGkTrKysoFAoMGzYMAC1c1I6duwoNjgd8PT0RFhYGKZPnw5nZ2dUV1cjLy8PO3fuhIeHh%2BjwtM7JyQnXrl1D165dNdpTU1PRqlUrQVHp1pQpU/DXv/4VYWFhMDOrfT9YU1ODr776CqGhoYKj0561a9eqf27atCkWLFgAT09PuLi41JkzGRUVpe/wdMrBwQG5ublwdXXVaM/MzFS/wTRGt27dwrfffotDhw6hoKAA3t7eWLt2LQYNGoRHjx5hyZIlmD9/Pr788kvRoZKBYkJmwD7%2B%2BGMsXboUxcXFWLVqFaytrVFUVIRly5YZxfyx/7Zy5UosX74cERERKC8vBwA0adIE/fv3x/LlywVHp32TJk3C9OnTERQUBKVSiR07duDatWs4cuQI5s%2BfLzo8nThz5gx%2B%2BeUXxMbGwtXVFTU1NcjNzUVVVRVeeOEF/POf/1Tv%2B/e//11gpNKkp6drPG7Xrh3u3buHe/fuabTX9wMtjcno0aPx9ttvY9q0aaipqUFycjKuXr2KuLg4TJo0SXR4OjFu3DhcvXoVL730Et555x34%2BflpJJ%2B2trZYunQp%2BvbtKzBKMnSc1N8IVVRUPHVysDEpKipCZWUl7O3tjfoTSz/88AP279%2BP27dvQy6Xw9XVFSEhIRgwYIDo0HQiJiam3vvOnj1bh5GQrqhUKuzYseOpr%2BugoCCjTEI3b96MwMBAODs7P3e/c%2BfOcaI/PRMTMgOWnZ2NVatWqathK1euVC97sWrVKnTu3FlwhNr3eBmI/Px8LFy4EIDxLwNh6h48eIDmzZuLDkNnqqqqsGnTJgwaNAheXl4AgIMHD%2BLGjRuYPXs2LC0tBUeoXQUFBXBwcBAdhs4dOHCg3vu%2B9tprOoyEjAUTMgP21ltvwdXVFYsXL8bp06cRHh6OzZs348KFCzh9%2BjT%2B7//%2BT3SIWmVqy0AsWLDgmdvMzMzQunVrDBkyBL1799ZjVLp16dIlLFq0CAkJCQCAiIgIHD16FC1btsTmzZuNcq7gRx99hIsXL%2BKzzz7DCy%2B8AAC4fPkylixZAjc3NyxevFhwhNrl4eGBc%2BfOqecIGqtBgwZpPC4uLkZVVRXs7OygUqlQXFwMuVyO1q1b4%2BjRo4KipMaECZkB8/LyQmpqKuRyOT7%2B%2BGOoVCosWbIEVVVVGDRoEH7%2B%2BWfRIWrV6NGjERERgREjRqBnz57IyMgAAJw%2BfRrLli3DoUOHBEeoXR9%2B%2BCGSkpJgbW2NF198EWZmZrh8%2BTIqKirQt29f3Lt3D%2Bnp6YiOjkZQUJDocLVi4sSJGDx4SXnIvQAADwBJREFUMMLCwpCcnIzo6Gjs2bMH58%2Bfx65du7Br1y7RIWrdgAEDcPjwYbRs2VKjvbCwEAEBATh58qSgyHRjxYoVkMvlmDFjhsmsXr93715cunQJERER6t%2BzQqHA%2BvXr4eHhgfHjxwuOkBoD452cYwTMzc3VSwGkpqaqV3JXqVSoqqoSGZpOmNoyEC1atEBoaGidTxx%2B8cUXsLCwwDvvvIPU1FQsW7bMaBKy69ev429/%2BxuA2oroqFGj4OTkhLZt2xpdpegxpVL51HlTVVVVqKioEBCRbqWmpkKhUGDr1q2ws7Ors5xJamqqoMh0JyYmBkePHoVcLle3OTo64oMPPsCoUaOYkFG9MCEzYH369MHixYthYWGByspKdYl8x44dcHd3Fxyd9pnaMhB79uxBamqqxtCOmZkZ3n77bQwfPhzvvPMOBg4ciPz8fIFRapeVlRWqqqogk8nwz3/%2BE6tWrQIAPHr0CDU1NYKj041XXnkFs2bNwltvvQVnZ2eoVCrcunUL27ZtM7pheKB2qoWpKS8vR25ubp3liO7fv2%2BUSTfpBhMyA7Z48WJ8/vnnKCwsVFdNHjx4gG%2B%2B%2BQbr168XHZ7WmdoyEBYWFkhJScGIESM02k%2BdOqW%2BCfWJEyfQtm1bEeHpxJAhQzBnzhw0adIEtra2ePnll1FVVYV169bB09NTdHg6sXDhQqxZswYLFixQ38/Szs4O48aNw3vvvSc4Ou0bO3bsM7etWbNGj5HoT0BAAEJDQzF69Gi4uLhAqVQiNzcXhw8fhq%2Bvr%2BjwqJHgHLJGas2aNUZ5MTelZSD27t2Ljz76CO7u7nB2dkaTJk1w9%2B5dZGZmIjIyElOnToWnpydWrFhhNJWU8vJy7NixAw8fPsSkSZPg7OyMR48eITw8HMuXLzfam08/VlhYCDMzM6P%2BVClQ%2B0bi4sWL6jcWAJCfn4%2BkpCScP39eYGS6oVQqsW/fPiQnJyMvLw%2BVlZVwdHTEkCFDMHXqVFhYWIgOkRoBJmQGzpQubKa4vMXly5eRkpKCgoIC1NTUoFWrVujXrx92796NtWvXIicnBy4uLqLD1Lqamhrcv38fVlZWsLOzEx2OTnl5eeHs2bNGuf7W02zcuBHbt2%2BHm5sbMjIy4OHhgaysLLRu3RozZ87Eq6%2B%2BKjpEIoPEIUsD9rwLmzGuXD9x4kS0bdsWo0aNwqhRo4xyntyTlEol0tPT8euvv6oT7uLiYvz888%2B4fv06ABhdMlZQUIBFixbh5MmTqK6uBgDI5XKMGDECCxYsgL29veAItW/w4MHYs2eP%2Bv6Oxm7fvn3Ys2cPunTpgp49eyIuLg4VFRVYvHixUS/yvHv3bhw5cgR37tyBTCZDu3btMHbsWIwZM0Z0aNRIsEJmwIYOHYpt27apL2wZGRnqC9vw4cMxcuRI0SFqVWFhIY4dO4bk5GScOnUKTk5O6uTMGBfBjY6OxvHjx%2BHl5YXvv/8e/v7%2BuHLlCiwtLbFw4UKjnFM1bdo0VFdXY%2BrUqWjXrh1UKhVu376NnTt3wsrKyijv8zdz5kz88ssvMDc3R5s2beokJY35FlFP4%2Bnpqa7ee3h4IC0tDebm5rh37x4mTpyIpKQkwRFq3/r167Fv3z4EBgaiffv2AICsrCwcOHAAkZGRCAkJERwhNQbG%2B3bFCDx8%2BBBdunQBULsEhlKphJWVFaKiojBx4kSjS8hatmyJoKAgBAUFobS0FCkpKUhKSsKkSZPQpk0bJCYmig5Rq5KTk7Fv3z60adMGSUlJWLlyJVQqFVavXo1r164ZZUJ24cIFpKSkoFmzZuq2F154Af369cOwYcPEBaZD3bt3R/fu3UWHoTcdOnRAQkICxo4dCycnJyQnJ8PX1xfV1dW4f/%2B%2B6PB0IiEhAV9%2B%2BWWdT4j7%2B/vj/fffZ0JG9cKEzICZ4oXtsaZNm8LBwQFt2rRB69atkZeXJzokrauoqFBPYjc3N0dlZSUsLS3xzjvvYPTo0Zg4caLgCLXPxcUFjx490kjIAM1/C2PzvHtyGuNCuO%2B99x7Cw8PxyiuvYMqUKYiKikKnTp2Ql5dntEl3SUmJ%2Bs3zk7p16waFQiEgImqMmJAZsKioKMyZM%2BepF7bhw4eLDk/rKisrkZqaiuTkZJw4cQJKpRIjRozAvHnz0L9/f9Hhad0LL7yAmJgY/O///i86duyIvXv3YvLkycjNzcWjR49Eh6c1t27dUv88Y8YMvPfee5g8eTI6d%2B4MMzMz3Lp1C3FxcQgPDxcYpW5dv34dly5dqvPhnK%2B%2B%2BgqTJk0SGJl2BAQEqO%2BksWzZMvz000%2BQy%2BWYMGECXF1dkZmZCWdnZ/j5%2BQmOVDe6dOmCffv21amEJSQkqIcwiX4P55AZuLKyMlhbWwOoXZ/q8YXN19fX6CbIenh4wNLSEj4%2BPvDz88OAAQOM7hyflJmZiaioKCQmJuKnn37C3LlzYWlpiYqKCkyePBkffPCB6BC1wt3dHTKZDL93qZHJZLhy5YqeotKf3bt3Y%2BnSpWjVqhXu3buH1q1bQ6FQwNnZGZMnT8bUqVNFhyjZ0KFD4eHhgXbt2mH79u2YPn36M3/fUVFReo5O99LS0jBjxgw4Ozur57tmZWUhJycHGzduxJAhQwRHSI0BEzIyGCkpKUafhD3PzZs3ceXKFTg7OxvVDcXv3LlT732dnZ11GIkYI0aMwLJly/Dyyy%2BrP5xTUFCA5cuX44033oCXl5foECX7%2BeefERsbi5KSEpw9e/aZ5ySTybBz5049R6cf9%2B/fR2JiIv79738DqJ1y4ufnBycnJ7GBUaPBhMzAPL49Un0Ywz3hNmzYgDlz5gAA1q5d%2B9x9jfGdtSmoqqpSL4z55JDd01haWuojJL3y8PBAeno6AKB3795IT0%2BHTCbDnTt3MHPmTHz77beCI9Su0NBQ9f1KTcXj5VxSU1PVy7lYW1sb9XIupH2mWYowYMa4%2Bv7z/PLLL%2BqfH/%2Bn9TSmsqimMfLy8lL/nnv27Pnc36UxDlk6OTnh9OnTePnll%2BHg4IC0tDT06dMHzZo1Q05OjujwtM7UkjEAmDdvHpRKJT7//PM6y7m8//77RrmcC2kfK2QGrri4GEqlEi1btgQA5OTkwMbGRv3YmBQUFMDBwUF0GKRlaWlp6iGsM2fO4P79%2B%2BqbxT98%2BFB937%2BuXbuib9%2B%2BIkPViYMHD2LBggU4ffo04uLi8OWXX6JPnz64efMm2rVrh23btokOkSTy8PCos5wLUPv6HjZsGM6dOycoMmpMzEQHQM92%2BvRpeHt749SpU%2Bq2f/zjHxg5ciR%2B/vlngZHpxtChQ/Hmm28iPj4eRUVFosMhLXlyPlFeXh4WLlyIvn37okePHli5ciXWrFmDL774AtnZ2QKj1J0xY8bg6NGjsLW1RXBwMGbOnInWrVsjKCgI69atEx0eacHj5Vz%2BmzEv50LaxwqZARs7dizefPNNjB07VqP98OHD2LZtG7755htBkenG5cuXkZycjKSkJNy6dQv9%2BvWDv78/Ro4cWeedJzVOr776Kj788EMMHDgQf//73xEXF4cDBw7gxo0biIqKwuHDh0WHqHX3799HdHQ0UlJSUFlZCZVKBWtrawwdOhQfffQR5xc1Uk8u55KRkaFetua/l3MJCQkx2uU%2BSLuYkBmwJ2878qSqqir07dv3uXOuGrtff/0VSUlJSEpKwtWrVzFw4EBs3rxZdFgkUe/evXHhwgUAQFhYGHr06IF3330XgObkd2MSGhoKmUyGqVOnqj9xl5OTg9jYWJiZmSE2NlZwhPRHmPpyLqR9nNRvwNq1a4ekpKQ6764OHDhg9B%2Blbt%2B%2BPfz9/WFtbY0mTZogJSVFdEikBfb29sjPz4elpSVOnTqFiIgIALWLpMrlcsHR6UZGRgZSU1M1qrzu7u7w8vLC0KFDBUZGUhw7dkx0CGRkmJAZsHnz5iE8PBxbtmyBi4sLampqcPPmTeTl5RntROBr164hOTkZx44dw/Xr1%2BHl5YXRo0dj48aNokMjLQgJCUFQUBDMzc3Rr18/uLm5oaSkBJGRkUY7rOPq6ory8vI6w%2B5KpRKurq6CoiKpjHHNPBKLQ5YGLj8/H0eOHEFOTg5UKhXat2%2BPgIAA9afUjImPjw/y8/Px0ksvwc/PD76%2BvpxfY4TS09NRXFyM/v37w9LSEtXV1di%2BfTumTZumXq/MmPzwww/4%2Buuv8cYbb6BDhw6oqanB7du3sXv3bowZM0ZjEeCOHTsKjJSIRGJCZsAeTwb%2Bxz/%2BoV5Q09raGsOGDcOiRYuMLlnZsmULxo8fb5TJJpkud3f3525/PA%2BJc42ITBsTMgNmapOBPTw8cO7cOZiZcTUWMh6mfusoIqofJmQGrFevXnUmAwNAUVERhg4dqrHKvTFYsWIF5HI5ZsyYARsbG9HhEBER6Q0n9RswU5sMnJqaCoVCga1bt8LOzq7Och/GcO9OIiKip2GFzICZ2mTg31vo9r8XyCUiIjIWTMgMGCcDExERmQYmZAbM1CYDL1iw4LnbP/30Uz1FQkREpF%2BcQ2bAjCHJaoiKigqNx0qlEtnZ2cjNzYW/v7%2BgqIiIiHSPCRkZjLVr1z61/ZtvvsG1a9f0HA0REZH%2BcMiSDJ5SqUT//v1x5swZ0aEQERHpBCtkZDAe343gSeXl5Th69KhR3lKHiIjoMSZkZDB69uwJmUxWp93c3Bx//vOfBURERESkHxyyJINx5swZ3L9/X30vy4cPH%2BJf//oXBgwYgJ49ewqOjoiISHeYkJHBOHjwIKKjo3H%2B/HmUlZXhtddeAwA8ePAA8%2BbNw%2Buvvy44QiIiIt3gXZzJYHzxxRfYuHEjACAxMRGWlpY4cuQIYmNjsX37dsHRERER6Q4TMjIYubm5GDhwIAAgJSUFo0aNgrm5Odzc3HD37l3B0REREekOEzIyGPb29sjPz0dhYSFOnToFb29vAEB%2Bfj7kcrng6IiIiHSHn7IkgxESEoKgoCCYm5ujX79%2BcHNzQ0lJCSIjI%2BHn5yc6PCIiIp3hpH4yKOnp6SguLkb//v1haWmJ6upqbN%2B%2BHdOmTeNaZEREZLSYkBEREREJxjlkRERERIIxISMiIiISjAkZERERkWBMyIiIiIgEY0JGREREJBgTMiIiIiLBmJARERERCcaEjIiIiEiw/wfrggSkZM039gAAAABJRU5ErkJggg%3D%3D&quot; class=&quot;center-img&quot;&gt; &lt;h1&gt;Sample&lt;/h1&gt; &lt;/div&gt; &lt;div class=&quot;row variablerow&quot;&gt; &lt;div class=&quot;col-md-12&quot; style=&quot;overflow:scroll; width: 100%%; overflow-y: hidden;&quot;&gt; &lt;table border=&quot;1&quot; class=&quot;dataframe sample&quot;&gt; pclass survived name sex age sibsp parch ticket fare cabin embarked boat body home.dest 0 1 1 Allen, Miss. Elisabeth Walton female 29.0000 0 0 24160 211.3375 B5 S 2 NaN St Louis, MO 1 1 1 Allison, Master. Hudson Trevor male 0.9167 1 2 113781 151.5500 C22 C26 S 11 NaN Montreal, PQ / Chesterville, ON 2 1 0 Allison, Miss. Helen Loraine female 2.0000 1 2 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON 3 1 0 Allison, Mr. Hudson Joshua Creighton male 30.0000 1 2 113781 151.5500 C22 C26 S NaN 135.0 Montreal, PQ / Chesterville, ON 4 1 0 Allison, Mrs. Hudson J C (Bessie Waldo Daniels) female 25.0000 1 2 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON &lt;/div&gt; df.shape (1309, 14) df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pclass survived age sibsp parch fare body count 1309.000000 1309.000000 1046.000000 1309.000000 1309.000000 1308.000000 121.000000 mean 2.294882 0.381971 29.881135 0.498854 0.385027 33.295479 160.809917 std 0.837836 0.486055 14.413500 1.041658 0.865560 51.758668 97.696922 min 1.000000 0.000000 0.166700 0.000000 0.000000 0.000000 1.000000 25% 2.000000 0.000000 21.000000 0.000000 0.000000 7.895800 72.000000 50% 3.000000 0.000000 28.000000 0.000000 0.000000 14.454200 155.000000 75% 3.000000 1.000000 39.000000 1.000000 0.000000 31.275000 256.000000 max 3.000000 1.000000 80.000000 8.000000 9.000000 512.329200 328.000000 df.describe().iloc[:,:2] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pclass survived count 1309.000000 1309.000000 mean 2.294882 0.381971 std 0.837836 0.486055 min 1.000000 0.000000 25% 2.000000 0.000000 50% 3.000000 0.000000 75% 3.000000 1.000000 max 3.000000 1.000000 df.isnull().sum() pclass 0 survived 0 name 0 sex 0 age 263 sibsp 0 parch 0 ticket 0 fare 1 cabin 1014 embarked 2 boat 823 body 1188 home.dest 564 dtype: int64 "],["statistical-learning.html", "Section 60 Statistical Learning 60.1 What is Statistical Learning? 60.2 Assessing Model Accuracy 60.3 Footnotes", " Section 60 Statistical Learning 60.1 What is Statistical Learning? Given paired data \\((X, Y)\\), assume a relationship between \\(X\\) and \\(Y\\) modeled by \\[ Y = f(X) + \\epsilon \\] where \\(f:\\mathbb{R}^p \\rightarrow \\mathbb{R}\\) is a function and \\(\\epsilon\\) is a random error term with \\(\\mathbb{E}(\\epsilon) = 0\\). Statistical learning is a set of approaches for estimating \\(f\\)0 60.1.1 Why Estimate \\(f\\)? 60.1.1.0.1 Prediction We may want to predict the output \\(Y\\) from an estimate \\(\\hat{f}\\) of \\(f\\). The predicted value for a given \\(Y\\) is then \\[ \\hat{Y} = \\hat{f}(X)\\]. In prediction, we often treat \\(f\\) as a black-box The mean squared-error2 \\(\\mathbf{mse}(\\hat{Y})=\\mathbb{E}(Y-\\hat{Y})^2\\) is a good measure of the accuracy of \\(\\hat{Y}\\) as a predictor for \\(Y\\). One can write \\[ \\mathbf{mse}(\\hat{Y}) = \\left(f(X) - \\hat{f}(X)\\right)^2 + \\mathbb{V}(\\epsilon) \\] These two terms are known as the reducible error and irreducible error, respectively3 60.1.1.0.2 Inference Instead of predicting \\(Y\\) from \\(X\\), we may be more interested how \\(Y\\) changes as a function of \\(X\\). In inference, we usually do not treat \\(f\\) as a black box. Examples of important inference questions: Which predictors have the largest influence on the response? What is the relationship between the response and each predictor? *Is f linear or non-linear? 60.1.2 How to Estimate \\(f\\)? 60.1.2.0.1 Parametric methods Steps for parametric method: Assume a parametric model for \\(f\\), that is assume a specific functional form4 \\[f = f(X, \\boldsymbol{\\beta}) \\] for some vector of parameters \\(\\boldsymbol{\\beta} = (\\beta_1,\\dots,\\beta_p)^T\\) Use the training data to fit or train the model, that is to choose \\(\\beta_i\\) such that \\[Y \\approx f(X, \\boldsymbol{\\beta})\\] 60.1.2.0.2 Non-parametric methods These methods make no assumptions about the functional form of \\(f\\). 60.1.3 Accuracy vs. Interpretability In inference, generally speaking the more flexible the method, the less interpretable. In prediction, generally speaking the more flexible the method, the less accurate 60.1.4 Supervised vs. Unsupervised Learning In supervised learning, training data consists of pairs \\((X, Y)\\) where \\(X\\) is a vector of predictors and \\(Y\\) a response. Prediction and inference are supervised learning problems, and the response variable (or the relationship between the response and the predictors) supervises the analysis of model In unsupervised learning, training data lacks a response variable. 60.1.5 Regression vs. Classification Problems with a quantitative response (\\(Y\\in S \\subseteq \\mathbb{R}\\)) tend to be called regression problems Problems with a qualitative, or categorical response (\\(Y \\in \\{y_1, \\dots, y_n\\})\\) tend to be called classification problems 60.2 Assessing Model Accuracy There is no free lunch in statistics 60.2.1 Measuring Quality of Fit To evaluate the performance of a method on a data set, we need measure model accuracy (how well predictions match observed data). In regression, the most common measure is the mean-squared error \\[MSE = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2\\] where \\(y_i\\) and \\(\\hat{f}(x_i)\\) are the \\(i\\) true and predicting responses, respectively. We are usually not interested in minimizing MSE with respect to training data but rather to test data. There is no guarantee low training MSE will translate to low test MSE. Having low training MSE but high test MSE is called overfitting 60.2.2 The Bias-Variance Tradeoff For a given \\(x_0\\), the expected 5 MSE can be written \\[\\begin{align*} \\mathbb{E}\\left[\\left(y_0 - \\hat{f}(x_0)\\right)^2\\right] &amp;= \\left(\\mathbb{E}\\left[\\hat{f}(x) \\right] - f(x)\\right)^2 + \\mathbb{E}\\left[\\left(\\hat{f}(x_0) - \\mathbb{E}\\left[\\hat{f}(x_0)\\right]\\right)^2\\right] + \\mathbb{E}\\left[\\left(\\epsilon - \\mathbb{E}[\\epsilon]\\right)^2\\right]\\\\ &amp;= \\mathbf{bias}^2\\left(\\hat{f}(x_0))\\right) + \\mathbb{V}\\left(\\hat{f}(x_0)\\right) + \\mathbb{V}(\\epsilon) \\end{align*}\\] A good method minimizes variance and bias simultaneously. As a general rule, these quantities are inversely proportional. More flexible methods have lower bias but higher variance, while less flexible methods have the opposite. This is the bias-variance tradeoff In practice the mse, variance and bias cannot be calculated exactly but one must keep the bias-variance tradeoff in mind. 60.2.3 The Classification Setting In the classification setting, the most common measure of model accuracy is the error rate 6 \\[\\frac{1}{n}\\sum_{i=1}^n I(y_i \\neq \\hat{y}_i)\\] As with the regression, we are interested in minimizing the test error rate, not the training error rate. 60.2.3.0.1 The Bayes Classifier Given \\(K\\) classes, the Bayes Classifier predicts \\[ \\hat{y_0} = \\underset{1\\leqslant j \\leqslant K}{\\text{argmax}\\,} \\mathbb{P}\\left(Y=j\\ |\\ X = x_0\\right)\\] The set of points \\[\\{x_0\\in\\mathbb{R}^p\\ |\\ \\mathbb{P}\\left(Y=j\\ |\\ X = x_0\\right) = \\mathbb{P}\\left(Y=k\\ |\\ X = x_0\\right)\\ \\text{for all}\\ 1\\leqslant j,k \\leqslant K\\}\\] is called the Bayes decision boundary The test error rate of the Bayes classifier is the Bayes error rate, which is minimal among classifiers. It is given by \\[ 1 - \\mathbb{E}\\left(\\underset{j}{\\max} \\mathbb{P}\\left(Y=j\\ |\\ X\\right)\\right)\\] The Bayes classifier is optimal, but in practice we don’t know \\(\\mathbb{P}\\left(Y\\ |\\ X\\right)\\). 60.2.3.0.2 K-Nearest Neighbors The K-nearest neighbors classifier works by estimating \\(\\mathbb{P}\\left(Y\\ |\\ X\\right)\\) as follows. Given \\(K\\geqslant 1\\) and \\(x_0\\), find the set of points \\[ \\mathcal{N}_0 = \\{K\\ \\text{nearest points to}\\ x_0\\}\\subseteq\\mathbb{R}^p \\] For each class \\(j\\) set \\[ \\mathbb{P}\\left(Y=j\\ |\\ X\\right) = \\frac{1}{K}\\sum_{x_i\\in\\mathcal{N}_0}I(y_i = j)\\] Predict \\[ \\hat{y_0} = \\underset{1\\leqslant j \\leqslant K}{\\text{argmax}\\,} \\mathbb{P}\\left(Y=j\\ |\\ X = x_0\\right)\\] 60.3 Footnotes Reading the rest of the chapter, one realized this is the situation for supervised learning, which is the vast majority of this book is concerned with. ↩︎ Here \\(X=(X_1,\\dots, X_p)^T\\) is a vector. ↩︎ This is usual definition of the mean squared-error of \\(\\hat{Y}\\) as an estimator of the (non-parametric) quantity \\(Y=f(X)\\). ↩︎ We can in principle control the reducible error by improving the estimate \\(\\hat{f}\\), but we cannot control the irreducible error. ↩︎ For example, a simple but popular assumption is that f is linear in both the parameters and the features, that is: \\[f(X) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\\] This is linear regression. ↩︎ Here the random variable is \\(\\hat{f}(x_0)\\), so the average is taken over all data sets ↩︎ This is just the proportion of misclassified observations. ↩︎ "],["linear-regression-practice.html", "Section 61 Linear Regression: Practice 61.1 3.1 Simple Linear Regression 61.2 3.2 Multiple Linear Regression 61.3 3.3 Other Considerations in the Regression Model", " Section 61 Linear Regression: Practice Load Datasets 3.1 Simple Linear Regression 3.2 Multiple Linear Regression 3.3 Other Considerations in the Regression Model import pandas as pd import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import axes3d import seaborn as sns from sklearn.preprocessing import scale import sklearn.linear_model as skl_lm from sklearn.metrics import mean_squared_error, r2_score import statsmodels.api as sm import statsmodels.formula.api as smf %matplotlib inline plt.style.use(&#39;seaborn-white&#39;) 61.0.1 Load Datasets Datasets available on https://statlearning.com/data.html !../data_grabber.sh data/ ../data_urls.txt Error: data/ not found. Will make directory and place files in data/. advertising = pd.read_csv(&#39;data/Advertising.csv&#39;, usecols=[1,2,3,4]) advertising.info() &lt;class ‘pandas.core.frame.DataFrame’&gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 4 columns): # Column Non-Null Count Dtype — —— ————– —– 0 TV 200 non-null float64 1 radio 200 non-null float64 2 newspaper 200 non-null float64 3 sales 200 non-null float64 dtypes: float64(4) memory usage: 6.4 KB credit = pd.read_csv(&#39;data/Credit.csv&#39;, usecols=list(range(1,12))) credit[&#39;Student2&#39;] = credit.Student.map({&#39;No&#39;:0, &#39;Yes&#39;:1}) credit.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Income Limit Rating Cards Age Education Gender Student Married Ethnicity Balance Student2 0 14.891 3606 283 2 34 11 Male No Yes Caucasian 333 0 1 106.025 6645 483 3 82 15 Female Yes Yes Asian 903 1 2 104.593 7075 514 4 71 11 Male No No Asian 580 0 auto = pd.read_table(&#39;data/Auto.data&#39;, na_values=&#39;?&#39;).dropna() auto.info() auto.head() &lt;class ‘pandas.core.frame.DataFrame’&gt; Int64Index: 0 entries Data columns (total 14 columns): # Column Non-Null Count Dtype — —— ————– —– 0 mpg 0 non-null object 1 cylinders 0 non-null object 2 displacement 0 non-null float64 3 horsepower weight 0 non-null float64 4 acceleration 0 non-null float64 5 year 0 non-null float64 6 origin 0 non-null float64 7 name 0 non-null float64 8 Unnamed: 8 0 non-null float64 9 Unnamed: 9 0 non-null float64 10 Unnamed: 10 0 non-null float64 11 Unnamed: 11 0 non-null float64 12 Unnamed: 12 0 non-null float64 13 Unnamed: 13 0 non-null float64 dtypes: float64(12), object(2) memory usage: 0.0+ bytes .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mpg cylinders displacement horsepower weight acceleration year origin name Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13 61.1 3.1 Simple Linear Regression 61.1.1 Figure 3.1 - Least squares fit sns.regplot(advertising.TV, advertising.Sales, order=1, ci=None, scatter_kws={&#39;color&#39;:&#39;r&#39;, &#39;s&#39;:9}) plt.xlim(-10,310) plt.ylim(ymin=0); png 61.1.2 Figure 3.2 - Regression coefficients - RSS Note that the text in the book describes the coefficients based on uncentered data, whereas the plot shows the model based on centered data. The latter is visually more appealing for explaining the concept of a minimum RSS. I think that, in order not to confuse the reader, the values on the axis of the B0 coefficients have been changed to correspond with the text. The axes on the plots below are unaltered. # Regression coefficients (Ordinary Least Squares) regr = skl_lm.LinearRegression() X = scale(advertising.TV, with_mean=True, with_std=False).reshape(-1,1) y = advertising.Sales regr.fit(X,y) print(regr.intercept_) print(regr.coef_) 14.0225 [ 0.04753664] # Create grid coordinates for plotting B0 = np.linspace(regr.intercept_-2, regr.intercept_+2, 50) B1 = np.linspace(regr.coef_-0.02, regr.coef_+0.02, 50) xx, yy = np.meshgrid(B0, B1, indexing=&#39;xy&#39;) Z = np.zeros((B0.size,B1.size)) # Calculate Z-values (RSS) based on grid of coefficients for (i,j),v in np.ndenumerate(Z): Z[i,j] =((y - (xx[i,j]+X.ravel()*yy[i,j]))**2).sum()/1000 # Minimized RSS min_RSS = r&#39;$\\beta_0$, $\\beta_1$ for minimized RSS&#39; min_rss = np.sum((regr.intercept_+regr.coef_*X - y.values.reshape(-1,1))**2)/1000 min_rss 2.1025305831313514 fig = plt.figure(figsize=(15,6)) fig.suptitle(&#39;RSS - Regression coefficients&#39;, fontsize=20) ax1 = fig.add_subplot(121) ax2 = fig.add_subplot(122, projection=&#39;3d&#39;) # Left plot CS = ax1.contour(xx, yy, Z, cmap=plt.cm.Set1, levels=[2.15, 2.2, 2.3, 2.5, 3]) ax1.scatter(regr.intercept_, regr.coef_[0], c=&#39;r&#39;, label=min_RSS) ax1.clabel(CS, inline=True, fontsize=10, fmt=&#39;%1.1f&#39;) # Right plot ax2.plot_surface(xx, yy, Z, rstride=3, cstride=3, alpha=0.3) ax2.contour(xx, yy, Z, zdir=&#39;z&#39;, offset=Z.min(), cmap=plt.cm.Set1, alpha=0.4, levels=[2.15, 2.2, 2.3, 2.5, 3]) ax2.scatter3D(regr.intercept_, regr.coef_[0], min_rss, c=&#39;r&#39;, label=min_RSS) ax2.set_zlabel(&#39;RSS&#39;) ax2.set_zlim(Z.min(),Z.max()) ax2.set_ylim(0.02,0.07) # settings common to both plots for ax in fig.axes: ax.set_xlabel(r&#39;$\\beta_0$&#39;, fontsize=17) ax.set_ylabel(r&#39;$\\beta_1$&#39;, fontsize=17) ax.set_yticks([0.03,0.04,0.05,0.06]) ax.legend() png 61.1.3 Confidence interval on page 67 &amp; Table 3.1 &amp; 3.2 - Statsmodels est = smf.ols(&#39;Sales ~ TV&#39;, advertising).fit() est.summary().tables[1] coef std err t P&gt;|t| [0.025 0.975] Intercept 7.0326 0.458 15.360 0.000 6.130 7.935 TV &lt;td&gt; 0.0475&lt;/td&gt; &lt;td&gt; 0.003&lt;/td&gt; &lt;td&gt; 17.668&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 0.042&lt;/td&gt; &lt;td&gt; 0.053&lt;/td&gt; # RSS with regression coefficients ((advertising.Sales - (est.params[0] + est.params[1]*advertising.TV))**2).sum()/1000 2.1025305831313514 61.1.4 Table 3.1 &amp; 3.2 - Scikit-learn regr = skl_lm.LinearRegression() X = advertising.TV.values.reshape(-1,1) y = advertising.Sales regr.fit(X,y) print(regr.intercept_) print(regr.coef_) 7.03259354913 [ 0.04753664] Sales_pred = regr.predict(X) r2_score(y, Sales_pred) 0.61187505085007099 61.2 3.2 Multiple Linear Regression 61.2.1 Table 3.3 - Statsmodels est = smf.ols(&#39;Sales ~ Radio&#39;, advertising).fit() est.summary().tables[1] coef std err t P&gt;|t| [0.025 0.975] Intercept 9.3116 0.563 16.542 0.000 8.202 10.422 Radio 0.2025 0.020 9.921 0.000 0.162 0.243 est = smf.ols(&#39;Sales ~ Newspaper&#39;, advertising).fit() est.summary().tables[1] coef std err t P&gt;|t| [0.025 0.975] Intercept 12.3514 0.621 19.876 0.000 11.126 13.577 Newspaper 0.0547 0.017 3.300 0.001 0.022 0.087 61.2.2 Table 3.4 &amp; 3.6 - Statsmodels est = smf.ols(&#39;Sales ~ TV + Radio + Newspaper&#39;, advertising).fit() est.summary() OLS Regression Results Dep. Variable: &lt;td&gt;Sales&lt;/td&gt; &lt;th&gt; R-squared: &lt;/th&gt; &lt;td&gt; 0.897&lt;/td&gt; Model: &lt;td&gt;OLS&lt;/td&gt; &lt;th&gt; Adj. R-squared: &lt;/th&gt; &lt;td&gt; 0.896&lt;/td&gt; Method: &lt;td&gt;Least Squares&lt;/td&gt; &lt;th&gt; F-statistic: &lt;/th&gt; &lt;td&gt; 570.3&lt;/td&gt; Date: &lt;td&gt;Tue, 09 Jan 2018&lt;/td&gt; &lt;th&gt; Prob (F-statistic):&lt;/th&gt; &lt;td&gt;1.58e-96&lt;/td&gt; Time: &lt;td&gt;23:14:15&lt;/td&gt; &lt;th&gt; Log-Likelihood: &lt;/th&gt; &lt;td&gt; -386.18&lt;/td&gt; No. Observations: &lt;td&gt; 200&lt;/td&gt; &lt;th&gt; AIC: &lt;/th&gt; &lt;td&gt; 780.4&lt;/td&gt; Df Residuals: &lt;td&gt; 196&lt;/td&gt; &lt;th&gt; BIC: &lt;/th&gt; &lt;td&gt; 793.6&lt;/td&gt; Df Model: &lt;td&gt; 3&lt;/td&gt; &lt;th&gt; &lt;/th&gt; &lt;td&gt; &lt;/td&gt; Covariance Type: &lt;td&gt;nonrobust&lt;/td&gt; &lt;th&gt; &lt;/th&gt; &lt;td&gt; &lt;/td&gt; coef std err t P&gt;|t| [0.025 0.975] Intercept 2.9389 0.312 9.422 0.000 2.324 3.554 TV &lt;td&gt; 0.0458&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt; 32.809&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 0.043&lt;/td&gt; &lt;td&gt; 0.049&lt;/td&gt; Radio 0.1885 0.009 21.893 0.000 0.172 0.206 Newspaper -0.0010 0.006 -0.177 0.860 -0.013 0.011 Omnibus: &lt;td&gt;60.414&lt;/td&gt; &lt;th&gt; Durbin-Watson: &lt;/th&gt; &lt;td&gt; 2.084&lt;/td&gt; Prob(Omnibus): 0.000 Jarque-Bera (JB): 151.241 Skew: &lt;td&gt;-1.327&lt;/td&gt; &lt;th&gt; Prob(JB): &lt;/th&gt; &lt;td&gt;1.44e-33&lt;/td&gt; Kurtosis: &lt;td&gt; 6.332&lt;/td&gt; &lt;th&gt; Cond. No. &lt;/th&gt; &lt;td&gt; 454.&lt;/td&gt; 61.2.3 Table 3.5 - Correlation Matrix advertising.corr() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TV Radio Newspaper Sales TV 1.000000 0.054809 0.056648 0.782224 Radio 0.054809 1.000000 0.354104 0.576223 Newspaper 0.056648 0.354104 1.000000 0.228299 Sales 0.782224 0.576223 0.228299 1.000000 61.2.4 Figure 3.5 - Multiple Linear Regression regr = skl_lm.LinearRegression() X = advertising[[&#39;Radio&#39;, &#39;TV&#39;]].as_matrix() y = advertising.Sales regr.fit(X,y) print(regr.coef_) print(regr.intercept_) [ 0.18799423 0.04575482] 2.92109991241 # What are the min/max values of Radio &amp; TV? # Use these values to set up the grid for plotting. advertising[[&#39;Radio&#39;, &#39;TV&#39;]].describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Radio TV count 200.000000 200.000000 mean 23.264000 147.042500 std 14.846809 85.854236 min 0.000000 0.700000 25% 9.975000 74.375000 50% 22.900000 149.750000 75% 36.525000 218.825000 max 49.600000 296.400000 # Create a coordinate grid Radio = np.arange(0,50) TV = np.arange(0,300) B1, B2 = np.meshgrid(Radio, TV, indexing=&#39;xy&#39;) Z = np.zeros((TV.size, Radio.size)) for (i,j),v in np.ndenumerate(Z): Z[i,j] =(regr.intercept_ + B1[i,j]*regr.coef_[0] + B2[i,j]*regr.coef_[1]) # Create plot fig = plt.figure(figsize=(10,6)) fig.suptitle(&#39;Regression: Sales ~ Radio + TV Advertising&#39;, fontsize=20) ax = axes3d.Axes3D(fig) ax.plot_surface(B1, B2, Z, rstride=10, cstride=5, alpha=0.4) ax.scatter3D(advertising.Radio, advertising.TV, advertising.Sales, c=&#39;r&#39;) ax.set_xlabel(&#39;Radio&#39;) ax.set_xlim(0,50) ax.set_ylabel(&#39;TV&#39;) ax.set_ylim(ymin=0) ax.set_zlabel(&#39;Sales&#39;); png 61.3 3.3 Other Considerations in the Regression Model 61.3.1 Figure 3.6 sns.pairplot(credit[[&#39;Balance&#39;,&#39;Age&#39;,&#39;Cards&#39;,&#39;Education&#39;,&#39;Income&#39;,&#39;Limit&#39;,&#39;Rating&#39;]]); png 61.3.2 Table 3.7 est = smf.ols(&#39;Balance ~ Gender&#39;, credit).fit() est.summary().tables[1] coef std err t P&gt;|t| [0.025 0.975] Intercept &lt;td&gt; 509.8031&lt;/td&gt; &lt;td&gt; 33.128&lt;/td&gt; &lt;td&gt; 15.389&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 444.675&lt;/td&gt; &lt;td&gt; 574.931&lt;/td&gt; Gender[T.Female] 19.7331 46.051 0.429 0.669 -70.801 110.267 61.3.3 Table 3.8 est = smf.ols(&#39;Balance ~ Ethnicity&#39;, credit).fit() est.summary().tables[1] coef std err t P&gt;|t| [0.025 0.975] Intercept &lt;td&gt; 531.0000&lt;/td&gt; &lt;td&gt; 46.319&lt;/td&gt; &lt;td&gt; 11.464&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 439.939&lt;/td&gt; &lt;td&gt; 622.061&lt;/td&gt; Ethnicity[T.Asian] -18.6863 65.021 -0.287 0.774 -146.515 109.142 Ethnicity[T.Caucasian] -12.5025 56.681 -0.221 0.826 -123.935 98.930 61.3.4 Table 3.9 - Interaction Variables est = smf.ols(&#39;Sales ~ TV + Radio + TV*Radio&#39;, advertising).fit() est.summary().tables[1] coef std err t P&gt;|t| [0.025 0.975] Intercept 6.7502 0.248 27.233 0.000 6.261 7.239 TV &lt;td&gt; 0.0191&lt;/td&gt; &lt;td&gt; 0.002&lt;/td&gt; &lt;td&gt; 12.699&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 0.016&lt;/td&gt; &lt;td&gt; 0.022&lt;/td&gt; Radio 0.0289 0.009 3.241 0.001 0.011 0.046 TV:Radio 0.0011 5.24e-05 20.727 0.000 0.001 0.001 61.3.5 Figure 3.7 - Interaction between qualitative and quantative variables est1 = smf.ols(&#39;Balance ~ Income + Student2&#39;, credit).fit() regr1 = est1.params est2 = smf.ols(&#39;Balance ~ Income + Income*Student2&#39;, credit).fit() regr2 = est2.params print(&#39;Regression 1 - without interaction term&#39;) print(regr1) print(&#39;\\nRegression 2 - with interaction term&#39;) print(regr2) Regression 1 - without interaction term Intercept 211.142964 Income 5.984336 Student2 382.670539 dtype: float64 Regression 2 - with interaction term Intercept 200.623153 Income 6.218169 Student2 476.675843 Income:Student2 -1.999151 dtype: float64 # Income (x-axis) income = np.linspace(0,150) # Balance without interaction term (y-axis) student1 = np.linspace(regr1[&#39;Intercept&#39;]+regr1[&#39;Student2&#39;], regr1[&#39;Intercept&#39;]+regr1[&#39;Student2&#39;]+150*regr1[&#39;Income&#39;]) non_student1 = np.linspace(regr1[&#39;Intercept&#39;], regr1[&#39;Intercept&#39;]+150*regr1[&#39;Income&#39;]) # Balance with iteraction term (y-axis) student2 = np.linspace(regr2[&#39;Intercept&#39;]+regr2[&#39;Student2&#39;], regr2[&#39;Intercept&#39;]+regr2[&#39;Student2&#39;]+ 150*(regr2[&#39;Income&#39;]+regr2[&#39;Income:Student2&#39;])) non_student2 = np.linspace(regr2[&#39;Intercept&#39;], regr2[&#39;Intercept&#39;]+150*regr2[&#39;Income&#39;]) # Create plot fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) ax1.plot(income, student1, &#39;r&#39;, income, non_student1, &#39;k&#39;) ax2.plot(income, student2, &#39;r&#39;, income, non_student2, &#39;k&#39;) for ax in fig.axes: ax.legend([&#39;student&#39;, &#39;non-student&#39;], loc=2) ax.set_xlabel(&#39;Income&#39;) ax.set_ylabel(&#39;Balance&#39;) ax.set_ylim(ymax=1550) png 61.3.6 Figure 3.8 - Non-linear relationships # With Seaborn&#39;s regplot() you can easily plot higher order polynomials. plt.scatter(auto.horsepower, auto.mpg, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.5) sns.regplot(auto.horsepower, auto.mpg, ci=None, label=&#39;Linear&#39;, scatter=False, color=&#39;orange&#39;) sns.regplot(auto.horsepower, auto.mpg, ci=None, label=&#39;Degree 2&#39;, order=2, scatter=False, color=&#39;lightblue&#39;) sns.regplot(auto.horsepower, auto.mpg, ci=None, label=&#39;Degree 5&#39;, order=5, scatter=False, color=&#39;g&#39;) plt.legend() plt.ylim(5,55) plt.xlim(40,240); png 61.3.7 Table 3.10 auto[&#39;horsepower2&#39;] = auto.horsepower**2 auto.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mpg cylinders displacement horsepower weight acceleration year origin name horsepower2 0 18.0 8 307.0 130.0 3504 12.0 70 1 chevrolet chevelle malibu 16900.0 1 15.0 8 350.0 165.0 3693 11.5 70 1 buick skylark 320 27225.0 2 18.0 8 318.0 150.0 3436 11.0 70 1 plymouth satellite 22500.0 est = smf.ols(&#39;mpg ~ horsepower + horsepower2&#39;, auto).fit() est.summary().tables[1] coef std err t P&gt;|t| [0.025 0.975] Intercept 56.9001 1.800 31.604 0.000 53.360 60.440 horsepower -0.4662 0.031 -14.978 0.000 -0.527 -0.405 horsepower2 0.0012 0.000 10.080 0.000 0.001 0.001 61.3.8 Figure 3.9 regr = skl_lm.LinearRegression() # Linear fit X = auto.horsepower.values.reshape(-1,1) y = auto.mpg regr.fit(X, y) auto[&#39;pred1&#39;] = regr.predict(X) auto[&#39;resid1&#39;] = auto.mpg - auto.pred1 # Quadratic fit X2 = auto[[&#39;horsepower&#39;, &#39;horsepower2&#39;]].as_matrix() regr.fit(X2, y) auto[&#39;pred2&#39;] = regr.predict(X2) auto[&#39;resid2&#39;] = auto.mpg - auto.pred2 fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) # Left plot sns.regplot(auto.pred1, auto.resid1, lowess=True, ax=ax1, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;lw&#39;:1}, scatter_kws={&#39;facecolors&#39;:&#39;None&#39;, &#39;edgecolors&#39;:&#39;k&#39;, &#39;alpha&#39;:0.5}) ax1.hlines(0,xmin=ax1.xaxis.get_data_interval()[0], xmax=ax1.xaxis.get_data_interval()[1], linestyles=&#39;dotted&#39;) ax1.set_title(&#39;Residual Plot for Linear Fit&#39;) # Right plot sns.regplot(auto.pred2, auto.resid2, lowess=True, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;lw&#39;:1}, ax=ax2, scatter_kws={&#39;facecolors&#39;:&#39;None&#39;, &#39;edgecolors&#39;:&#39;k&#39;, &#39;alpha&#39;:0.5}) ax2.hlines(0,xmin=ax2.xaxis.get_data_interval()[0], xmax=ax2.xaxis.get_data_interval()[1], linestyles=&#39;dotted&#39;) ax2.set_title(&#39;Residual Plot for Quadratic Fit&#39;) for ax in fig.axes: ax.set_xlabel(&#39;Fitted values&#39;) ax.set_ylabel(&#39;Residuals&#39;) png 61.3.9 Figure 3.14 fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) # Left plot ax1.scatter(credit.Limit, credit.Age, facecolor=&#39;None&#39;, edgecolor=&#39;r&#39;) ax1.set_ylabel(&#39;Age&#39;) # Right plot ax2.scatter(credit.Limit, credit.Rating, facecolor=&#39;None&#39;, edgecolor=&#39;r&#39;) ax2.set_ylabel(&#39;Rating&#39;) for ax in fig.axes: ax.set_xlabel(&#39;Limit&#39;) ax.set_xticks([2000,4000,6000,8000,12000]) png 61.3.10 Figure 3.15 y = credit.Balance # Regression for left plot X = credit[[&#39;Age&#39;, &#39;Limit&#39;]].as_matrix() regr1 = skl_lm.LinearRegression() regr1.fit(scale(X.astype(&#39;float&#39;), with_std=False), y) print(&#39;Age/Limit\\n&#39;,regr1.intercept_) print(regr1.coef_) # Regression for right plot X2 = credit[[&#39;Rating&#39;, &#39;Limit&#39;]].as_matrix() regr2 = skl_lm.LinearRegression() regr2.fit(scale(X2.astype(&#39;float&#39;), with_std=False), y) print(&#39;\\nRating/Limit\\n&#39;,regr2.intercept_) print(regr2.coef_) Age/Limit 520.015 [-2.29148553 0.17336497] Rating/Limit 520.015 [ 2.20167217 0.02451438] # Create grid coordinates for plotting B_Age = np.linspace(regr1.coef_[0]-3, regr1.coef_[0]+3, 100) B_Limit = np.linspace(regr1.coef_[1]-0.02, regr1.coef_[1]+0.02, 100) B_Rating = np.linspace(regr2.coef_[0]-3, regr2.coef_[0]+3, 100) B_Limit2 = np.linspace(regr2.coef_[1]-0.2, regr2.coef_[1]+0.2, 100) X1, Y1 = np.meshgrid(B_Limit, B_Age, indexing=&#39;xy&#39;) X2, Y2 = np.meshgrid(B_Limit2, B_Rating, indexing=&#39;xy&#39;) Z1 = np.zeros((B_Age.size,B_Limit.size)) Z2 = np.zeros((B_Rating.size,B_Limit2.size)) Limit_scaled = scale(credit.Limit.astype(&#39;float&#39;), with_std=False) Age_scaled = scale(credit.Age.astype(&#39;float&#39;), with_std=False) Rating_scaled = scale(credit.Rating.astype(&#39;float&#39;), with_std=False) # Calculate Z-values (RSS) based on grid of coefficients for (i,j),v in np.ndenumerate(Z1): Z1[i,j] =((y - (regr1.intercept_ + X1[i,j]*Limit_scaled + Y1[i,j]*Age_scaled))**2).sum()/1000000 for (i,j),v in np.ndenumerate(Z2): Z2[i,j] =((y - (regr2.intercept_ + X2[i,j]*Limit_scaled + Y2[i,j]*Rating_scaled))**2).sum()/1000000 fig = plt.figure(figsize=(12,5)) fig.suptitle(&#39;RSS - Regression coefficients&#39;, fontsize=20) ax1 = fig.add_subplot(121) ax2 = fig.add_subplot(122) min_RSS = r&#39;$\\beta_0$, $\\beta_1$ for minimized RSS&#39; # Left plot CS = ax1.contour(X1, Y1, Z1, cmap=plt.cm.Set1, levels=[21.25, 21.5, 21.8]) ax1.scatter(regr1.coef_[1], regr1.coef_[0], c=&#39;r&#39;, label=min_RSS) ax1.clabel(CS, inline=True, fontsize=10, fmt=&#39;%1.1f&#39;) ax1.set_ylabel(r&#39;$\\beta_{Age}$&#39;, fontsize=17) # Right plot CS = ax2.contour(X2, Y2, Z2, cmap=plt.cm.Set1, levels=[21.5, 21.8]) ax2.scatter(regr2.coef_[1], regr2.coef_[0], c=&#39;r&#39;, label=min_RSS) ax2.clabel(CS, inline=True, fontsize=10, fmt=&#39;%1.1f&#39;) ax2.set_ylabel(r&#39;$\\beta_{Rating}$&#39;, fontsize=17) ax2.set_xticks([-0.1, 0, 0.1, 0.2]) for ax in fig.axes: ax.set_xlabel(r&#39;$\\beta_{Limit}$&#39;, fontsize=17) ax.legend() png 61.3.11 Variance Inflation Factor - page 102 est_Age = smf.ols(&#39;Age ~ Rating + Limit&#39;, credit).fit() est_Rating = smf.ols(&#39;Rating ~ Age + Limit&#39;, credit).fit() est_Limit = smf.ols(&#39;Limit ~ Age + Rating&#39;, credit).fit() print(1/(1-est_Age.rsquared)) print(1/(1-est_Rating.rsquared)) print(1/(1-est_Limit.rsquared)) 1.01138468607 160.668300959 160.592879786 "],["linear-regression-theory.html", "Section 62 Linear Regression: Theory 62.1 Simple Linear Regression 62.2 Multiple Linear Regression 62.3 Other Considerations In the Regression Model 62.4 The Marketing Plan 62.5 Comparison of Linear Regression and K-Nearest Neighbors 62.6 Footnotes", " Section 62 Linear Regression: Theory 62.1 Simple Linear Regression For data (X, Y), \\(X, Y\\in\\mathbb{R}\\), simple linear regression models \\(Y\\) as a linear function of \\(X\\) \\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\] and predicts \\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X\\] where \\(\\hat{\\beta}_i\\) is the estimate for \\(\\beta_i\\). 62.1.1 Estimating the Coefficients Estimates of the coefficients \\(\\beta_0, \\beta_1\\) arize from minimizing residual sum of squares \\[RSS = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\] using calculus one finds estimates7 \\[\\begin{align*} \\hat{\\beta}_1 &amp;= \\frac{\\sum_i (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_i (x_i - \\overline{x}) ^2}\\\\ \\hat{\\beta}_0 &amp;= \\overline{y}-\\hat{\\beta}_1\\overline{x} \\end{align*}\\] These are sometimes called the least squares estimates. 62.1.2 Assessing the Accuracy of the Coefficent Estimates The population regression line8 is the line given by \\[ Y = \\beta_0 + \\beta_1 X \\] and the least squares regression line is the line given by \\[ Y = \\hat{\\beta}_0 + \\hat{\\beta}_1 X \\] The least squares estimate is an unbiased estimator 9 Assuming errors \\(\\epsilon_i\\) are uncorrelated with common variance \\(\\sigma^2=\\mathbb{V}(\\epsilon)\\), the standard errors of \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\) are \\[ \\mathbf{se}(\\hat{\\beta}_0) = \\sigma\\sqrt{\\left[\\frac{1}{n} + \\frac{\\overline{x}}{\\sum_i (x_i - \\overline{x})^2}\\right]} \\] \\[ \\mathbf{se}(\\hat{\\beta}_1) = \\sigma\\sqrt{\\frac{1}{\\sum_i (x_i - \\overline{x})^2}} \\] The estimated standard errors \\(\\hat{\\mathbf{se}}(\\hat{\\beta}_0), \\hat{\\mathbf{se}}(\\hat{\\beta}_0)\\) are found by estimating \\(\\sigma\\) with the residual standard error 10 \\[ \\hat{\\sigma} = RSE := \\sqrt{\\frac{RSS}{n-2}} \\] Approximate \\(1 - \\alpha\\) confidence intervals 11 for the least squares estimators are \\[ \\hat{\\beta_i} \\pm t_{\\alpha/2}\\hat{\\mathbf{se}}(\\hat{\\beta}_i) \\] Most common hypothesis tests for the least squares estimates are \\[H_0: \\beta_i = 0\\] \\[H_a: \\beta_i \\neq 0\\] the rejection region is \\[\\{ x\\in \\mathbb{R}\\ |\\ T &gt; t \\}\\] where \\(t\\) is the test-statistic 12 \\[ t = \\frac{\\hat{\\beta}_i - \\beta_i}{\\hat{\\mathbf{se}}(\\hat{\\beta_i})} \\] 62.1.3 Assessing the Accuracy of the Model Quality of fit (model accuracy) is commonly assessed using \\(RSE\\) and the \\(R^2\\) statistic. 62.1.4 Residual Standard Errors The RSE is a measure of the overall difference between the observed responses \\(y_i\\) and the predicted responses \\(\\hat{y}_i\\). Thus it provides a measure of lack-of-fit of the model – higher RSE indicates worse fit. RSE is measured in units of \\(Y\\) so it provides an absolute measure of lack of fit, which is sometimes difficult to interpret 62.1.5 \\(R^2\\) Statistic The \\(R^2\\) statistic is \\[ R^2 = \\frac{TSS - RSS}{TSS}\\] where \\(TSS = \\sum_i (y_i - \\overline{y})^2\\) is the total sum of squares. \\(TSS\\) measures the total variability in \\(Y\\), while \\(RSS\\) measures the variability left after modeling \\(Y\\) by \\(f(X)\\). Thus, \\(R^2\\) measures the proportion of variability in \\(Y\\) that can be explained by the model. \\(R^2\\) is dimensionless so it provides a good relative measure of lack-of-fit. As \\(R^2 \\rightarrow 1\\), the model explains more of the variability in \\(Y\\). As \\(R^2 \\rightarrow 0\\), the model explains less 13. What constitutes a good \\(R^2\\) value depends on context We can also think of \\(R^2\\) as a measure of the linear relationship between \\(Y\\) and \\(X\\). Another such measure is the correlation \\(\\text{corr}(X,Y)\\), which is estimated by the sample correlation \\(r\\). In the case of simple linear regression, \\(R^2 = r^2\\). 62.2 Multiple Linear Regression For data (X, Y), \\(X=(X_1,\\dots,X_p)^T\\in\\mathbb{R}^p\\),\\(Y\\in\\mathbb{R}\\), multiple linear regression models \\(Y\\) as a linear function 14 of \\(X\\) \\[Y = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p + \\epsilon\\] and predicts \\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\cdots + \\hat{\\beta}_p X_p + \\epsilon \\] where \\(\\hat{\\beta}_i\\) is the estimate of \\(\\beta_i\\) If we form the \\(n \\times (p + 1)\\) matrix \\(\\mathbf{X}\\) with rows \\((1, X_{i1}, \\dots, X_{ip})\\), response vector \\(Y=(Y_1,\\dots,Y_n)\\), parameter vector \\(\\beta = (\\beta_0, \\dots, \\beta_p)\\) and noise vector \\(\\epsilon = (\\epsilon_1, \\dots, \\epsilon_n)\\) then the model can be written in matrix form \\[ Y = \\mathbf{X}\\beta + \\epsilon \\] 62.2.1 Estimating the Regression Coefficients RSS is defined and estimates \\(\\hat{\\beta}_i\\) for the parameters \\(\\beta_i\\) are chosen to minimize RSS 15 as in the case of simple regression. If the data matrix \\(\\mathbf{X}\\) has full rank, then the estimate 16 \\(\\hat{\\beta}\\) for the parameter vector is \\[ \\hat{\\beta} = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\beta \\] 62.2.2 Important Questions 62.2.2.1 Is There a Relationship Between the Response and Predictors? One way to answer this question is a hypothesis test \\[\\begin{align*} H_0:&amp; \\beta_i = 0 &amp;\\text{for all}\\ 1 \\leqslant i \\leqslant p\\\\ H_a:&amp; \\beta_i \\neq 0&amp;\\text{for some}\\ 1 \\leqslant i \\leqslant p \\end{align*}\\] The test statistic is the \\(F\\)-statistic17 \\[ F = \\frac{(TSS - RSS)/p}{RSS/(n - p - 1)} \\] where \\(TSS, RSS\\) are defined as in simple linear regression. Assuming the model is correct, \\[ \\mathbb{E}\\left(\\frac{RSS}{n-p-1}\\right) = \\sigma^2 \\] where again, \\(\\sigma^2 = \\mathbb{V}(\\epsilon)\\). Further assuming \\(H_0\\) is true, \\[ \\mathbb{E}\\left(\\frac{TSS - TSS}{p}\\right) = \\sigma^2 \\] hence \\(H_0 \\Rightarrow F \\approx 1\\) and \\(H_a \\Rightarrow F &gt; 1\\)18. Another way to answer this question is a hypothesis test on a subset of the predictors of size \\(q\\) \\[\\begin{align*} H_0:&amp; \\beta_{i} = 0 &amp;\\text{for all}\\ p - q + 1 \\leqslant i \\leqslant p\\\\ H_a:&amp; \\beta_i \\neq 0 &amp;\\text{for some}\\ p - q + 1 \\leqslant i \\leqslant p \\end{align*}\\] where \\(RSS_0\\) is the residual sum of squares for a second model ommitting the last \\(q\\) predictors. The \\(F\\)-statistic is \\[ F = \\frac{(RSS_0 - RSS)/p}{RSS/(n - p - 1)} \\] These hypothesis tests help us conclude that at least one of the predictors is related to the response (the second test narrows it down a bit), but don’t indicate which ones. 62.2.2.2 Deciding on Important Variables The task of finding which predictors are related to the response is sometimes known as variable selection.19 Various statistics can be used to judge the quality of models using different subsets of the predictors. Examples are Mallows \\(C_p\\) criterion, Akaike Information Criterion (AIC), Bayesian Information Criterion and adjusted \\(R^2\\). Since the number of distinct linear regression models grows exponentially with \\(p\\) exhaustive search is infeasible unless \\(p\\) is small. Common approaches to consider a smaller set of possible models are Forward Selection Start with the null model \\(M_0\\) (an intercept but no predictors). Fit \\(p\\) simple regressions and add to the null model the one with lowest \\(RSS\\), resulting in a new model \\(M_1\\). Iterate until a stopping rule is reached. Backward Selection Start with a model \\(M_p\\) consisting of all predictors. Remove the variable with largest \\(p\\)-value, resulting in a new model \\(M_{p-1}\\). Iterate until a stopping rule is reached. Mixed Selection Proceed with forward selection, but remove any predictors whose \\(p\\)-value is too large. 62.2.2.3 Model Fit As in simple regression, \\(RSE\\) and \\(R^2\\) are two common measures of model fit In multiple regression, \\(R^2 = Corr(Y, \\hat{Y})^2\\), with the same interpretation as in simple regression. The model \\(\\hat{Y}\\) maximizes \\(R^2\\) among all linear models. \\(R^2\\) increases monotonically in the number of predictors, but small increases indicate the low relative value of the corresponding predictor. In multiple regression \\[ RSS = \\sqrt{\\frac{RSS}{n - p - 1}} \\] Visualization can be helpful in assessing model fit, e.g. by suggesting the inclusion of interaction terms 62.2.2.4 Predictions There are 3 types of uncertainty associated with predicting \\(Y\\) by \\(\\hat{Y}\\) Estimation Error. \\(\\hat{Y} = \\hat{f}(X)\\) is only an estimate \\(f(X)\\). This error is reducible. We can compute confidence intervals to quantify it. Model Bias. A linear form for \\(f(X)\\) may be inappropriate. This error is also reducible Noise. The noise term \\(\\epsilon\\) is a random variable. This error is irreducible. We can compute predeiction intervals to quantify it. 62.3 Other Considerations In the Regression Model 62.3.1 Qualitative Predictors If the \\(i\\)-th predictor \\(X_i\\) is a factor (qualitative) with \\(K\\) levels (that is \\(K\\) possible values) then we model it by \\(K-1\\) indicator variables (sometimes called a dummy variables). Two commons definitions of the dummy variables are \\[ \\tilde{X}_{i} = \\begin{cases} 1 &amp; X_i = k\\\\ 0 &amp; X_i \\neq k \\end{cases}\\] \\[ \\tilde{X}_{i} = \\begin{cases} 1 &amp; X_i = k\\\\ -1 &amp; X_i \\neq k \\end{cases}\\] for \\(1 \\leqslant k \\leqslant K\\). The corresponding regression model is \\[ Y = \\beta_0 + \\sum_{i} \\beta_i\\tilde{X}_i + \\epsilon \\] since we can only have \\(\\tilde{X}_i = 1\\) if \\(\\tilde{X}_j \\neq 1\\) for \\(j \\neq i\\), this model can be seen as \\(K\\) distinct models \\[ Y = \\begin{cases} \\beta_0 &amp; X_i = 1 \\\\ \\beta_0 + \\beta_1 &amp; X_i = 2 \\\\ \\vdots &amp; \\vdots \\\\ \\beta_0 + \\beta_K &amp; X_i = K \\end{cases} \\] 62.3.2 Extensions of the Linear Model The standard linear regression we have been discussing relies on the twin assumptions Additivity: The effect of \\(X_i\\) on \\(Y\\) is independent of the effect of \\(X_j\\) for \\(j\\neq i\\). Linearity: \\(Y\\) is linear in \\(X_i\\) for all \\(i\\). We can extend the model by relaxing these assumptions 62.3.2.1 Removing the Additive Assumption Dropping the assumption of additivity leads to the possible inclusion of interaction or synergy effects among predictors. One way to model an interaction effect between predictors \\(X_i\\) and \\(X_j\\) is to include an interaction term, \\(\\beta_{i + j}X_iX_j\\). The non-interaction terms \\(\\beta_i X_i\\) model the main effects. We can perform hypothesis tests as in the standard linear model to select important terms/variables. However, the hierarchical principle dictates that, if we include an interaction effect, we should include the corresponding main effects, even if the latter aren’t statistically significant. 62.3.2.2 Non-linear Relationships Dropping the assumption of linearity leads to the possible includion of non-linear effects. One common way to model non-linearity is to use polynomial regression 20, that is model \\(f(X)\\) with a polynomial in the predictors. For example in the case of a single predictor \\(X\\) \\[Y = \\beta_0 + \\beta_1 X + \\dots + \\beta_d X^s \\] models \\(Y\\) as a degree \\(d\\) polynomial in \\(X\\) In general one can model a non-linear effect of predictors \\(X_i\\) by including a non-linear function of the \\(X_i\\) in the model 62.3.3 Potential Problems 62.3.3.1 Non-linearity of the Data Residual plots are a useful way of vizualizing non-linearity. The presence of a discernible pattern may indicate a problem with the linearity of the model. 62.3.3.2 Correlation of Error Terms Standard linear regression assumes \\(\\text{Corr}(\\epsilon_i,\\epsilon_j) = 0\\) for \\(i\\neq j\\). Correlated error terms frequently occur in the context of time series. Positively correlated error terms may display tracking behavior (adjacent residuals may have similar values). 62.3.3.3 Non-constant Variance of Error Terms Standard linear regression assumes the variance of errors is constant across observations, i.e. \\(\\mathbb{V}(\\epsilon_i) = \\sigma^2\\) for all \\(1 \\leqslant i \\leqslant n\\) Hetereoscedasticity, or variance which changes across observations can be identified by a funnel shape in the residual plot. One way to reduce hetereoscedasticity is to transform \\(Y\\) by a concave function such as \\(\\log Y\\) or \\(\\sqrt{Y}\\). Another way to do this is weighted least squares. This weights terms in \\(RSS\\) with weights \\(w_i\\) inversely proportional to \\(\\sigma_i^2\\) where \\(\\sigma_i^2 = \\mathbb{V}(\\epsilon_i)\\). 62.3.3.4 Outliers An outlier is an observation for which the value of \\(y_i\\) given \\(x_i\\) is unusual, i.e. such that the squared-error \\((y_i - \\hat{y}_i)^2\\) is large Outliers can have disproportionate effects on statistics e.g. \\(R^2\\), which in turn affect the entire analysis (e.g. confidence intervals, hypothesis tests). Residual plots can identify outliers. In practice, we plot studentized residuals \\[\\frac{\\hat{\\epsilon}_i}{\\hat{\\mathbf{se}}(\\hat{\\epsilon}_i)} \\] If an outlier is due to a data collection error it can be removed, but great care should be taken when doing this. 62.3.3.5 High Leverage Points A high leverage point is a point with an unusual value of \\(x_i\\). High leverage points tend to have a sizable impact on \\(\\hat{f}\\). To quantify the leverage of \\(x_i\\), we use the leverage statistic. In simple linear regression this is \\[ h_i = \\frac{1}{n} + \\frac{(X_j - \\overline{X})^2}{\\sum_{j} (X_{j} - \\overline{X})^2} \\] 62.3.3.6 Collinearity Collinearity is a linear relationship among two or more predictors. Collinearity reduces the accuracy of coefficient estimates 21 Collinearity reduces the power22 of the hypothesis test Collinearity between two variables can be detected by the sample correlation matrix \\(\\hat{\\Sigma}\\). A high value for \\[|(\\hat{\\Sigma})_{ij}| = |\\hat{\\text{corr}(X_i, X_j)}|\\] indicates high correlation between \\(X_i, X_j\\) hence high collinearity in the data23. Multicollinearity is a linear relationship among more than two predictors. Multicollinearity can be detected using the variance inflation factor (VIF)24. \\[ VIF(\\hat{\\beta}_i) = \\frac{1}{1-R^2_{X_i|X_{-i}}}\\] where \\(R^2_{X_i|X_{-i}}\\) is the \\(R^2\\) from regression of \\(X_i\\) onto all other predictors. One solution to the presence of collinearity is to drop one of the problematic variables, which is usually not an issue, since correlation among variables is seen as redundant. Another solution is to combine the problematic variables into a single predictor (e.g. an average) 62.4 The Marketing Plan Skip 62.5 Comparison of Linear Regression and K-Nearest Neighbors Linear regression is a parametric model for regression (with parameter \\(\\beta = (\\beta_0, \\dots, \\beta_p)\\)). KNN regression is a popular non-parametric model, which estimates \\[\\hat{f}(x_0) = \\frac{1}{K}\\sum_{x_i\\in\\mathcal{N}_0} y_i \\] In general, a parametric model will outperform a non-parametric model if the parametric estimation \\(\\hat{f}\\) is close to the true \\(f\\). KNN regression suffers from the curse of dimensionality - as the dimension increases the data become sparse. Effectively this is a reduction in sample size, hence KNN performance commonly decreases as the dimension \\(p\\) increases. In general parametric methods outperform non-parametric methods when there is a small number of observations per predictor. Even if performance of KNN and linear regression is comparable, the latter may be favored for interpretability. 62.6 Footnotes The value \\((\\hat{\\beta}_0, \\hat{\\beta}_1)\\) is the local minimum in \\(\\mathbb{R}^2\\) of the “loss function” given by RSS ↩︎ Here estimate means the same as “estimator,” found elsewhere in the statistics literature. The population regression line is given by the “true” (population) values \\((\\beta_0, \\beta_1)\\) of the parameter, while the least squares line is given by the estimator \\((\\hat{\\beta}_0, \\hat{\\beta}_1)\\) ↩︎ In other words, \\(\\mathbb{E}\\left((\\hat{\\beta}_0, \\hat{\\beta}_1)\\right) = (\\beta_0, \\beta_1)\\) ↩︎ The factor \\(\\frac{1}{n-2}\\) is a correction to make this an unbiased estimator, the quantity \\(n - 2\\) is known as the “degrees of freedom.” Note this is a special case of \\(n - p - 1\\) degrees of freedom for \\(p\\) predictors where \\(p = 1\\). ↩︎ This appears to be based on the assumption (no doubt proved in the literature) that the least squares estimators are asymptotically t-distributed, \\(\\hat{\\beta}_i \\approx Student_{n-2}(\\beta_i, \\hat{\\mathbf{se}}(\\hat{\\beta}_i))\\). ↩︎ This is the Wald test for the statistic \\(T\\), which (by footnote 4) has \\(T \\approx Student_{n - 2}(0, 1)\\). ↩︎ This can happen if either the model is wrong (i.e. a linear form for \\(f(X)\\) isn’t a good choice) or because \\(\\mathbb{V}(\\epsilon)\\) is large. ↩︎ This estimation method is known as Ordinary Least Squares (OLS). The estimate is the solution to the quadratic minimzation problem \\[ \\hat{\\beta} = \\underset{\\beta}{\\text{argmin}\\,} || y - \\mathbf{X}\\beta ||^2 \\] ↩︎ The estimate is any solution to the quadratic minimzation problem \\[ \\hat{\\beta} = \\underset{\\beta}{\\text{argmin}\\,} || y - \\mathbf{X}\\beta ||^2 \\] which can be found by solving the normal equations \\[\\mathbf{X}^\\top\\mathbf{X}\\hat{\\beta} = \\mathbf{X}^\\top y\\] ↩︎ If \\(\\mathbf{X}\\) has full rank then \\(\\mathbf{X}^\\top\\mathbf{X}\\) is invertible and the normal equations have a unique solution ↩︎ Assuming the \\(\\epsilon_i\\) are normally distributed, \\(\\epsilon_i \\sim N(\\mu_i, \\sigma^2)\\) where \\(\\mu = \\beta_0 + \\sum \\beta_i X_i\\)), the \\(F\\)-statistic has an \\(F\\)-distribution with \\(p, n-p\\) degrees of freedom (\\(F\\) has this asymptotic distribution even without the normality assumption). The use of the \\(F\\) statistic arises from ANOVA among the predictors, which is beyond our scope. There is some qualitative discussion of the motivation for the \\(F\\) statistic on page 77 of the text. It is an appropriate statistic in the case \\(p\\) i ↩︎ How much \\(F &gt; 1\\) should be before we rejct \\(H_0\\) depends on \\(n\\) and \\(p\\). If \\(n\\) is large, \\(F\\) need not be much greater than 1, and if it’s small, ↩︎ This is discussed extensively in chapter 6. ↩︎ This is discussed in chapter 7. ↩︎ This is due to issues identifying the global minimum of \\(RSS\\). In the example in the text, in the presence of collinearity, the global minimum is in a long “valley.” The coefficient estimates are very sensitive to the data – small changes in the data yeild large changes in the estimates. ↩︎ The power of the test is the probability of correctly rejecting \\(H_0: \\beta_i = 0\\), i.e. correctly accepting \\(H_a: \\beta_i \\neq 0\\). Since it uncreases uncertainty of the coefficient estimates, it increases \\(\\hat{se}(\\hat{\\beta_i})\\), hence reduces the \\(t\\)-statistic, making it less likely \\(H_0\\) is rejected. ↩︎ However, the converse is not true – absence of such entries in the sample correlation matrix doesn’t indicate absence of collinearity. The matrix only detects pairwise correlation, and a predictor may correlate two or more other predictors. ↩︎ This is defined the ratio of the (sample) variance of \\(\\hat{\\beta_i}\\) when fitting the full model divided by the variance of \\(\\hat{\\beta_i}\\) when fit on it’s own. It can be computed using the given formula. ↩︎ TEST HERE "],["classification-practice.html", "Section 63 Classification: Practice 63.1 4.3 Logistic Regression 63.2 4.4 Linear Discriminant Analysis", " Section 63 Classification: Practice Load dataset The Default data set 4.3 Logistic Regression 4.4 Linear Discriminant Analysis Lab: 4.6.3 Linear Discriminant Analysis Lab: 4.6.4 Quadratic Discriminant Analysis Lab: 4.6.5 K-Nearest Neighbors Lab: 4.6.6 An Application to Caravan Insurance Data import pandas as pd import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns import sklearn.linear_model as skl_lm from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis from sklearn.metrics import confusion_matrix, classification_report, precision_score from sklearn import preprocessing from sklearn import neighbors import statsmodels.api as sm import statsmodels.formula.api as smf %matplotlib inline plt.style.use(&#39;seaborn-white&#39;) 63.0.1 Load dataset # In R, I exported the dataset from package &#39;ISLR&#39; to an Excel file df = pd.read_excel(&#39;Data/Default.xlsx&#39;) # Note: factorize() returns two objects: a label array and an array with the unique values. # We are only interested in the first object. df[&#39;default2&#39;] = df.default.factorize()[0] df[&#39;student2&#39;] = df.student.factorize()[0] df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } default student balance income default2 student2 1 No No 729.526495 44361.625074 0 0 2 No Yes 817.180407 12106.134700 0 1 3 No No 1073.549164 31767.138947 0 0 63.0.2 Figure 4.1 - Default data set fig = plt.figure(figsize=(12,5)) gs = mpl.gridspec.GridSpec(1, 4) ax1 = plt.subplot(gs[0,:-2]) ax2 = plt.subplot(gs[0,-2]) ax3 = plt.subplot(gs[0,-1]) # Take a fraction of the samples where target value (default) is &#39;no&#39; df_no = df[df.default2 == 0].sample(frac=0.15) # Take all samples where target value is &#39;yes&#39; df_yes = df[df.default2 == 1] df_ = df_no.append(df_yes) ax1.scatter(df_[df_.default == &#39;Yes&#39;].balance, df_[df_.default == &#39;Yes&#39;].income, s=40, c=&#39;orange&#39;, marker=&#39;+&#39;, linewidths=1) ax1.scatter(df_[df_.default == &#39;No&#39;].balance, df_[df_.default == &#39;No&#39;].income, s=40, marker=&#39;o&#39;, linewidths=&#39;1&#39;, edgecolors=&#39;lightblue&#39;, facecolors=&#39;white&#39;, alpha=.6) ax1.set_ylim(ymin=0) ax1.set_ylabel(&#39;Income&#39;) ax1.set_xlim(xmin=-100) ax1.set_xlabel(&#39;Balance&#39;) c_palette = {&#39;No&#39;:&#39;lightblue&#39;, &#39;Yes&#39;:&#39;orange&#39;} sns.boxplot(&#39;default&#39;, &#39;balance&#39;, data=df, orient=&#39;v&#39;, ax=ax2, palette=c_palette) sns.boxplot(&#39;default&#39;, &#39;income&#39;, data=df, orient=&#39;v&#39;, ax=ax3, palette=c_palette) gs.tight_layout(plt.gcf()) png 63.1 4.3 Logistic Regression 63.1.1 Figure 4.2 X_train = df.balance.values.reshape(-1,1) y = df.default2 # Create array of test data. Calculate the classification probability # and predicted classification. X_test = np.arange(df.balance.min(), df.balance.max()).reshape(-1,1) clf = skl_lm.LogisticRegression(solver=&#39;newton-cg&#39;) clf.fit(X_train,y) prob = clf.predict_proba(X_test) fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5)) # Left plot sns.regplot(df.balance, df.default2, order=1, ci=None, scatter_kws={&#39;color&#39;:&#39;orange&#39;}, line_kws={&#39;color&#39;:&#39;lightblue&#39;, &#39;lw&#39;:2}, ax=ax1) # Right plot ax2.scatter(X_train, y, color=&#39;orange&#39;) ax2.plot(X_test, prob[:,1], color=&#39;lightblue&#39;) for ax in fig.axes: ax.hlines(1, xmin=ax.xaxis.get_data_interval()[0], xmax=ax.xaxis.get_data_interval()[1], linestyles=&#39;dashed&#39;, lw=1) ax.hlines(0, xmin=ax.xaxis.get_data_interval()[0], xmax=ax.xaxis.get_data_interval()[1], linestyles=&#39;dashed&#39;, lw=1) ax.set_ylabel(&#39;Probability of default&#39;) ax.set_xlabel(&#39;Balance&#39;) ax.set_yticks([0, 0.25, 0.5, 0.75, 1.]) ax.set_xlim(xmin=-100) png 63.1.2 Table 4.1 y = df.default2 63.1.2.0.1 scikit-learn # Using newton-cg solver, the coefficients are equal/closest to the ones in the book. # I do not know the details on the differences between the solvers. clf = skl_lm.LogisticRegression(solver=&#39;newton-cg&#39;) X_train = df.balance.values.reshape(-1,1) clf.fit(X_train,y) print(clf) print(&#39;classes: &#39;,clf.classes_) print(&#39;coefficients: &#39;,clf.coef_) print(&#39;intercept :&#39;, clf.intercept_) LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class=‘ovr,’ n_jobs=1, penalty=‘l2,’ random_state=None, solver=‘newton-cg,’ tol=0.0001, verbose=0, warm_start=False) classes: [0 1] coefficients: [[ 0.00549891]] intercept : [-10.65131761] 63.1.2.0.2 statsmodels X_train = sm.add_constant(df.balance) est = smf.Logit(y.ravel(), X_train).fit() est.summary2().tables[1] Optimization terminated successfully. Current function value: 0.079823 Iterations 10 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Coef. Std.Err. z P&gt;|z| [0.025 0.975] const -10.651331 0.361169 -29.491287 3.723665e-191 -11.359208 -9.943453 balance 0.005499 0.000220 24.952404 2.010855e-137 0.005067 0.005931 63.1.3 Table 4.2 X_train = sm.add_constant(df.student2) y = df.default2 est = smf.Logit(y, X_train).fit() est.summary2().tables[1] Optimization terminated successfully. Current function value: 0.145434 Iterations 7 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Coef. Std.Err. z P&gt;|z| [0.025 0.975] const -3.504128 0.070713 -49.554094 0.000000 -3.642723 -3.365532 student2 0.404887 0.115019 3.520177 0.000431 0.179454 0.630320 63.1.4 Table 4.3 - Multiple Logistic Regression X_train = sm.add_constant(df[[&#39;balance&#39;, &#39;income&#39;, &#39;student2&#39;]]) est = smf.Logit(y, X_train).fit() est.summary2().tables[1] Optimization terminated successfully. Current function value: 0.078577 Iterations 10 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Coef. Std.Err. z P&gt;|z| [0.025 0.975] const -10.869045 0.492273 -22.079320 4.995499e-108 -11.833882 -9.904209 balance 0.005737 0.000232 24.736506 4.331521e-135 0.005282 0.006191 income 0.000003 0.000008 0.369808 7.115254e-01 -0.000013 0.000019 student2 -0.646776 0.236257 -2.737595 6.189022e-03 -1.109831 -0.183721 63.1.5 Figure 4.3 - Confounding # balance and default vectors for students X_train = df[df.student == &#39;Yes&#39;].balance.values.reshape(df[df.student == &#39;Yes&#39;].balance.size,1) y = df[df.student == &#39;Yes&#39;].default2 # balance and default vectors for non-students X_train2 = df[df.student == &#39;No&#39;].balance.values.reshape(df[df.student == &#39;No&#39;].balance.size,1) y2 = df[df.student == &#39;No&#39;].default2 # Vector with balance values for plotting X_test = np.arange(df.balance.min(), df.balance.max()).reshape(-1,1) clf = skl_lm.LogisticRegression(solver=&#39;newton-cg&#39;) clf2 = skl_lm.LogisticRegression(solver=&#39;newton-cg&#39;) clf.fit(X_train,y) clf2.fit(X_train2,y2) prob = clf.predict_proba(X_test) prob2 = clf2.predict_proba(X_test) df.groupby([&#39;student&#39;,&#39;default&#39;]).size().unstack(&#39;default&#39;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } default No Yes student No 6850 206 Yes 2817 127 # creating plot fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5)) # Left plot ax1.plot(X_test, pd.DataFrame(prob)[1], color=&#39;orange&#39;, label=&#39;Student&#39;) ax1.plot(X_test, pd.DataFrame(prob2)[1], color=&#39;lightblue&#39;, label=&#39;Non-student&#39;) ax1.hlines(127/2817, colors=&#39;orange&#39;, label=&#39;Overall Student&#39;, xmin=ax1.xaxis.get_data_interval()[0], xmax=ax1.xaxis.get_data_interval()[1], linestyles=&#39;dashed&#39;) ax1.hlines(206/6850, colors=&#39;lightblue&#39;, label=&#39;Overall Non-Student&#39;, xmin=ax1.xaxis.get_data_interval()[0], xmax=ax1.xaxis.get_data_interval()[1], linestyles=&#39;dashed&#39;) ax1.set_ylabel(&#39;Default Rate&#39;) ax1.set_xlabel(&#39;Credit Card Balance&#39;) ax1.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1.]) ax1.set_xlim(450,2500) ax1.legend(loc=2) # Right plot sns.boxplot(&#39;student&#39;, &#39;balance&#39;, data=df, orient=&#39;v&#39;, ax=ax2, palette=c_palette); png 63.2 4.4 Linear Discriminant Analysis 63.2.1 Table 4.4 X = df[[&#39;balance&#39;, &#39;income&#39;, &#39;student2&#39;]].as_matrix() y = df.default2.as_matrix() lda = LinearDiscriminantAnalysis(solver=&#39;svd&#39;) y_pred = lda.fit(X, y).predict(X) df_ = pd.DataFrame({&#39;True default status&#39;: y, &#39;Predicted default status&#39;: y_pred}) df_.replace(to_replace={0:&#39;No&#39;, 1:&#39;Yes&#39;}, inplace=True) df_.groupby([&#39;Predicted default status&#39;,&#39;True default status&#39;]).size().unstack(&#39;True default status&#39;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } True default status No Yes Predicted default status No 9645 254 Yes 22 79 print(classification_report(y, y_pred, target_names=[&#39;No&#39;, &#39;Yes&#39;])) precision recall f1-score support No 0.97 1.00 0.99 9667 Yes 0.78 0.24 0.36 333 avg / total 0.97 0.97 0.97 10000 63.2.2 Table 4.5 Instead of using the probability of 50% as decision boundary, we say that a probability of default of 20% is to be classified as ‘Yes.’ decision_prob = 0.2 y_prob = lda.fit(X, y).predict_proba(X) df_ = pd.DataFrame({&#39;True default status&#39;: y, &#39;Predicted default status&#39;: y_prob[:,1] &gt; decision_prob}) df_.replace(to_replace={0:&#39;No&#39;, 1:&#39;Yes&#39;, &#39;True&#39;:&#39;Yes&#39;, &#39;False&#39;:&#39;No&#39;}, inplace=True) df_.groupby([&#39;Predicted default status&#39;,&#39;True default status&#39;]).size().unstack(&#39;True default status&#39;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } True default status No Yes Predicted default status No 9435 140 Yes 232 193 "],["lab.html", "Section 64 Lab", " Section 64 Lab 64.0.1 4.6.3 Linear Discriminant Analysis df = pd.read_csv(&#39;Data/Smarket.csv&#39;, usecols=range(1,10), index_col=0, parse_dates=True) X_train = df[:&#39;2004&#39;][[&#39;Lag1&#39;,&#39;Lag2&#39;]] y_train = df[:&#39;2004&#39;][&#39;Direction&#39;] X_test = df[&#39;2005&#39;:][[&#39;Lag1&#39;,&#39;Lag2&#39;]] y_test = df[&#39;2005&#39;:][&#39;Direction&#39;] lda = LinearDiscriminantAnalysis() pred = lda.fit(X_train, y_train).predict(X_test) lda.priors_ array([ 0.49198397, 0.50801603]) lda.means_ array([[ 0.04279022, 0.03389409], [-0.03954635, -0.03132544]]) # These do not seem to correspond to the values from the R output in the book? lda.coef_ array([[-0.05544078, -0.0443452 ]]) confusion_matrix(y_test, pred).T array([[ 35, 35], [ 76, 106]]) print(classification_report(y_test, pred, digits=3)) precision recall f1-score support Down 0.500 0.315 0.387 111 Up 0.582 0.752 0.656 141 avg / total 0.546 0.560 0.538 252 pred_p = lda.predict_proba(X_test) np.unique(pred_p[:,1]&gt;0.5, return_counts=True) (array([False, True], dtype=bool), array([ 70, 182])) np.unique(pred_p[:,1]&gt;0.9, return_counts=True) (array([False], dtype=bool), array([252])) 64.0.2 4.6.4 Quadratic Discriminant Analysis qda = QuadraticDiscriminantAnalysis() pred = qda.fit(X_train, y_train).predict(X_test) qda.priors_ array([ 0.49198397, 0.50801603]) qda.means_ array([[ 0.04279022, 0.03389409], [-0.03954635, -0.03132544]]) confusion_matrix(y_test, pred).T array([[ 30, 20], [ 81, 121]]) print(classification_report(y_test, pred, digits=3)) precision recall f1-score support Down 0.600 0.270 0.373 111 Up 0.599 0.858 0.706 141 avg / total 0.599 0.599 0.559 252 64.0.3 4.6.5 K-Nearest Neighbors knn = neighbors.KNeighborsClassifier(n_neighbors=1) pred = knn.fit(X_train, y_train).predict(X_test) print(confusion_matrix(y_test, pred).T) print(classification_report(y_test, pred, digits=3)) [[43 58] [68 83]] precision recall f1-score support Down 0.426 0.387 0.406 111 Up 0.550 0.589 0.568 141 avg / total 0.495 0.500 0.497 252 knn = neighbors.KNeighborsClassifier(n_neighbors=3) pred = knn.fit(X_train, y_train).predict(X_test) print(confusion_matrix(y_test, pred).T) print(classification_report(y_test, pred, digits=3)) [[48 55] [63 86]] precision recall f1-score support Down 0.466 0.432 0.449 111 Up 0.577 0.610 0.593 141 avg / total 0.528 0.532 0.529 252 64.0.4 4.6.6 An Application to Caravan Insurance Data 64.0.4.1 K-Nearest Neighbors # In R, I exported the dataset from package &#39;ISLR&#39; to a csv file df = pd.read_csv(&#39;Data/Caravan.csv&#39;) y = df.Purchase X = df.drop(&#39;Purchase&#39;, axis=1).astype(&#39;float64&#39;) X_scaled = preprocessing.scale(X) X_train = X_scaled[1000:,:] y_train = y[1000:] X_test = X_scaled[:1000,:] y_test = y[:1000] def KNN(n_neighbors=1, weights=&#39;uniform&#39;): clf = neighbors.KNeighborsClassifier(n_neighbors, weights) clf.fit(X_train, y_train) pred = clf.predict(X_test) score = clf.score(X_test, y_test) return(pred, score, clf.classes_) def plot_confusion_matrix(cm, classes, n_neighbors, title=&#39;Confusion matrix (Normalized)&#39;, cmap=plt.cm.Blues): plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=plt.cm.Blues) plt.title(&#39;Normalized confusion matrix: KNN-{}&#39;.format(n_neighbors)) plt.colorbar() plt.xticks(np.arange(2), classes) plt.yticks(np.arange(2), classes) plt.tight_layout() plt.xlabel(&#39;True label&#39;,rotation=&#39;horizontal&#39;, ha=&#39;right&#39;) plt.ylabel(&#39;Predicted label&#39;) plt.show() for i in [1,3,5]: pred, score, classes = KNN(i) cm = confusion_matrix(y_test, pred) cm_normalized = cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis] plot_confusion_matrix(cm_normalized.T, classes, n_neighbors=i) cm_df = pd.DataFrame(cm.T, index=classes, columns=classes) cm_df.index.name = &#39;Predicted&#39; cm_df.columns.name = &#39;True&#39; print(cm_df) print(pd.DataFrame(precision_score(y_test, pred, average=None), index=classes, columns=[&#39;Precision&#39;])) png True No Yes Predicted No 882 48 Yes 59 11 Precision No 0.948387 Yes 0.157143 png True No Yes Predicted No 921 53 Yes 20 6 Precision No 0.945585 Yes 0.230769 png True No Yes Predicted No 934 55 Yes 7 4 Precision No 0.944388 Yes 0.363636 64.0.4.2 Logistic Regression regr = skl_lm.LogisticRegression() regr.fit(X_train, y_train) LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class=‘ovr,’ n_jobs=1, penalty=‘l2,’ random_state=None, solver=‘liblinear,’ tol=0.0001, verbose=0, warm_start=False) pred = regr.predict(X_test) cm_df = pd.DataFrame(confusion_matrix(y_test, pred).T, index=regr.classes_, columns=regr.classes_) cm_df.index.name = &#39;Predicted&#39; cm_df.columns.name = &#39;True&#39; print(cm_df) print(classification_report(y_test, pred)) True No Yes Predicted No 935 59 Yes 6 0 precision recall f1-score support No 0.94 0.99 0.97 941 Yes 0.00 0.00 0.00 59 avg / total 0.89 0.94 0.91 1000 pred_p = regr.predict_proba(X_test) cm_df = pd.DataFrame({&#39;True&#39;: y_test, &#39;Pred&#39;: pred_p[:,1] &gt; .25}) cm_df.Pred.replace(to_replace={True:&#39;Yes&#39;, False:&#39;No&#39;}, inplace=True) print(cm_df.groupby([&#39;True&#39;, &#39;Pred&#39;]).size().unstack(&#39;True&#39;).T) print(classification_report(y_test, cm_df.Pred)) Pred No Yes True No 919 22 Yes 48 11 precision recall f1-score support No 0.95 0.98 0.96 941 Yes 0.33 0.19 0.24 59 avg / total 0.91 0.93 0.92 1000 "],["classification-theory.html", "Section 65 Classification: Theory 65.1 An Overview of Classification 65.2 Why Not Linear Regression? 65.3 Logistic Regression 65.4 Linear Discriminant Analysis 65.5 A Comparison of Classification Methods 65.6 Footnotes", " Section 65 Classification: Theory 65.1 An Overview of Classification In classification we consider paired data \\((\\mathbf{X}, Y)\\), where \\(Y\\) is a qualitative variable, that is, a finite random variable. The values \\(Y\\) takes are called classes 65.2 Why Not Linear Regression? Because a linear regression model implies an ordering on the values of the response and in general there is no natural ordering on the values of a qualitative variable 65.3 Logistic Regression 65.3.1 The Logistic Model Consider a quantitiative predictor \\(X\\) binary response variable \\(Y \\in {0,1}\\) We want to model the conditional probability of \\(Y=1\\) given \\(X\\) \\[ P(X) := P\\left(Y=1 | X\\right)\\] We model \\(P(X)\\) with the logistic function \\[ P(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}} \\] The logistic model can be considered a linear model for the log-odds or logit \\[\\log\\left(\\frac{P(X)}{1 - P(X)}\\right) = \\beta_0 + \\beta_1 X\\] 65.3.2 Estimating the Regression Coefficients The likelihood function for the logistic regression parameter \\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1)\\) is \\[\\begin{align*} \\ell(\\boldsymbol{\\beta}) &amp;= \\prod_{i = 1}^n p(x_i)\\\\ &amp;= \\prod_{i: y_i = 1}p(x_i) \\prod_{i: y_i = 0} (1 - p(x_i)) \\end{align*}\\] The maximum likelihood estimate (MLE) for the regression parameter is \\[ \\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\beta}\\in \\mathbb{R}^2}{\\text{argmax}\\,} \\ell(\\boldsymbol{\\beta})\\] There isn’t a closed form solution for \\(\\hat{\\boldsymbol{\\beta}}\\) so it must be found using numerical methods 65.3.3 Making Predictions The MLE \\(\\hat{\\boldsymbol{\\beta}}\\) results in an estimate for the conditional probability \\(\\hat{P}(X)\\) which can be used to predict the class \\(Y\\) 65.3.4 Multiple Logistic Regression Multiple logistic regression considers the case of multiple predictors \\(\\mathbf{X} = (X_1,\\dots, X_p)^\\top\\). If we write the predictors as \\(\\mathbf{X} = (1, X_1, \\dots, X_p)^\\top\\), and the parameter \\(\\boldsymbol(\\beta) = (\\beta_0, \\dots, \\beta_p)^\\top\\) then multiple logistic regression models \\[p(X) = \\frac{\\exp(\\boldsymbol{\\beta}^\\top \\mathbf{X})}{1 + \\exp(\\boldsymbol{\\beta}^\\top \\mathbf{X})} \\] 65.3.5 Logistic Regression for more than two response classes This isn’t used often (a softmax is often used) 65.4 Linear Discriminant Analysis This is a method for modeling the conditional probability of a qualitative response \\(Y\\) given quantitative predictors \\(\\mathbf{X}\\) when \\(Y\\) takes more than two values. It is useful because: Parameter estimates for logistic regression are suprisingly unstable when the classes are well separated, but LDA doesn’t have this problem If \\(n\\) is small and the \\(X_i\\) are approximately normal in the classes (i.e. the conditional \\(X_i | Y = k\\) is approximately normal) LDA is more stable LDA can accomodate more than two clases 65.4.1 Bayes Theorem for Classification Consider a quantitiative input \\(\\mathbf{X}\\) and qualitative response \\(Y \\in {1, \\dots K}\\). Let \\(\\pi_k := \\mathbb{P}(Y = k)\\) be the prior probability that \\(Y=k\\), let \\(p_k(x) := \\mathbb{P}(Y = k\\ |\\ X = x)\\) be the posterior probability that \\(Y = k\\), and let \\(f_k(x):= \\mathbb{P}(X = x\\ |\\ Y = k)\\). Then Bayes’ theorem says: \\[ p_k(x) = \\frac{\\pi_k f_k(x)}{\\sum_{l}\\pi_l f_l(x)} \\] We can form an estimate \\(\\hat{p}_k(x)\\) for \\(p_k(x)\\) with estimates of \\(\\pi_k\\) and \\(f_k(x)\\) for each k, and for \\(x\\) predicts 25 \\[\\hat{y} = \\underset{1 \\leqslant k \\leqslant K}{\\text{argmax}\\,} \\hat{p}_k(x) \\] 65.4.2 Linear Discriminant Analysis for p=1 Assume that the conditional $ X| Y = k (_k, _k^2)$ and that the variances are equal across classes \\(\\sigma_1^2 = \\cdots = \\sigma_K^2 = \\sigma^2\\). The Bayes classifier predicts \\(Y = k\\) where \\(p_k(x)\\) is largest or equivalently \\[\\begin{align*} \\hat{y} &amp;= \\underset{1 \\leqslant k \\leqslant K}{\\text{argmax}\\ } \\delta_k(x)\\\\ \\end{align*}\\] where \\[ \\delta_k(x) := \\left(\\frac{\\mu_k}{\\sigma^2}\\right) - \\frac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k)\\] is the discriminant function26. The LDA classifier 27 estimates the parameters \\[\\begin{align*} \\hat{\\mu}_k &amp;= \\frac{1}{n_k}\\sum_{i: y_i = k} x_i\\\\ \\hat{\\sigma}_k &amp;= \\frac{1}{n-K} \\sum_{k = 1}^K \\sum_{i: y_i = k} \\left(x_i - \\hat{\\mu}_k\\right)^2 \\end{align*}\\] Where \\(n_k\\) is the number of observations in class \\(k\\) 28 and predicts \\[\\begin{align*} \\hat{y} &amp;= \\underset{1 \\leqslant k \\leqslant K}{\\text{argmax}\\ } \\hat{\\sigma}_k(x)) \\\\ \\end{align*}\\] 65.4.3 Linear Discriminant Analysis for p &gt; 1 Assume that the conditional \\((\\mathbf{X}| Y = k) \\sim \\text{Normal}(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\\) and that the covariance matrices are equal across classes \\(\\boldsymbol{\\Sigma}_1 = \\cdots = \\boldsymbol{\\Sigma}_K = \\boldsymbol{\\Sigma}\\). The discriminant functions are \\[\\sigma_k(x) = x^\\top\\boldsymbol{\\Sigma}^{-1}\\mu_k - \\frac{1}{2}\\mu_k {\\Sigma}^{-1} \\mu_k + \\log(\\pi_k)\\] LDA estimates \\(\\boldsymbol{\\mu}_k\\) and \\(\\boldsymbol{\\Sigma}_k\\) componentwise \\[\\begin{align*} (\\hat{\\mu}_k)_j &amp;= \\frac{1}{n_k}\\sum_{i: y_i = k} x_{ij}\\\\ (\\hat{\\sigma}_k)_j &amp;= \\frac{1}{n-K} \\sum_{k = 1}^K \\sum_{i: y_i = k} \\left(x_{ij} - (\\hat{\\mu}_k)_j\\right)^2 \\end{align*}\\] for \\(1 \\leqslant j \\leqslant p\\) as above and predicts \\[\\begin{align*} \\hat{y} &amp;= \\underset{1 \\leqslant k \\leqslant K}{\\text{argmax}\\ } \\hat{\\sigma}_k(x)) \\\\ \\end{align*}\\] as above Confusion matrices help analyze misclassifications for an LDA model 29 The Bayes decision boundary may not be agreeable in every context so sometimes a different decsision boundary (threshold) is used. An ROC curve is useful for vizualising true vs false positives over different decision thresholds in the binary response case. 65.4.4 Quadratic Discriminant Analysis Assume that the conditional \\((\\mathbf{X}| Y = k) \\sim \\text{Normal}(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\\) but assume that the covariance matrices \\(\\boldsymbol{\\Sigma}_k\\) are not equal across classes The discriminant functions are now quadratic in \\(x\\) \\[\\sigma_k(x) = x^\\top\\boldsymbol{\\Sigma}_k^{-1}\\mu_k - \\frac{1}{2}\\mu_k {\\Sigma}_k^{-1} \\mu_k + \\log(\\pi_k)\\] QDA has more degrees of freedom than LDA 30 so generally has lower bias but higher variance. 65.5 A Comparison of Classification Methods So far our classification methods are KNN, logistic regression (LogReg), LDA and QDA LDA and LogReg both produce linear decision boundaries. They often give similar performance results, although LDA tends to outperform when the conditionals \\(X | Y = k\\) are normally distributed, and not when they aren’t As a non-parametric approach, KNN produces a non-linear decision boundary, so tends to outperform LDA and LogReg when the true decision boundary is highly non-linear. It doesn’t help with selecting important predictors With a quadratic decision boundary, QDA is a compromise between the non-linear KNN and the linear LDA/LogReg 65.6 Footnotes Recall that that the Bayes classifier predicts \\[\\hat{y} = \\underset{1 \\leqslant k \\leqslant K}{\\text{argmax}\\,} p_k(x) \\] So we can think of LDA as an esimate of the Bayes Classifier ↩︎ The Bayes decision boundary corresponds to the parameter values \\(\\boldsymbol{\\mu} = (\\mu_1, \\dots, \\mu_K)\\), \\(\\boldsymbol{\\sigma} = (\\sigma_1, \\dots, \\sigma_K)\\) such that \\(\\delta_k(x) = \\delta_j(x)\\) for all \\(1 \\leqslant j,k \\leqslant K\\). The Bayes classifier assigns a class \\(y\\) to an input \\(x\\) based on where \\(x\\) falls with respect to this boundary. For the case \\(K=2\\), this is equivalent to assigning \\(x\\) to class 1 if \\(2x(\\mu_1 - \\mu_2) &gt; \\mu_1^2 - \\mu_2^2\\). ↩︎ The functions \\(\\hat{\\delta}_k(x)\\) are called discriminant functions, and since they’re linear in \\(x\\), the method is called linear discriminant analysis. ↩︎ \\(\\hat{\\mu}_k\\) is the average of all observation inputs in the \\(k\\)-th class, and \\(\\hat{\\sigma}_k\\) is a weighted average of the samples variances over the \\(K\\) classes, ↩︎ False positives and false negatives in the binary case. ↩︎ \\(K\\binom{p}{2}\\) for QDA versus \\(\\binom{p}{2}\\) for LDA. ↩︎ "],["resampling-practice.html", "Section 66 Resampling: Practice 66.1 5.1 Cross-Validation", " Section 66 Resampling: Practice Load dataset Cross-Validation import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import sklearn.linear_model as skl_lm from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split, LeaveOneOut, KFold, cross_val_score from sklearn.preprocessing import PolynomialFeatures %matplotlib inline plt.style.use(&#39;seaborn-white&#39;) 66.0.1 Load dataset Dataset available on http://www-bcf.usc.edu/~gareth/ISL/data.html df1 = pd.read_csv(&#39;Data/Auto.csv&#39;, na_values=&#39;?&#39;).dropna() df1.info() &lt;class ‘pandas.core.frame.DataFrame’&gt; Int64Index: 392 entries, 0 to 396 Data columns (total 9 columns): mpg 392 non-null float64 cylinders 392 non-null int64 displacement 392 non-null float64 horsepower 392 non-null float64 weight 392 non-null int64 acceleration 392 non-null float64 year 392 non-null int64 origin 392 non-null int64 name 392 non-null object dtypes: float64(4), int64(4), object(1) memory usage: 30.6+ KB 66.1 5.1 Cross-Validation 66.1.1 Figure 5.2 - Validation Set Approach Using Polynomial feature generation in scikit-learn http://scikit-learn.org/dev/modules/preprocessing.html#generating-polynomial-features t_prop = 0.5 p_order = np.arange(1,11) r_state = np.arange(0,10) X, Y = np.meshgrid(p_order, r_state, indexing=&#39;ij&#39;) Z = np.zeros((p_order.size,r_state.size)) regr = skl_lm.LinearRegression() # Generate 10 random splits of the dataset for (i,j),v in np.ndenumerate(Z): poly = PolynomialFeatures(int(X[i,j])) X_poly = poly.fit_transform(df1.horsepower.values.reshape(-1,1)) X_train, X_test, y_train, y_test = train_test_split(X_poly, df1.mpg.ravel(), test_size=t_prop, random_state=Y[i,j]) regr.fit(X_train, y_train) pred = regr.predict(X_test) Z[i,j]= mean_squared_error(y_test, pred) fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4)) # Left plot (first split) ax1.plot(X.T[0],Z.T[0], &#39;-o&#39;) ax1.set_title(&#39;Random split of the data set&#39;) # Right plot (all splits) ax2.plot(X,Z) ax2.set_title(&#39;10 random splits of the data set&#39;) for ax in fig.axes: ax.set_ylabel(&#39;Mean Squared Error&#39;) ax.set_ylim(15,30) ax.set_xlabel(&#39;Degree of Polynomial&#39;) ax.set_xlim(0.5,10.5) ax.set_xticks(range(2,11,2)); png 66.1.2 Figure 5.4 p_order = np.arange(1,11) r_state = np.arange(0,10) # LeaveOneOut CV regr = skl_lm.LinearRegression() loo = LeaveOneOut() loo.get_n_splits(df1) scores = list() for i in p_order: poly = PolynomialFeatures(i) X_poly = poly.fit_transform(df1.horsepower.values.reshape(-1,1)) score = cross_val_score(regr, X_poly, df1.mpg, cv=loo, scoring=&#39;neg_mean_squared_error&#39;).mean() scores.append(score) # k-fold CV folds = 10 elements = len(df1.index) X, Y = np.meshgrid(p_order, r_state, indexing=&#39;ij&#39;) Z = np.zeros((p_order.size,r_state.size)) regr = skl_lm.LinearRegression() for (i,j),v in np.ndenumerate(Z): poly = PolynomialFeatures(X[i,j]) X_poly = poly.fit_transform(df1.horsepower.values.reshape(-1,1)) kf_10 = KFold(n_splits=folds, random_state=Y[i,j]) Z[i,j] = cross_val_score(regr, X_poly, df1.mpg, cv=kf_10, scoring=&#39;neg_mean_squared_error&#39;).mean() fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4)) # Note: cross_val_score() method return negative values for the scores. # https://github.com/scikit-learn/scikit-learn/issues/2439 # Left plot ax1.plot(p_order, np.array(scores)*-1, &#39;-o&#39;) ax1.set_title(&#39;LOOCV&#39;) # Right plot ax2.plot(X,Z*-1) ax2.set_title(&#39;10-fold CV&#39;) for ax in fig.axes: ax.set_ylabel(&#39;Mean Squared Error&#39;) ax.set_ylim(15,30) ax.set_xlabel(&#39;Degree of Polynomial&#39;) ax.set_xlim(0.5,10.5) ax.set_xticks(range(2,11,2)); png "],["resampling-theory.html", "Section 67 Resampling: Theory 67.1 Cross-validation 67.2 The Bootstrap 67.3 Footnotes", " Section 67 Resampling: Theory Resampling methods involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model Two of the most commonly used resampling methods are cross-validation and the bootstrap Resampling methods can be useful in model assessment, the process of evaluating a model’s performance, or in model selection, the process of selecting the proper level of flexibility. 67.1 Cross-validation 67.1.1 The Validation Set Approach Randomly divide the data into a training set and validation set. The model is fit on the training set and its prediction performance on the test set provides an estimate of overall performance. In the case of a quantitative response, the prediction performance is measured by the mean-squared-error. The validation estimates the “true” \\(\\text{MSE}\\) with the mean-squared error \\(\\text{MSE}_{validation}\\) computed on the validation set. 67.1.1.0.1 Advantages conceptual simplicity ease of implementation low computational resources 67.1.1.0.2 Disadvantages the validation estimate is highly variable - it is highly dependent on the train/validation set split since the model is trained on a subset of the dataset, it may tend to overestimate the test error rate if it was trained on the entire dataset 67.1.2 Leave-One-Out Cross Validation Given paired observations \\(\\mathcal{D} = \\{(x_1, y_1), \\dots, (x_n, y_n)\\}\\), for each \\(1 \\leqslant i \\leqslant n\\): - Divide the data \\(\\mathcal{D}\\) into a training set \\(\\mathcal{D}_{(i)} = \\mathcal{D}\\ \\{(x_i, y_i)\\}\\) and a validation set \\(\\{(x_i, y_i)\\}\\). - Train a model \\(\\mathcal{M}_i\\) on \\(\\mathcal{D}_{(i)}\\) and use it to predict \\(\\hat{y}_i\\). - The LOOCV estimate for \\(\\text{MSE}_{test}\\) is \\[CV_{(n)} = \\frac{1}{n}\\sum_{i=1}^n \\text{MSE}_i\\] where \\(\\text{MSE}_i = (y_i - \\hat{y}_i)\\)31 67.1.2.0.1 Advantages approximately unbiased deterministic - doesn’t depend on a random train/test split. computationally fast in least squares regression \\[CV_{(n)} = \\frac{1}{n}\\sum_{i=1}^n \\left(\\frac{y_i - \\hat{y}_i}{1 - h_i}\\right)^2\\] where \\(h_i\\) is the leverage of point i 67.1.2.0.2 Disdvantages Computationally expensive32 in general 67.1.3 \\(k\\)-fold Cross-Validation Given paired observations \\(\\mathcal{D} = \\{(x_1, y_1), \\dots, (x_n, y_n)\\}\\), divide the data \\(\\mathcal{D}\\) into \\(K\\) folds (sets) \\(\\mathcal{D}_1, \\dots, \\mathcal{D}_K\\) of roughly equal size.33 Then for each \\(1 \\leqslant k \\leqslant K\\): Train a model on \\(\\mathcal{M}_k\\) on \\(\\cup_{j\\neq k} \\mathcal{D}_{j}\\) and validate on \\(\\mathcal{D}_k\\). The \\(k\\)-fold CV estimate for \\(\\text{MSE}_{test}\\) is \\[CV_{(k)} = \\frac{1}{k}\\sum_{i=1}^k \\text{MSE}_k\\] where \\(\\text{MSE}_k\\) is the mean-squared-error on the validation set \\(\\mathcal{D}_k\\) 67.1.3.0.1 Advantages computationally faster than \\(LOOCV\\) if \\(k &gt; 1\\) less variance than validation set approach or LOOCV 67.1.3.0.2 Disdvantages more biased than LOOCV if \\(k &gt; 1\\). 67.1.4 Bias-Variance Tradeoff for \\(k\\)-fold Cross Validation As \\(k \\rightarrow n\\), bias \\(\\downarrow\\) but variance \\(\\uparrow\\) 67.1.5 Cross-Validation on Classification Problems In the classification setting, we define the LOOCV estimate \\[CV_{(n)} = \\frac{1}{n}\\sum_{i=1}^n \\text{Err}_i\\] where \\(\\text{Err}_i = I(y_i \\neq \\hat{y}_i)\\). The \\(k\\)-fold CV and validation error rates are defined analogously. 67.2 The Bootstrap The bootstrap is a method for estimating the standard error of a statistic34 or statistical learning process. In the case of an estimator \\(\\hat{S}\\) for a statistic \\(S\\) proceeds as follows: Given a dataset \\(\\mathcal{D}\\) with \\(|\\mathcal{D}=n|\\), for \\(1 \\leqslant i \\leqslant B\\): - Create a bootstrap dataset \\(\\mathcal{D}^\\ast_i\\) by sampling uniformly \\(n\\) times from \\(\\mathcal{D}\\) - Calculate the statistic \\(S\\) on \\(\\mathcal{D}^\\ast_i\\) to get a bootstrap estimate \\(S^\\ast_i\\) of \\(S\\) Then the bootstrap estimate for the \\(\\mathbf{se}(S)\\) the sample standard deviation of the boostrap estimates \\(S^\\ast_1, \\dots, S^\\ast_B\\): \\[\\hat{se}(\\hat{S}) = \\sqrt{\\frac{1}{B-1} \\sum_{i = 1}^ B \\left(S^\\ast_i - \\overline{S^\\ast}\\right)^2}\\] 67.3 Footnotes \\(\\text{MSE}_i\\) is just the mean-squared error of the model \\(\\mathcal{M}_i\\) on the validation set \\(\\{(x_i, y_i)\\}\\). It is an approximately unbiased estimator of \\(\\text{MSE}_{test}\\) but it has high variance. But as the average of the \\(\\text{MSE}_i\\), \\(CV_{(n)}\\) has much lower variance.\\ \\(CV_{(n)}\\) is sometimes called the LOOCV error rate – it can be seen as the average error rate over the singleton validation sets \\(\\{(x_i, y_i)\\}\\) ↩︎ Specifically \\(O(n * \\text{model fit time})\\) ↩︎ LOOCV is then \\(k\\)-fold CV in the case \\(k=n\\). Analogous, \\(CV_{k}\\) is sometimes called the \\(k\\)-fold CV error rate, the average error over the folds. ↩︎ Recall a statistic \\(S\\) is just a function of a sample \\(S = S(X_1,\\dots, X_n)\\) ↩︎ "],["linear-model-selection-and-regularization-practice.html", "Section 68 Linear Model Selection and Regularization: Practice", " Section 68 Linear Model Selection and Regularization: Practice Lab 2: Ridge Regression Lab 2: The Lasso Lab 3: Principal Components Regression Lab 3: Partial Least Squares import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import glmnet as gln from sklearn.preprocessing import scale from sklearn import model_selection from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV from sklearn.decomposition import PCA from sklearn.cross_decomposition import PLSRegression from sklearn.model_selection import KFold, cross_val_score from sklearn.metrics import mean_squared_error %matplotlib inline plt.style.use(&#39;seaborn-white&#39;) "],["lab-2.html", "Section 69 Lab 2", " Section 69 Lab 2 # In R, I exported the dataset from package &#39;ISLR&#39; to a csv file. df = pd.read_csv(&#39;Data/Hitters.csv&#39;, index_col=0).dropna() df.index.name = &#39;Player&#39; df.info() &lt;class ‘pandas.core.frame.DataFrame’&gt; Index: 263 entries, -Alan Ashby to -Willie Wilson Data columns (total 20 columns): AtBat 263 non-null int64 Hits 263 non-null int64 HmRun 263 non-null int64 Runs 263 non-null int64 RBI 263 non-null int64 Walks 263 non-null int64 Years 263 non-null int64 CAtBat 263 non-null int64 CHits 263 non-null int64 CHmRun 263 non-null int64 CRuns 263 non-null int64 CRBI 263 non-null int64 CWalks 263 non-null int64 League 263 non-null object Division 263 non-null object PutOuts 263 non-null int64 Assists 263 non-null int64 Errors 263 non-null int64 Salary 263 non-null float64 NewLeague 263 non-null object dtypes: float64(1), int64(16), object(3) memory usage: 43.1+ KB df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI CWalks League Division PutOuts Assists Errors Salary NewLeague Player -Alan Ashby 315 81 7 24 38 39 14 3449 835 69 321 414 375 N W 632 43 10 475.0 N -Alvin Davis 479 130 18 66 72 76 3 1624 457 63 224 266 263 A W 880 82 14 480.0 A -Andre Dawson 496 141 20 65 78 37 11 5628 1575 225 828 838 354 N E 200 11 3 500.0 N -Andres Galarraga 321 87 10 39 42 30 2 396 101 12 48 46 33 N E 805 40 4 91.5 N -Alfredo Griffin 594 169 4 74 51 35 11 4408 1133 19 501 336 194 A W 282 421 25 750.0 A dummies = pd.get_dummies(df[[&#39;League&#39;, &#39;Division&#39;, &#39;NewLeague&#39;]]) dummies.info() print(dummies.head()) &lt;class ‘pandas.core.frame.DataFrame’&gt; Index: 263 entries, -Alan Ashby to -Willie Wilson Data columns (total 6 columns): League_A 263 non-null uint8 League_N 263 non-null uint8 Division_E 263 non-null uint8 Division_W 263 non-null uint8 NewLeague_A 263 non-null uint8 NewLeague_N 263 non-null uint8 dtypes: uint8(6) memory usage: 3.6+ KB League_A League_N Division_E Division_W NewLeague_A Player -Alan Ashby 0 1 0 1 0 -Alvin Davis 1 0 0 1 1 -Andre Dawson 0 1 1 0 0 -Andres Galarraga 0 1 1 0 0 -Alfredo Griffin 1 0 0 1 1 NewLeague_N Player -Alan Ashby 1 -Alvin Davis 0 -Andre Dawson 1 -Andres Galarraga 1 -Alfredo Griffin 0 y = df.Salary # Drop the column with the independent variable (Salary), and columns for which we created dummy variables X_ = df.drop([&#39;Salary&#39;, &#39;League&#39;, &#39;Division&#39;, &#39;NewLeague&#39;], axis=1).astype(&#39;float64&#39;) # Define the feature set X. X = pd.concat([X_, dummies[[&#39;League_N&#39;, &#39;Division_W&#39;, &#39;NewLeague_N&#39;]]], axis=1) X.info() &lt;class ‘pandas.core.frame.DataFrame’&gt; Index: 263 entries, -Alan Ashby to -Willie Wilson Data columns (total 19 columns): AtBat 263 non-null float64 Hits 263 non-null float64 HmRun 263 non-null float64 Runs 263 non-null float64 RBI 263 non-null float64 Walks 263 non-null float64 Years 263 non-null float64 CAtBat 263 non-null float64 CHits 263 non-null float64 CHmRun 263 non-null float64 CRuns 263 non-null float64 CRBI 263 non-null float64 CWalks 263 non-null float64 PutOuts 263 non-null float64 Assists 263 non-null float64 Errors 263 non-null float64 League_N 263 non-null uint8 Division_W 263 non-null uint8 NewLeague_N 263 non-null uint8 dtypes: float64(16), uint8(3) memory usage: 35.7+ KB X.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI CWalks PutOuts Assists Errors League_N Division_W NewLeague_N Player -Alan Ashby 315.0 81.0 7.0 24.0 38.0 39.0 14.0 3449.0 835.0 69.0 321.0 414.0 375.0 632.0 43.0 10.0 1 1 1 -Alvin Davis 479.0 130.0 18.0 66.0 72.0 76.0 3.0 1624.0 457.0 63.0 224.0 266.0 263.0 880.0 82.0 14.0 0 1 0 -Andre Dawson 496.0 141.0 20.0 65.0 78.0 37.0 11.0 5628.0 1575.0 225.0 828.0 838.0 354.0 200.0 11.0 3.0 1 0 1 -Andres Galarraga 321.0 87.0 10.0 39.0 42.0 30.0 2.0 396.0 101.0 12.0 48.0 46.0 33.0 805.0 40.0 4.0 1 0 1 -Alfredo Griffin 594.0 169.0 4.0 74.0 51.0 35.0 11.0 4408.0 1133.0 19.0 501.0 336.0 194.0 282.0 421.0 25.0 0 1 0 69.0.0.1 I executed the R code and downloaded the exact same training/test sets used in the book. X_train = pd.read_csv(&#39;Data/Hitters_X_train.csv&#39;, index_col=0) y_train = pd.read_csv(&#39;Data/Hitters_y_train.csv&#39;, index_col=0) X_test = pd.read_csv(&#39;Data/Hitters_X_test.csv&#39;, index_col=0) y_test = pd.read_csv(&#39;Data/Hitters_y_test.csv&#39;, index_col=0) 69.0.1 6.6.1 Ridge Regression 69.0.2 Scikit-learn The glmnet algorithms in R optimize the objective function using cyclical coordinate descent, while scikit-learn Ridge regression uses linear least squares with L2 regularization. They are rather different implementations, but the general principles are the same. The glmnet() function in R optimizes: ### \\[ \\frac{1}{N}|| X\\beta-y||^2_2+\\lambda\\bigg(\\frac{1}{2}(1−\\alpha)||\\beta||^2_2 \\ +\\ \\alpha||\\beta||_1\\bigg) \\] (See R documentation and https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet_beta.pdf) The function supports L1 and L2 regularization. For just Ridge regression we need to use $= 0 $. This reduces the above cost function to ### \\[ \\frac{1}{N}|| X\\beta-y||^2_2+\\frac{1}{2}\\lambda ||\\beta||^2_2 \\] The sklearn Ridge() function optimizes: ### \\[ ||X\\beta - y||^2_2 + \\alpha ||\\beta||^2_2 \\] which is equivalent to optimizing ### \\[ \\frac{1}{N}||X\\beta - y||^2_2 + \\frac{\\alpha}{N} ||\\beta||^2_2 \\] alphas = 10**np.linspace(10,-2,100)*0.5 ridge = Ridge() coefs = [] for a in alphas: ridge.set_params(alpha=a) ridge.fit(scale(X), y) coefs.append(ridge.coef_) ax = plt.gca() ax.plot(alphas, coefs) ax.set_xscale(&#39;log&#39;) ax.set_xlim(ax.get_xlim()[::-1]) # reverse axis plt.axis(&#39;tight&#39;) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;weights&#39;) plt.title(&#39;Ridge coefficients as a function of the regularization&#39;); png The above plot shows that the Ridge coefficients get larger when we decrease alpha. 69.0.2.1 Alpha = 4 from sklearn.preprocessing import StandardScaler scaler = StandardScaler().fit(X_train) ridge2 = Ridge(alpha=len(X_)*11498/2) ridge2.fit(scaler.transform(X_train), y_train) pred = ridge2.predict(scaler.transform(X_test)) mean_squared_error(y_test, pred) 193147.46143016344 pd.Series(ridge2.coef_.flatten(), index=X.columns) AtBat 0.015146 Hits 0.016050 HmRun 0.013561 Runs 0.015681 RBI 0.016782 Walks 0.019662 Years 0.010390 CAtBat 0.016570 CHits 0.017627 CHmRun 0.015072 CRuns 0.018771 CRBI 0.016697 CWalks 0.016821 PutOuts 0.003228 Assists -0.007600 Errors 0.013672 League_N 0.003519 Division_W 0.003339 NewLeague_N 0.003499 dtype: float64 69.0.2.2 Alpha = \\(10^{10}\\) This big penalty shrinks the coefficients to a very large degree and makes the model more biased, resulting in a higher MSE. ridge2.set_params(alpha=10**10) ridge2.fit(scale(X_train), y_train) pred = ridge2.predict(scale(X_test)) mean_squared_error(y_test, pred) 193253.09741651407 69.0.2.3 Compute the regularization path using RidgeCV ridgecv = RidgeCV(alphas=alphas, scoring=&#39;neg_mean_squared_error&#39;) ridgecv.fit(scale(X_train), y_train) RidgeCV(alphas=array([5.00000e+09, 3.78232e+09, …, 6.60971e-03, 5.00000e-03]), cv=None, fit_intercept=True, gcv_mode=None, normalize=False, scoring=‘neg_mean_squared_error,’ store_cv_values=False) ridgecv.alpha_ 115.5064850041579 ridge2.set_params(alpha=ridgecv.alpha_) ridge2.fit(scale(X_train), y_train) mean_squared_error(y_test, ridge2.predict(scale(X_test))) 97384.92959172592 pd.Series(ridge2.coef_.flatten(), index=X.columns) AtBat 7.576771 Hits 22.596030 HmRun 18.971990 Runs 20.193945 RBI 21.063875 Walks 55.713281 Years -4.687149 CAtBat 20.496892 CHits 29.230247 CHmRun 14.293124 CRuns 35.881788 CRBI 20.212172 CWalks 24.419768 PutOuts 16.128910 Assists -44.102264 Errors 54.624503 League_N 5.771464 Division_W -0.293713 NewLeague_N 11.137518 dtype: float64 69.0.3 python-glmnet (update 2016-08-29) This relatively new module is a wrapper for the fortran library used in the R package glmnet. It gives mostly the exact same results as described in the book. However, the predict() method does not give you the regression coefficients for lambda values not in the lambda_path. It only returns the predicted values. https://github.com/civisanalytics/python-glmnet grid = 10**np.linspace(10,-2,100) ridge3 = gln.ElasticNet(alpha=0, lambda_path=grid) ridge3.fit(X, y) ElasticNet(alpha=0, cut_point=1.0, fit_intercept=True, lambda_path=array([1.00000e+10, 7.56463e+09, …, 1.32194e-02, 1.00000e-02]), max_iter=100000, min_lambda_ratio=0.0001, n_jobs=1, n_lambda=100, n_splits=3, random_state=None, scoring=None, standardize=True, tol=1e-07, verbose=False) 69.0.3.1 Lambda 11498 ridge3.lambda_path_[49] 11497.569953977356 print(&#39;Intercept: {:.3f}&#39;.format(ridge3.intercept_path_[49])) Intercept: 407.356 pd.Series(np.round(ridge3.coef_path_[:,49], decimals=3), index=X.columns) AtBat 0.037 Hits 0.138 HmRun 0.525 Runs 0.231 RBI 0.240 Walks 0.290 Years 1.108 CAtBat 0.003 CHits 0.012 CHmRun 0.088 CRuns 0.023 CRBI 0.024 CWalks 0.025 PutOuts 0.016 Assists 0.003 Errors -0.021 League_N 0.085 Division_W -6.215 NewLeague_N 0.301 dtype: float64 np.sqrt(np.sum(ridge3.coef_path_[:,49]**2)) 6.3606122865384505 69.0.3.2 Lambda 705 ridge3.lambda_path_[59] 705.4802310718645 print(&#39;Intercept: {:.3f}&#39;.format(ridge3.intercept_path_[59])) Intercept: 54.325 pd.Series(np.round(ridge3.coef_path_[:,59], decimals=3), index=X.columns) AtBat 0.112 Hits 0.656 HmRun 1.180 Runs 0.938 RBI 0.847 Walks 1.320 Years 2.596 CAtBat 0.011 CHits 0.047 CHmRun 0.338 CRuns 0.094 CRBI 0.098 CWalks 0.072 PutOuts 0.119 Assists 0.016 Errors -0.704 League_N 13.684 Division_W -54.659 NewLeague_N 8.612 dtype: float64 np.sqrt(np.sum(ridge3.coef_path_[:,59]**2)) 57.11003436702412 69.0.3.3 Fit model using just the training set. ridge4 = gln.ElasticNet(alpha=0, lambda_path=grid, scoring=&#39;mean_squared_error&#39;, tol=1e-12) ridge4.fit(X_train, y_train.values.ravel()) ElasticNet(alpha=0, cut_point=1.0, fit_intercept=True, lambda_path=array([1.00000e+10, 7.56463e+09, …, 1.32194e-02, 1.00000e-02]), max_iter=100000, min_lambda_ratio=0.0001, n_jobs=1, n_lambda=100, n_splits=3, random_state=None, scoring=‘mean_squared_error,’ standardize=True, tol=1e-12, verbose=False) # prediction using lambda = 4 pred = ridge4.predict(X_test, lamb=4) mean_squared_error(y_test.values.ravel(), pred) 101036.83230892917 69.0.3.4 Lambda chosen by cross validation ridge5 = gln.ElasticNet(alpha=0, scoring=&#39;mean_squared_error&#39;) ridge5.fit(X_train, y_train.values.ravel()) ElasticNet(alpha=0, cut_point=1.0, fit_intercept=True, lambda_path=None, max_iter=100000, min_lambda_ratio=0.0001, n_jobs=1, n_lambda=100, n_splits=3, random_state=None, scoring=‘mean_squared_error,’ standardize=True, tol=1e-07, verbose=False) # Lambda with best CV performance ridge5.lambda_max_ 255.04348848905948 # Lambda larger than lambda_max_, but with a CV score that is within 1 standard deviation away from lambda_max_ ridge5.lambda_best_ array([1974.70910641]) plt.figure(figsize=(15,6)) plt.errorbar(np.log(ridge5.lambda_path_), -ridge5.cv_mean_score_, color=&#39;r&#39;, linestyle=&#39;None&#39;, marker=&#39;o&#39;, markersize=5, yerr=ridge5.cv_standard_error_, ecolor=&#39;lightgrey&#39;, capsize=4) for ref, txt in zip([ridge5.lambda_best_, ridge5.lambda_max_], [&#39;Lambda best&#39;, &#39;Lambda max&#39;]): plt.axvline(x=np.log(ref), linestyle=&#39;dashed&#39;, color=&#39;lightgrey&#39;) plt.text(np.log(ref), .95*plt.gca().get_ylim()[1], txt, ha=&#39;center&#39;) plt.xlabel(&#39;log(Lambda)&#39;) plt.ylabel(&#39;Mean-Squared Error&#39;); png # MSE for lambda with best CV performance pred = ridge5.predict(X_test, lamb=ridge5.lambda_max_) mean_squared_error(y_test, pred) 96006.84514850576 69.0.3.5 Fit model to full data set ridge6= gln.ElasticNet(alpha=0, scoring=&#39;mean_squared_error&#39;, n_splits=10) ridge6.fit(X, y) ElasticNet(alpha=0, cut_point=1.0, fit_intercept=True, lambda_path=None, max_iter=100000, min_lambda_ratio=0.0001, n_jobs=1, n_lambda=100, n_splits=10, random_state=None, scoring=‘mean_squared_error,’ standardize=True, tol=1e-07, verbose=False) # These are not really close to the ones in the book. pd.Series(ridge6.coef_path_[:,ridge6.lambda_max_inx_], index=X.columns) AtBat -0.681594 Hits 2.772311 HmRun -1.365704 Runs 1.014812 RBI 0.713030 Walks 3.378558 Years -9.066826 CAtBat -0.001200 CHits 0.136102 CHmRun 0.697992 CRuns 0.295890 CRBI 0.257072 CWalks -0.278966 PutOuts 0.263887 Assists 0.169878 Errors -3.685656 League_N 53.209503 Division_W -122.834334 NewLeague_N -18.102528 dtype: float64 69.0.4 6.6.2 The Lasso 69.0.5 Scikit-learn For both glmnet in R and sklearn Lasso() function the standard L1 penalty is: ### \\[ \\lambda |\\beta|_1 \\] lasso = Lasso(max_iter=10000) coefs = [] for a in alphas*2: lasso.set_params(alpha=a) lasso.fit(scale(X_train), y_train) coefs.append(lasso.coef_) ax = plt.gca() ax.plot(alphas*2, coefs) ax.set_xscale(&#39;log&#39;) ax.set_xlim(ax.get_xlim()[::-1]) # reverse axis plt.axis(&#39;tight&#39;) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;weights&#39;) plt.title(&#39;Lasso coefficients as a function of the regularization&#39;); png lassocv = LassoCV(alphas=None, cv=10, max_iter=10000) lassocv.fit(scale(X_train), y_train.values.ravel()) LassoCV(alphas=None, copy_X=True, cv=10, eps=0.001, fit_intercept=True, max_iter=10000, n_alphas=100, n_jobs=1, normalize=False, positive=False, precompute=‘auto,’ random_state=None, selection=‘cyclic,’ tol=0.0001, verbose=False) lassocv.alpha_ 30.013822564464284 lasso.set_params(alpha=lassocv.alpha_) lasso.fit(scale(X_train), y_train) mean_squared_error(y_test, lasso.predict(scale(X_test))) 102924.90954696963 # Some of the coefficients are now reduced to exactly zero. pd.Series(lasso.coef_, index=X.columns) AtBat 0.000000 Hits 0.000000 HmRun 2.154219 Runs 0.000000 RBI 30.835560 Walks 104.071528 Years -0.000000 CAtBat 0.000000 CHits 0.000000 CHmRun 0.000000 CRuns 132.858095 CRBI 0.000000 CWalks 0.000000 PutOuts 1.896185 Assists -51.058752 Errors 76.779641 League_N 0.000000 Division_W 0.000000 NewLeague_N 0.000000 dtype: float64 69.0.6 python-glmnet lasso2 = gln.ElasticNet(alpha=1, lambda_path=grid, scoring=&#39;mean_squared_error&#39;, n_splits=10) lasso2.fit(X_train, y_train.values.ravel()) ElasticNet(alpha=1, cut_point=1.0, fit_intercept=True, lambda_path=array([1.00000e+10, 7.56463e+09, …, 1.32194e-02, 1.00000e-02]), max_iter=100000, min_lambda_ratio=0.0001, n_jobs=1, n_lambda=100, n_splits=10, random_state=None, scoring=‘mean_squared_error,’ standardize=True, tol=1e-07, verbose=False) l1_norm = np.sum(np.abs(lasso2.coef_path_), axis=0) plt.figure(figsize=(10,6)) plt.plot(l1_norm, lasso2.coef_path_.T) plt.xlabel(&#39;L1 norm&#39;) plt.ylabel(&#39;Coefficients&#39;); png 69.0.6.1 Let glmnet() create a grid to use in CV lasso3 = gln.ElasticNet(alpha=1, scoring=&#39;mean_squared_error&#39;, n_splits=10) lasso3.fit(X_train, y_train.values.ravel()) ElasticNet(alpha=1, cut_point=1.0, fit_intercept=True, lambda_path=None, max_iter=100000, min_lambda_ratio=0.0001, n_jobs=1, n_lambda=100, n_splits=10, random_state=None, scoring=‘mean_squared_error,’ standardize=True, tol=1e-07, verbose=False) plt.figure(figsize=(15,6)) plt.errorbar(np.log(lasso3.lambda_path_), -lasso3.cv_mean_score_, color=&#39;r&#39;, linestyle=&#39;None&#39;, marker=&#39;o&#39;, markersize=5, yerr=lasso3.cv_standard_error_, ecolor=&#39;lightgrey&#39;, capsize=4) for ref, txt in zip([lasso3.lambda_best_, lasso3.lambda_max_], [&#39;Lambda best&#39;, &#39;Lambda max&#39;]): plt.axvline(x=np.log(ref), linestyle=&#39;dashed&#39;, color=&#39;lightgrey&#39;) plt.text(np.log(ref), .95*plt.gca().get_ylim()[1], txt, ha=&#39;center&#39;) plt.xlabel(&#39;log(Lambda)&#39;) plt.ylabel(&#39;Mean-Squared Error&#39;); png pred = lasso3.predict(X_test, lamb=lasso3.lambda_max_) mean_squared_error(y_test, pred) 101294.32852317697 69.0.6.2 Fit model on full dataset lasso4 = gln.ElasticNet(alpha=1, lambda_path=grid, scoring=&#39;mean_squared_error&#39;, n_splits=10) lasso4.fit(X, y) ElasticNet(alpha=1, cut_point=1.0, fit_intercept=True, lambda_path=array([1.00000e+10, 7.56463e+09, …, 1.32194e-02, 1.00000e-02]), max_iter=100000, min_lambda_ratio=0.0001, n_jobs=1, n_lambda=100, n_splits=10, random_state=None, scoring=‘mean_squared_error,’ standardize=True, tol=1e-07, verbose=False) # These are not really close to the ones in the book. pd.Series(lasso4.coef_path_[:,lasso4.lambda_max_inx_], index=X.columns) AtBat -1.560098 Hits 5.693168 HmRun 0.000000 Runs 0.000000 RBI 0.000000 Walks 4.750540 Years -9.518024 CAtBat 0.000000 CHits 0.000000 CHmRun 0.519161 CRuns 0.660407 CRBI 0.391541 CWalks -0.532687 PutOuts 0.272620 Assists 0.174816 Errors -2.056721 League_N 32.109569 Division_W -119.258342 NewLeague_N 0.000000 dtype: float64 "],["lab-3.html", "Section 70 Lab 3", " Section 70 Lab 3 70.0.1 6.7.1 Principal Components Regression Scikit-klearn does not have an implementation of PCA and regression combined like the ‘pls’ package in R. https://cran.r-project.org/web/packages/pls/vignettes/pls-manual.pdf pca = PCA() X_reduced = pca.fit_transform(scale(X)) print(pca.components_.shape) pd.DataFrame(pca.components_.T).loc[:4,:5] (19, 19) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 0 0.198290 -0.383784 0.088626 0.031967 0.028117 -0.070646 1 0.195861 -0.377271 0.074032 0.017982 -0.004652 -0.082240 2 0.204369 -0.237136 -0.216186 -0.235831 0.077660 -0.149646 3 0.198337 -0.377721 -0.017166 -0.049942 -0.038536 -0.136660 4 0.235174 -0.314531 -0.073085 -0.138985 0.024299 -0.111675 The above loadings are the same as in R. print(X_reduced.shape) pd.DataFrame(X_reduced).loc[:4,:5] (263, 19) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 0 -0.009649 1.870522 1.265145 -0.935481 1.109636 1.211972 1 0.411434 -2.429422 -0.909193 -0.264212 1.232031 1.826617 2 3.466822 0.825947 0.555469 -1.616726 -0.857488 -1.028712 3 -2.558317 -0.230984 0.519642 -2.176251 -0.820301 1.491696 4 1.027702 -1.573537 1.331382 3.494004 0.983427 0.513675 The above principal components are the same as in R. # Variance explained by the principal components np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100) array([38.31, 60.15, 70.84, 79.03, 84.29, 88.63, 92.26, 94.96, 96.28, 97.25, 97.97, 98.64, 99.14, 99.46, 99.73, 99.88, 99.95, 99.98, 99.99]) # 10-fold CV, with shuffle n = len(X_reduced) kf_10 = KFold(n_splits=10, shuffle=True, random_state=1) regr = LinearRegression() mse = [] # Calculate MSE with only the intercept (no principal components in regression) score = -1*cross_val_score(regr, np.ones((n,1)), y.ravel(), cv=kf_10, scoring=&#39;neg_mean_squared_error&#39;).mean() mse.append(score) # Calculate MSE using CV for the 19 principle components, adding one component at the time. for i in np.arange(1, 20): score = -1*cross_val_score(regr, X_reduced[:,:i], y.ravel(), cv=kf_10, scoring=&#39;neg_mean_squared_error&#39;).mean() mse.append(score) plt.plot(mse, &#39;-v&#39;) plt.xlabel(&#39;Number of principal components in regression&#39;) plt.ylabel(&#39;MSE&#39;) plt.title(&#39;Salary&#39;) plt.xlim(xmin=-1); png The above plot indicates that the lowest training MSE is reached when doing regression on 18 components. regr_test = LinearRegression() regr_test.fit(X_reduced, y) regr_test.coef_ array([ 106.36859204, -21.60350456, 24.2942534 , -36.9858579 , -58.41402748, 62.20632652, 24.63862038, 15.82817701, 29.57680773, 99.64801199, -30.11209105, 20.99269291, 72.40210574, -276.68551696, -74.17098665, 422.72580227, -347.05662353, -561.59691587, -83.25441536]) 70.0.1.1 Fitting PCA with training data pca2 = PCA() X_reduced_train = pca2.fit_transform(scale(X_train)) n = len(X_reduced_train) # 10-fold CV, with shuffle kf_10 = KFold(n_splits=10, shuffle=False, random_state=1) mse = [] # Calculate MSE with only the intercept (no principal components in regression) score = -1*cross_val_score(regr, np.ones((n,1)), y_train, cv=kf_10, scoring=&#39;neg_mean_squared_error&#39;).mean() mse.append(score) # Calculate MSE using CV for the 19 principle components, adding one component at the time. for i in np.arange(1, 20): score = -1*cross_val_score(regr, X_reduced_train[:,:i], y_train, cv=kf_10, scoring=&#39;neg_mean_squared_error&#39;).mean() mse.append(score) plt.plot(np.array(mse), &#39;-v&#39;) plt.xlabel(&#39;Number of principal components in regression&#39;) plt.ylabel(&#39;MSE&#39;) plt.title(&#39;Salary&#39;) plt.xlim(xmin=-1); png The above plot indicates that the lowest training MSE is reached when doing regression on 6 components. 70.0.1.2 Transform test data with PCA loadings and fit regression on 6 principal components X_reduced_test = pca2.transform(scale(X_test))[:,:7] # Train regression model on training data regr = LinearRegression() regr.fit(X_reduced_train[:,:7], y_train) # Prediction with test data pred = regr.predict(X_reduced_test) mean_squared_error(y_test, pred) 96320.02078250324 70.0.2 6.7.2 Partial Least Squares Scikit-learn PLSRegression gives same results as the pls package in R when using ‘method=’oscorespls.’ In the LAB excercise, the standard method is used which is ‘kernelpls.’ When doing a slightly different fitting in R, the result is close to the one obtained using scikit-learn. pls.fit=plsr(Salary~., data=Hitters, subset=train, scale=TRUE, validation=“CV,” method=‘oscorespls’) validationplot(pls.fit,val.type=“MSEP,” intercept = FALSE) See documentation: http://scikit-learn.org/dev/modules/generated/sklearn.cross_decomposition.PLSRegression.html#sklearn.cross_decomposition.PLSRegression n = len(X_train) # 10-fold CV, with shuffle kf_10 = KFold(n_splits=10, shuffle=False, random_state=1) mse = [] for i in np.arange(1, 20): pls = PLSRegression(n_components=i) score = cross_val_score(pls, scale(X_train), y_train, cv=kf_10, scoring=&#39;neg_mean_squared_error&#39;).mean() mse.append(-score) plt.plot(np.arange(1, 20), np.array(mse), &#39;-v&#39;) plt.xlabel(&#39;Number of principal components in regression&#39;) plt.ylabel(&#39;MSE&#39;) plt.title(&#39;Salary&#39;) plt.xlim(xmin=-1); png pls = PLSRegression(n_components=2) pls.fit(scale(X_train), y_train) mean_squared_error(y_test, pls.predict(scale(X_test))) 102234.27995999217 "],["linear-model-selection-and-regularization-theory.html", "Section 71 Linear Model Selection and Regularization: Theory 71.1 Subset Selection 71.2 Shrinkage Methods 71.3 Dimension Reduction Methods 71.4 Considerations in High Dimensions 71.5 Footnotes", " Section 71 Linear Model Selection and Regularization: Theory Alternatives to the least squares fitting procedures can yield better prediction accuracy model interpretability 71.1 Subset Selection Methods for selecting a subset of the predictors to improve test performance 71.1.1 Best Subset Selection 71.1.1.0.0.1 Algorithm: Best Subset Selection (BSS) for linear regression Let \\(\\mathcal{M}_0\\) denote the null model35 For \\(1 \\leqslant k \\leqslant p\\): Fit all \\(\\binom{p}{k}\\) linear regression models with \\(k\\) predictors Let \\(\\mathcal{M}_k = \\underset{\\text{models}}{\\text{argmin}}\\ RSS\\) Choose the best model \\(\\mathcal{M}_i, 1 \\leqslant i \\leqslant p\\) based on estimated test error 36 For logistic regression, in step 2.A., let \\(\\mathcal{M}_k = \\underset{\\text{models}}{\\text{argmin}}\\ D(y, \\hat{y})\\) where \\(D(y, \\hat{y})\\) is the deviance37 of the model 71.1.1.1 Advantages Slightly faster than brute force. Model evaluation is \\(O(p)\\) as opposed to \\(O(2^p)\\) for brute force. Conceptually simple 71.1.1.2 Disadvantages Still very slow. Fitting is \\(O(2^p)\\) as for brute force Overfitting and high variance of coefficient estimates when \\(p\\) is large 71.1.2 Stepwise Selection 71.1.2.1 Forward Stepwise Selection 71.1.2.1.1 Algorithm: Forward Stepwise Selection (FSS) for linear regression38 Let \\(\\mathcal{M}_0\\) denote the null model For \\(0 \\leqslant k \\leqslant p - 1\\): Fit all \\(p-k\\) linear regression models that augment model \\(\\mathcal{M}_k\\) with one additional predictor Let \\(\\mathcal{M}_{k+1} = \\underset{\\text{models}}{\\text{argmin}}\\ RSS\\) Choose the best model \\(\\mathcal{M}_i, 1 \\leqslant i \\leqslant p\\) based on estimated test error 71.1.2.1.2 Advantages Faster than BSS. Fitting is \\(O(p^2)\\) and evaluation is \\(O(p)\\) Can be applied in the high-dimensional setting \\(n &lt; p\\) 71.1.2.1.3 Disadvantages Evaluation is more challenging since it compares models with different numbers of predictors. Searches less of the parameter space, hence may be suboptimal 71.1.2.2 Backward Stepwise Selection 71.1.2.2.1 Algorithm: Backward Stepwise Selection (BKSS) for linear regression 39 is Let \\(\\mathcal{M}_p\\) denote the full model 40 For \\(k = p, p-1, \\dots, 1\\): Fit all \\(k\\) linear regression models of \\(k-1\\) predictors that contain all but one of the predictors in \\(\\mathcal{M}_k\\). Let \\(\\mathcal{M}_{k-1} = \\underset{\\text{models}}{\\text{argmin}}\\ RSS\\) Choose the best model \\(\\mathcal{M}_i, 1 \\leqslant i \\leqslant p\\) based on estimated test error 71.1.2.2.2 Advantages As fast as FSS 71.1.2.2.3 Disadvantages Same disadvantages as FSS Cannot be used when \\(n &lt; p\\) 71.1.2.3 Hybrid Approaches Other approaches exist which may add variables sequentially (as with FSS) but may also remove variables (as with BSS). These methods strike a balance between optimality (e.g. BSS) and speed (FSS/BSS) 71.1.3 Choosing the Optimal Model Two common approaches to estimating the test error: Estimate indirectly by adjusting the training error to account for overfitting bias Estimate directly using a validation approach 71.1.3.1 \\(C_p\\), AIC, BIC and Adjusted \\(R^2\\) Train MSE underestimates test MSE and decreases as \\(p\\) increases, so it cannot be used to select from models with different numbers of predictors. However we may adjust the training error to account for the model size, and use this to estimate the test MSE For least squares models, the \\(C_p\\) estimate41 of the test MSE for a model with \\(d\\) predictors is \\[ C_p = \\frac{1}{n}(RSS + 2d\\hat{\\sigma}^2) \\] where \\(\\hat{\\sigma} = \\hat{\\mathbb{V}}(\\epsilon)\\). For maximum likelihood models42, the Akaike Information Criterion (AIC) estimate of the test MSE is \\[ AIC = \\frac{1}{n\\hat{\\sigma}^2}(RSS + 2d\\hat{\\sigma}^2) \\] For least squares models, the Bayes Information Criterion (BIC) estimate43 of the test MSE is \\[ BIC = \\frac{1}{n}(RSS + \\log(n)d\\hat{\\sigma}^2) \\] For least squares models, the adjusted \\(R^2\\) statistic44 is \\[AdjR^2 = 1 - \\frac{RSS/(n - d - 1)}{TSS/(n - 1)}\\] 71.1.3.2 Validation and Cross-Validation Instead using adjusted training error to estimate test error indirectly, we can directly estimate using validation or cross-validation In the past this was computationally prohibitive but advances in computation have made this method very attractive. In this approach, we can select a model using the one-standard-error rule, i.e. selecting the model for which the estimated standard error is within one standard error of the \\(p\\) vs. error curve. 71.2 Shrinkage Methods Methods for constraining or regularizing the coefficient estimates, i.e. shrinking them towards zero. This can significantly reduce their variance. 71.2.1 Ridge Regression Ridge regression introduces an \\(L^2\\)-penalty45 for the training error and estimates \\[\\hat{\\beta}^R = RSS+\\lambda\\|\\tilde{\\beta}\\|_2^2\\] where \\(\\lambda\\) is a tuning parameter46 and \\(\\tilde{\\beta} = (\\beta_1, \\dots, \\beta_p)\\)47. The term \\(\\lambda\\|\\beta\\|_2^2\\) is called a shrinkage penalty Selecting a good value for \\(\\lambda\\) is critical, see section 6.2.3 Standardizing the predictors \\(X_i \\mapsto \\frac{X_i - \\mu_i}{s_i}\\) is advised. 71.2.1.0.1 Advantages Takes advantage of bias-variance tradeoff by decreasing flexibility 48 thus decreasing variance. Preferable to least squares in situations when the latter has high variance (close to linear relationship, \\(p \\lesssim n\\) In contrast to least squares, works when \\(p &gt; n\\) 71.2.1.0.2 Disadvantages Lower variance means higher bias. Will not eliminate any predictors which can be an issue for interpretation when \\(p\\) is large. 71.2.2 The Lasso Lasso regression introduces an \\(L^1\\)-penalty 49 for the training error and estimates \\[\\hat{\\beta}^R = RSS+\\lambda\\|\\tilde{\\beta}\\|^2_1\\] 71.2.2.0.1 Advantages Same advantages as ridge regression. Improves over ridge regression by yielding sparse models (i.e. performs variable selection) when \\(\\lambda\\) is sufficiently large 71.2.2.0.2 Disadvantages Lower variance means higher bias. 71.2.2.0.3 Another Formulation for Ridge Regression and the Lasso Ridge Regression is equivalent to the quadratic optimization problem: \\[\\begin{align*} \\min&amp;\\ RSS + \\|\\tilde{\\beta}\\|_2\\\\ \\text{s.t.}&amp;\\ \\| \\tilde{\\beta} \\|_2^2 \\leqslant s \\end{align*}\\] Lasso Regression is equivalent to the quadratic optimization problem: \\[\\begin{align*} \\min&amp;\\ RSS + \\|\\tilde{\\beta}\\|_1\\\\ \\text{s.t.}&amp;\\ \\| \\tilde{\\beta} \\|_1 \\leqslant s \\end{align*}\\] 71.2.2.0.4 Bayesian Interpretation for Ridge and Lasso Regression Given Gaussian errors, and simple assumptions on the prior \\(p(\\beta)\\), ridge and lasso regression emerge as solutions If the \\(\\beta_i \\sim \\text{Normal}(0, h(\\lambda))\\) iid for some function \\(h=h(\\lambda)\\) then the posterior mode for \\(\\beta\\) (i.e. \\(\\underset{\\beta}{\\text{argmax}} p(\\beta| X, Y)\\)) is the ridge regression solution If the \\(\\beta_i \\sim \\text{Laplace}(0, h(\\lambda))\\) iid then the posterior mode is the lasso regression solution. 71.2.3 Selecting the Tuning Parameter Compute the cross-validation error \\(CV_{(n),i}\\) for for a “grid” (evenly-spaced discrete set) of values \\(\\lambda_i\\), and choose \\[ \\lambda = \\underset{i}{\\text{argmin}\\ CV_{(n),i}}\\] 71.3 Dimension Reduction Methods Dimension reduction methods transform the predictors \\(X_1, \\dots, X_p\\) into a smaller set of predictors \\(Z_1, \\dots, Z_M\\), \\(M &lt; p\\). When \\(p &gt;&gt; n\\), \\(M &lt;&lt; p\\) can greatly reduce the variance of the coefficient estimates. In this section we consider linear transformations \\[Z_m = \\sum_{j = 1}^p \\phi_{jm}X_j\\] and a least squares regression model \\[ Y = \\mathbf{Z}\\theta + \\epsilon \\] where \\(\\mathbf{Z} = (1,Z_1, \\dots, Z_M)\\) 71.3.1 Principal Components Regression Principal Components Analysis is a popular unsupervised approach 50 that can be used for dimensional reduction 71.3.1.0.1 An Overview of Principal Components Analysis The principal components of a data matrix \\(n\\times p\\) matrix \\(\\mathbf{X}\\) can be seen (among many different perspectives) as the right singular eigenvectors \\(v_1, \\dots, v_p\\) of the \\(p\\times p\\) sample covariance matrix \\(C\\), i.e. the eigenvectors of \\(C^{\\top}C\\)) ordered by decreasing absolute value of the corresponding eigenvalues. Let \\(\\sigma_1^2,\\dots, \\sigma_k^2\\) be the singular values of \\(C\\) (the squares of the eigenvalues of \\(C^{\\top}C\\)) and let \\(v_1, \\dots, v_p\\) be the corresponding eigenvectors of \\(C\\). Then \\(\\sigma_i^2\\) is the variance of the data along the direction \\(v_i\\), and \\(\\sigma_1^2\\) is the direction of maximal variance. 71.3.1.0.2 The Principal Components Regression Approach Principal Components Regression takes \\(Z_1,\\dots, Z_M\\) to be the first \\(M\\) principal components of \\(\\mathbf{X}\\) and then fits a least squares model on these components. The assumption is that, since the principal components correspond to the directions of greatest variation of the data, they show the most association with \\(Y\\). Furthermore, they are ordered by decreasing magnitude of association. Typically \\(M\\) is chosen by cross-validation. 71.3.1.0.2.1 Advantages If the assumption holds then the least squares model on \\(Z_1, \\dots, Z_M\\) will perform better than \\(X_1, \\dots, X_p\\), since it will contain most of the information related to the response 51, and by choosing \\(M&lt;&lt;p\\) we can mitigate overfitting. Decreased variance of coefficient estimates relative to OLS regression 71.3.1.0.2.2 Disadvantages Is not a feature selection method, since each \\(Z_i\\) is a linear function of the predictors 71.3.1.0.2.3 Recommendations Data should usually be standarized prior to finding the principal components. 71.3.2 Partial Least Squares A supervised dimension reduction method which proceeds roughly as follows Standardize the variables Compute \\(Z_1\\) by setting \\(\\phi_{j1} = \\hat{\\beta_j}\\) the ordinary least squares estimate 52 For $ 1 &lt; m &lt; M$, \\(Z_m\\) is determined by Adjust the data \\(X_j = \\epsilon_j\\) where \\(\\epsilon_j\\) is the residual from regression of \\(Z_{m - 1}\\) onto \\(X_j\\) Compute \\(Z_m\\) in the same fashion as \\(Z_1\\) on the adjusted data As with PCR, \\(M\\) is chosen by cross-validation 71.3.2.0.1 Advantages Decreased variance of coefficient estimates relative to OLS regression Supervised dimension reduction may reduce bias 71.3.2.0.2 Disadvantages May increase variance relative to PCR (which is unsupervised). May be no better than PCR in practice 71.4 Considerations in High Dimensions 71.4.1 High-Dimensional Data Low dimensional means \\(p &lt;&lt; n\\), high dimensional is \\(p \\gtrsim n\\) 71.4.2 What Goes Wrong in High Dimensions? If \\(p \\gtrsim n\\), then linear models will create a perfect fit, hence overfit (usually badly) \\(C_p\\), \\(AIC\\), \\(BIC\\), and \\(R^2\\) approaches don’t work in well in this setting 71.4.3 Regression in High Dimensions Regularization or shrinkage plays a key role in high-dimensional problems. Appropriate tuning parameter selection is crucial for good predictive performance. The test error tends to increase as the dimensionality of the problem increases if the additional features aren’t truly associated with the response (the curse of dimensionality) 71.4.4 Interpreting Results in High Dimensions Multicollinearity problem is maximal in high dimensional setting This makes interpretation difficult, since models obtained from highly multicollinear data fail to identify which features are “preferred” Care must be taken to measure performance 53 71.5 Footnotes This is the model that predicts \\(\\hat{y} = \\overline{y}\\), i.e. \\(\\hat{\\beta_i} = 0\\) for \\(i &gt; 1\\) and \\(\\hat{\\beta_0} = \\overline{y}\\). ↩︎ Estimates of test error can come from CV, \\(C_p (AIC)\\), \\(BIC\\) or adjusted \\(R^2\\) ↩︎ Here \\(D(y, \\hat{y}) = -2\\log(p(y\\ |\\ \\hat{\\beta})\\) where \\(\\hat{\\beta}\\) is the MLE for \\(\\beta\\).The author’s definition of deviance can be found in the comment on the Wikipedia entry if \\(\\hat{\\theta}_0\\) ↩︎ As with BSS, we can use FSS for logistic regression by replacing \\(RSS\\) with the deviance in step 2B. ↩︎ As with BSS, we can use BackSS for logistic regression by replacing \\(RSS\\) with the deviance in step 2B. ↩︎ Here full means contains all \\(p\\) predictors. ↩︎ Thus \\(C_p\\) is RSS plus a penalty which depends on the number of predictors and the estimate of the error variance. One can show that if \\(\\hat{\\sigma}^2\\) is unbiased then then \\(C_p\\) is unbiased. ↩︎ For Gaussian errors, the least squares estimate is the maximumlikelihood estimate so in that case \\(C_p\\) and \\(AIC\\) are proportional. ↩︎ The BIC places a heavier penalty than \\(C_p\\) when \\(n &gt; 7\\) due to the \\(\\log(n)d\\hat{\\sigma}^2\\) term. The book says this means BIC places a heavier penalty than \\(C_p\\) on models with many variables although this isn’t clear. It would seem it places a penalty on large numbers of observation (unless somehow larger numbers of observations are correlated with larger numbers of predictors). ↩︎ \\(C_p, AIC\\) and \\(BIC\\) are all estimates of the test \\(MSE\\) so smaller values are better. By contrast, larger values of adjusted \\(R^2\\), but this is equivalent to minimizing \\(RSS/(n - d - 1)\\) which likely can be thought of as a test MSE estimate. Note that curiously, adjusted \\(R^2\\) is not defined when \\(d = n - 1\\). ↩︎ Here \\(L^2\\) is a reference to the \\(L^p\\) norm (denoted \\(\\| \\cdot \\|_2\\)) when \\(p=2\\) (see also p216), which is just the standard Euclidean norm. ↩︎ The tuning parameter \\(\\lambda\\) is actually a Lagrange multiplier used to turn the constrained optimization problem \\[ \\begin{align*} \\min&amp;\\ RSS\\\\ \\text{s.t.}&amp;\\ \\| \\tilde{\\beta} \\|_2^2 \\leqslant s \\end{align*} \\] into the unconstrained optimization problem \\[ \\begin{align*} \\min&amp;\\ RSS + \\lambda\\| \\tilde{\\beta} \\|_2^2 \\end{align*} \\] see this section ↩︎ We use \\(\\tilde{\\beta}\\) instead of \\(\\beta\\) because we don’t want to shrink the intercept \\(\\beta_0\\). If the data have been centered about their mean then \\(\\hat{\\beta}_0 = \\overline{y}\\) ↩︎ Flexibility decreases because the shrinkage penalty effectively decreases the size of the parameter space? ↩︎ The \\(L^1\\) norm is \\(\\|\\tilde{\\beta}\\|_1 = \\sum_{i=1}^p |\\beta_p|\\) ↩︎ Unsupervised since it only takes the predictors \\(\\mathbf{X}\\) and not the response \\(\\mathbf{Y}\\) as input. ↩︎ Under certain assumptions, PCA is an optimal dimension reduction method from an information theoretic perspective ↩︎ One can show that \\(\\hat{\\beta}_j \\sim \\text{corr}(Y, X_j)\\), so \\(Z_1\\) effectively weights the variables by correlation. The intuition is, that at each iteration, the residuals (hence the variable \\(Z_m\\)) contain information not accounted for by the previous variable \\(Z_{m - 1}\\). ↩︎ For example SSE, \\(p\\)-values, and \\(R^2\\) statistics from the training data are useless in this setting. Thus it is important to e.g. evaluate performance on an independent test set or use resampling methods ↩︎ 71.5.1 blah "],["chapter-7-moving-beyond-linearity.html", "Section 72 Chapter 7 - Moving Beyond Linearity 72.1 Lab", " Section 72 Chapter 7 - Moving Beyond Linearity Lab: 7.8.1 Polynomial Regression and Step Functions Lab: 7.8.2 Splines # %load ../standard_import.txt import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.preprocessing import PolynomialFeatures import statsmodels.api as sm import statsmodels.formula.api as smf from patsy import dmatrix %matplotlib inline plt.style.use(&#39;seaborn-white&#39;) 72.0.1 Load dataset Using write.csv in R, I exported the dataset from package ‘ISLR’ to a csv file. df = pd.read_csv(&#39;Data/Wage.csv&#39;) df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Unnamed: 0 year age sex maritl race education region jobclass health health_ins logwage wage 0 231655 2006 18 Male Never Married White &lt; HS Grad Middle Atlantic Industrial &lt;=Good No 4.318063 75.043154 1 86582 2004 24 Male Never Married White College Grad Middle Atlantic Information &gt;=Very Good No 4.255273 70.476020 2 161300 2003 45 Male Married White Some College Middle Atlantic Industrial &lt;=Good Yes 4.875061 130.982177 df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 3000 entries, 0 to 2999 Data columns (total 13 columns): Unnamed: 0 3000 non-null int64 year 3000 non-null int64 age 3000 non-null int64 sex 3000 non-null object maritl 3000 non-null object race 3000 non-null object education 3000 non-null object region 3000 non-null object jobclass 3000 non-null object health 3000 non-null object health_ins 3000 non-null object logwage 3000 non-null float64 wage 3000 non-null float64 dtypes: float64(2), int64(3), object(8) memory usage: 304.8+ KB 72.1 Lab 72.1.1 7.8.1 Polynomial Regression and Step Functions Create polynomials for ‘age.’ These correspond to those in R, when using raw=TRUE in poly() function. X1 = PolynomialFeatures(1).fit_transform(df.age.values.reshape(-1,1)) X2 = PolynomialFeatures(2).fit_transform(df.age.values.reshape(-1,1)) X3 = PolynomialFeatures(3).fit_transform(df.age.values.reshape(-1,1)) X4 = PolynomialFeatures(4).fit_transform(df.age.values.reshape(-1,1)) X5 = PolynomialFeatures(5).fit_transform(df.age.values.reshape(-1,1)) y = (df.wage &gt; 250).map({False:0, True:1}).values print(&#39;X4:\\n&#39;, X4[:5]) print(&#39;y:\\n&#39;, y[:5]) X4: [[1.000000e+00 1.800000e+01 3.240000e+02 5.832000e+03 1.049760e+05] [1.000000e+00 2.400000e+01 5.760000e+02 1.382400e+04 3.317760e+05] [1.000000e+00 4.500000e+01 2.025000e+03 9.112500e+04 4.100625e+06] [1.000000e+00 4.300000e+01 1.849000e+03 7.950700e+04 3.418801e+06] [1.000000e+00 5.000000e+01 2.500000e+03 1.250000e+05 6.250000e+06]] y: [0 0 0 0 0] 72.1.1.1 Linear regression model. (Degree 4) fit2 = sm.GLS(df.wage, X4).fit() fit2.summary().tables[1] coef std err t P&gt;|t| [0.025 0.975] const -184.1542 60.040 -3.067 0.002 -301.879 -66.430 x1 21.2455 5.887 3.609 0.000 9.703 32.788 x2 -0.5639 0.206 -2.736 0.006 -0.968 -0.160 x3 0.0068 0.003 2.221 0.026 0.001 0.013 x4 -3.204e-05 1.64e-05 -1.952 0.051 -6.42e-05 1.45e-07 Selecting a suitable degree for the polynomial of age. fit_1 = sm.GLS(df.wage, X1).fit() fit_2 = sm.GLS(df.wage, X2).fit() fit_3 = sm.GLS(df.wage, X3).fit() fit_4 = sm.GLS(df.wage, X4).fit() fit_5 = sm.GLS(df.wage, X5).fit() sm.stats.anova_lm(fit_1, fit_2, fit_3, fit_4, fit_5, typ=1) /Users/jordi/anaconda3/envs/jwv/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater return (self.a &lt; x) &amp; (x &lt; self.b) /Users/jordi/anaconda3/envs/jwv/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less return (self.a &lt; x) &amp; (x &lt; self.b) /Users/jordi/anaconda3/envs/jwv/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1821: RuntimeWarning: invalid value encountered in less_equal cond2 = cond0 &amp; (x &lt;= self.a) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } df_resid ssr df_diff ss_diff F Pr(&gt;F) 0 2998.0 5.022216e+06 0.0 NaN NaN NaN 1 2997.0 4.793430e+06 1.0 228786.010128 143.593107 2.363850e-32 2 2996.0 4.777674e+06 1.0 15755.693664 9.888756 1.679202e-03 3 2995.0 4.771604e+06 1.0 6070.152124 3.809813 5.104620e-02 4 2994.0 4.770322e+06 1.0 1282.563017 0.804976 3.696820e-01 The polynomial degree 4 seems best. X = X4 Scikit-learn implements a regularized logistic regression model particularly suitable for high dimensional data. Since we just have one feature (age) we use the GLM model from statsmodels. clf = sm.GLM(y, X, family=sm.families.Binomial(sm.families.links.logit)) res = clf.fit() Create array of test data. Transform to polynomial degree 4 and run prediction. age_grid = np.arange(df.age.min(), df.age.max()).reshape(-1,1) X_test = PolynomialFeatures(4).fit_transform(age_grid) pred = res.predict(X_test) 72.1.2 Figure 7.1 # creating plots fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5)) fig.suptitle(&#39;Degree-4 Polynomial&#39;, fontsize=14) # Scatter plot with polynomial regression line ax1.scatter(df.age, df.wage, facecolor=&#39;None&#39;, edgecolor=&#39;k&#39;, alpha=0.3) sns.regplot(df.age, df.wage, order = 4, truncate=True, scatter=False, ax=ax1) ax1.set_ylim(ymin=0) # Logistic regression showing Pr(wage&gt;250) for the age range. ax2.plot(age_grid, pred, color=&#39;b&#39;) # Rug plot showing the distribution of wage&gt;250 in the training data. # &#39;True&#39; on the top, &#39;False&#39; on the bottom. ax2.scatter(df.age, y/5, s=30, c=&#39;grey&#39;, marker=&#39;|&#39;, alpha=0.7) ax2.set_ylim(-0.01,0.21) ax2.set_xlabel(&#39;age&#39;) ax2.set_ylabel(&#39;Pr(wage&gt;250|age)&#39;); png 72.1.2.1 Step function df_cut, bins = pd.cut(df.age, 4, retbins=True, right=True) df_cut.value_counts(sort=False) (17.938, 33.5] 750 (33.5, 49.0] 1399 (49.0, 64.5] 779 (64.5, 80.0] 72 Name: age, dtype: int64 df_steps = pd.concat([df.age, df_cut, df.wage], keys=[&#39;age&#39;,&#39;age_cuts&#39;,&#39;wage&#39;], axis=1) df_steps.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age age_cuts wage 0 18 (17.938, 33.5] 75.043154 1 24 (17.938, 33.5] 70.476020 2 45 (33.5, 49.0] 130.982177 3 43 (33.5, 49.0] 154.685293 4 50 (49.0, 64.5] 75.043154 # Create dummy variables for the age groups df_steps_dummies = pd.get_dummies(df_steps[&#39;age_cuts&#39;]) # Statsmodels requires explicit adding of a constant (intercept) df_steps_dummies = sm.add_constant(df_steps_dummies) df_steps_dummies.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } const (17.938, 33.5] (33.5, 49.0] (49.0, 64.5] (64.5, 80.0] 0 1.0 1 0 0 0 1 1.0 1 0 0 0 2 1.0 0 1 0 0 3 1.0 0 1 0 0 4 1.0 0 0 1 0 # Using statsmodels because it has a more complete output for coefficients fit3 = sm.GLM(df_steps.wage, df_steps_dummies.drop(df_steps_dummies.columns[1], axis=1)).fit() fit3.summary().tables[1] coef std err z P&gt;|z| [0.025 0.975] const &lt;td&gt; 94.1584&lt;/td&gt; &lt;td&gt; 1.476&lt;/td&gt; &lt;td&gt; 63.790&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 91.265&lt;/td&gt; &lt;td&gt; 97.051&lt;/td&gt; (33.5, 49.0] 24.0535 1.829 13.148 0.000 20.468 27.639 (49.0, 64.5] 23.6646 2.068 11.443 0.000 19.611 27.718 (64.5, 80.0] 7.6406 4.987 1.532 0.126 -2.135 17.416 # Put the test data in the same bins as the training data. bin_mapping = np.digitize(age_grid.ravel(), bins) bin_mapping array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]) # Get dummies, drop first dummy category, add constant X_test2 = sm.add_constant(pd.get_dummies(bin_mapping).drop(1, axis=1)) X_test2.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } const 2 3 4 0 1.0 0 0 0 1 1.0 0 0 0 2 1.0 0 0 0 3 1.0 0 0 0 4 1.0 0 0 0 72.1.2.2 Linear Regression pred2 = fit3.predict(X_test2) 72.1.2.3 Logistic Regression clf2 = sm.GLM(y, df_steps_dummies.drop(df_steps_dummies.columns[1], axis=1), family=sm.families.Binomial(sm.families.links.logit)) res2 = clf2.fit() pred3 = res2.predict(X_test2) 72.1.3 Figure 7.2 # creating plots fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5)) fig.suptitle(&#39;Piecewise Constant&#39;, fontsize=14) # Scatter plot with polynomial regression line ax1.scatter(df.age, df.wage, facecolor=&#39;None&#39;, edgecolor=&#39;k&#39;, alpha=0.3) ax1.plot(age_grid, pred2, c=&#39;b&#39;) ax1.set_xlabel(&#39;age&#39;) ax1.set_ylabel(&#39;wage&#39;) ax1.set_ylim(ymin=0) # Logistic regression showing Pr(wage&gt;250) for the age range. ax2.plot(np.arange(df.age.min(), df.age.max()).reshape(-1,1), pred3, color=&#39;b&#39;) # Rug plot showing the distribution of wage&gt;250 in the training data. # &#39;True&#39; on the top, &#39;False&#39; on the bottom. ax2.scatter(df.age, y/5, s=30, c=&#39;grey&#39;, marker=&#39;|&#39;, alpha=0.7) ax2.set_ylim(-0.01,0.21) ax2.set_xlabel(&#39;age&#39;) ax2.set_ylabel(&#39;Pr(wage&gt;250|age)&#39;); png 72.1.4 7.8.2 Splines Using patsy to create non-linear transformations of the input data. See http://patsy.readthedocs.org/en/latest/ I have not found functions to create smoothing splines or GAMs or do local regression. 72.1.4.1 Cubic splines # Specifying 3 knots transformed_x = dmatrix(&quot;bs(df.age, knots=(25,40,60), degree=3, include_intercept=False)&quot;, {&quot;df.age&quot;: df.age}, return_type=&#39;dataframe&#39;) fit4 = sm.GLM(df.wage, transformed_x).fit() pred4 = fit4.predict(dmatrix(&quot;bs(age_grid, knots=(25,40,60), degree=3, include_intercept=False)&quot;, {&quot;age_grid&quot;: age_grid}, return_type=&#39;dataframe&#39;)) fit4.params Intercept 60.493714 bs(df.age, knots=(25, 40, 60), degree=3, include_intercept=False)[0] 3.980500 bs(df.age, knots=(25, 40, 60), degree=3, include_intercept=False)[1] 44.630980 bs(df.age, knots=(25, 40, 60), degree=3, include_intercept=False)[2] 62.838788 bs(df.age, knots=(25, 40, 60), degree=3, include_intercept=False)[3] 55.990830 bs(df.age, knots=(25, 40, 60), degree=3, include_intercept=False)[4] 50.688098 bs(df.age, knots=(25, 40, 60), degree=3, include_intercept=False)[5] 16.606142 dtype: float64 # Specifying 6 degrees of freedom transformed_x2 = dmatrix(&quot;bs(df.age, df=6, degree=3, include_intercept=False)&quot;, {&quot;df.age&quot;: df.age}, return_type=&#39;dataframe&#39;) fit5 = sm.GLM(df.wage, transformed_x2).fit() pred5 = fit5.predict(dmatrix(&quot;bs(age_grid, df=6, degree=3, include_intercept=False)&quot;, {&quot;age_grid&quot;: age_grid}, return_type=&#39;dataframe&#39;)) fit5.params Intercept 56.313841 bs(df.age, df=6, degree=3, include_intercept=False)[0] 27.824002 bs(df.age, df=6, degree=3, include_intercept=False)[1] 54.062546 bs(df.age, df=6, degree=3, include_intercept=False)[2] 65.828391 bs(df.age, df=6, degree=3, include_intercept=False)[3] 55.812734 bs(df.age, df=6, degree=3, include_intercept=False)[4] 72.131473 bs(df.age, df=6, degree=3, include_intercept=False)[5] 14.750876 dtype: float64 72.1.4.2 Natural splines # Specifying 4 degrees of freedom transformed_x3 = dmatrix(&quot;cr(df.age, df=4)&quot;, {&quot;df.age&quot;: df.age}, return_type=&#39;dataframe&#39;) fit6 = sm.GLM(df.wage, transformed_x3).fit() pred6 = fit6.predict(dmatrix(&quot;cr(age_grid, df=4)&quot;, {&quot;age_grid&quot;: age_grid}, return_type=&#39;dataframe&#39;)) fit6.params Intercept 79.642095 cr(df.age, df=4)[0] -14.667784 cr(df.age, df=4)[1] 36.811142 cr(df.age, df=4)[2] 35.934874 cr(df.age, df=4)[3] 21.563863 dtype: float64 plt.scatter(df.age, df.wage, facecolor=&#39;None&#39;, edgecolor=&#39;k&#39;, alpha=0.3) plt.plot(age_grid, pred4, color=&#39;b&#39;, label=&#39;Specifying three knots&#39;) plt.plot(age_grid, pred5, color=&#39;r&#39;, label=&#39;Specifying df=6&#39;) plt.plot(age_grid, pred6, color=&#39;g&#39;, label=&#39;Natural spline df=4&#39;) [plt.vlines(i , 0, 350, linestyles=&#39;dashed&#39;, lw=2, colors=&#39;b&#39;) for i in [25,40,60]] plt.legend(bbox_to_anchor=(1.5, 1.0)) plt.xlim(15,85) plt.ylim(0,350) plt.xlabel(&#39;age&#39;) plt.ylabel(&#39;wage&#39;); png Table of Contents 7 Moving Beyond Linearity 7.1 Polynomial Regression 7.2 Step Functions 7.3 Basis Functions 7.4 Regression Splines 7.4.1 Piecewise Polynomials 7.4.2 Constraints and Splines 7.4.3 The Spline Basis Representation 7.4.4 Choosing the Number and the Locations of the Knots 7.4.5 Comparison to Polynomial Regression 7.5 Smoothing Splines 7.5.1 An Overview of Smoothing Splines 7.5.2 Choosing the Smoothing Parameter \\(\\lambda\\) 7.6 Local Regression 7.7 Generalized Additive Models 7.7.1 GAMs for Regression Problems 7.7.2 GAMs for Classification Problems 7.8 Footnotes "],["moving-beyond-linearity.html", "Section 73 Moving Beyond Linearity 73.1 Polynomial Regression 73.2 Step Functions 73.3 Basis Functions 73.4 Regression Splines 73.5 Smoothing Splines 73.6 Local Regression 73.7 Generalized Additive Models 73.8 Footnotes", " Section 73 Moving Beyond Linearity 73.1 Polynomial Regression Simple polynomial regression is a regression model which is polynomial54 in the feature variable X \\[Y = \\beta_0 + \\sum_{i = 1}^d \\beta_iX^d\\] - The model can be fit as a simple linear regression model with predictors \\(X_1, \\dots, X_d = X, \\dots X^d\\). - It is rare to take \\(d \\geqslant 4\\) because it lead strange curves 73.1.0.0.1 Advantages Interpretability More flexibility than linear regression, can better model non-linear relationships 73.1.0.0.2 Disadvantages Greater flexibility can lead to overfitting (can be mitigating by keeping \\(d\\) low) Imposes global structure on target function (as does linear regression) 73.2 Step Functions Step functions model the target function as locally constant by converting the continuous variable \\(X\\) into an ordered categorical variable.as follows Choose \\(K\\) points \\(c_1, \\dots, c_K \\in [\\min(X), \\max(X)]\\) Define \\(K + 1\\) “dummy” variables \\[\\begin{align*} C_0(X) &amp;= I(X &lt; c_1)\\\\ C_i(X) &amp;= I(c_i \\leqslant X &lt; c_{i+1})\\qquad 1 \\leqslant i \\leqslant K - 1\\\\ C_K(X) &amp;= I(c_K \\leqslant X) \\end{align*}\\] Fit a linear regression model to the predictors \\(C_1, \\dots, C_K\\)55 73.2.0.0.1 Advantages Flexibility to model non-linear relationships Can model local behavior better than global models (e.g. linear and polynomial regression) 73.2.0.0.2 Disadvantages Locally constant assumption is strong, breakpoints in data may not be realized. 73.3 Basis Functions In general, we can fit a regression model \\[Y = \\beta_0 + \\sum_{i=1}^Kb_i(X)\\] where the \\(b_i(X)\\) are called basis functions 56 73.3.0.0.1 Advantages Different choices of basis functions are useful for modeling different types of relationships (for example, Fourier basis functions can model periodic behavior). 73.3.0.0.2 Disadvantages As usual, greater flexibility can lead to overfitting Some choices of basis functions (i.e. basis functions which are not suited to the assumed true functional relationship) will likely have poor performance. 73.4 Regression Splines Regression splines are a flexible (and common choice of) class of basis functions which extend both polynomial and piecewise constant basis functions. 73.4.1 Piecewise Polynomials Piecewise polynomials fit separate low-degree polynomials over different regions of \\(X\\). The points where the coefficients change are called knots. 73.4.1.0.1 Advantages Flexibility to model non-linear relationships (as with all non-linear methods discussed in this chapter) Sensitivity to local behavior (less rigid than global model). 73.4.1.0.2 Disadvantages Overly flexible - each piece has independent degrees of freedom Can have unnatural breaks at knots without appropriate constraints Possibility of overfitting (as with all non-linear methods discussed in this chapter) 73.4.2 Constraints and Splines To remedy overflexibility of piecewise polynomials, we can impose constraints at the knots, e.g. continuity, differentiability of various orders (smoothness). A spline is a piecewise degree \\(d\\) polynomial that has continuous derivatives up to order \\(d-1\\) at each knot (hence everywhere). 73.4.2.0.1 Advantages Same advantages to piecewise polynomials, while improving on the disadvantages 73.4.2.0.2 Disadvantages Overfitting Poor match to the true relationship 73.4.3 The Spline Basis Representation Regression splines can be modeled using an appropriate basis, of which there are many choices. For example, we can model a \\(d\\) degree spline with \\(K\\) knots using truncated power basis \\[b_1(X), \\dots, b_{K+d}(X) = x, \\dots, x^d, h(X, \\xi_1), \\dots, h(X, \\xi_K)\\] where \\(\\xi_i\\) is the \\(i-th\\) knot and \\[h(X - \\xi_i) = \\begin{cases} (X-\\xi_i)^d &amp; X &gt; \\xi_i\\\\ 0 &amp; X \\leqslant \\xi_i \\end{cases}\\] is the truncated power function of degree \\(d\\). 73.4.3.0.1 Advantages Ibid. 73.4.3.0.2 Disadvantages Beyond those mentioned above, splines can have a high variance near \\(\\min(X), \\max(X)\\) (this can be overcome by using natural splines which impose boundary constraints, i.e constraints on the form of the model on \\([\\min(X), \\xi_1]\\), \\([\\max(X), \\xi_K]\\) (e.g. linearity) 73.4.4 Choosing the Number and the Locations of the Knots In practice, we place knots in uniform fashion, e.g. by specifying the desired degrees of freedom and using software to place the knots at uniform quantiles of the data. The desired degrees of freedom (hence number of knots) can be obtained using cross-validation. 73.4.5 Comparison to Polynomial Regression Often gives superior results to polynomial regression – the latter must use higher degrees (imposing global structure) while the former can increase the number of knots while leaving the degree fixed (sensitivity to local behavior) as well as varying the density of knots (i.e. placing more where the response varies rapidly, less where it is more stable) 73.5 Smoothing Splines 73.5.1 An Overview of Smoothing Splines A smoothing spline 57 is a function \\[\\hat{g}_\\lambda = \\underset{g}{\\text{argmin}\\,}\\sum_{i=1}^n(y_i - g(x_i))^2 + \\lambda \\int g&#39;&#39;(t)^2\\,dt\\] where \\(\\lambda = 0\\) is a tuning parameter58 - \\(\\lambda\\) controls the bias-variance tradeoff. \\(\\lambda = 0\\) corresponds to the interpolation spline which fits all the data points exactly and will be thus woefull overfit. In the limit \\(\\lambda \\rightarrow \\infty\\), \\(\\hat{g}_\\lambda\\) approaches the least squares line - It can be show that the function \\(\\hat{g}_\\lambda\\) is a piecewise cubic polynomial with knots at the unique \\(x_i\\) and continuous first and second derivatives at the knots 59 73.5.2 Choosing the Smoothing Parameter \\(\\lambda\\) The parameter \\(\\lambda\\) controls the effective degrees of freedom \\(df_{\\lambda}\\). As \\(\\lambda\\) goes from $0 $ to \\(\\infty\\), \\(df_\\lambda\\) goes from \\(n\\) to \\(2\\). The effective degress of freedom is defined to be \\[df_\\lambda = \\text{trace}(S_\\lambda)\\] where \\(S_\\lambda\\) is the matrix such that \\(\\mathbf{\\hat{g}}_\\lambda = S_\\lambda \\mathbf{y}\\) where \\(\\mathbf{\\hat{g}}\\) is the vector of fitted values. \\(\\lambda\\) can be chosen by cross-validation. LOOCV is particularly efficient to compute 60 \\[RSS_{cv}(\\lambda) = \\sum_{i=1}^n (y_i - \\hat{g}_\\lambda^{(-i)}(x_i))^2 = \\sum_{i=1}^n\\left(\\frac{y_i - \\hat{g}_\\lambda(x_i)}{1-tr(S_{\\lambda})}\\right)^2 \\] 73.5.2.0.1 Advantages Flexibility/nonlinearity As a shrinkage method, effective degrees of freedom are reduced, helping to balance bias-variance tradeoff and avoid overfitting. 73.5.2.0.2 Disadvantages As usual, flexibility can lead to overfitting 73.6 Local Regression Computes the fit at a target point by regressing on nearby training observations Is memory-based - all the training data is necessary for computing a prediction In multiple linear regression, variable coefficient models fit global regression to some variables and local to others 73.6.0.0.1 Algorithm: \\(K\\)-nearest neighbors regression Fix the parameter61 \\(1 \\leqslant k \\leqslant n\\). For each \\(X_=x_0\\): 1. Get the neighborhood \\(N_{i0}= \\{k\\ \\text{closest}\\ x_i\\}\\). 2. Assign a weight \\(K_{i0} = K(x_i, x_0)\\) to each point \\(x_i\\) such that such that - each point outside \\(x_i\\notin N_{i0}\\) has \\(K_{i0}(x_i)=0\\). - the furthest point \\(x_i\\in N_{i0}\\) has weight zero - the closest point \\(x_i\\in N_{i0}\\) has the highest weight. 3. Fit a weighted least squares regression \\[ (\\hat{\\beta_0}, \\hat{\\beta_1}) = \\sum_{i=1}^nK_{i0}(y_i - \\beta_0 - \\beta_1 x_i)^2\\] 4. Predict \\(\\hat{f}(x_0) = \\hat{\\beta_0} + \\hat{\\beta_1}x_0\\). 73.7 Generalized Additive Models A Generalized additive model is a model which is a sum of nonlinear functions of the individual predictors. 73.7.1 GAMs for Regression Problems A GAM for regression 62 is a model \\[Y =\\beta_0 + \\sum_{j=1}^p f_j(X_j) + \\epsilon\\] where the functions \\(f_j\\) are smooth non-linear functions. GAMs can be used to combine methods from this chapter – one can fit different nonlinear functions \\(f_j\\) to the predictors \\(X_j\\) 63 Standard software can fit GAMs with smoothing splines via backfitting 73.7.1.0.1 Advantages Nonlinearity hence flexibility Automatically introduces nonlinearity - obviates the need to experiment with different nonlinear transformations Interpretability/inference - the \\(f_j\\) allow to consider the effect of each feature \\(X_j\\) independently of the others. Smoothness of individual \\(f_j\\) can be summarized via degrees of freedom. Represents a nice compromise betwee linear and fully non-parametric models (see §8). 73.7.1.0.2 Disadvantages Usual disadvantages of nonlinearity Doesn’t allow for interactions between features (this can be overcome by including nonlinear functios of the interaction terms \\(f(X_j,X_k)\\) The additive constraint is strong, restricts flexibility. 73.7.2 GAMs for Classification Problems GAMs can be used for classification. For example, a GAM for logistic regression is \\[\\log\\left(\\frac{p_k(X)}{1 - p_k(X)}\\right) =\\beta_0 + \\sum_{j=1}^p f_j(X_j) + \\epsilon\\] where \\(p_k(X) =\\text{Pr}(Y = k\\ |\\ X)\\). 73.8 Footnotes In statistical literature, polynomial regression is sometimes referred to as linear regression. This is because the model is linear in the population parameters \\(\\beta_i\\). ↩︎ The variable \\(C_0(X)\\) accounts for an intercept. Alternatively fit a linear model to \\(C_0, \\dots, C_K\\) with no intercept. ↩︎ Such a model amounts to the assumption that the target function lives in a finite-dimensional subspace of the vector space of all functions \\(f:X\\rightarrow Y\\). ↩︎ The function \\(g\\) is not guaranteed to be smooth in the sense of infinitely differentiable. The penalty on the second derivative (curvature) penalizes the “roughness” or “wiggliness” of \\(g\\), hence “smoothes out” noise in the data. Other penalties have been used ↩︎ A tuning parameter is also called a hyperparameter ↩︎ Thus \\(\\hat{g}\\) is a natural cubic spline with knots at the \\(x_i\\). However, it is not the spline one obtains in §7.4.3. It is a “shrunken” version, where \\(\\lambda\\) controls the shrinkage. ↩︎ Compare to a similar formula in §5.1.2 ↩︎ Our description of the algorithm deviates a bit from the book, but it’s equivalent. ↩︎ “Additive” because we are summing the \\(f_i\\). “Generalized” because it generalizes from the linear functions \\(\\beta_jX_j\\) in ordinary linear regression. ↩︎ It’s not hard to see that (with the exception of local regression), all the models discussed in this chapter can be seen as special cases of GAM. ↩︎ "],["tree-based-methods-theory.html", "Section 74 Tree Based Methods: Theory 74.1 The Basics of Decision Trees 74.2 Bagging, Random Forests, Boosting 74.3 Footnotes", " Section 74 Tree Based Methods: Theory 74.1 The Basics of Decision Trees 74.1.1 Regression Trees 74.1.1.0.1 Overview There are two main steps: Partition predictor space \\(\\mathbb{R}^p\\) into regions \\(R_1, \\dots, R_M\\). For all \\(X = (X_1, \\dots, X_p) \\in R_m\\), predict the average over the responses in \\(R_m\\) \\[\\hat{f}(X) = \\hat{y}_{R_m} := \\frac{1}{N_m}\\sum_{i: y_i\\in R_m} y_i\\] where \\(N_m = |\\{y_i\\ |\\ y_i \\in R_m\\}|\\) In practice, we take the regions of the partition to be rectangular for simplicity and ease of interpretation. We choose the partition to minimize the RSS \\[ \\sum_{m = 1}^M \\sum_{i: y_i \\in R_m} (y_i - \\hat{y}_{R_m})^2 \\] We search the space of partitions using a recursive binary splitting64 strategy. 74.1.1.0.2 Algorithm: Recursive Binary Decision Tree for Linear Regression Start with top node \\(\\mathbb{R}^p\\) While a stopping criterion is unsatisfied: Let \\[(\\hat{i}, \\hat{j}) = \\underset{(i, j)}{\\text{argmin}}\\left( \\sum_{i: x_i\\in R_1} (y_i - \\hat{y}_{R_1})^ 2 + \\sum_{i: x_i\\in R_2} (y_i - \\hat{y}_{R_2})^ 2\\right)\\] where \\[R_{1} = \\{X| X_j &lt; x_{i,j}\\}\\] \\[R_{2} = \\{X| X_j \\geqslant x_{i,j}\\}\\] Add nodes \\[\\hat{R}_{1} = \\{X| X_\\hat{j} &lt; x_{\\hat{i},\\hat{j}}\\}\\] \\[\\hat{R}_{2} = \\{X| X_\\hat{j} \\geqslant x_{\\hat{i},\\hat{j}}\\}\\] to the partition, and recurse on one of the nodes 74.1.1.0.3 Tree-pruning Complex trees can overfit, but simpler trees may avoid it 65. To get a simpler tree, we can grow a large tree \\(T_0\\) and prune it to obtain a subtree. Cost complexity or weakest link pruning is a method for finding an optimal subtree 66. For \\(\\alpha &gt; 0\\), we obtain a subtree \\[ T_\\alpha = \\underset{T\\ \\subset T_0}{\\text{argmin}} \\left(\\sum_{m=1}^{|T|} \\sum_{i: x_i \\in R_m} \\left(y_i - \\hat{y}_{R_m}\\right)^2 + \\alpha|T|\\right)\\] where \\(|T|\\) is the number of terminal nodes of \\(T\\), \\(R_m\\) is the rectangle corresponding to the \\(m\\)-th terminal node, and \\(\\hat{y}_{R_m}\\) is the predicted response (average of \\(y_i\\in R_m\\)) 67. 74.1.1.0.4 Algorithm: Weakest link regression tree with \\(K\\)-fold cross-validation For each68 \\(\\alpha &gt; 0\\): For \\(k = 1, \\dots K\\): Let \\(\\mathcal{D}_k = \\mathcal{D} \\backslash \\{k-\\text{th fold}\\}\\) Use recursive binary splitting to grow a tree \\(T_{k}\\), stopping when each node has fewer than some minimum number of observations \\(M\\) is reached 69 Use weakest link pruning to find a subtree \\(T_{k, \\alpha}\\) Let \\(CV_{(k)}(\\alpha)\\) be the \\(K\\)-fold cross-validation estimate of the mean squared test error Choose \\[ \\hat{\\alpha} = \\underset{\\alpha}{\\text{argmin}}\\ CV_{(k)}(\\alpha) \\] Return \\(\\hat{T} = T_{\\hat{\\alpha}}\\) 74.1.2 Classification Trees Classification trees are very similar to regression trees, but they predict qualitative responses. The predicted class for an observation \\((x_i, y_i)\\) in \\(R_m\\) is 70 is \\[ \\hat{k}_m = \\underset{k}{\\text{argmax}}\\ \\hat{p}_{m,k} \\] where \\(\\hat{p}_{m,k}\\) is the fraction of observations \\((x_i, y_i)\\) in the region \\(R_m\\) such that \\(y_i = k\\). One performance measure is the Classification error rate71 for the region \\(R_m\\) is \\[ E_m = 1 - \\hat{p}_{m, \\hat{k}} \\] A better performance measure is the Gini index for the region \\(R_m\\), a measure of total variance 72 across classes \\[ G_m = \\sum_{k = 1}^K \\hat{p}_{m,k}(1 - \\hat{p}_{m,k})\\] Another better performance measure is the entropy for the region \\(R_m\\)73 \\[ D_m = \\sum_{k = 1}^K - \\hat{p}_{m,k}\\log(\\hat{p}_{m,k}) \\] Typically the Gini index or entropy is used to prune, due to their sensitivity to node purity. However, if prediction accuracy is the goal then classification error rate is preferable. 74.1.3 Trees Versus Linear Models A linear regression model is of the form \\[f(X) = \\beta_0 + \\sum_{j = 1}^p \\beta_j X_j\\] while a regression tree model is of the form \\[ f(X) = \\sum_{m = 1}^M c_m I(X \\in R_m)\\] Linear regression will tend to perform better if the relationship between features and response is well-approximated by a linear function, whereas the regression tree will tend perform better if the relationship is non-linear or complex. 74.1.4 Advantages and Disadvantages of Trees 74.1.4.0.1 Advantages Conceptual simplicity May mirror human decision-making better than previous regression and classification methods Readily visualizable and easily interpreted Can handle qualitative predictors without the need for dummy variables 74.1.4.0.2 Disadvantages Less accurate prediction than previous regression and classification methods Non-robust to changes in data – small changes in data lead to large changes in estimated tree. 74.2 Bagging, Random Forests, Boosting These are methods for improving the prediction accuracy of decision trees. 74.2.1 Bagging The decision trees in § 8.1 suffer from high variance. Bagging is a method of reducing the variance of a statistical learning process 74. The bagging estimate of the target function of the process with dataset \\(\\mathcal{D}\\) is \\[\\hat{f}_{\\text{bag}}(x) = \\frac{1}{B} \\sum_{b = 1}^B \\hat{f}^{*b}(x) \\] where \\(\\hat{f^*}^b(x)\\) is the estimate of target function on the boostrap dataset \\(\\mathcal{D}_b\\) 75. Bagging can be used for any statistical learning method but it is particularly useful for decision trees.76 74.2.1.0.1 Out-of-bag Error Estimation On average, a bagged tree uses about 2/3 of the data – the remaining 1/3 is the out-of-bag (OOB) data. We can predict the response for each observation using the trees for which it was OOB, yielding about B/3 prediction. If we’re doing regression, we can average these predicted responses, or if we’re doing classification, take a majority vote, to get a single OOB prediction for each observation. Test error can be estimated using these predictions. 74.2.1.0.2 Variable Importance Measures Bagging typically results in improved prediction accuracy over single trees, at the expense of interpretability The RSS (for bagging regression trees) and Gini index (for bagging classification trees) can provide measures of variable importance. For both loss functions (RSS/Gini) the amount the loss is decreases due to a split over a given predictor, averaged over the B bagged trees. The greater the decrease, the more important the predictor 74.2.2 Random Forests Random forests works as follows: at each split in the tree, choose a predictor from among a new random sample of \\(1 \\leqslant m \\leqslant p\\) predictors. The random predictor sampling overcomes the tendency of bagged trees to look similar given strong predictors (e.g. the strongest predictor will be at the top of most or all of the bagged trees). On average, \\(\\frac{p-m}{p}\\) of the splits will not consider a given predictor, giving other predictors a chance to be chosen. This decorrelation of the trees improves the reduction in variance achieved by bagged. \\(m=p\\) corresponds to bagging. \\(m &lt;&lt; p\\) is useful when there is a large number of correlated predictors. Typically we choose \\(m \\approx \\sqrt{p}\\) 74.2.3 Boosting Boosting is another method of improving prediction accuracy that can be applied to many statistical learning methods. In decision trees, each tree is build using information from the previous trees. Instead of bootstrapped datasets, the datasets are modified based on the previously grown trees. The boosting approach learns slowly, by slowly improving in areas where it underperforms. It has 3 parameters: Number of trees \\(B\\). Unlike bagging and random forests, boosting can overfit if \\(B\\) is too big, although this happens slowly. The shrinkage parameter \\(\\lambda &gt; 0\\). Typical values are \\(\\lambda = 0.01, 0.001\\). Very small \\(\\lambda\\) can require very large \\(B\\) to get good performance. The number of tree splits \\(d\\), which controls the complexity of the boosted ensemble. Often \\(d\\) works well (the resulting tree is called a stump). 77 74.2.3.0.1 Algorithm: Boosting for Regression Trees Set \\(\\hat{f}(x) = 0\\) and \\(r_i = y_i\\), \\(1 \\leqslant i \\leqslant n\\) For \\(b = 1, 2, \\dots, B\\): Fit a tree \\(\\hat{f}^b\\) with \\(d\\) splits to \\((X, r)\\) Update the model \\(\\hat{f}\\) by adding a shrunk version of the new tree: \\[ \\hat{f}(x) \\leftarrow \\hat{f}(x) + \\lambda \\hat{f}^b(x)\\] Update the residuals: \\[ r_i \\leftarrow r_i - \\lambda \\hat{f}^b(x)\\] Output the boosted model \\[ \\hat{f}(x) = \\sum_{b = 1}^B \\lambda \\hat{f}^b(x)\\] 74.3 Footnotes This strategy results in a binary tree with the partition regions as leaves, and binary splits as nodes. It is “top-down” because it starts at the top of the partition tree (with a single region), “binary” because it splits the predictor space into two regions at each node in the tree, “recursive” because it calls itself at each node, and “greedy” because at each node, it chooses the optimal split at that node. ↩︎ That is, it may lead to lower variance and better interpretation at the cost of a higher bias ↩︎ We want a subtree with minimal estimated test error but it’s infeasible to compute this for all subtrees. ↩︎ This is the RSS for the partition given by the nodes of the tree \\(T\\), with a weighted penalty \\(\\alpha|T|\\) for the number of nodes (hence the complexity). ↩︎ Even though \\(\\alpha \\in [0, \\infty)\\) is a continuous parameter here, in practice it will be selected from a finite set of values. In fact (cf. comment on pg 309 of the text), as \\(\\alpha\\) increases,“branches get pruned in a nested and predictable fashion,” resulting in a sequence of subtrees as a function of \\(\\alpha\\). One can then find a sequence \\(\\alpha_1, \\dots, \\alpha_N\\) such that at each \\(\\alpha_i\\), a branch is removed, and since the tree is finite, the algorithm is guaranteed to terminate. ↩︎ The smallest possible number of observations per node is \\(M=1\\), which results in a partition with only one point in each region. This is clearly a maximal complexity tree, so we probably take \\(M &gt;&gt; 1\\) in practice. ↩︎ That is, the predicted class for observations in \\(R_m\\) is the most frequently occuring class in \\(R_m\\). ↩︎ The classification error rate isn’t sufficiently sensitive to “node purity,” that is degree to which a node contains observations from a single class. ↩︎ The Gini index is a measure of “node purity” – it is minimized when all \\(\\hat{p}_{m, k} \\in \\{0, 1\\}\\), that is, when all nodes contain observations from a single class. ↩︎ The \\(\\hat{p}_{m,k}\\) are the empirical pmf estimates of the conditional probabilities \\(p_{m, k} = P(Y = k | X \\in R_m)\\), so \\(D\\) is an estimate of the conditional entropy, i.e. the entropy of \\(Y\\ |\\ X \\in R_m\\). Thus \\(D\\) is a measure of information that the empirical pmf, and hence the corresponding tree provides, that is, of its average suprisal. As with the Gini index, \\(D\\) is minimized when all \\(\\hat{p}_{m, k} \\in \\{0, 1\\}\\). An average surprisal of zero means the tree provides all information, that is, it perfectly separates the classes. ↩︎ Bagging is another name for bootstrapping. It appears that the latter is usually used in the context of estimating the standard error of a statistic, while the former is used in the context of a statistical learning process (even though these are essentially the same). ↩︎ Really this is the bootstrap estimate of the average of the target function estimate over many datasets. For a given dataset \\(\\mathcal{D}\\), the function \\(\\hat{f}(x)\\) produced by the learning process is an estimate of the target function \\(f(x)\\). Repeating the process \\(1 \\leqslant b \\leqslant B\\) times over datasets \\(\\mathcal{D}_b\\), we get estimates \\(\\hat{f}^b(x)\\). Assuming these are iid, they have common variance \\(\\sigma^2\\), but their average \\[\\hat{f}_{\\text{avg}} = \\frac{1}{B} \\sum_{b = 1}^B \\hat{f}^{b}(x)\\] has variance \\(\\frac{\\sigma^2}{B}\\). Given \\(B\\) large enough, this variance is low. Bagging/bootstrapping gets around the lack of separate datasets \\(\\mathcal{D}_b\\) in practice by repeated sampling with replacement from a single dataset \\(\\mathcal{D}\\). ↩︎ For regression, one grows \\(B\\) deep (unpruned) regression trees on \\(B\\) bootstrapped datasets, each of which has low bias but high variance, then averages them to get a bootstrap estimate which has the same low bias, but much lower variance. For classification (since we can’t average over the classes of the bootstrapped trees) a simple approach is to predict the majority class over the bootstrapped trees. ↩︎ In the case of a stump, the boosted ensemble is fitting an additive model, since each term is a single variable. More generally, \\(d\\) is the interaction depth – since \\(d\\) splits can involve at most \\(d\\) variables, this controls the interaction order of the boosted model (i.e. the model can fit interaction terms up to degree \\(d\\)). ↩︎ 74.3.1 blah "],["tree-based-methods-practice.html", "Section 75 Tree Based Methods: Practice 75.1 Lab", " Section 75 Tree Based Methods: Practice 8.1.1 Regression Trees 8.1.2 Classification Trees Lab: 8.3.1 Fitting Classification Trees Lab: 8.3.2 Fitting Regression Trees Lab: 8.3.3 Bagging and Random Forests Lab: 8.3.4 Boosting import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import pydot from IPython.display import Image from sklearn.model_selection import train_test_split, cross_val_score from sklearn.externals.six import StringIO from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, export_graphviz from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor from sklearn.metrics import mean_squared_error,confusion_matrix, classification_report %matplotlib inline plt.style.use(&#39;seaborn-white&#39;) # This function creates images of tree models using pydot def print_tree(estimator, features, class_names=None, filled=True): tree = estimator names = features color = filled classn = class_names dot_data = StringIO() export_graphviz(estimator, out_file=dot_data, feature_names=features, class_names=classn, filled=filled) graph = pydot.graph_from_dot_data(dot_data.getvalue()) return(graph) 75.0.1 8.1.1 Regression Trees In R, I exported the dataset from package ‘ISLR’ to a csv file. df = pd.read_csv(&#39;Data/Hitters.csv&#39;).dropna() df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 263 entries, 1 to 321 Data columns (total 21 columns): Unnamed: 0 263 non-null object AtBat 263 non-null int64 Hits 263 non-null int64 HmRun 263 non-null int64 Runs 263 non-null int64 RBI 263 non-null int64 Walks 263 non-null int64 Years 263 non-null int64 CAtBat 263 non-null int64 CHits 263 non-null int64 CHmRun 263 non-null int64 CRuns 263 non-null int64 CRBI 263 non-null int64 CWalks 263 non-null int64 League 263 non-null object Division 263 non-null object PutOuts 263 non-null int64 Assists 263 non-null int64 Errors 263 non-null int64 Salary 263 non-null float64 NewLeague 263 non-null object dtypes: float64(1), int64(16), object(4) memory usage: 45.2+ KB X = df[[&#39;Years&#39;, &#39;Hits&#39;]].as_matrix() y = np.log(df.Salary.as_matrix()) fig, (ax1, ax2) = plt.subplots(1,2, figsize=(11,4)) ax1.hist(df.Salary.as_matrix()) ax1.set_xlabel(&#39;Salary&#39;) ax2.hist(y) ax2.set_xlabel(&#39;Log(Salary)&#39;); png regr = DecisionTreeRegressor(max_leaf_nodes=3) regr.fit(X, y) DecisionTreeRegressor(criterion=&#39;mse&#39;, max_depth=None, max_features=None, max_leaf_nodes=3, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter=&#39;best&#39;) 75.0.2 Figure 8.1 graph, = print_tree(regr, features=[&#39;Years&#39;, &#39;Hits&#39;]) Image(graph.create_png()) png 75.0.3 Figure 8.2 df.plot(&#39;Years&#39;, &#39;Hits&#39;, kind=&#39;scatter&#39;, color=&#39;orange&#39;, figsize=(7,6)) plt.xlim(0,25) plt.ylim(ymin=-5) plt.xticks([1, 4.5, 24]) plt.yticks([1, 117.5, 238]) plt.vlines(4.5, ymin=-5, ymax=250) plt.hlines(117.5, xmin=4.5, xmax=25) plt.annotate(&#39;R1&#39;, xy=(2,117.5), fontsize=&#39;xx-large&#39;) plt.annotate(&#39;R2&#39;, xy=(11,60), fontsize=&#39;xx-large&#39;) plt.annotate(&#39;R3&#39;, xy=(11,170), fontsize=&#39;xx-large&#39;); png 75.0.4 Pruning This is currently not supported in scikit-learn. See first point under ’disadvantages of decision trees in the documentation. Implementation has been discussed but Random Forests have better predictive qualities than a single pruned tree anyway if I understand correctly. 75.0.5 8.1.2 Classification Trees Dataset available on http://www-bcf.usc.edu/~gareth/ISL/data.html df2 = pd.read_csv(&#39;Data/Heart.csv&#39;).drop(&#39;Unnamed: 0&#39;, axis=1).dropna() df2.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 297 entries, 0 to 301 Data columns (total 14 columns): Age 297 non-null int64 Sex 297 non-null int64 ChestPain 297 non-null object RestBP 297 non-null int64 Chol 297 non-null int64 Fbs 297 non-null int64 RestECG 297 non-null int64 MaxHR 297 non-null int64 ExAng 297 non-null int64 Oldpeak 297 non-null float64 Slope 297 non-null int64 Ca 297 non-null float64 Thal 297 non-null object AHD 297 non-null object dtypes: float64(2), int64(9), object(3) memory usage: 34.8+ KB df2.ChestPain = pd.factorize(df2.ChestPain)[0] df2.Thal = pd.factorize(df2.Thal)[0] X2 = df2.drop(&#39;AHD&#39;, axis=1) y2 = pd.factorize(df2.AHD)[0] clf = DecisionTreeClassifier(max_depth=None, max_leaf_nodes=6, max_features=3) clf.fit(X2,y2) DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=3, max_leaf_nodes=6, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter=&#39;best&#39;) clf.score(X2,y2) 0.82491582491582494 graph2, = print_tree(clf, features=X2.columns, class_names=[&#39;No&#39;, &#39;Yes&#39;]) Image(graph2.create_png()) png 75.1 Lab 75.1.1 8.3.1 Fitting Classification Trees In R, I exported the dataset from package ‘ISLR’ to a csv file. df3 = pd.read_csv(&#39;Data/Carseats.csv&#39;).drop(&#39;Unnamed: 0&#39;, axis=1) df3.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Sales CompPrice Income Advertising Population Price ShelveLoc Age Education Urban US 0 9.50 138 73 11 276 120 Bad 42 17 Yes Yes 1 11.22 111 48 16 260 83 Good 65 10 Yes Yes 2 10.06 113 35 10 269 80 Medium 59 12 Yes Yes 3 7.40 117 100 4 466 97 Medium 55 14 Yes Yes 4 4.15 141 64 3 340 128 Bad 38 13 Yes No df3[&#39;High&#39;] = df3.Sales.map(lambda x: 1 if x&gt;8 else 0) df3.ShelveLoc = pd.factorize(df3.ShelveLoc)[0] df3.Urban = df3.Urban.map({&#39;No&#39;:0, &#39;Yes&#39;:1}) df3.US = df3.US.map({&#39;No&#39;:0, &#39;Yes&#39;:1}) df3.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 400 entries, 0 to 399 Data columns (total 12 columns): Sales 400 non-null float64 CompPrice 400 non-null int64 Income 400 non-null int64 Advertising 400 non-null int64 Population 400 non-null int64 Price 400 non-null int64 ShelveLoc 400 non-null int64 Age 400 non-null int64 Education 400 non-null int64 Urban 400 non-null int64 US 400 non-null int64 High 400 non-null int64 dtypes: float64(1), int64(11) memory usage: 37.6 KB df3.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Sales CompPrice Income Advertising Population Price ShelveLoc Age Education Urban US High 0 9.50 138 73 11 276 120 0 42 17 1 1 1 1 11.22 111 48 16 260 83 1 65 10 1 1 1 2 10.06 113 35 10 269 80 2 59 12 1 1 1 3 7.40 117 100 4 466 97 2 55 14 1 1 0 4 4.15 141 64 3 340 128 0 38 13 1 0 0 X = df3.drop([&#39;Sales&#39;, &#39;High&#39;], axis=1) y = df3.High X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0) clf = DecisionTreeClassifier(max_depth=6) clf.fit(X, y) DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=6, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter=&#39;best&#39;) print(classification_report(y, clf.predict(X))) precision recall f1-score support 0 0.89 0.99 0.93 236 1 0.98 0.82 0.89 164 avg / total 0.92 0.92 0.92 400 graph3, = print_tree(clf, features=X.columns, class_names=[&#39;No&#39;, &#39;Yes&#39;]) Image(graph3.create_png()) png clf.fit(X_train, y_train) pred = clf.predict(X_test) cm = pd.DataFrame(confusion_matrix(y_test, pred).T, index=[&#39;No&#39;, &#39;Yes&#39;], columns=[&#39;No&#39;, &#39;Yes&#39;]) cm.index.name = &#39;Predicted&#39; cm.columns.name = &#39;True&#39; cm .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } True No Yes Predicted No 100 31 Yes 18 51 # Precision of the model using test data is 74% print(classification_report(y_test, pred)) precision recall f1-score support 0 0.76 0.85 0.80 118 1 0.74 0.62 0.68 82 avg / total 0.75 0.76 0.75 200 Pruning not implemented in scikit-learn. 75.1.2 8.3.2 Fitting Regression Trees In R, I exported the dataset from package ‘MASS’ to a csv file. boston_df = pd.read_csv(&#39;Data/Boston.csv&#39;) boston_df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 506 entries, 0 to 505 Data columns (total 14 columns): crim 506 non-null float64 zn 506 non-null float64 indus 506 non-null float64 chas 506 non-null int64 nox 506 non-null float64 rm 506 non-null float64 age 506 non-null float64 dis 506 non-null float64 rad 506 non-null int64 tax 506 non-null int64 ptratio 506 non-null float64 black 506 non-null float64 lstat 506 non-null float64 medv 506 non-null float64 dtypes: float64(11), int64(3) memory usage: 55.4 KB X = boston_df.drop(&#39;medv&#39;, axis=1) y = boston_df.medv X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0) # Pruning not supported. Choosing max depth 3) regr2 = DecisionTreeRegressor(max_depth=3) regr2.fit(X_train, y_train) pred = regr2.predict(X_test) graph, = print_tree(regr2, features=X.columns) Image(graph.create_png()) png plt.scatter(pred, y_test, label=&#39;medv&#39;) plt.plot([0, 1], [0, 1], &#39;--k&#39;, transform=plt.gca().transAxes) plt.xlabel(&#39;pred&#39;) plt.ylabel(&#39;y_test&#39;) Text(0,0.5,&#39;y_test&#39;) png mean_squared_error(y_test, pred) 26.023230850097445 75.1.3 8.3.3 Bagging and Random Forests # There are 13 features in the dataset X.shape (506, 13) # Bagging: using all features regr1 = RandomForestRegressor(max_features=13, random_state=1) regr1.fit(X_train, y_train) RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=13, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1, oob_score=False, random_state=1, verbose=0, warm_start=False) pred = regr1.predict(X_test) plt.scatter(pred, y_test, label=&#39;medv&#39;) plt.plot([0, 1], [0, 1], &#39;--k&#39;, transform=plt.gca().transAxes) plt.xlabel(&#39;pred&#39;) plt.ylabel(&#39;y_test&#39;) Text(0,0.5,&#39;y_test&#39;) png mean_squared_error(y_test, pred) 18.301366007905138 # Random forests: using 6 features regr2 = RandomForestRegressor(max_features=6, random_state=1) regr2.fit(X_train, y_train) RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=6, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1, oob_score=False, random_state=1, verbose=0, warm_start=False) pred = regr2.predict(X_test) mean_squared_error(y_test, pred) 16.469374703557314 Importance = pd.DataFrame({&#39;Importance&#39;:regr2.feature_importances_*100}, index=X.columns) Importance.sort_values(&#39;Importance&#39;, axis=0, ascending=True).plot(kind=&#39;barh&#39;, color=&#39;r&#39;, ) plt.xlabel(&#39;Variable Importance&#39;) plt.gca().legend_ = None png 75.1.4 8.3.4 Boosting regr = GradientBoostingRegressor(n_estimators=500, learning_rate=0.01, random_state=1) regr.fit(X_train, y_train) GradientBoostingRegressor(alpha=0.9, criterion=&#39;friedman_mse&#39;, init=None, learning_rate=0.01, loss=&#39;ls&#39;, max_depth=3, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=500, presort=&#39;auto&#39;, random_state=1, subsample=1.0, verbose=0, warm_start=False) feature_importance = regr.feature_importances_*100 rel_imp = pd.Series(feature_importance, index=X.columns).sort_values(inplace=False) print(rel_imp) rel_imp.T.plot(kind=&#39;barh&#39;, color=&#39;r&#39;, ) plt.xlabel(&#39;Variable Importance&#39;) plt.gca().legend_ = None zn 0.170382 rad 1.593909 chas 1.844703 indus 3.045285 nox 3.284683 tax 5.007437 black 5.082208 age 5.587239 crim 6.750284 ptratio 8.226473 dis 10.248698 rm 22.134290 lstat 27.024410 dtype: float64 png mean_squared_error(y_test, regr.predict(X_test)) 15.529710264059759 "],["support-vector-machines-practice.html", "Section 76 Support Vector Machines: Practice 76.1 LAB", " Section 76 Support Vector Machines: Practice Lab: 9.6.1 Support Vector Classifier Lab: 9.6.2 Support Vector Machine Lab: 9.6.3 ROC Curves Lab: 9.6.4 SVM with Multiple Classes Lab: 9.6.5 Application to Gene Expression Data # %load ../standard_import.txt import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.preprocessing import label_binarize from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.svm import SVC, LinearSVC from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report %matplotlib inline plt.style.use(&#39;seaborn-white&#39;) 76.1 LAB 76.1.1 9.6.1 Support Vector Classifier Define a function to plot a classifier with support vectors. def plot_svc(svc, X, y, h=0.02, pad=0.25): x_min, x_max = X[:, 0].min()-pad, X[:, 0].max()+pad y_min, y_max = X[:, 1].min()-pad, X[:, 1].max()+pad xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = svc.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.2) plt.scatter(X[:,0], X[:,1], s=70, c=y, cmap=plt.cm.Paired) # Support vectors indicated in plot by vertical lines sv = svc.support_vectors_ plt.scatter(sv[:,0], sv[:,1], c=&#39;k&#39;, marker=&#39;|&#39;, s=100, linewidths=&#39;1&#39;) plt.xlim(x_min, x_max) plt.ylim(y_min, y_max) plt.xlabel(&#39;X1&#39;) plt.ylabel(&#39;X2&#39;) plt.show() print(&#39;Number of support vectors: &#39;, svc.support_.size) # Generating random data: 20 observations of 2 features and divide into two classes. np.random.seed(5) X = np.random.randn(20,2) y = np.repeat([1,-1], 10) X[y == -1] = X[y == -1] +1 plt.scatter(X[:,0], X[:,1], s=70, c=y, cmap=plt.cm.Paired) plt.xlabel(&#39;X1&#39;) plt.ylabel(&#39;X2&#39;); png # Support Vector Classifier with linear kernel. svc = SVC(C= 1.0, kernel=&#39;linear&#39;) svc.fit(X, y) plot_svc(svc, X, y) png Number of support vectors: 13 # When using a smaller cost parameter (C=0.1) the margin is wider, resulting in more support vectors. svc2 = SVC(C=0.1, kernel=&#39;linear&#39;) svc2.fit(X, y) plot_svc(svc2, X, y) png Number of support vectors: 16 # Select the optimal C parameter by cross-validation tuned_parameters = [{&#39;C&#39;: [0.001, 0.01, 0.1, 1, 5, 10, 100]}] clf = GridSearchCV(SVC(kernel=&#39;linear&#39;), tuned_parameters, cv=10, scoring=&#39;accuracy&#39;, return_train_score=True) clf.fit(X, y) clf.cv_results_ {&#39;mean_fit_time&#39;: array([ 0.00041504, 0.00026286, 0.00026855, 0.00027678, 0.00026004, 0.00027893, 0.00039349]), &#39;mean_score_time&#39;: array([ 0.00026352, 0.00018048, 0.0001838 , 0.0001822 , 0.00018001, 0.00018055, 0.00017881]), &#39;mean_test_score&#39;: array([ 0.8 , 0.8 , 0.8 , 0.75, 0.75, 0.75, 0.75]), &#39;mean_train_score&#39;: array([ 0.79444444, 0.79444444, 0.75 , 0.77777778, 0.76666667, 0.76666667, 0.76666667]), &#39;param_C&#39;: masked_array(data = [0.001 0.01 0.1 1 5 10 100], mask = [False False False False False False False], fill_value = ?), &#39;params&#39;: [{&#39;C&#39;: 0.001}, {&#39;C&#39;: 0.01}, {&#39;C&#39;: 0.1}, {&#39;C&#39;: 1}, {&#39;C&#39;: 5}, {&#39;C&#39;: 10}, {&#39;C&#39;: 100}], &#39;rank_test_score&#39;: array([1, 1, 1, 4, 4, 4, 4], dtype=int32), &#39;split0_test_score&#39;: array([ 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]), &#39;split0_train_score&#39;: array([ 0.83333333, 0.83333333, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778]), &#39;split1_test_score&#39;: array([ 0.5, 0.5, 0.5, 0. , 0. , 0. , 0. ]), &#39;split1_train_score&#39;: array([ 0.83333333, 0.83333333, 0.83333333, 0.88888889, 0.88888889, 0.88888889, 0.88888889]), &#39;split2_test_score&#39;: array([ 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]), &#39;split2_train_score&#39;: array([ 0.83333333, 0.83333333, 0.77777778, 0.83333333, 0.83333333, 0.83333333, 0.83333333]), &#39;split3_test_score&#39;: array([ 1., 1., 1., 1., 1., 1., 1.]), &#39;split3_train_score&#39;: array([ 0.77777778, 0.77777778, 0.72222222, 0.72222222, 0.72222222, 0.72222222, 0.72222222]), &#39;split4_test_score&#39;: array([ 1., 1., 1., 1., 1., 1., 1.]), &#39;split4_train_score&#39;: array([ 0.77777778, 0.77777778, 0.72222222, 0.77777778, 0.77777778, 0.77777778, 0.77777778]), &#39;split5_test_score&#39;: array([ 1., 1., 1., 1., 1., 1., 1.]), &#39;split5_train_score&#39;: array([ 0.77777778, 0.77777778, 0.72222222, 0.72222222, 0.72222222, 0.72222222, 0.72222222]), &#39;split6_test_score&#39;: array([ 1., 1., 1., 1., 1., 1., 1.]), &#39;split6_train_score&#39;: array([ 0.77777778, 0.77777778, 0.72222222, 0.77777778, 0.72222222, 0.72222222, 0.72222222]), &#39;split7_test_score&#39;: array([ 1., 1., 1., 1., 1., 1., 1.]), &#39;split7_train_score&#39;: array([ 0.72222222, 0.72222222, 0.72222222, 0.77777778, 0.72222222, 0.72222222, 0.72222222]), &#39;split8_test_score&#39;: array([ 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]), &#39;split8_train_score&#39;: array([ 0.83333333, 0.83333333, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778]), &#39;split9_test_score&#39;: array([ 1., 1., 1., 1., 1., 1., 1.]), &#39;split9_train_score&#39;: array([ 0.77777778, 0.77777778, 0.72222222, 0.72222222, 0.72222222, 0.72222222, 0.72222222]), &#39;std_fit_time&#39;: array([ 1.29045113e-04, 2.34619509e-05, 4.18230226e-05, 3.99072322e-05, 4.80760882e-06, 2.40586557e-05, 4.76230235e-05]), &#39;std_score_time&#39;: array([ 1.05007676e-04, 1.09293532e-05, 1.94490093e-05, 1.08948682e-05, 1.90122934e-05, 1.49680733e-05, 2.26685797e-06]), &#39;std_test_score&#39;: array([ 0.24494897, 0.24494897, 0.24494897, 0.3354102 , 0.3354102 , 0.3354102 , 0.3354102 ]), &#39;std_train_score&#39;: array([ 0.03557291, 0.03557291, 0.0372678 , 0.0496904 , 0.05443311, 0.05443311, 0.05443311])} # 0.001 is best according to GridSearchCV. clf.best_params_ {&#39;C&#39;: 0.001} # Generating test data np.random.seed(1) X_test = np.random.randn(20,2) y_test = np.random.choice([-1,1], 20) X_test[y_test == 1] = X_test[y_test == 1] -1 plt.scatter(X_test[:,0], X_test[:,1], s=70, c=y_test, cmap=plt.cm.Paired) plt.xlabel(&#39;X1&#39;) plt.ylabel(&#39;X2&#39;); png # svc2 : C = 0.1 y_pred = svc2.predict(X_test) pd.DataFrame(confusion_matrix(y_test, y_pred),index=svc.classes_, columns=svc.classes_) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } -1 1 -1 2 6 1 0 12 svc3 = SVC(C=0.001, kernel=&#39;linear&#39;) svc3.fit(X, y) # svc3 : C = 0.001 y_pred = svc3.predict(X_test) pd.DataFrame(confusion_matrix(y_test, y_pred), index=svc3.classes_, columns=svc3.classes_) # The misclassification is the same .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } -1 1 -1 2 6 1 0 12 # Changing the test data so that the classes are really seperable with a hyperplane. X_test[y_test == 1] = X_test[y_test == 1] -1 plt.scatter(X_test[:,0], X_test[:,1], s=70, c=y_test, cmap=plt.cm.Paired) plt.xlabel(&#39;X1&#39;) plt.ylabel(&#39;X2&#39;); png svc4 = SVC(C=10.0, kernel=&#39;linear&#39;) svc4.fit(X_test, y_test) SVC(C=10.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto&#39;, kernel=&#39;linear&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) plot_svc(svc4, X_test, y_test) png Number of support vectors: 4 # Increase the margin. Now there is one misclassification: increased bias, lower variance. svc5 = SVC(C=1, kernel=&#39;linear&#39;) svc5.fit(X_test, y_test) SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto&#39;, kernel=&#39;linear&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) plot_svc(svc5, X_test, y_test) png Number of support vectors: 5 76.1.2 9.6.2 Support Vector Machine # Generating test data np.random.seed(8) X = np.random.randn(200,2) X[:100] = X[:100] +2 X[101:150] = X[101:150] -2 y = np.concatenate([np.repeat(-1, 150), np.repeat(1,50)]) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=2) plt.scatter(X[:,0], X[:,1], s=70, c=y, cmap=plt.cm.Paired) plt.xlabel(&#39;X1&#39;) plt.ylabel(&#39;X2&#39;); png svm = SVC(C=1.0, kernel=&#39;rbf&#39;, gamma=1) svm.fit(X_train, y_train) SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=1, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) plot_svc(svm, X_train, y_train) png Number of support vectors: 51 # Increasing C parameter, allowing more flexibility svm2 = SVC(C=100, kernel=&#39;rbf&#39;, gamma=1.0) svm2.fit(X_train, y_train) SVC(C=100, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=1.0, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) plot_svc(svm2, X_train, y_train) png Number of support vectors: 36 # Set the parameters by cross-validation tuned_parameters = [{&#39;C&#39;: [0.01, 0.1, 1, 10, 100], &#39;gamma&#39;: [0.5, 1,2,3,4]}] clf = GridSearchCV(SVC(kernel=&#39;rbf&#39;), tuned_parameters, cv=10, scoring=&#39;accuracy&#39;, return_train_score=True) clf.fit(X_train, y_train) clf.cv_results_ {&#39;mean_fit_time&#39;: array([ 0.00057094, 0.00040917, 0.00044692, 0.00046566, 0.0004539 , 0.00037632, 0.00042117, 0.00052576, 0.00053477, 0.00054688, 0.00039585, 0.00043306, 0.00058219, 0.00061436, 0.00059385, 0.00039713, 0.00043871, 0.00058134, 0.0006536 , 0.00065069, 0.00043354, 0.00047519, 0.0005713 , 0.00062847, 0.00062041]), &#39;mean_score_time&#39;: array([ 0.00028653, 0.00020535, 0.00020931, 0.00020568, 0.00019453, 0.00019224, 0.00019584, 0.0002043 , 0.00020387, 0.00020037, 0.0001987 , 0.00019855, 0.00020149, 0.00020573, 0.00021515, 0.0001905 , 0.00019574, 0.00020428, 0.00020165, 0.00020831, 0.00020461, 0.00019076, 0.0001929 , 0.00019503, 0.00019488]), &#39;mean_test_score&#39;: array([ 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.92, 0.92, 0.9 , 0.88, 0.85, 0.92, 0.89, 0.86, 0.86, 0.87, 0.84, 0.83, 0.86, 0.87, 0.87]), &#39;mean_train_score&#39;: array([ 0.77002017, 0.77002017, 0.77002017, 0.77002017, 0.77002017, 0.77002017, 0.77002017, 0.77002017, 0.77002017, 0.77002017, 0.93557504, 0.94777366, 0.95998477, 0.96333059, 0.98112195, 0.95113224, 0.9666642 , 0.99112277, 0.99112277, 0.99112277, 0.9688989 , 0.99112277, 0.99112277, 0.99112277, 0.99112277]), &#39;param_C&#39;: masked_array(data = [0.01 0.01 0.01 0.01 0.01 0.1 0.1 0.1 0.1 0.1 1 1 1 1 1 10 10 10 10 10 100 100 100 100 100], mask = [False False False False False False False False False False False False False False False False False False False False False False False False False], fill_value = ?), &#39;param_gamma&#39;: masked_array(data = [0.5 1 2 3 4 0.5 1 2 3 4 0.5 1 2 3 4 0.5 1 2 3 4 0.5 1 2 3 4], mask = [False False False False False False False False False False False False False False False False False False False False False False False False False], fill_value = ?), &#39;params&#39;: [{&#39;C&#39;: 0.01, &#39;gamma&#39;: 0.5}, {&#39;C&#39;: 0.01, &#39;gamma&#39;: 1}, {&#39;C&#39;: 0.01, &#39;gamma&#39;: 2}, {&#39;C&#39;: 0.01, &#39;gamma&#39;: 3}, {&#39;C&#39;: 0.01, &#39;gamma&#39;: 4}, {&#39;C&#39;: 0.1, &#39;gamma&#39;: 0.5}, {&#39;C&#39;: 0.1, &#39;gamma&#39;: 1}, {&#39;C&#39;: 0.1, &#39;gamma&#39;: 2}, {&#39;C&#39;: 0.1, &#39;gamma&#39;: 3}, {&#39;C&#39;: 0.1, &#39;gamma&#39;: 4}, {&#39;C&#39;: 1, &#39;gamma&#39;: 0.5}, {&#39;C&#39;: 1, &#39;gamma&#39;: 1}, {&#39;C&#39;: 1, &#39;gamma&#39;: 2}, {&#39;C&#39;: 1, &#39;gamma&#39;: 3}, {&#39;C&#39;: 1, &#39;gamma&#39;: 4}, {&#39;C&#39;: 10, &#39;gamma&#39;: 0.5}, {&#39;C&#39;: 10, &#39;gamma&#39;: 1}, {&#39;C&#39;: 10, &#39;gamma&#39;: 2}, {&#39;C&#39;: 10, &#39;gamma&#39;: 3}, {&#39;C&#39;: 10, &#39;gamma&#39;: 4}, {&#39;C&#39;: 100, &#39;gamma&#39;: 0.5}, {&#39;C&#39;: 100, &#39;gamma&#39;: 1}, {&#39;C&#39;: 100, &#39;gamma&#39;: 2}, {&#39;C&#39;: 100, &#39;gamma&#39;: 3}, {&#39;C&#39;: 100, &#39;gamma&#39;: 4}], &#39;rank_test_score&#39;: array([16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 1, 1, 4, 6, 13, 1, 5, 10, 10, 7, 14, 15, 10, 7, 7], dtype=int32), &#39;split0_test_score&#39;: array([ 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.90909091, 0.90909091, 0.81818182, 0.81818182, 0.81818182, 0.90909091, 0.81818182, 0.72727273, 0.72727273, 0.72727273, 0.81818182, 0.81818182, 0.72727273, 0.72727273, 0.72727273]), &#39;split0_train_score&#39;: array([ 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.94382022, 0.95505618, 0.96629213, 0.96629213, 0.98876404, 0.95505618, 0.97752809, 1. , 1. , 1. , 0.97752809, 1. , 1. , 1. , 1. ]), &#39;split1_test_score&#39;: array([ 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 1. , 0.90909091, 0.90909091, 0.90909091, 0.81818182, 0.90909091, 0.81818182, 0.90909091, 0.90909091, 0.90909091, 0.81818182, 0.81818182, 0.90909091, 1. , 1. ]), &#39;split1_train_score&#39;: array([ 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.92134831, 0.93258427, 0.95505618, 0.96629213, 0.97752809, 0.96629213, 0.96629213, 0.98876404, 0.98876404, 0.98876404, 0.97752809, 0.98876404, 0.98876404, 0.98876404, 0.98876404]), &#39;split2_test_score&#39;: array([ 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 1. , 1. , 1. , 0.90909091, 0.81818182, 1. , 1. , 1. , 0.90909091, 0.90909091, 1. , 1. , 1. , 0.90909091, 0.90909091]), &#39;split2_train_score&#39;: array([ 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.93258427, 0.94382022, 0.95505618, 0.95505618, 0.97752809, 0.94382022, 0.95505618, 0.98876404, 0.98876404, 0.98876404, 0.95505618, 0.98876404, 0.98876404, 0.98876404, 0.98876404]), &#39;split3_test_score&#39;: array([ 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 1. , 1. , 1. , 1. , 1. , 0.9, 0.9, 1. , 1. , 1. , 0.9, 0.9, 1. , 1. , 1. ]), &#39;split3_train_score&#39;: array([ 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.94444444, 0.95555556, 0.95555556, 0.95555556, 0.97777778, 0.94444444, 0.95555556, 0.98888889, 0.98888889, 0.98888889, 0.95555556, 0.98888889, 0.98888889, 0.98888889, 0.98888889]), &#39;split4_test_score&#39;: array([ 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.9, 0.7, 0.7, 0.7, 0.8, 0.7, 0.7, 0.7, 0.7, 0.7]), &#39;split4_train_score&#39;: array([ 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.93333333, 0.94444444, 0.95555556, 0.97777778, 0.98888889, 0.95555556, 0.97777778, 0.98888889, 0.98888889, 0.98888889, 0.98888889, 0.98888889, 0.98888889, 0.98888889, 0.98888889]), &#39;split5_test_score&#39;: array([ 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 1. , 1. , 1. , 1. , 0.9, 1. , 1. , 1. , 1. , 1. , 0.9, 1. , 1. , 1. , 1. ]), &#39;split5_train_score&#39;: array([ 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.94444444, 0.94444444, 0.95555556, 0.95555556, 0.97777778, 0.94444444, 0.96666667, 0.98888889, 0.98888889, 0.98888889, 0.96666667, 0.98888889, 0.98888889, 0.98888889, 0.98888889]), &#39;split6_test_score&#39;: array([ 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.7, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.7, 0.7, 0.7, 0.7, 0.6, 0.7, 0.7, 0.7]), &#39;split6_train_score&#39;: array([ 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.95555556, 0.96666667, 0.96666667, 0.96666667, 0.98888889, 0.95555556, 0.96666667, 1. , 1. , 1. , 0.96666667, 1. , 1. , 1. , 1. ]), &#39;split7_test_score&#39;: array([ 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.88888889, 0.77777778, 0.88888889, 0.88888889, 0.77777778, 0.77777778, 0.77777778, 0.88888889, 0.88888889]), &#39;split7_train_score&#39;: array([ 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.94505495, 0.95604396, 0.96703297, 0.96703297, 0.97802198, 0.95604396, 0.95604396, 0.98901099, 0.98901099, 0.98901099, 0.95604396, 0.98901099, 0.98901099, 0.98901099, 0.98901099]), &#39;split8_test_score&#39;: array([ 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 1. , 1. , 0.88888889, 0.88888889, 0.88888889, 1. , 1. , 0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889]), &#39;split8_train_score&#39;: array([ 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.91208791, 0.93406593, 0.96703297, 0.96703297, 0.97802198, 0.94505495, 0.96703297, 0.98901099, 0.98901099, 0.98901099, 0.96703297, 0.98901099, 0.98901099, 0.98901099, 0.98901099]), &#39;split9_test_score&#39;: array([ 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 1. , 1. , 1. , 0.88888889, 0.88888889, 1. , 1. , 0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.77777778, 0.88888889, 0.88888889, 0.88888889]), &#39;split9_train_score&#39;: array([ 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.92307692, 0.94505495, 0.95604396, 0.95604396, 0.97802198, 0.94505495, 0.97802198, 0.98901099, 0.98901099, 0.98901099, 0.97802198, 0.98901099, 0.98901099, 0.98901099, 0.98901099]), &#39;std_fit_time&#39;: array([ 1.29946046e-04, 2.37428996e-05, 3.44466995e-05, 4.06270366e-05, 1.08891275e-05, 9.16545024e-06, 3.26168903e-05, 4.05703804e-05, 3.52196780e-05, 5.12081594e-05, 4.58430076e-05, 5.77187892e-05, 6.30817400e-05, 5.33768882e-05, 3.02056298e-05, 1.66381495e-05, 2.50651415e-05, 3.66048308e-05, 4.32769062e-05, 1.73285536e-05, 2.26485603e-05, 3.73201545e-05, 1.64487395e-05, 2.56888689e-05, 2.57250885e-05]), &#39;std_score_time&#39;: array([ 5.44625275e-05, 1.39864760e-05, 2.87118636e-05, 1.88631289e-05, 2.26798611e-06, 3.38697068e-06, 9.65003433e-06, 1.13169492e-05, 1.33860726e-05, 8.99264819e-06, 1.90952892e-05, 1.06008097e-05, 9.35649405e-06, 1.96293797e-05, 3.91622674e-05, 4.30343884e-06, 8.26594154e-06, 2.02857086e-05, 1.05884695e-05, 2.38947401e-05, 4.54962125e-05, 2.53767431e-06, 1.47145025e-06, 2.89460186e-06, 1.58220864e-06]), &#39;std_test_score&#39;: array([ 0.03128559, 0.03128559, 0.03128559, 0.03128559, 0.03128559, 0.03128559, 0.03128559, 0.03128559, 0.03128559, 0.03128559, 0.10933222, 0.08867145, 0.09000561, 0.07563869, 0.06384166, 0.07656779, 0.10140926, 0.11898561, 0.10832051, 0.09712535, 0.09132028, 0.11988631, 0.11898561, 0.11629378, 0.11629378]), &#39;std_train_score&#39;: array([ 0.00360388, 0.00360388, 0.00360388, 0.00360388, 0.00360388, 0.00360388, 0.00360388, 0.00360388, 0.00360388, 0.00360388, 0.01280087, 0.01001371, 0.00553851, 0.00712229, 0.00506044, 0.0072475 , 0.00861247, 0.00443945, 0.00443945, 0.00443945, 0.01086333, 0.00443945, 0.00443945, 0.00443945, 0.00443945])} clf.best_params_ {&#39;C&#39;: 1, &#39;gamma&#39;: 0.5} confusion_matrix(y_test, clf.best_estimator_.predict(X_test)) array([[67, 6], [ 9, 18]]) # 15% of test observations misclassified clf.best_estimator_.score(X_test, y_test) 0.84999999999999998 76.1.3 9.6.3 ROC Curves Comparing the ROC curves of two models on train/test data. One model is more flexible than the other. svm3 = SVC(C=1, kernel=&#39;rbf&#39;, gamma=2) svm3.fit(X_train, y_train) SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=2, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) # More flexible model svm4 = SVC(C=1, kernel=&#39;rbf&#39;, gamma=50) svm4.fit(X_train, y_train) SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=50, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) y_train_score3 = svm3.decision_function(X_train) y_train_score4 = svm4.decision_function(X_train) false_pos_rate3, true_pos_rate3, _ = roc_curve(y_train, y_train_score3) roc_auc3 = auc(false_pos_rate3, true_pos_rate3) false_pos_rate4, true_pos_rate4, _ = roc_curve(y_train, y_train_score4) roc_auc4 = auc(false_pos_rate4, true_pos_rate4) fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(14,6)) ax1.plot(false_pos_rate3, true_pos_rate3, label=&#39;SVM $\\gamma = 1$ ROC curve (area = %0.2f)&#39; % roc_auc3, color=&#39;b&#39;) ax1.plot(false_pos_rate4, true_pos_rate4, label=&#39;SVM $\\gamma = 50$ ROC curve (area = %0.2f)&#39; % roc_auc4, color=&#39;r&#39;) ax1.set_title(&#39;Training Data&#39;) y_test_score3 = svm3.decision_function(X_test) y_test_score4 = svm4.decision_function(X_test) false_pos_rate3, true_pos_rate3, _ = roc_curve(y_test, y_test_score3) roc_auc3 = auc(false_pos_rate3, true_pos_rate3) false_pos_rate4, true_pos_rate4, _ = roc_curve(y_test, y_test_score4) roc_auc4 = auc(false_pos_rate4, true_pos_rate4) ax2.plot(false_pos_rate3, true_pos_rate3, label=&#39;SVM $\\gamma = 1$ ROC curve (area = %0.2f)&#39; % roc_auc3, color=&#39;b&#39;) ax2.plot(false_pos_rate4, true_pos_rate4, label=&#39;SVM $\\gamma = 50$ ROC curve (area = %0.2f)&#39; % roc_auc4, color=&#39;r&#39;) ax2.set_title(&#39;Test Data&#39;) for ax in fig.axes: ax.plot([0, 1], [0, 1], &#39;k--&#39;) ax.set_xlim([-0.05, 1.0]) ax.set_ylim([0.0, 1.05]) ax.set_xlabel(&#39;False Positive Rate&#39;) ax.set_ylabel(&#39;True Positive Rate&#39;) ax.legend(loc=&quot;lower right&quot;) png As expected, the more flexible model scores better on training data but worse on the test data. 76.1.4 9.6.4 SVM with Multiple Classes # Adding a third class of observations np.random.seed(8) XX = np.vstack([X, np.random.randn(50,2)]) yy = np.hstack([y, np.repeat(0,50)]) XX[yy ==0] = XX[yy == 0] +4 plt.scatter(XX[:,0], XX[:,1], s=70, c=yy, cmap=plt.cm.prism) plt.xlabel(&#39;XX1&#39;) plt.ylabel(&#39;XX2&#39;); png svm5 = SVC(C=1, kernel=&#39;rbf&#39;) svm5.fit(XX, yy) SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto&#39;, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) plot_svc(svm5, XX, yy) png Number of support vectors: 133 76.1.5 9.6.5 Application to Gene Expression Data In R, I exported the dataset from package ‘ISLR’ to csv files. X_train = pd.read_csv(&#39;Data/Khan_xtrain.csv&#39;).drop(&#39;Unnamed: 0&#39;, axis=1) y_train = pd.read_csv(&#39;Data/Khan_ytrain.csv&#39;).drop(&#39;Unnamed: 0&#39;, axis=1).as_matrix().ravel() X_test = pd.read_csv(&#39;Data/Khan_xtest.csv&#39;).drop(&#39;Unnamed: 0&#39;, axis=1) y_test = pd.read_csv(&#39;Data/Khan_ytest.csv&#39;).drop(&#39;Unnamed: 0&#39;, axis=1).as_matrix().ravel() # y_train counts pd.Series(y_train).value_counts(sort=False) 1 8 2 23 3 12 4 20 dtype: int64 # y_test counts pd.Series(y_test).value_counts(sort=False) 1 3 2 6 3 6 4 5 dtype: int64 # This model gives identical results to the svm() of the R package e1071, also based on libsvm library. svc = SVC(kernel=&#39;linear&#39;) # This model is based on liblinear library and gives 100 score on the test data. #svc = LinearSVC() svc.fit(X_train, y_train) SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto&#39;, kernel=&#39;linear&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) cm = confusion_matrix(y_train, svc.predict(X_train)) cm_df = pd.DataFrame(cm.T, index=svc.classes_, columns=svc.classes_) cm_df.index.name = &#39;Predicted&#39; cm_df.columns.name = &#39;True&#39; print(cm_df) True 1 2 3 4 Predicted 1 8 0 0 0 2 0 23 0 0 3 0 0 12 0 4 0 0 0 20 cm = confusion_matrix(y_test, svc.predict(X_test)) cm_df = pd.DataFrame(cm.T, index=svc.classes_, columns=svc.classes_) cm_df.index.name = &#39;Predicted&#39; cm_df.columns.name = &#39;True&#39; print(cm_df) True 1 2 3 4 Predicted 1 3 0 0 0 2 0 6 2 0 3 0 0 4 0 4 0 0 0 5 "],["support-vector-machines-theory.html", "Section 77 Support Vector Machines: Theory 77.1 Maximal Margin Classifier 77.2 Support Vector Classifiers 77.3 Support Vector Machines 77.4 SVMs with More than Two Classes 77.5 Relationship to Logistic Regression 77.6 Footnotes", " Section 77 Support Vector Machines: Theory 77.1 Maximal Margin Classifier 77.1.1 What Is a Hyperplane? A hyperplane in \\(\\mathbb{R}^p\\) is an affine subspace of dimension \\(p-1\\). Every hyperplane is the set of solutions \\(X\\) to \\(\\beta^\\top X = 0\\) for some \\(\\beta\\in\\mathbb{R}^p\\). A hyperplane \\(\\beta^\\top X = 0\\) partitions \\(\\mathbb{R}^p\\) into two halfspaces: \\[H_+ = \\{X\\in\\mathbb{R}^p\\ |\\ \\beta^\\top X &gt; 0\\}\\] \\[H_- = \\{X\\in\\mathbb{R}^p\\ |\\ \\beta^\\top X &gt; 0\\}\\] corresponding to either side of the plane, or equivalently, \\[H_+ = \\{X\\in\\mathbb{R}^p\\ |\\ \\text{sgn}(\\beta^\\top X) = 1\\}\\] \\[H_- = \\{X\\in\\mathbb{R}^p\\ |\\ \\text{sgn}(\\beta^\\top X) = -1\\}\\] 77.1.2 Classification Using a Separating Hyperplane Given data \\((x_i, y_i)\\), \\(i = 1,\\dots n\\) with response classes \\(y_i \\in \\{ \\pm 1\\}\\), a hyperplane \\(\\beta^\\top X = 0\\) is separating if \\[\\text{sgn}(\\beta^\\top x_i) = y_i\\] for all \\(i\\). - Given a separating hyperplane, we may predict \\[\\hat{y}_i = \\text{sgn}(\\beta^\\top x_i)\\] 77.1.3 The Maximal Margin Classifier Separating hyperplanes are not unique (if one exists then uncountably many exist). A natural choice is the maximal margin hyperplane (or optimal separating hyperplane) The margin is the minimal perpendicular distance to the hyperplane over the sample points \\[ M = \\underset{i}{\\min}\\{\\ ||x_i - P x_i||\\ \\}\\] where \\(P\\) is the projection matrix onto the hyperplane. The points \\((x_i, y_i)\\) “on the margin” (where \\(||x_i - P x_i|| = M\\)) are called support vectors 77.1.4 Construction of the Maximal Margin Classifier The maximal margin classifier is the solution to the optimization problem: \\[\\begin{align*} \\underset{\\boldsymbol{\\beta}}{\\text{argmax}}&amp;\\ M\\\\ \\text{subject to}&amp;\\ ||\\,\\boldsymbol{\\beta}\\,|| = 1\\\\ &amp; \\mathbf{y}^\\top(X\\boldsymbol{\\beta}) \\geqslant \\mathbf{M}\\\\ \\end{align*}\\] where \\(\\mathbf{M} = (M, \\dots, M) \\in \\mathbb{R}^n\\) 78 77.1.5 The Non-separable Case The maximal margin classifier is a natural classifier, but a separating hyperplane is not guaranteed to exist If a separating hyperplane doesn’t exist, we can choose an “almost” separating hyperplane by using a “soft” margin. 77.2 Support Vector Classifiers 77.2.1 Overview of the Support Vector Classifier Separating hyperplanes don’t always exist, and even if they do, they may be undesirable. The distance to the hyperplane can be thought of as a measure of confidence in the classification. For very small margins, the separating hyperplane is very sensitive to individual observations – we have low confidence in the classification of nearby observations. In these situations, we may prefer a hyperplane that doesn’t perfectly separate in the interest of: Greater robustness to individual observations Better classification of most of the training observations This is achieved by the support vector classifier or soft margin classifier 79 77.2.2 Details of the Support Vector Classifier The support vector classifier is the solution to the optimization problem: \\[\\begin{align*} \\underset{\\boldsymbol{\\beta}}{\\text{argmax}}&amp;\\ M\\\\ \\text{subject to}&amp;\\ ||\\,\\boldsymbol{\\beta}\\,|| = 1\\\\ &amp; y_i(\\boldsymbol{\\beta}^\\top x_i) \\geqslant M(1-\\epsilon_i)\\\\ &amp; \\epsilon_i \\geqslant 0\\\\ &amp; \\sum_i \\epsilon_i \\leqslant C \\end{align*}\\] where \\(C \\geqslant 0\\) is a tuning parameter, \\(M\\) is the margin, and the \\(\\epsilon_i\\) are slack variables. 80 Observations on the margin or on the wrong side of the margin are called support vectors 77.3 Support Vector Machines 77.3.1 Classification with Non-Linear Decision Boundaries The support vector classifier is a natural choice for two response classes when the class boundary is linear, but may perform poorly when the boundary is non-linear. Non-linear transformations of the features will lead to a non-linear class boundary, but enlarging the feature space too much can lead to intractable computations. The support vector machine enlarges the feature space in a way which is computationally efficient. 77.3.2 The Support Vector Machine It can be shown that: the linear support vector classifier is a model of the form \\[f(x) = \\beta_0 + \\sum_{i = 1}^n \\alpha_i \\langle x, x_i\\rangle \\] the parameter estimates \\(\\hat{\\alpha}_i, \\hat{\\beta}_0\\) can be computed from the \\(\\binom{n}{2}\\) inner products \\(\\langle x, x_i \\rangle\\) The support vector machine is a model of the form \\[f(x) = \\beta_0 + \\sum_{i = 1}^n \\alpha_i K(x, x_i) \\] where \\(K\\) is a kernel function 81 Popular kernels 82 are The polynomial kernel \\[K(x_i, x_i&#39;) = (1 + x_i^\\top x_i&#39;)^d\\] The radial kernel \\[K(x_i, x_i&#39;) = \\exp(-\\gamma\\,||x_i - x_i&#39;||^2)\\] 77.4 SVMs with More than Two Classes 77.4.1 One-Versus-One Classification This approach works as follows: 1. Fit \\(\\binom{K}{2}\\) SVMs, one for each pair of classes \\(k,k&#39;\\) encoded as \\(\\pm 1\\), respectively. 2. For each observation \\(x\\), classify using each of the predictors in 1, and let \\(N_k\\) be the number of times \\(x\\) was assigned to class \\(k\\). 3. Predict \\[ \\hat{f}(x) = \\underset{k}{\\text{argmax}}\\, N_k\\] 77.4.2 One-Versus-All Classification This approach works as follows: 1. Fit \\(K\\) SVMs, comparing each class \\(k\\) to other \\(K-1\\) classes, encoded as \\(\\pm 1\\), respectively. Let \\(\\beta_k (\\beta_{0k}, \\dots, \\beta_{pk})\\) be resulting parameters. 2. Predict \\[\\hat{f}(x) = \\underset{k}{\\text{argmax}}\\, \\beta_k^\\top x\\] 77.5 Relationship to Logistic Regression The optimization problem leading to the support vector classifier can be rewritten as \\[\\underset{\\beta}{\\text{argmin}}\\left(\\sum_{i = 1}^n \\max\\{0, 1 - y_i(\\beta^\\top x_i)\\} + \\lambda\\,||\\beta||^2\\right)\\] where $ $ is a tuning parameter 83 . The hinge loss 84 is very similar to the logistic regression loss, so both methods tend to give similar results. However, SVMs tend to perform better when the classes are well separated, while logistic regression tends to perform better when they are not. 77.6 Footnotes The constraint \\(|| \\boldsymbol{\\beta} || = 1\\) ensures that the perpendicular distance \\(||x_i - P x_i||\\) is given by \\(y_i(\\beta^\\top x_i)\\). ↩︎ Sometimes the maximal margin and support vector classifiers are called “hard margin” and “soft margin” support vector classifiers, respectively. ↩︎ For each \\(i\\), if \\(\\epsilon_i = 0\\) the \\(i\\)-th observation is on the correct side of the margin. If \\(\\epsilon_i &gt; 0\\) then it is on the wrong side of the margin, and if \\(\\epsilon_i &gt; 1\\) then it is on the wrong side of the hyperplane. The parameter \\(C\\) is a “margin violation tolerance” – it bounds the \\(\\epsilon_i\\) and thus the number/size of margin violations. Greater \\(C\\) implies greater tolerance. The case \\(C = 0\\) is the maximal margin hyperplane. ↩︎ 81.In this context a kernel function is a positive-definite kernel . Among other things, it is a generalization of an inner product (every inner product \\(\\langle x, y \\rangle\\) is a kernel function), and is one way of quantifying similarity between points.\\ In the context of statistical and machine learning, a kernel method is one which makes use of the “kernel trick.” The kernel function \\(K(x_i, x_i&#39;)\\) encodes the similarity of the observations \\(x_i, x_i&#39;\\) in a transformed feature space, but it is more computationally efficient to compute the \\(\\binom{n}{k}\\) kernels themselves than to transform the data. The kernel fits a support vector classifier (hence a linear classification boundary) in the transformed feature space, which corresponds to a non-linear boundary in the original feature space. The polynomial kernel is effectively the inner product on the space of \\(d\\)-degree polynomials in the features \\(X_j\\). The radial kernel is a similarity measure in an infinite dimensional feature space. This is another instance of a general form of a “regularized loss” or “loss + penalty” \\[\\underset{\\beta}{\\text{argmin}}L(\\mathbf{X}, \\mathbf{y}, \\beta) + \\lambda P(\\beta)\\] where the loss function \\(L(\\mathbf{X}, \\mathbf{y}, \\beta)\\) quantifies how well the parameter model with parameter \\(\\beta\\) fits the data \\((\\mathbf{X}, \\mathbf{y})\\), and \\(P(\\beta)\\) is a penalty function controlled by \\(\\lambda\\). In this case \\[L(\\mathbf{X}, \\mathbf{y}, \\beta) = \\sum_{i = 1}^n \\{0, 1 - y_i(\\beta^\\top x_i\\}\\] is called the hinge loss. "],["unsupervised-learning-practice.html", "Section 78 Unsupervised Learning: Practice 78.1 Lab 1: Principal Component Analysis 78.2 Lab 2: Clustering 78.3 Lab 3: NCI60 Data Example", " Section 78 Unsupervised Learning: Practice Lab 1: Principal Component Analysis Lab 2: K-Means Clustering Lab 2: Hierarchical Clustering Lab 3: NCI60 Data Example import pandas as pd import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns from sklearn.preprocessing import scale from sklearn.decomposition import PCA from sklearn.cluster import KMeans from scipy.cluster import hierarchy %matplotlib inline plt.style.use(&#39;seaborn-white&#39;) 78.1 Lab 1: Principal Component Analysis # In R, I exported the dataset to a csv file. It is part of the base R distribution. df = pd.read_csv(&#39;Data/USArrests.csv&#39;, index_col=0) df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 50 entries, Alabama to Wyoming Data columns (total 4 columns): Murder 50 non-null float64 Assault 50 non-null int64 UrbanPop 50 non-null int64 Rape 50 non-null float64 dtypes: float64(2), int64(2) memory usage: 2.0+ KB df.mean() Murder 7.788 Assault 170.760 UrbanPop 65.540 Rape 21.232 dtype: float64 df.var() Murder 18.970465 Assault 6945.165714 UrbanPop 209.518776 Rape 87.729159 dtype: float64 X = pd.DataFrame(scale(df), index=df.index, columns=df.columns) # The loading vectors pca_loadings = pd.DataFrame(PCA().fit(X).components_.T, index=df.columns, columns=[&#39;V1&#39;, &#39;V2&#39;, &#39;V3&#39;, &#39;V4&#39;]) pca_loadings .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } V1 V2 V3 V4 Murder 0.535899 0.418181 -0.341233 0.649228 Assault 0.583184 0.187986 -0.268148 -0.743407 UrbanPop 0.278191 -0.872806 -0.378016 0.133878 Rape 0.543432 -0.167319 0.817778 0.089024 # Fit the PCA model and transform X to get the principal components pca = PCA() df_plot = pd.DataFrame(pca.fit_transform(X), columns=[&#39;PC1&#39;, &#39;PC2&#39;, &#39;PC3&#39;, &#39;PC4&#39;], index=X.index) df_plot .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PC1 PC2 PC3 PC4 Alabama 0.985566 1.133392 -0.444269 0.156267 Alaska 1.950138 1.073213 2.040003 -0.438583 Arizona 1.763164 -0.745957 0.054781 -0.834653 Arkansas -0.141420 1.119797 0.114574 -0.182811 California 2.523980 -1.542934 0.598557 -0.341996 Colorado 1.514563 -0.987555 1.095007 0.001465 Connecticut -1.358647 -1.088928 -0.643258 -0.118469 Delaware 0.047709 -0.325359 -0.718633 -0.881978 Florida 3.013042 0.039229 -0.576829 -0.096285 Georgia 1.639283 1.278942 -0.342460 1.076797 Hawaii -0.912657 -1.570460 0.050782 0.902807 Idaho -1.639800 0.210973 0.259801 -0.499104 Illinois 1.378911 -0.681841 -0.677496 -0.122021 Indiana -0.505461 -0.151563 0.228055 0.424666 Iowa -2.253646 -0.104054 0.164564 0.017556 Kansas -0.796881 -0.270165 0.025553 0.206496 Kentucky -0.750859 0.958440 -0.028369 0.670557 Louisiana 1.564818 0.871055 -0.783480 0.454728 Maine -2.396829 0.376392 -0.065682 -0.330460 Maryland 1.763369 0.427655 -0.157250 -0.559070 Massachusetts -0.486166 -1.474496 -0.609497 -0.179599 Michigan 2.108441 -0.155397 0.384869 0.102372 Minnesota -1.692682 -0.632261 0.153070 0.067317 Mississippi 0.996494 2.393796 -0.740808 0.215508 Missouri 0.696787 -0.263355 0.377444 0.225824 Montana -1.185452 0.536874 0.246889 0.123742 Nebraska -1.265637 -0.193954 0.175574 0.015893 Nevada 2.874395 -0.775600 1.163380 0.314515 New Hampshire -2.383915 -0.018082 0.036855 -0.033137 New Jersey 0.181566 -1.449506 -0.764454 0.243383 New Mexico 1.980024 0.142849 0.183692 -0.339534 New York 1.682577 -0.823184 -0.643075 -0.013484 North Carolina 1.123379 2.228003 -0.863572 -0.954382 North Dakota -2.992226 0.599119 0.301277 -0.253987 Ohio -0.225965 -0.742238 -0.031139 0.473916 Oklahoma -0.311783 -0.287854 -0.015310 0.010332 Oregon 0.059122 -0.541411 0.939833 -0.237781 Pennsylvania -0.888416 -0.571100 -0.400629 0.359061 Rhode Island -0.863772 -1.491978 -1.369946 -0.613569 South Carolina 1.320724 1.933405 -0.300538 -0.131467 South Dakota -1.987775 0.823343 0.389293 -0.109572 Tennessee 0.999742 0.860251 0.188083 0.652864 Texas 1.355138 -0.412481 -0.492069 0.643195 Utah -0.550565 -1.471505 0.293728 -0.082314 Vermont -2.801412 1.402288 0.841263 -0.144890 Virginia -0.096335 0.199735 0.011713 0.211371 Washington -0.216903 -0.970124 0.624871 -0.220848 West Virginia -2.108585 1.424847 0.104775 0.131909 Wisconsin -2.079714 -0.611269 -0.138865 0.184104 Wyoming -0.629427 0.321013 -0.240659 -0.166652 fig , ax1 = plt.subplots(figsize=(9,7)) ax1.set_xlim(-3.5,3.5) ax1.set_ylim(-3.5,3.5) # Plot Principal Components 1 and 2 for i in df_plot.index: ax1.annotate(i, (df_plot.PC1.loc[i], -df_plot.PC2.loc[i]), ha=&#39;center&#39;) # Plot reference lines ax1.hlines(0,-3.5,3.5, linestyles=&#39;dotted&#39;, colors=&#39;grey&#39;) ax1.vlines(0,-3.5,3.5, linestyles=&#39;dotted&#39;, colors=&#39;grey&#39;) ax1.set_xlabel(&#39;First Principal Component&#39;) ax1.set_ylabel(&#39;Second Principal Component&#39;) # Plot Principal Component loading vectors, using a second y-axis. ax2 = ax1.twinx().twiny() ax2.set_ylim(-1,1) ax2.set_xlim(-1,1) ax2.tick_params(axis=&#39;y&#39;, colors=&#39;orange&#39;) ax2.set_xlabel(&#39;Principal Component loading vectors&#39;, color=&#39;orange&#39;) # Plot labels for vectors. Variable &#39;a&#39; is a small offset parameter to separate arrow tip and text. a = 1.07 for i in pca_loadings[[&#39;V1&#39;, &#39;V2&#39;]].index: ax2.annotate(i, (pca_loadings.V1.loc[i]*a, -pca_loadings.V2.loc[i]*a), color=&#39;orange&#39;) # Plot vectors ax2.arrow(0,0,pca_loadings.V1[0], -pca_loadings.V2[0]) ax2.arrow(0,0,pca_loadings.V1[1], -pca_loadings.V2[1]) ax2.arrow(0,0,pca_loadings.V1[2], -pca_loadings.V2[2]) ax2.arrow(0,0,pca_loadings.V1[3], -pca_loadings.V2[3]); png # Standard deviation of the four principal components np.sqrt(pca.explained_variance_) array([ 1.5908673 , 1.00496987, 0.6031915 , 0.4206774 ]) pca.explained_variance_ array([ 2.53085875, 1.00996444, 0.36383998, 0.17696948]) pca.explained_variance_ratio_ array([ 0.62006039, 0.24744129, 0.0891408 , 0.04335752]) plt.figure(figsize=(7,5)) plt.plot([1,2,3,4], pca.explained_variance_ratio_, &#39;-o&#39;, label=&#39;Individual component&#39;) plt.plot([1,2,3,4], np.cumsum(pca.explained_variance_ratio_), &#39;-s&#39;, label=&#39;Cumulative&#39;) plt.ylabel(&#39;Proportion of Variance Explained&#39;) plt.xlabel(&#39;Principal Component&#39;) plt.xlim(0.75,4.25) plt.ylim(0,1.05) plt.xticks([1,2,3,4]) plt.legend(loc=2); png 78.2 Lab 2: Clustering 78.2.1 10.5.1 K-Means Clustering # Generate data np.random.seed(2) X = np.random.standard_normal((50,2)) X[:25,0] = X[:25,0]+3 X[:25,1] = X[:25,1]-4 78.2.1.1 K = 2 km1 = KMeans(n_clusters=2, n_init=20) km1.fit(X) KMeans(algorithm=&#39;auto&#39;, copy_x=True, init=&#39;k-means++&#39;, max_iter=300, n_clusters=2, n_init=20, n_jobs=1, precompute_distances=&#39;auto&#39;, random_state=None, tol=0.0001, verbose=0) km1.labels_ array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], dtype=int32) See plot for K=2 below. 78.2.1.2 K = 3 np.random.seed(4) km2 = KMeans(n_clusters=3, n_init=20) km2.fit(X) KMeans(algorithm=&#39;auto&#39;, copy_x=True, init=&#39;k-means++&#39;, max_iter=300, n_clusters=3, n_init=20, n_jobs=1, precompute_distances=&#39;auto&#39;, random_state=None, tol=0.0001, verbose=0) pd.Series(km2.labels_).value_counts() 1 21 0 20 2 9 dtype: int64 km2.cluster_centers_ array([[-0.27876523, 0.51224152], [ 2.82805911, -4.11351797], [ 0.69945422, -2.14934345]]) km2.labels_ array([1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2], dtype=int32) # Sum of distances of samples to their closest cluster center. km2.inertia_ 68.973792009397258 fig, (ax1, ax2) = plt.subplots(1,2, figsize=(14,5)) ax1.scatter(X[:,0], X[:,1], s=40, c=km1.labels_, cmap=plt.cm.prism) ax1.set_title(&#39;K-Means Clustering Results with K=2&#39;) ax1.scatter(km1.cluster_centers_[:,0], km1.cluster_centers_[:,1], marker=&#39;+&#39;, s=100, c=&#39;k&#39;, linewidth=2) ax2.scatter(X[:,0], X[:,1], s=40, c=km2.labels_, cmap=plt.cm.prism) ax2.set_title(&#39;K-Means Clustering Results with K=3&#39;) ax2.scatter(km2.cluster_centers_[:,0], km2.cluster_centers_[:,1], marker=&#39;+&#39;, s=100, c=&#39;k&#39;, linewidth=2); png 78.2.2 10.5.3 Hierarchical Clustering 78.2.2.1 scipy fig, (ax1,ax2,ax3) = plt.subplots(3,1, figsize=(15,18)) for linkage, cluster, ax in zip([hierarchy.complete(X), hierarchy.average(X), hierarchy.single(X)], [&#39;c1&#39;,&#39;c2&#39;,&#39;c3&#39;], [ax1,ax2,ax3]): cluster = hierarchy.dendrogram(linkage, ax=ax, color_threshold=0) ax1.set_title(&#39;Complete Linkage&#39;) ax2.set_title(&#39;Average Linkage&#39;) ax3.set_title(&#39;Single Linkage&#39;); png 78.3 Lab 3: NCI60 Data Example 78.3.1 § 10.6.1 PCA # In R, I exported the two elements of this ISLR dataset to csv files. # There is one file for the features and another file for the classes/types. df2 = pd.read_csv(&#39;Data/NCI60_X.csv&#39;).drop(&#39;Unnamed: 0&#39;, axis=1) df2.columns = np.arange(df2.columns.size) df2.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 64 entries, 0 to 63 Columns: 6830 entries, 0 to 6829 dtypes: float64(6830) memory usage: 3.3 MB X = pd.DataFrame(scale(df2)) X.shape (64, 6830) y = pd.read_csv(&#39;Data/NCI60_y.csv&#39;, usecols=[1], skiprows=1, names=[&#39;type&#39;]) y.shape (64, 1) y.type.value_counts() RENAL 9 NSCLC 9 MELANOMA 8 BREAST 7 COLON 7 OVARIAN 6 LEUKEMIA 6 CNS 5 PROSTATE 2 MCF7D-repro 1 K562B-repro 1 K562A-repro 1 MCF7A-repro 1 UNKNOWN 1 Name: type, dtype: int64 # Fit the PCA model and transform X to get the principal components pca2 = PCA() df2_plot = pd.DataFrame(pca2.fit_transform(X)) fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6)) color_idx = pd.factorize(y.type)[0] cmap = plt.cm.hsv # Left plot ax1.scatter(df2_plot.iloc[:,0], -df2_plot.iloc[:,1], c=color_idx, cmap=cmap, alpha=0.5, s=50) ax1.set_ylabel(&#39;Principal Component 2&#39;) # Right plot ax2.scatter(df2_plot.iloc[:,0], df2_plot.iloc[:,2], c=color_idx, cmap=cmap, alpha=0.5, s=50) ax2.set_ylabel(&#39;Principal Component 3&#39;) # Custom legend for the classes (y) since we do not create scatter plots per class (which could have their own labels). handles = [] labels = pd.factorize(y.type.unique()) norm = mpl.colors.Normalize(vmin=0.0, vmax=14.0) for i, v in zip(labels[0], labels[1]): handles.append(mpl.patches.Patch(color=cmap(norm(i)), label=v, alpha=0.5)) ax2.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.) # xlabel for both plots for ax in fig.axes: ax.set_xlabel(&#39;Principal Component 1&#39;) png pd.DataFrame([df2_plot.iloc[:,:5].std(axis=0, ddof=0).as_matrix(), pca2.explained_variance_ratio_[:5], np.cumsum(pca2.explained_variance_ratio_[:5])], index=[&#39;Standard Deviation&#39;, &#39;Proportion of Variance&#39;, &#39;Cumulative Proportion&#39;], columns=[&#39;PC1&#39;, &#39;PC2&#39;, &#39;PC3&#39;, &#39;PC4&#39;, &#39;PC5&#39;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PC1 PC2 PC3 PC4 PC5 Standard Deviation 27.853469 21.481355 19.820465 17.032556 15.971807 Proportion of Variance 0.113589 0.067562 0.057518 0.042476 0.037350 Cumulative Proportion 0.113589 0.181151 0.238670 0.281145 0.318495 df2_plot.iloc[:,:10].var(axis=0, ddof=0).plot(kind=&#39;bar&#39;, rot=0) plt.ylabel(&#39;Variances&#39;); png fig , (ax1,ax2) = plt.subplots(1,2, figsize=(15,5)) # Left plot ax1.plot(pca2.explained_variance_ratio_, &#39;-o&#39;) ax1.set_ylabel(&#39;Proportion of Variance Explained&#39;) ax1.set_ylim(ymin=-0.01) # Right plot ax2.plot(np.cumsum(pca2.explained_variance_ratio_), &#39;-ro&#39;) ax2.set_ylabel(&#39;Cumulative Proportion of Variance Explained&#39;) ax2.set_ylim(ymax=1.05) for ax in fig.axes: ax.set_xlabel(&#39;Principal Component&#39;) ax.set_xlim(-1,65) png 78.3.2 § 10.6.2 Clustering X= pd.DataFrame(scale(df2), index=y.type, columns=df2.columns) fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(20,20)) for linkage, cluster, ax in zip([hierarchy.complete(X), hierarchy.average(X), hierarchy.single(X)], [&#39;c1&#39;,&#39;c2&#39;,&#39;c3&#39;], [ax1,ax2,ax3]): cluster = hierarchy.dendrogram(linkage, labels=X.index, orientation=&#39;right&#39;, color_threshold=0, leaf_font_size=10, ax=ax) ax1.set_title(&#39;Complete Linkage&#39;) ax2.set_title(&#39;Average Linkage&#39;) ax3.set_title(&#39;Single Linkage&#39;); png plt.figure(figsize=(10,20)) cut4 = hierarchy.dendrogram(hierarchy.complete(X), labels=X.index, orientation=&#39;right&#39;, color_threshold=140, leaf_font_size=10) plt.vlines(140,0,plt.gca().yaxis.get_data_interval()[1], colors=&#39;r&#39;, linestyles=&#39;dashed&#39;); png 78.3.2.0.1 KMeans np.random.seed(2) km4 = KMeans(n_clusters=4, n_init=50) km4.fit(X) KMeans(algorithm=&#39;auto&#39;, copy_x=True, init=&#39;k-means++&#39;, max_iter=300, n_clusters=4, n_init=50, n_jobs=1, precompute_distances=&#39;auto&#39;, random_state=None, tol=0.0001, verbose=0) km4.labels_ array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3], dtype=int32) # Observations per KMeans cluster pd.Series(km4.labels_).value_counts().sort_index() 0 8 1 23 2 24 3 9 dtype: int64 78.3.2.0.2 Hierarchical # Observations per Hierarchical cluster cut4b = hierarchy.dendrogram(hierarchy.complete(X), truncate_mode=&#39;lastp&#39;, p=4, show_leaf_counts=True) png # Hierarchy based on Principal Components 1 to 5 plt.figure(figsize=(10,20)) pca_cluster = hierarchy.dendrogram(hierarchy.complete(df2_plot.iloc[:,:5]), labels=y.type.values, orientation=&#39;right&#39;, color_threshold=100, leaf_font_size=10) png cut4c = hierarchy.dendrogram(hierarchy.complete(df2_plot), truncate_mode=&#39;lastp&#39;, p=4, show_leaf_counts=True) # See also color coding in plot above. png "],["unsupervised-learning-theory.html", "Section 79 Unsupervised Learning: Theory 79.1 The Challenge of Unsupervised Learning 79.2 Principal Components Analysis 79.3 Clustering Methods 79.4 Footnotes", " Section 79 Unsupervised Learning: Theory 79.1 The Challenge of Unsupervised Learning Unsupervised learning is learning in the absence of a response. It is often part of exploratory data analysis (EDA). Without a response, we aren’t interested in prediction or classification, rather we are interested in discovering interesting things about the data. This can be difficult because such a goal is somewhat subjective. Objective performance criteria for unsupervised learning can also be challenging. 79.2 Principal Components Analysis Principal components were discussed earlier as a dimensional reduction method in the context of regression. They provide a low-dimensional representation of the data that contains as much variation as possible. Principal Components Analysis is the process of computing principal components and using them in data analysis. 79.2.1 What Are Principal Components? The first principal component of features \\(X_1, \\dots, X_p\\) is the normalized linear combination \\[ Z_1 = \\hat{\\phi}_1^\\top X\\] where \\(X = (X_1, \\dots, X_p), \\hat{\\phi}_1\\in \\mathbb{R}^p\\) and \\(|| \\hat{\\phi} || = 1\\). The vector \\(\\hat{\\phi}_1\\) is called the loading vector (its entries are called the loadings) and \\[ \\hat{\\phi}_1 = \\underset{\\underset{||\\phi|| = 1}{\\phi \\in \\mathbb{R}^p}}{\\text{argmax}}\\left(\\frac{1}{n}\\sum_{i=1}^n \\left(\\phi^\\top x_i\\right)^2\\right)\\] Assume we have data \\(X_i\\) with features \\(X_1, \\dots, X_p\\) which is centered in the features (each feature has mean zero). The objective function in the above optimization problem can be rewritten \\[ \\hat{\\phi}_1 = \\underset{\\phi \\in \\mathbb{R}^p}{\\text{argmax}}\\left(\\frac{1}{n}\\sum_{i=1}^n ||z_i ||^2\\right)\\] which is just the sample variance. The \\(z_{i1}\\) are called the scores of the first principal component \\(Z_1\\). The first principal component has a nice geometric interpretation 85. The loading vector \\(\\phi_{1}\\) defines a direction in \\(\\mathbb{R}^p\\) along which the variation is maximized. The principal component scores \\(z_{i1}\\) are the projections of the data \\(x_i\\) onto \\(\\phi_1\\) – that is, the components of the \\(x_i\\) along this direction. For \\(j = 2,...,p\\) we can compute the \\(j\\)-th principal component \\(\\phi_j\\) recursively \\[ \\hat{\\phi}_j = \\underset{\\phi \\in \\mathbb{R}^p}{\\text{argmax}}\\left(\\frac{1}{n}\\sum_{i=1}^n \\left(\\phi^\\top x_i\\right)^2\\right)\\] subject to 86 \\[\\phi_j^\\top \\phi_{j - 1} = 0\\]. - We can plot the principal components against each other for a low-dimensional visualization of the data. For example a biplot plots both the scores and the loading vectors 87. 79.2.2 Another Interpretation of Principal Components Principal components can also be seen as providing low-dimensional surfaces that are “closest” to the observations. The span of the first \\(M\\) loading vectors \\(\\phi_1, \\dots, \\phi_M\\) can be seen as the \\(M\\)-dimensional linear subspaces of \\(\\mathbb{R}^p\\) which is closest to the observations \\(x_i\\) 88 Together the principal components \\(Z_1, \\dots, Z_M\\) and loading vectors \\(\\phi_1, \\dots, \\phi_M\\) can be seen as an \\(M\\)-dimensional approximation89 of each observation \\[x_{ij} \\approx \\sum_{m = 1}^M z_{im}\\phi_{jm}\\] 79.2.3 More on PCA PCA requires that the variables are centered to have mean zero PCA is sensitive to scaling, so we usually scale each variable to have standard deviation 1. Scaling to standard deviation 1 is particularly important when variables are measured in different units, however if they are measured in the same units we may not wish to do this. 79.2.3.0.1 Uniqueness of the Principal Components The loading vectors and score vectors are unique up to sign flips. 79.2.3.0.2 The Proportion of Variance Explained How much of the information in a given data set is lost by projecting onto the principal components? More precisely, what is the proportion of variance explained (PVE) by each principal component? Assuming centered data, the total variance 90 is \\[\\text{var}_{total} := \\sum_{j = 1}^p \\mathbb{V}(X_j) = \\sum_{i = 1}^p \\left(\\frac{1}{n} \\sum_{i = 1}^n x_{ij}^2 \\right)\\] while the variance explained by the \\(m\\)-th principal component is \\[\\text{var}_{m} := \\frac{1}{n} \\sum_{i=1}^n z_{im}^2 = \\frac{1}{n} \\sum_{i = 1}^n \\left(\\sum_{i = 1}^p \\phi_{jm}x_{ij} \\right)^2 \\]. The PVE of the \\(m\\)-th component is then \\[\\text{PVE}_m := \\frac{\\text{var}_{m}}{\\text{var}_{total}}\\] and the cumulative PVE of the first \\(M\\) components 91 is \\[\\sum_{m = 1}^M \\text{PVE}_m \\] 79.2.3.0.3 Deciding How Many Principal Components to Use In general choose we may not be interested in using all principal components, but just enough to get a “good” understanding of the data 92. A scree plot, which plots \\(\\text{PVM}_m\\) vs. \\(m\\), can help identify a good number of principal components to use, is one visual method for identifying a good number of principal components. We look for an elbow - a value of \\(m\\) such that \\(\\text{PVM}_m\\) drops off thereafter. In general, the question of how many principal components are “enough” is ill-defined, and depends on the application and the dataset. We maybe look at the first few principal components in order to find interesting patterns. If none are evident, then we conclude further components are unlikely to be of use. If some are evident, we continue looking at components until no more interesting patterns are found. In an unpervised setting, these methods are all ad hoc, and reflect the fact that PCA is generally used in EDA 93. 79.2.4 Other Uses for Principal Components Many statistical techniques (regression, classification, clustering) can be adapted to the \\(n \\times M\\) PCA matrix with columns the first \\(M &lt;&lt; p\\) principal component score vectors. The PCA matrix can be seen as a “de-noising” 94 of the original data, since the signal (as opposed to the noise) is weighted towards the earlier principal components 79.3 Clustering Methods This is a broad set of techniques for finding clusters (or subgroups) of the data set. Observations should be “similar” within clusters and dissimilar across clusters. The definition of “similar” is context dependent. Clustering is popular in many fields, so there exist a great number of methods. 79.3.1 \\(K\\)-Means Clustering \\(K\\)-means clustering seeks to partition the data into a pre-specified number \\(K\\) of distinct, non-overlapping clusters. More precisely, we seek a partition \\(\\hat{C}_1, \\dots \\hat{C}_K\\) of the set of indices \\(\\{1, \\dots n\\}\\) \\[\\hat{C}_1, \\dots \\hat{C}_K = \\underset{C_1, \\dots, C_k}{\\text{argmin}}\\left(\\sum_{k = 1}^K W(C_k)\\right)\\] where \\(W(C_k)\\) is some measure of the variation within cluster \\(C_k\\). - A typical choice of \\(W(C_k)\\) is the average 95 squared Euclidean distance between points in \\(C_k\\): \\[W(C)_k = \\frac{1}{|C_k|}\\sum_{i, i&#39; \\in C_k} ||x_i - x_i&#39;||^2\\] - A brute force algorithm for finding the global minimum is \\(O(K^n)\\) but there is a much faster algorithm which is guaranteed to find a local minimum. It uses a random initialization so it should be performed several times. 79.3.1.0.1 Algorithm: \\(K\\)-Means Clustering Initialize by randomly assigning a cluster number \\(1,\\dots K\\) to each observation. While the cluster assignments change: For each \\(k = 1, \\dots K\\), compute the centroid of the \\(k\\)-th cluster (the vector of feature means for the observations in the cluster). Assign to each observation the number of the cluster whose centroid is closest. 79.3.1.0.2 Advantages 79.3.1.0.3 Disadvantages 79.3.2 Hierarchical Clustering Hierarchical clustering is an alternative clustering method which doesn’t require a specified number of clusters and results in an attractive tree-based representation of the data called a dendrogram. Bottom-up or agglomerative hierarchical clustering builds a dendrogram from the leaves up to the trunk. 79.3.2.0.1 Interpreting a Dendrogram A dendrogram is a tree (visualized as upside down) with leaves corresponding to observations. As we move up the tree, similar observations fuse into branches, and similar branches again fuse. The earlier fusions occur, the more similar the corresponding groups of observations. The height at which two observations are joined by this fusing is a measure of this similarity. At each height in the dendrogram, a horizontal cut splits the observations into \\(k\\) clusters (corresponding to each of the branches cut) where \\(1 \\leqslant k \\leqslant n\\). The best choice of cut (hence number \\(k\\) of clusters) is often obtained by inspecting the diagram. 79.3.2.0.2 The Hierarchical Clustering Algorithm This algorithm uses a notion of dissimilarity defined for clusters, called a linkage. Let \\(A, B\\) be clusters, and let \\(d(a, b)\\) be a dissimilarity measure 95 for observations \\(a, b\\). A linkage defines a dissimilarity measure \\(d(A,B)\\) between the clusters \\(A, B\\). The four most common types of linkage are complete: \\[d_{comp}(A, B) = \\underset{(a, b) \\in A \\times B}{\\max} d(a, b)\\] single: \\[d_{sing}(A, B) = \\underset{(a, b) \\in A \\times B}{\\min} d(a, b)\\] average \\[d_{avg}(A, B) = \\frac{1}{|A||B|} \\underset{(a, b) \\in A \\times B}{\\sum} d(a, b)\\] centroid \\[d_{cent}(A, B) = d(x_a, x_b)\\], where \\(x_a\\) (resp. \\(x_b\\)) is the centroid of \\(A\\) (resp. \\(B\\)). Average, complete, and single linkages are preferred by statisticians. Average and complete linkages are generally preferred as they result in more balanced dendrograms. Centroid linkage is often used in genomics, but suffers from the possibility of an inversion, in which two clusters are fused at a height below the individual clusters, which makes interpretation difficult. 79.3.2.0.2.1 Choice of Dissimilarity Measure The squared Euclidean distance is often used as a dissimilarity measure. An alternative is the correlation-based distance The choice of dissimilarity measure is very important and has a strong effect on the resulting dendrogram. The choice of measure should be determined by context. One should consider scaling the data before choosing the dissimilarity measure. 79.3.2.0.2.2 Algorithm: Hierarchical Clustering Initialize with \\(n\\) clusters, one for each observation, and compute the dissimilarities \\(d(x_i, x_j)\\) for each pair. For \\(i = n, \\dots, 2\\): Compute all dissimilarites among the \\(i\\) clusters, and fuse the two clusters which are the least dissimilar. This dissimilarity is the height in the dendrogram where the fusion is placed. Compute the dissimilarities among the new \\(i -1\\) clusters. 79.3.2.0.2.3 Advantages 79.3.2.0.2.4 Disadvantages 79.3.3 Practical Issues in Clustering 79.3.3.0.1 Small Decisions with Big Consequences Should observations or features be standardized in some way? For hierarchical clustering: What dissimilarity measure should be used? What type of linkage should be used? Where should we cut the dendrogram to determine the number of clusters? For \\(K\\)-means clustering, what is the choice of \\(K\\)? 79.3.3.0.2 Validating the Clusters Obtained It is important to decide whether the clusters obtained reflect true subgroups in the data or are a result of “clustering the noise.” There exist techniques for making this decision, such as obtaining \\(p\\)-values for each cluster. 79.3.3.0.3 Other Considerations in Clustering Both \\(K\\)-means and hierarchical clustering assign all observations to some cluster. This can be problematic, for example in the presence of outliers that don’t clearly belong to any cluster. “Mixture models” are an attractive approach to accommodating outliers (they amount to a “soft” clustering approach). Clustering methods are not robust to perturbations. 79.3.3.0.4 A Tempered Approach to Interpreting the Results of Clustering Clustering can be a very useful and valid statistical tool if used properly. To overcome the sensitivity to hyperparameters, is recommended to try hyperparameter optimization. To overcome the sensitivity to perturbations, it is recommended to cluster on subsets of the data. Finally, results of cluster analysis should be considered a part of EDA and not taken too seriously 79.4 Footnotes The linear algebra interpretation is also nice ↩︎ This constraint is equivalent to \\[\\text{corr}(Z_j, Z_{j-1}) = 0\\] ↩︎ See book figure 10.1 and corresponding discussion. ↩︎ That is, the linear subspace in \\(\\mathbb{R}^p\\) which minimizes the sum of the squared euclidean distances to the points \\(x_i\\). ↩︎ When \\(M = \\min\\{n - 1, p\\}\\), the approximation is exact. ↩︎ More accurately, the sum on the right is an estimate of the sum on the left. In general there are \\(\\min\\{n-1, p\\}\\) principal components and \\[\\sum_{m = 1}^{\\min\\{n-1, p\\}} \\text{PVE}_m = 1\\] Indeed, if we take \\(M &lt; p\\) principal components, then we are truly doing dimensional reduction. In a supervised setting, however, we can treat the number of components as a tuning parameter. There is a nice information-theoretic interpretation of this statement. That we are taking an average is probably the reason for the “means” in “\\(K\\)-means.” For example, the commonly used squared Euclidean distance. See Choice of Dissimilarity Measure "]]
