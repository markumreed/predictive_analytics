[["index.html", "Predictive Analytics Chapter 1 Predictive Analytics", " Predictive Analytics Markum Reed 2021-01-06 Chapter 1 Predictive Analytics This course aims to go beyond the classical statistical methods. As computing power has increased many new, highly computational, regression, or “Machine Learning,” methods have been developed. There has been a significant expansion of the number of possible approaches. Since these methods are so new, the business community is generally unaware of their huge potential. With the explosion of “Big Data” problems, machine learning has become a hot field in many scientific areas as well as marketing, finance and other business disciplines. People with machine learning skills are in high demand. 1.0.1 Course Text In this course we will be following James, Gareth, et al. An introduction to statistical learning. Vol. 112. New York: springer, 2013.. 1.0.2 Course Material Course Material Reading Assignment Labs Due Date Introduction to Machine Learning Ch. 1,2 Intro to Python Linear Regression Ch. 3 Linear Regression Classification Ch. 4 Logistic Regression LDA QDA KNN Resampling Ch. 5 Cross-Validation and the Bootstrap Model Selection Ch. 6 Subset Selection Methods Elastic Net Regression PCR and PLS Moving Beyond Linearity Ch. 7 Non-linear Modeling Tree Based Methods Ch. 8 Decision Trees Support Vector Machines Ch. 9 Support Vector Machines Unsupervised Learning Ch. 10 Principal Components Analysis Clustering NCI60 Example "],["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter 4. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (R-bookdown?) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],["literature.html", "Chapter 3 Literature", " Chapter 3 Literature Here is a review of existing methods. "],["methods.html", "Chapter 4 Methods", " Chapter 4 Methods We describe our methods in this chapter. "],["applications.html", "Chapter 5 Applications 5.1 Example one 5.2 Example two", " Chapter 5 Applications Some significant applications are demonstrated in this chapter. 5.1 Example one 5.2 Example two "],["final-words.html", "Chapter 6 Final Words", " Chapter 6 Final Words We have finished a nice book. "],["references.html", "References", " References "],["conditionals-and-recursion.html", "Chapter 7 Conditionals and Recursion 7.1 Floor division and modulus 7.2 Boolean expressions 7.3 Logical operators 7.4 Conditional execution 7.5 Alternative execution 7.6 Chained conditionals 7.7 Nested conditionals 7.8 Recursion 7.9 Infinite recursion 7.10 Keyboard Input", " Chapter 7 Conditionals and Recursion if statement executes code depending on the state of the program 7.1 Floor division and modulus The floor division operator, //, divides two numbers and rounds down to an integer. Suppose you have a movie with a runtime of 105 minutes. You might want to know how long that is in hours. Conventional division returns a floating-point minutes = 105 minutes / 60 1.75 But we don’t normally write hours with decimal points. Floor division returns the integer number of hours, rounding down: hours = minutes // 60 hours 1 Modulus operator, %, which divides two numbers and returns the remainder. remainder = minutes % 60 remainder 45 def movie_time(minutes): &quot;&quot;&quot;Converts movie time from minutes to hours minutes. Output: ========= X hrs Y mins &quot;&quot;&quot; hrs = minutes // 60 mins = minutes % 60 print(str(hrs) + &#39; hrs &#39; + str(mins) +&#39; mins&#39;) movie_time(105) 1 hrs 45 mins 7.2 Boolean expressions A boolean expression is an expression that is either true or false. The following examples use the operator ==, which compares two operands and produces True if they are equal and False otherwise: 5 == 5 True 5 == 6 False type(True) bool type(False) bool The == operator is one of the relational operators; the others are: Relational Operator Description x != y x is not equal to y x &gt; y x is greater than y x &lt; y x is less than y x &gt;= y x is greater than or equal to y x &lt;= y x is less than or equal to y 7.3 Logical operators There are three logical operators: - and, or, and not. The meaning of these operators is similar to their meaning in English. For example, x &gt; 0 and x &lt; 10 is true only if x is greater than 0 and less than 10. 5 &gt; 0 and 5 &lt; 10 True 20 &gt; 0 and 20 &lt; 10 False n%2 == 0 or n%3 == 0 is true if either or both of the conditions is true, that is, if the number is divisible by 2 or 3 4 % 2 == 0 or 4 % 3 == 0 True 9 % 2 == 0 or 9 % 3 == 0 True 9 % 2 == 0 or 10 % 3 == 0 False the not operator negates a boolean expression, so not (x &gt; y) is true if x &gt; y is false, that is, if x is less than or equal to y. not (10 &gt; 5) False (10 &gt; 5) True 7.4 Conditional execution In order to write useful programs, we almost always need the ability to check conditions and change the behavior of the program accordingly. Conditional statements give us this ability. The simplest form is the if statement: x = 10 if x &gt; 0: print(&#39;x is positive&#39;) x is positive The boolean expression after if is called the condition. If it is true, the indented statement runs. If not, nothing happens. if statements have the same structure as function definitions: a header followed by an indented body. Statements like this are called compound statements. 7.5 Alternative execution A second form of the if statement is “alternative execution,” in which there are two possibilities and the condition determines which one runs. The alternatives are called branches, because they are branches in the flow of execution. x = 10 if x % 2 ==0: print(&#39;x is even&#39;) else: print(&#39;x is odd&#39;) x is even x = 9 if x % 2 ==0: print(&#39;x is even&#39;) else: print(&#39;x is odd&#39;) 7.6 Chained conditionals Sometimes there are more than two possibilities and we need more than two branches. elif is an abbreviation of “else if.” Exactly one branch will run. There is no limit on the number of elif statements. If there is an else clause, it has to be at the end, but there doesn’t have to be one. One way to express a computation like that is a chained conditional: x = 10 y = 11 if x &lt; y: print(&#39;x is less than y&#39;) elif x &gt; y: print(&#39;x is greater than y&#39;) else: print(&#39;x and y are equal&#39;) x is less than y x = 12 y = 11 if x &lt; y: print(&#39;x is less than y&#39;) elif x &gt; y: print(&#39;x is greater than y&#39;) else: print(&#39;x and y are equal&#39;) x = 12 y = 12 if x &lt; y: print(&#39;x is less than y&#39;) elif x &gt; y: print(&#39;x is greater than y&#39;) else: print(&#39;x and y are equal&#39;) 7.7 Nested conditionals One conditional can also be nested within another. The outer conditional contains two branches. The first branch contains a simple statement. The second branch contains another if statement, which has two branches of its own. Those two branches are both simple statements, although they could have been conditional statements as well. x = 12 y = 12 if x == y: print(&#39;x and y are equal&#39;) else: if x &lt; y: print(&#39;x is less than y&#39;) else: print(&#39;x is greater than y&#39;) x and y are equal x = 12 y = 11 if x == y: print(&#39;x and y are equal&#39;) else: if x &lt; y: print(&#39;x is less than y&#39;) else: print(&#39;x is greater than y&#39;) x = 10 y = 11 if x == y: print(&#39;x and y are equal&#39;) else: if x &lt; y: print(&#39;x is less than y&#39;) else: print(&#39;x is greater than y&#39;) Although the indentation of the statements makes the structure apparent, nested conditionals become difficult to read very quickly. It is a good idea to avoid them when you can. 7.8 Recursion It is legal for one function to call another; it is also legal for a function to call itself. It may not be obvious why that is a good thing, but it turns out to be one of the most magical things a program can do. 7.8.1 What is recursion in Python? Recursion is the process of defining something in terms of itself. A physical world example would be to place two parallel mirrors facing each other. Any object in between them would be reflected recursively. def countdown(n): if n &lt;= 0: print(&#39;Blastoff!&#39;) else: print(n) countdown(n-1) If n is 0 or negative, it outputs the word, “Blastoff!” Otherwise, it outputs n and then calls a function named countdown—itself—passing n-1 as an argument. countdown(5) 5 4 3 2 1 Blastoff! 7.9 Infinite recursion Infinite recursion is when a recursion never reaches a base case, it goes on making recursive calls forever, and the program never terminates. In most programming environments, a program with infinite recursion does not really run forever. Python reports an error message when the maximum recursion depth is reached: def recursion(): recursion() recursion() --------------------------------------------------------------------------- RecursionError Traceback (most recent call last) &lt;ipython-input-58-c6e0f7eb0cde&gt; in &lt;module&gt;() ----&gt; 1 recursion() &lt;ipython-input-57-d9b9ba688751&gt; in recursion() 1 def recursion(): ----&gt; 2 recursion() ... last 1 frames repeated, from the frame below ... &lt;ipython-input-57-d9b9ba688751&gt; in recursion() 1 def recursion(): ----&gt; 2 recursion() RecursionError: maximum recursion depth exceeded 7.10 Keyboard Input Python provides a built-in function called input that stops the program and waits for the user to type something. When the user presses Return or Enter, the program resumes and input returns what the user typed as a string. text = input() blah text &#39;blah&#39; number = input(&#39;Pick a number between 1 and 3:\\n&#39;) Pick a number between 1 and 3: 2 number &#39;2&#39; int(number) 2 What if they typed out the digits instead? number = input(&#39;Pick a number between 1 and 3:\\n&#39;) Pick a number between 1 and 3: two int(number) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-65-23a75fe4b6c6&gt; in &lt;module&gt;() ----&gt; 1 int(number) ValueError: invalid literal for int() with base 10: &#39;two&#39; "],["dictionaries.html", "Chapter 8 Dictionaries 8.1 Dictionary as a collection of counters 8.2 Looping and dictionaries 8.3 Reverse Lookup", " Chapter 8 Dictionaries A dictionary is like a list, but more general. In a list, the indices have to be integers; in a dictionary they can be (almost) any type. A dictionary contains a collection of indices, which are called keys, and a collection of values Each key is associated with a single value. The association of a key and a value is called a key-value pair Dictionaries represent a mapping from keys to values, so you can also say that each key “maps to” a value The function dict creates a new dictionary with no items en2ch = dict() en2ch {} squiggly-brackets, {}, represent an empty dictionary. To add items to the dictionary, you can use square brackets: en2ch[&#39;one&#39;] = &#39;yi&#39; This line creates an item that maps from the key ‘one’ to the value ‘yi.’ If we print the dictionary again, we see a key-value pair with a colon between the key and value: en2ch {&#39;one&#39;: &#39;yi&#39;} This output format is also an input format. For example, you can create a new dictionary with three items: en2ch = {&#39;one&#39;:&#39;yi&#39;,&#39;two&#39;:&#39;er&#39;,&#39;three&#39;:&#39;san&#39;} en2ch {&#39;one&#39;: &#39;yi&#39;, &#39;two&#39;: &#39;er&#39;, &#39;three&#39;: &#39;san&#39;} The order of the key-value pairs might not be the same If you type the same example on your computer, you might get a different result The order of items in a dictionary is unpredictable. Not a problem since the elements of a dictionary are never indexed with integer indices Instead, you use the keys to look up the corresponding values: en2ch[&#39;two&#39;] &#39;er&#39; The key ‘two’ always maps to the value ‘er’ so the order of the items doesn’t matter. If the key isn’t in the dictionary, you get an exception: en2ch[&#39;four&#39;] --------------------------------------------------------------------------- KeyError Traceback (most recent call last) &lt;ipython-input-14-bf386d070566&gt; in &lt;module&gt;() ----&gt; 1 en2ch[&#39;four&#39;] KeyError: &#39;four&#39; The len function works on dictionaries; it returns the number of key-value pairs: len(en2ch) 3 The in operator works on dictionaries; it tells you whether something appears as a key in the dictionary &#39;one&#39; in en2ch True &#39;yi&#39; in en2ch False To see whether something appears as a value in a dictionary, you can use the method values, which returns a collection of values, and then use the in operator: &#39;yi&#39; in en2ch.values() True en2ch.values() dict_values([&#39;yi&#39;, &#39;er&#39;, &#39;san&#39;]) 8.1 Dictionary as a collection of counters Suppose you are given a string and you want to count how many times each letter appears. An advantage of a dictionary implementation is that we don’t have to know ahead of time which letters appear in the string and we only have to make room for the letters that do appear def alpha_count(s): d = dict() for c in s: if c not in d: d[c] = 1 else: d[c] += 1 return d The first line of the function creates an empty dictionary. The for loop traverses the string. Each time through the loop, if the character c is not in the dictionary, we create a new item with key c and the initial value 1 (since we have seen this letter once). If c is already in the dictionary we increment d[c] h = alpha_count(&#39;the quick brown fox jumped over the lazy dog&#39;) h {&#39;t&#39;: 2, &#39;h&#39;: 2, &#39;e&#39;: 4, &#39; &#39;: 8, &#39;q&#39;: 1, &#39;u&#39;: 2, &#39;i&#39;: 1, &#39;c&#39;: 1, &#39;k&#39;: 1, &#39;b&#39;: 1, &#39;r&#39;: 2, &#39;o&#39;: 4, &#39;w&#39;: 1, &#39;n&#39;: 1, &#39;f&#39;: 1, &#39;x&#39;: 1, &#39;j&#39;: 1, &#39;m&#39;: 1, &#39;p&#39;: 1, &#39;d&#39;: 2, &#39;v&#39;: 1, &#39;l&#39;: 1, &#39;a&#39;: 1, &#39;z&#39;: 1, &#39;y&#39;: 1, &#39;g&#39;: 1} h indicates that the letters ‘t,’ ‘h’ appeared twice; ‘e’ appears four times; etc. Dictionaries have a method called get that takes a key and a default value. If the key appears in the dictionary, get returns the corresponding value; otherwise it returns the default value: h = alpha_count(&#39;a&#39;) h {&#39;a&#39;: 1} h.get(&#39;a&#39;,0) # Find &#39;a&#39;, if not return 0 1 h.get(&#39;b&#39;,0) 0 8.1.0.1 Exercise Use get to write alpha_count more concisely. def alpha_count(s): d = dict() for c in s: if c not in d: d[c] = 1 else: d[c] += 1 return d Hint: You should be able to eliminate the if statement. def alpha_count(s): d = dict() for c in s: d[c] = d.get(c, 0) + 1 return d alpha_count(&#39;carrot&#39;) {&#39;c&#39;: 1, &#39;a&#39;: 1, &#39;r&#39;: 2, &#39;o&#39;: 1, &#39;t&#39;: 1} 8.2 Looping and dictionaries If you use a dictionary in a for statement, it traverses the keys of the dictionary. For example, print_count prints each key and the corresponding value: def print_count(h): for k in h: print(k, h[k]) h = alpha_count(&#39;parrot&#39;) h {&#39;p&#39;: 1, &#39;a&#39;: 1, &#39;r&#39;: 2, &#39;o&#39;: 1, &#39;t&#39;: 1} print_count(h) p 1 a 1 r 2 o 1 t 1 The keys are in no particular order. To traverse the keys in sorted order, you can use the built-in function sorted: for key in sorted(h): print(key, h[key]) a 1 o 1 p 1 r 2 t 1 8.2.0.1 Exercise Use the sorted function to have print_count print a sorted dictionary. def print_count(h): for c in h: print(c, h[c]) def print_count(h): for c in sorted(h): print(c, h[c]) h = alpha_count(&#39;Pparrot&#39;.lower()) print_count(h) a 1 o 1 p 2 r 2 t 1 8.3 Reverse Lookup Given a dictionary d and a key k, it is easy to find the corresponding value v = d[k]. This operation is called a lookup. But what if you have v and you want to find k? You have two problems: 1. There might be more than one key that maps to the value v. 2. There is no simple syntax to do a reverse lookup; you have to search. Here is a function that takes a value and returns the first key that maps to that value: def reverse_lookup(d, v): for k in d: if d[k] == v: return k raise LookupError(&#39;value does not appear in the dictionary&#39;) h {&#39;p&#39;: 1, &#39;a&#39;: 1, &#39;r&#39;: 2, &#39;o&#39;: 1, &#39;t&#39;: 1} reverse_lookup(h,2) &#39;r&#39; reverse_lookup(h,1) &#39;p&#39; A reverse lookup is much slower than a forward lookup; if you have to do it often, or if the dictionary gets big, the performance of your program will suffer. "],["fruitful-functions-for-business.html", "Chapter 9 Fruitful Functions (For Business) 9.1 Return values 9.2 Boolean Functions 9.3 Recursion (Factorial Example) 9.4 Fibonacci Example 9.5 Checking types: Factorial Example 2", " Chapter 9 Fruitful Functions (For Business) Many of the Python functions we have used (the math functions) produce return values. But the functions we’ve written are all void: they have an effect, like printing a value, but they don’t have a return value Today you’ll learn to write fruitful functions 9.1 Return values Calling the function generates a return value, which we (usually) assign to a variable or use as part of an expression Today we are going to write fruitful functions The first exampe is cash_flow, which returns your cash flow given income and expenses def cash_flow(income, expenses): cf = income - expenses return cf cash_flow(10000, 9000.0) 1000.0 We have seen the return statement before, but in a fruitful function the return statement includes an expression. This statement means: “Return immediately from this function and use the following expression as a return value.” We could have written this function more concisely: def cash_flow(income, expenses): return income - expenses 9.1.0.0.1 Incremental development As you write larger functions, you might find yourself spending more time debugging. To deal with increasingly complex programs, you might want to try a process called incremental development. 9.1.1 Example Suppose you want to calculate compound interest: Compound interest refers to calculating the compounded interest, not just the interest gained on the principal invested or borrowed amount. \\[A = P(1+\\frac{r}{n})^{nt}\\] where A = New Principal (principal + interest) P = Original Principal Amount r = Annual Nominal Interest Rate (Float) t = Overall length of time the interest is applied n = Compounding frequency 9.1.1.1 Step 1 The first step is to consider what a comp_interest function should look like. - What are the inputs (parameters) and what is the output (return value)? - In this case, the inputs are four numbers. - The return value is the accrued amount (principal + interest) AND the total compound interest (represented by a floating-point value) 9.1.1.2 Step 2 Write an outline of the function: def comp_interest(principal, rate, n, t): return 0.0 Obviously, this version doesn’t compute our values; it always returns zero. But it is syntactically correct, and it runs, which means that you can test it before you make it more complicated. comp_interest(5000, 0.08, 4, 2) # Easy to solve 0.0 At this point we have confirmed that the function is syntactically correct, and we can start adding code to the body. 9.1.1.3 Step 3 A reasonable next step is to find the subvalues. def comp_interest(principal, rate, n, t): r_n = 1 + (rate / n) nt = n * t print(&#39;Compound rate: &#39;, r_n) print(&#39;Frequency: &#39;, nt) return 0.0 If the function is working, it should display: comp_interest(5000,0.08,4,2) Compound rate: 1.02 Frequency: 8 9.1.1.4 Step 4 Next compute the result def comp_interest(principal, rate, n, t): r_n = 1 + (rate / n) nt = n * t result = principal*r_n**nt return round(result,2) When you start out, you should add only a line or two of code at a time. As you gain more experience, you might find yourself writing and debugging bigger chunks. Either way, incremental development can save you a lot of debugging time. The key aspects of the process are: Start with a working program and make small incremental changes. At any point, if there is an error, you should have a good idea where it is. Use variables to hold intermediate values so you can display and check them. Once the program is working, you might want to remove some of the scaffolding or consolidate multiple statements into compound expressions, but only if it does not make the program difficult to read. 9.1.2 Example 2 \\[B = R [\\frac{1-(1+i)^{-(n-x)}}{i}]\\] def rem_bal(reg_pay, i, num_pay, pay_made): return 0.0 def rem_bal(reg_pay, i, num_pay, pay_made): a = 1+i b = -(num_pay-pay_made) c = 1 - a**b d = c / i result = reg_pay * d return round(result,2) rem_bal(5000, 0.08, 4, 2) 8916.32 9.2 Boolean Functions Functions can return booleans, which is often convenient for hiding complicated tests inside functions. For example: def is_divisible(x, y): if x % y == 0: return True else: return False It is common to give boolean functions names that sound like yes/no questions The result of the == operator is a boolean, so we can write the function more concisely by returning it directly: def is_divisible(x, y): return x % y == 0 9.3 Recursion (Factorial Example) The factorial of a non-negative integer n, denoted by n!, is the product of all positive integers less than or equal to n. \\[0! = 1\\] \\[n! = n(n-1)!\\] If you can write a recursive definition of something, you can write a Python program to evaluate it 9.3.1 Step 1: Decide on parameters factorial takes an integer def factorial(n): 9.3.2 Step 2: Add basic conditional argument if the argument happens to be 0, we return 1: def factorial(n): if n == 0: return 1 9.3.3 Step 3: Make it recursive Otherwise, we have to make a recursive call to 1. find the factorial of n-1 2. multiply it by n def factorial(n): if n == 0: return 1 else: recurse = factorial(n-1) result = n * recurse return result This should look similar to our countdown example def factorial(n): if n == 0: return 1 else: recurse = factorial(n-1) result = n * recurse return result factorial(500) 1220136825991110068701238785423046926253574342803192842192413588385845373153881997605496447502203281863013616477148203584163378722078177200480785205159329285477907571939330603772960859086270429174547882424912726344305670173270769461062802310452644218878789465754777149863494367781037644274033827365397471386477878495438489595537537990423241061271326984327745715546309977202781014561081188373709531016356324432987029563896628911658974769572087926928871281780070265174507768410719624390394322536422605234945850129918571501248706961568141625359056693423813008856249246891564126775654481886506593847951775360894005745238940335798476363944905313062323749066445048824665075946735862074637925184200459369692981022263971952597190945217823331756934581508552332820762820023402626907898342451712006207714640979456116127629145951237229913340169552363850942885592018727433795173014586357570828355780158735432768888680120399882384702151467605445407663535984174430480128938313896881639487469658817504506926365338175055478128640000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 9.4 Fibonacci Example fibonacci(0) = 0 fibonacci(1) = 1 fibonacci(n) = fibonacci(n−1) + fibonacci(n−2) Translated into a Python function: def fibonacci(n): if n ==0: return 0 elif n == 1: return 1 else: return fibonacci(n-1) + fibonacci(n-2) fibonacci(4) 3 9.5 Checking types: Factorial Example 2 What happens if we call factorial and give it 1.5 as an argument? factorial(1.5) RuntimeError: Maximum recursion depth exceeded It looks like an infinite recursion. How can that be? The function has a base case—when n == 0. But if n is not an integer, we can miss the base case and recurse forever 9.5.1 Solution Use the built-in function isinstance to verify the type of the argument def factorial(n): if not isinstance(n, int): print(&#39;Factorial is only defined for integers.&#39;) return None elif n &lt; 0: print(&#39;Factorial is not defined for negative integers.&#39;) return None elif n == 0: return 1 else: return n * factorial(n-1) factorial(&#39;bob&#39;) Factorial is only defined for integers. factorial(-2) Factorial is not defined for negative integers. factorial(0) 1 "],["functions.html", "Chapter 10 Functions 10.1 Function calls 10.2 Math functions 10.3 New Functions 10.4 Definitions and Uses 10.5 Parameters and arguments 10.6 Variables and parameters are local 10.7 Why functions", " Chapter 10 Functions function is a named sequence of statements that performs a computation. When you define a function, you specify the name and the sequence of statements. Later, you can “call” the function by name. 10.1 Function calls We’ve already see a function call: type(42) int The name of the function is type. The expression in parentheses is called the argument of the function. The result, for this function, is the type of the argument. It is common to say that a function “takes” an argument and “returns” a result. The result is also called the return value. int(&#39;32&#39;) # string to int 32 int(&#39;Hello&#39;) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-3-6765ce49acfe&gt; in &lt;module&gt;() ----&gt; 1 int(&#39;Hello&#39;) ValueError: invalid literal for int() with base 10: &#39;Hello&#39; float(32) # int / string to float 32.0 float(&#39;3.14&#39;) 3.14 str(31) # int / float to string &#39;31&#39; 10.2 Math functions Python has a math module that provides most of the familiar mathematical functions. A module is a file that contains a collection of related functions. Before we can use the functions in a module, we have to import it with an import statement: import math This statement creates a module object named math. math &lt;module &#39;math&#39; from &#39;/anaconda3/lib/python3.6/lib-dynload/math.cpython-36m-darwin.so&#39;&gt; The module object contains the functions and variables defined in the module. To access one of the functions, you have to specify the name of the module and the name of the function, separated by a period. This format is called dot notation 10.2.1 Example degrees = 45 radians = degrees / 180.0 * math.pi math.sin(radians) 0.7071067811865475 The expression math.pi gets the variable pi from the math module. Its value is a floating-point approximation of \\(\\pi\\), accurate to about 15 digits. math.pi 3.141592653589793 10.3 New Functions A function definition specifies the name of a new function and the sequence of statements that run when the function is called. def print_lyrics(): print(&quot;Hello darkness my old friend&quot;) print(&quot;Pink fluffy unicorns!&quot;) def is a keyword that indicates that this is a function definition. Defining a function creates a function object, which has type function: type(print_lyrics) function print_lyrics() Hello darkness my old friend Pink fluffy unicorns! Once you have defined a function, you can use it inside another function. def repeat_lyrics(): print_lyrics() print_lyrics() repeat_lyrics() Hello darkness my old friend Pink fluffy unicorns! Hello darkness my old friend Pink fluffy unicorns! 10.4 Definitions and Uses Pulling together the code fragments from the previous section, the whole program looks like this: def print_lyrics(): print(&quot;I&#39;m a lumberjack, and I&#39;m okay.&quot;) print(&quot;I sleep all night and I work all day.&quot;) def repeat_lyrics(): print_lyrics() print_lyrics() repeat_lyrics() This program contains two function definitions: print_lyrics and repeat_lyrics. You have to create a function before you can run it. In other words, the function definition has to run before the function gets called. 10.5 Parameters and arguments Some of the functions we have seen require arguments. Inside the function, the arguments are assigned to variables called parameters. Here is a definition for a function that takes an argument: def print_twice(param): print(param) print(param) print_twice(&#39;Hello&#39;) Hello Hello print_twice(42) 42 42 print_twice(math.pi) 3.141592653589793 3.141592653589793 print_twice(&#39;Spam &#39; * 10) Spam Spam Spam Spam Spam Spam Spam Spam Spam Spam Spam Spam Spam Spam Spam Spam Spam Spam Spam Spam print_twice(math.cos(math.pi)) -1.0 -1.0 The argument is evaluated before the function is called, so in the examples the expressions 'Spam '*10 and math.cos(math.pi) are only evaluated once. spam = &#39;Spam is the king of breakfast!&#39; print_twice(spam) Spam is the king of breakfast! Spam is the king of breakfast! 10.6 Variables and parameters are local When you create a variable inside a function, it is local, which means that it only exists inside the function. For example: def cat_twice(part1, part2): cat = part1 + part2 print_twice(cat) This function takes two arguments, concatenates them, and prints the result twice. Here is an example that uses it: line1 = &#39;Hello Darkness! &#39; line2 = &#39;Big Fluffy Unicorns!&#39; cat_twice(line1, line2) Hello Darkness! Big Fluffy Unicorns! Hello Darkness! Big Fluffy Unicorns! When cat_twice terminates, the variable cat is destroyed. If we try to print it, we get an exception: print(cat) --------------------------------------------------------------------------- NameError Traceback (most recent call last) &lt;ipython-input-28-34599fba884e&gt; in &lt;module&gt;() ----&gt; 1 print(cat) NameError: name &#39;cat&#39; is not defined 10.7 Why functions It may not be clear why it is worth the trouble to divide a program into functions. There are several reasons: Creating a new function gives you an opportunity to name a group of statements, which makes your program easier to read and debug. Functions can make a program smaller by eliminating repetitive code. Later, if you make a change, you only have to make it in one place. Dividing a long program into functions allows you to debug the parts one at a time and then assemble them into a working whole. Well-designed functions are often useful for many programs. Once you write and debug one, you can reuse it. "],["introduction-types-expressions.html", "Chapter 11 Introduction, Types, &amp; Expressions 11.1 Why Learn to Code?", " Chapter 11 Introduction, Types, &amp; Expressions 11.1 Why Learn to Code? 11.1.1 Outcomes Fluency: (Python) procedural programming Use assignments, conditionals, &amp; loops Create Python modules and programs Competency: object-oriented programming Recognize and use objects and classes Knowledge: Foundations for Data Science "],["why-python.html", "Chapter 12 Why Python?", " Chapter 12 Why Python? Low overhead little to learn before you start ‘doing’ easier for beginners designed with ‘rapid prototyping’ in mind Highly relevant to non-CS majors NumPy, SciPy and Pandas heavily used Modern language Popular for web applications Applicable to mobile app development Data Scientists Toolkit "],["storing-and-computing-data.html", "Chapter 13 Storing and Computing Data 13.1 Expressions 13.2 Types 13.3 How to tell the type of a value 13.4 Type: float (floating point) 13.5 Types: int (integers) 13.6 Type: bool (boolean) 13.7 Type: str (string) for text", " Chapter 13 Storing and Computing Data 13.1 Expressions An expression represents something Python evaluates it (turns it into a value) Similar to a calculator 2.3 # Literal (Evaluates to self) 2.3 (3*7 + 2) * 0.1 # An expression with four literals and some operators 2.3000000000000003 13.2 Types A set of values and operations on these values Examples of operations: +,-,/,* Meaning of operations depend on type MEMORIZE THIS DEFINITION 13.3 How to tell the type of a value Command: type(&lt;value&gt;) type(2) int 13.4 Type: float (floating point) Values: (approximations of) real numbers - With a “.”: a float literal (e.g., 2.0) - Without a decimal: an int literal (e.g., 2) Operations: + ,- ,* ,/ ,** - Notice: operator meaning can change from type to type Exponent notation useful for large (or small) values \\(-22.51e6\\) is \\(-22.51 * 10^6\\) or \\(-22510000\\) \\(22.51e-6\\) is \\(-22.51 * 10^{-6}\\) or \\(0.00002251\\) 13.5 Types: int (integers) Values: \\(\\dots , -3, -2, -1, 0, 1, 2, 3, 4, 5, \\dots\\) Operations: + ,- ,* , ** ,/ , // , % 13.6 Type: bool (boolean) Values: True, False - Booleans literals True and False (MUST BE CAPITALIZED) Operations: not, and, or - not b: True if b is false and False if b is true - b and c: True if both b and c are true; False otherwise - b or c: True if b is true or c is true; False otherwise Often come from comparing int or float values - Order comparisons: \\(i&lt;j\\) \\(i&lt;=j\\) \\(i&gt;=j\\) \\(i&gt;j\\) - Equality, inequality: \\(i == j\\) \\(i!=j\\) 13.7 Type: str (string) for text Values: any sequence of characters Operation(s): + (contenation, or concatenation) - operator + changes from type to type String literal: sequence of characters in quotes - Double quotes: “abc#&amp;$g&lt;” or “Hello World!” - Single quotes: ‘Hello World’ Concatenation applies only to strings - “ab” + “cd” evaluates to “abcd” - “ab” + 2 produces an ERROR "],["iteration.html", "Chapter 14 Iteration 14.1 Reassignment 14.2 Updating variables 14.3 while statement 14.4 break 14.5 Example: Square roots", " Chapter 14 Iteration iteration: the ability to run a block of statements repeatedly Saw a kind of iteration with recursion and using for loop 14.1 Reassignment Python uses the equal sign = for assignment Legal to make more than one assignment to the same variable New assignment makes an existing variable refer to a new value (and stop referring to the old value) x = 5 x 5 x = 7 x 7 a = 5 b = a # a &amp; b are equal a = 3 # a &amp; b are NOT equal b 5 Reassigning variables is useful, but you should use it with caution If the values of variables change frequently, it can make the code difficult to read and debug 14.2 Updating variables Common reassignment is an update, where the new value depends on the old c = c + 1 --------------------------------------------------------------------------- NameError Traceback (most recent call last) &lt;ipython-input-5-511f86f677ea&gt; in &lt;module&gt;() ----&gt; 1 c = c + 1 NameError: name &#39;c&#39; is not defined Before you can update a variable, you have to initialize it, usually with a simple assignment: z = 0 z = z + 1 Updating a variable by adding 1 is called an increment; subtracting 1 is called a decrement. 14.3 while statement Repeating identical or similar tasks without making errors is something that computers do well and people do poorly Since iteration is so common: Python provides features to make it easier def countdown(n): while n &gt; 0: print(n) n = n - 1 print(&#39;Blastoff!&#39;) countdown(3) 3 2 1 Blastoff! def countdown(n): while n &gt; 0: print(n) n = n - 1 print(&#39;Blastoff!&#39;) Here is the flow of execution for a while statement: Determine whether the condition is true or false. If false, exit the while statement and continue execution at the next statement. If the condition is true, run the body and then go back to step 1. This type of flow is called a loop because the third step loops back around to the top. def countdown(n): while n &gt; 0: print(n) n = n - 1 print(&#39;Blastoff!&#39;) The body of the loop should change the value of one or more variables so that the condition becomes false eventually and the loop terminates. Otherwise the loop will repeat forever, which is called an infinite loop. 14.4 break Sometimes you don’t know it’s time to end a loop until you get half way through the body In that case you can use the break statement to jump out of the loop. Suppose you want to take input from the user until they type done. You could write: while True: line = input(&#39;&gt; &#39;) if line == &#39;done&#39;: break print(line) print(&#39;Done&#39;) &gt; not done not done &gt; done Done while True: line = input(&#39;&gt; &#39;) if line == &#39;done&#39;: break print(line) print(&#39;Done&#39;) The loop condition is True, which is always true, so the loop runs until it hits the break statement This way of writing while loops is common because you can check the condition anywhere in the loop (not just at the top) and you can express the stop condition affirmatively rather than negatively E.g. - “stop when this happens” - “keep going until that happens” 14.5 Example: Square roots Loops are often used in programs that compute numerical results by starting with an approximate answer and iteratively improving it Suppose that you want to know the square root of a If you start with almost any estimate, x, you can compute a better estimate with the following formula: \\[y=\\frac{x+a/x}{2}\\] For example, if a is 4 and x is 3: a = 4 x = 3 y = (x + a/x) / 2 y The result is closer to the correct answer (√4 = 2). If we repeat the process with the new estimate, it gets even closer: # Run this a few times # y will get closer to 2 x = y y = (x + a/x) / 2 y 2.0 In general we don’t know ahead of time how many steps it takes to get to the right answer, but we know when we get there because the estimate stops changing When y == x, we can stop. Here is a loop that starts with an initial estimate, x, and improves it until it stops changing: while True: print(x) y = (x + a/x) / 2 if y == x: break x = y 2.0000000000262146 2.0 while True: print(x) y = (x + a/x) / 2 if y == x: break x = y def mysquart(a, x): while True: y = (x + a/x) / 2 if y == x: break x = y return x mysquart(4, 3) 2.1666666666666665 "],["lists.html", "Chapter 15 Lists 15.1 List == Sequence 15.2 Lists are Mutable 15.3 Traversing a list 15.4 List operations 15.5 List slices 15.6 List methods 15.7 Map, filter, reduce 15.8 Deleting Elements 15.9 Lists and strings 15.10 List arguments 15.11 Debugging", " Chapter 15 Lists 15.1 List == Sequence list: sequence of values Values inside of lists are elements or items There are several ways to create a new list; the simplest is to enclose the elements in square brackets [] [10,20,30,40] # list of 4 integers [10, 20, 30, 40] [&#39;Eli&#39;,&#39;Xu&#39;,&#39;Markum&#39;] # list of 3 strings [&#39;Eli&#39;, &#39;Xu&#39;, &#39;Markum&#39;] Elements of a list do NOT have to be the same type: [&#39;string&#39;,2.0,5,[10,20], True] # list of string, float, integer, another list [&#39;string&#39;, 2.0, 5, [10, 20], True] A list within another list is nested A list that contains no items is an empty list [] [] You can assign values to variables people = [&#39;Eli&#39;,&#39;Xu&#39;,&#39;Markum&#39;] numbers = [3,6,9] empty = [] print(people, numbers, empty) [&#39;Eli&#39;, &#39;Xu&#39;, &#39;Markum&#39;] [3, 6, 9] [] 15.2 Lists are Mutable Syntax for accessing the elements of a list is the same as for accessing the characters of a string—the bracket operator. The expression inside the brackets specifies the index. Remember that the indices start at 0: people[0] &#39;Eli&#39; pep = [&#39;e&#39;,&#39;x&#39;,&#39;m&#39;,[0,&#39;target&#39;]] pep[3][1] &#39;target&#39; Unlike strings, lists are mutable. When the bracket operator appears on the left side of an assignment, it identifies the element of the list that will be assigned. numbers = [10,20,30] numbers[1] = 21 numbers [10, 21, 30] List indices work the same way as string indices: Any integer expression can be used as an index. If you try to read or write an element that does not exist, you get an IndexError. If an index has a negative value, it counts backward from the end of the list. The in operator also works on lists: &#39;Xu&#39; in people True &#39;Bob&#39; in people False 15.3 Traversing a list The most common way to traverse the elements of a list is with a for loop. The syntax is the same as for strings: for p in people: print(&#39;Hello, &#39;, p) Hello, Eli Hello, Xu Hello, Markum people [&#39;Eli&#39;, &#39;Xu&#39;, &#39;Markum&#39;] for num in [1,2,3]: num2 = num * 2 print(num2) 2 4 6 This works well if you only need to read the elements of the list. But if you want to write or update the elements, you need the indices. A common way to do that is to combine the built-in functions range and len: numbers = [10, 20, 30] for i in range(len(numbers)): numbers[i] = numbers[i] * 2 numbers [20, 40, 60] How would we translate the above code into a function? Call the function double_it that takes a list, t. def double_it(t): &quot;&quot;&quot; double_it takes a list and doubles the values inside. t: list &quot;&quot;&quot; for i in range(len(t)): t[i] = t[i] * 2 return t double_it([1,2,3]) [2, 4, 6] 15.4 List operations The + operator concatenates lists: a = [1,2,3] b = [4,5,6] c = a + b c [1, 2, 3, 4, 5, 6] The * operator repeats a list a given number of times: [0] * 6 [0, 0, 0, 0, 0, 0] [1, 2, 3] * 3 # Waltz sequence [1, 2, 3, 1, 2, 3, 1, 2, 3] 15.5 List slices t = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;] t[1:3] [&#39;b&#39;, &#39;c&#39;] t[:4] [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;] t[2:] [&#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;] t[:] [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;] A slice operator on the left side of an assignment can update multiple elements: t[1:3] = [&#39;Change&#39;, &#39;This&#39;] t [3, &#39;Change&#39;, &#39;This&#39;, &#39;test&#39;, &#39;test&#39;, &#39;test&#39;] 15.6 List methods Python provides methods that operate on lists. - append adds a new element to the end of a list: t = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;] t.append(&#39;d&#39;) t [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;] extend takes a list as an argument and appends all of the elements: t1 = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;] t2 = [&#39;d&#39;,&#39;e&#39;] t1.extend(t2) t1 [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;] 15.7 Map, filter, reduce 15.7.1 Reduce An operation that combines a sequence of elements into a single value is called reduce. To add up all the number in a list, you can use a loop: def add_all(t): total = 0 for x in t: total += x return total add_all([1,2,3]) 6 total is initialized to 0. Each time through the loop, x gets one element from the list. The += operator provides a short way to update a variable. total += x is equivalent to total = total + x Adding up the elements of a list is such a common operation that Python provides it as a built-in function, sum: t = [1,2,3] sum(t) 6 15.7.2 Map Sometimes you want to traverse one list while building another. For example, the following function takes a list of strings and returns a new list that contains capitalized strings: def capitalize_all(t): res = [] for s in t: res.append(s.capitalize()) return res capitalize_all([&#39;this&#39;,&#39;is&#39;,&#39;neat&#39;]) [&#39;This&#39;, &#39;Is&#39;, &#39;Neat&#39;] res is initialized with an empty list; each time through the loop, we append the next element. An operation like capitalize_all is called a map because it “maps” a function onto each of the elements in a sequence. 15.7.3 Filter An operation to select some of the elements from a list and return a sublist. For example, the following function takes a list of strings and returns a list that contains only the uppercase strings: def only_upper(t): res = [] for s in t: if s.isupper(): res.append(s) return res only_upper([&#39;A&#39;,&#39;b&#39;,&#39;C&#39;]) [&#39;A&#39;, &#39;C&#39;] isupper is a string method that returns True if the string contains only upper case letters. An operation like only_upper is called a filter because it selects some of the elements and filters out the others. 15.8 Deleting Elements There are several ways to delete elements from a list. If you know the index of the element you want, you can use pop: t = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;] x = t.pop(1) t [&#39;a&#39;, &#39;c&#39;] x &#39;b&#39; pop modifies the list and returns the element that was removed. If you don’t provide an index, it deletes and returns the last element. If you don’t need the removed value, you can use the del operator: t = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;] del t[1] t [&#39;a&#39;, &#39;c&#39;] If you know the element you want to remove (but not the index), you can use remove: t = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;] t.remove(&#39;b&#39;) t [&#39;a&#39;, &#39;c&#39;] To remove more than one element, you can use del with a slice index: t = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;] del t[1:5] t [&#39;a&#39;, &#39;f&#39;] 15.9 Lists and strings A string is a sequence of characters and a list is a sequence of values, but a list of characters is not the same as a string. To convert from a string to a list of characters, you can use list: test = &#39;test&#39; t = list(test) t [&#39;t&#39;, &#39;e&#39;, &#39;s&#39;, &#39;t&#39;] The list function breaks a string into individual letters. If you want to break a string into words, you can use the split method: test = &#39;this is a test&#39; t = test.split() t [&#39;this&#39;, &#39;is&#39;, &#39;a&#39;, &#39;test&#39;] An optional argument called a delimiter specifies which characters to use as word boundaries. The following example uses a hyphen as a delimiter: hyp = &#39;H-E-L-L-O&#39; t = hyp.split(&quot;-&quot;) t [&#39;H&#39;, &#39;E&#39;, &#39;L&#39;, &#39;L&#39;, &#39;O&#39;] join is the inverse of split. It takes a list of strings and concatenates the elements. join is a string method, so you have to invoke it on the delimiter and pass the list as a parameter: t = [&#39;H&#39;, &#39;E&#39;, &#39;L&#39;, &#39;L&#39;, &#39;O&#39;] &#39;-&#39;.join(t) &#39;H-E-L-L-O&#39; &#39;&amp;&#39;.join(t) &#39;H&amp;E&amp;L&amp;L&amp;O&#39; &#39; &#39;.join(t) &#39;H E L L O&#39; 15.10 List arguments When you pass a list to a function, the function gets a reference to the list. If the function modifies the list, the caller sees the change. For example, delete_head removes the first element from a list: def delete_head(t): del t[0] The parameter t and the variable letters are aliases for the same object. letters = list(&#39;abcd&#39;) letters [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;] delete_head(letters) letters [&#39;b&#39;, &#39;c&#39;, &#39;d&#39;] It is important to distinguish between operations that modify lists and operations that create new lists. For example, the append method modifies a list, but the + operator creates a new list. t1 = [1, 2] t1.append(3) t1 [1, 2, 3] t2 # Returns None The return value from append is None. t3 = t1 + [4] t1 [1, 2, 3] t3 [1, 2, 3, 4] The result of the operator is a new list, and the original list is unchanged. This difference is important when you write functions that are supposed to modify lists. For example, this function does not delete the head of a list: def bad_delete_head(t): t = t[1:] # WRONG! The slice operator creates a new list and the assignment makes t refer to it, but that doesn’t affect the caller. t4 = [1, 2, 3] bad_delete_head(t4) t4 [1, 2, 3] At the beginning of bad_delete_head, t and t4 refer to the same list. At the end, t refers to a new list, but t4 still refers to the original, unmodified list. An alternative is to write a function that creates and returns a new list. For example, tail returns all but the first element of a list: def tail(t): return t[1:] This function leaves the original list unmodified. Here’s how it is used: letters = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;] rest = tail(letters) rest [&#39;b&#39;, &#39;c&#39;] letters = list(&#39;abcdefghi&#39;) letters[::-1] [&#39;i&#39;, &#39;h&#39;, &#39;g&#39;, &#39;f&#39;, &#39;e&#39;, &#39;d&#39;, &#39;c&#39;, &#39;b&#39;, &#39;a&#39;] x = [1,2,3] x[::-1] [3, 2, 1] 15.11 Debugging Careless use of lists can lead to long hours of debugging. 15.11.1 Most list methods modify the argument and return None This is the opposite of the string methods, which return a new string and leave the original alone. If you are used to writing string code like this: word = word.strip() It is tempting to write list code like this: t = t.sort() # WRONG! Sort returns None, the next operation you perform with t is likely to fail. 15.11.2 Pick an idiom and stick with it Part of the problem with lists is that there are too many ways to do things. For example, to remove an element from a list, you can use pop, remove, del, or slice To add an element, you can use the append method or the + operator. Assuming that t is a list and x is a list element, These are correct: t.append(x) t = t + [x] t += [x] And these are wrong: t.append([x]) # WRONG! t = t.append(x) # WRONG! t + [x] # WRONG! t = t + x # WRONG! Try out each of these examples to make sure you understand what they do. Notice that only the last one causes a runtime error; the other three are legal, but they do the wrong thing. 15.11.3 Make copies to avoid aliasing. If you want to use a method like sort that modifies the argument, but you need to keep the original list as well, you can make a copy. t = [3, 1, 2] t2 = t[:] t2.sort() t [3, 1, 2] t2 [1, 2, 3] In this example you could also use the built-in function sorted, which returns a new, sorted list and leaves the original alone. t2 = sorted(t) t [3, 1, 2] t2 [1, 2, 3] "],["python-basics.html", "Chapter 16 Python Basics", " Chapter 16 Python Basics Here is the introduction to python programming series. It is based on Downey, A. (2012). Think Python. \" O’Reilly Media, Inc.\".. Introduction to Python Dictionaries Functions: Part 1 Functions: Part 2 Iteration Strings Conditionals and Recursios Lists "],["strings.html", "Chapter 17 Strings 17.1 A string is a sequence 17.2 len 17.3 Traversal with a for loop 17.4 Example: Concatenation 17.5 Example: In-class question 17.6 String slices 17.7 Strings are immutable 17.8 Search 17.9 Looping and counting 17.10 String methods 17.11 in operator 17.12 Example 17.13 String comparison", " Chapter 17 Strings Strings are not like integers, floats, and booleans. A string is a sequence, which means it is an ordered collection of other values. 17.1 A string is a sequence A string is a sequence of characters You can access the characters one at a time with the bracket operator: fruit = &#39;coconut&#39; letter = fruit[1] letter &#39;o&#39; The expression in brackets is called an index. The index indicates which character in the sequence you want (hence the name). fruit = &#39;coconut&#39; letter = fruit[1] letter &#39;o&#39; For most people, the first letter of ‘coconut’ is c, not o. But for computer scientists, the index is an offset from the beginning of the string, and the offset of the first letter is zero. letter = fruit[0] letter &#39;c&#39; 17.2 len len is a built-in function that returns the number of characters in a string: fruit = &#39;lime&#39; len(fruit) 4 To get the last letter of a string, you might be tempted to try something like this: length = len(fruit) fruit[4] --------------------------------------------------------------------------- IndexError Traceback (most recent call last) &lt;ipython-input-10-9c7dbe8f62db&gt; in &lt;module&gt;() 1 length = len(fruit) ----&gt; 2 fruit[4] IndexError: string index out of range The reason for the IndexError is that there is no letter in ’banana’ with the index 6. Since we started counting at zero, the six letters are numbered 0 to 5. To get the last character, you have to subtract 1 from length: fruit[len(fruit) - 1] &#39;a&#39; Or you can use negative indices, which count backward from the end of the string. fruit[-1] 17.3 Traversal with a for loop A lot of computations involve processing a string one character at a time Often they start at the beginning, select each character in turn, do something to it, and continue until the end This pattern of processing is called a traversal. One way to write a traversal is with a while loop: fruit = &#39;kiwi&#39; index = 0 while index &lt; len(fruit): letter = fruit[index] print(letter) index = index + 1 Another way to write a traversal is with a for loop: for letter in fruit: print(letter) 17.4 Example: Concatenation The following example shows how to use concatenation (string addition) and a for loop to generate an abecedarian series (that is, in alphabetical order). In Robert McCloskey’s book Make Way for Ducklings, the names of the ducklings are Jack, Kack, Lack, Mack, Nack, Ouack, Pack, and Quack. This loop outputs these names in order: prefixes = &#39;JKLMNOPQ&#39; suffix = &#39;ack&#39; for letter in prefixes: print(letter + suffix) Jack Kack Lack Mack Nack Oack Pack Qack 17.5 Example: In-class question Notice that Ouack and Quack are misspelled: How do you fix this? prefixes = &#39;JKLMNOPQ&#39; suffix = &#39;ack&#39; for letter in prefixes: if letter == &#39;O&#39; or letter == &#39;Q&#39;: print(letter + &#39;u&#39; + suffix) else: print(letter + suffix) Jack Kack Lack Mack Nack Ouack Pack Quack 17.6 String slices Slice: a segment of a string. - Selecting a slice is similar to selecting a character - The operator [n:m] returns the part of the string from the \\(n^{th}\\) character to the \\(m^{th}\\) character - including the first but excluding the last. s = &#39;Henderson Reddies&#39; s[0:9] s[10:17] If you omit the first index (before the colon), the slice starts at the beginning of the string. If you omit the second index, the slice goes to the end of the string: fruit = &#39;peach&#39; fruit[:3] fruit[3:] 17.7 Strings are immutable You cannot use the [] operator to change characters in a string: greeting = &#39;Hello, students!&#39; greeting[0] = &#39;J&#39; The reason for the error is that strings are immutable, which means you can’t change an existing string. The best you can do is create a new string that is a variation on the original: greeting = &#39;Hello, students!&#39; new_greeting = &#39;J&#39; + greeting[1:] new_greeting This example concatenates a new first letter onto a slice of greeting. It has no effect on the original string. 17.8 Search What does the following function do? def find(word, letter): index = 0 while index &lt; len(word): if word[index] == letter: return index index = index + 1 return -1 In a sense, find is the inverse of the [] operator. Instead of taking an index and extracting the corresponding character, it takes a character and finds the index where that character appears. If the character is not found, the function returns -1. This pattern of computation—traversing a sequence and returning when we find what we are looking for—is called a search. 17.9 Looping and counting This program counts the number of times the letter a appears in a string: word = &#39;mississippi&#39; count = 0 for letter in word: if letter == &#39;i&#39;: count = count + 1 print(count) This program demonstrates another pattern of computation called a counter. The variable count is initialized to 0 and then incremented each time an a is found. When the loop exits, count contains the result—the total number of a’s. 17.10 String methods Strings provide methods that perform a variety of useful operations. A method is similar to a function—it takes arguments and returns a value—but the syntax is different. Instead of the function syntax upper(word), it uses the method syntax word.upper(). word = &#39;reddie&#39; new_word = word.upper() new_word new_word = word.upper() This form of dot notation specifies the name of the method, upper, and the name of the string to apply the method to, word. The empty parentheses indicate that this method takes no arguments. A method call is called an invocation; 17.11 in operator The word in is a boolean operator that takes two strings and returns True if the first appears as a substring in the second: &#39;e&#39; in &#39;reddie&#39; &#39;f&#39; in &#39;reddie&#39; 17.12 Example The following function prints all the letters from word1 that also appear in word2: def in_both(word1, word2): for letter in word1: if letter in word2: print(letter) in_both(&#39;Henderson&#39;, &#39;Reddies&#39;) 17.13 String comparison The relational operators work on strings. To see if two strings are equal: word = &#39;apple&#39; if word == &#39;banana&#39;: print(&#39;All right, bananas.&#39;) Other relational operations are useful for putting words in alphabetical order: word = &#39;banana&#39; # Try apple and pineapple if word &lt; &#39;banana&#39;: print(&#39;Your word, &#39; + word + &#39;, comes before banana.&#39;) elif word &gt; &#39;banana&#39;: print(&#39;Your word, &#39; + word + &#39;, comes after banana.&#39;) else: print(&#39;All right, bananas.&#39;) "],["numpy.html", "Chapter 18 NumPy 18.1 Using NumPy", " Chapter 18 NumPy NumPy (or Numpy) is a Linear Algebra Library for Python, the reason it is so important for Data Science with Python is that almost all of the libraries in the PyData Ecosystem rely on NumPy as one of their main building blocks. Numpy is also incredibly fast, as it has bindings to C libraries. We will only learn the basics of NumPy 18.1 Using NumPy Import it as a library: import numpy as np Numpy has many built-in functions and capabilities. We won’t cover them all but instead we will focus on some of the most important aspects of Numpy: vectors,arrays,matrices, and number generation. Let’s start by discussing arrays. "],["numpy-arrays.html", "Chapter 19 Numpy Arrays 19.1 Creating NumPy Arrays 19.2 Built-in Methods 19.3 eye 19.4 Random 19.5 Array Attributes and Methods 19.6 Reshape 19.7 Shape", " Chapter 19 Numpy Arrays NumPy arrays are the main way we will use Numpy throughout the course. Numpy arrays essentially come in two flavors: vectors and matrices. Vectors are strictly 1-d arrays and matrices are 2-d (but you should note a matrix can still have only one row or one column). Let’s begin our introduction by exploring how to create NumPy arrays. 19.1 Creating NumPy Arrays 19.1.1 From a Python List We can create an array by directly converting a list or list of lists: my_list = [1,2,3] my_list [1, 2, 3] np.array(my_list) array([1, 2, 3]) my_matrix = [[1,2,3],[4,5,6],[7,8,9]] my_matrix [[1, 2, 3], [4, 5, 6], [7, 8, 9]] np.array(my_matrix) array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) 19.2 Built-in Methods There are lots of built-in ways to generate Arrays 19.2.1 arange Return evenly spaced values within a given interval. np.arange(0,10) array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) np.arange(0,11,2) array([ 0, 2, 4, 6, 8, 10]) 19.2.2 zeros and ones Generate arrays of zeros or ones np.zeros(3) array([0., 0., 0.]) np.zeros((5,5)) array([[ 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0.]]) np.ones(3) array([ 1., 1., 1.]) np.ones((3,3)) array([[ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.]]) 19.2.3 linspace Return evenly spaced numbers over a specified interval. np.linspace(0,10,3) array([ 0., 5., 10.]) np.linspace(0,10,50) array([ 0. , 0.20408163, 0.40816327, 0.6122449 , 0.81632653, 1.02040816, 1.2244898 , 1.42857143, 1.63265306, 1.83673469, 2.04081633, 2.24489796, 2.44897959, 2.65306122, 2.85714286, 3.06122449, 3.26530612, 3.46938776, 3.67346939, 3.87755102, 4.08163265, 4.28571429, 4.48979592, 4.69387755, 4.89795918, 5.10204082, 5.30612245, 5.51020408, 5.71428571, 5.91836735, 6.12244898, 6.32653061, 6.53061224, 6.73469388, 6.93877551, 7.14285714, 7.34693878, 7.55102041, 7.75510204, 7.95918367, 8.16326531, 8.36734694, 8.57142857, 8.7755102 , 8.97959184, 9.18367347, 9.3877551 , 9.59183673, 9.79591837, 10. ]) 19.3 eye Creates an identity matrix np.eye(4) array([[ 1., 0., 0., 0.], [ 0., 1., 0., 0.], [ 0., 0., 1., 0.], [ 0., 0., 0., 1.]]) 19.4 Random Numpy also has lots of ways to create random number arrays: 19.4.1 rand Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1). np.random.rand(2) array([ 0.11570539, 0.35279769]) np.random.rand(5,5) array([[ 0.66660768, 0.87589888, 0.12421056, 0.65074126, 0.60260888], [ 0.70027668, 0.85572434, 0.8464595 , 0.2735416 , 0.10955384], [ 0.0670566 , 0.83267738, 0.9082729 , 0.58249129, 0.12305748], [ 0.27948423, 0.66422017, 0.95639833, 0.34238788, 0.9578872 ], [ 0.72155386, 0.3035422 , 0.85249683, 0.30414307, 0.79718816]]) 19.4.2 randn Return a sample (or samples) from the “standard normal” distribution. Unlike rand which is uniform: np.random.randn(2) array([-0.27954018, 0.90078368]) np.random.randn(5,5) array([[ 0.70154515, 0.22441999, 1.33563186, 0.82872577, -0.28247509], [ 0.64489788, 0.61815094, -0.81693168, -0.30102424, -0.29030574], [ 0.8695976 , 0.413755 , 2.20047208, 0.17955692, -0.82159344], [ 0.59264235, 1.29869894, -1.18870241, 0.11590888, -0.09181687], [-0.96924265, -1.62888685, -2.05787102, -0.29705576, 0.68915542]]) 19.4.3 randint Return random integers from low (inclusive) to high (exclusive). np.random.randint(1,100) 44 np.random.randint(1,100,10) array([13, 64, 27, 63, 46, 68, 92, 10, 58, 24]) 19.5 Array Attributes and Methods Let’s discuss some useful attributes and methods or an array: arr = np.arange(25) ranarr = np.random.randint(0,50,10) arr array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]) ranarr array([10, 12, 41, 17, 49, 2, 46, 3, 19, 39]) 19.6 Reshape Returns an array containing the same data with a new shape. arr.reshape(5,5) array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]]) 19.6.1 max,min,argmax,argmin These are useful methods for finding max or min values. Or to find their index locations using argmin or argmax ranarr array([10, 12, 41, 17, 49, 2, 46, 3, 19, 39]) ranarr.max() 49 ranarr.argmax() 4 ranarr.min() 2 ranarr.argmin() 5 19.7 Shape Shape is an attribute that arrays have (not a method): # Vector arr.shape (25,) # Notice the two sets of brackets arr.reshape(1,25) array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]]) arr.reshape(1,25).shape (1, 25) arr.reshape(25,1) array([[ 0], [ 1], [ 2], [ 3], [ 4], [ 5], [ 6], [ 7], [ 8], [ 9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24]]) arr.reshape(25,1).shape (25, 1) 19.7.1 dtype You can also grab the data type of the object in the array: arr.dtype dtype(&#39;int64&#39;) "],["numpy-indexing-and-selection.html", "Chapter 20 NumPy Indexing and Selection 20.1 Bracket Indexing and Selection 20.2 Broadcasting 20.3 Broadcasting 20.4 Broadcasting (DANGERS) 20.5 Copying 20.6 Indexing a 2D array (matrices) 20.7 Indexing a 2D array (matrices) 20.8 Indexing a 2D array (matrices) 20.9 More Indexing Help 20.10 Selection 20.11 Selection 20.12 Selection", " Chapter 20 NumPy Indexing and Selection In this lecture we will discuss how to select elements or groups of elements from an array. #Creating sample array arr = np.arange(0,11) #Show arr array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) 20.1 Bracket Indexing and Selection The simplest way to pick one or some elements of an array looks very similar to python lists: #Get a value at an index arr[8] 8 #Get values in a range arr[1:5] array([1, 2, 3, 4]) #Get values in a range arr[0:5] array([0, 1, 2, 3, 4]) 20.2 Broadcasting Numpy arrays differ from a normal Python list because of their ability to broadcast: #Setting a value with index range (Broadcasting) arr[0:5]=100 #Show arr array([100, 100, 100, 100, 100, 5, 6, 7, 8, 9, 10]) # Reset array arr = np.arange(0,11) #Show arr array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) 20.3 Broadcasting #Important notes on Slices slice_of_arr = arr[0:6] #Show slice slice_of_arr array([0, 1, 2, 3, 4, 5]) #Change Slice slice_of_arr[:]=99 #Show Slice again slice_of_arr array([99, 99, 99, 99, 99, 99]) 20.4 Broadcasting (DANGERS) Now note the changes also occur in our original array! arr array([99, 99, 99, 99, 99, 99, 6, 7, 8, 9, 10]) Data is not copied, it’s a view of the original array! This avoids memory problems! 20.5 Copying #To get a copy, need to be explicit arr_copy = arr.copy() arr_copy array([99, 99, 99, 99, 99, 99, 6, 7, 8, 9, 10]) 20.6 Indexing a 2D array (matrices) The general format is arr_2d[row][col] or arr_2d[row,col]. I recommend usually using the comma notation for clarity. arr_2d = np.array(([5,10,15],[20,25,30],[35,40,45])) #Show arr_2d array([[ 5, 10, 15], [20, 25, 30], [35, 40, 45]]) #Indexing row arr_2d[1] array([20, 25, 30]) 20.7 Indexing a 2D array (matrices) # Format is arr_2d[row][col] or arr_2d[row,col] # Getting individual element value arr_2d[1][0] 20 # Getting individual element value arr_2d[1,0] 20 20.8 Indexing a 2D array (matrices) # 2D array slicing #Shape (2,2) from top right corner arr_2d[:2,1:] array([[10, 15], [25, 30]]) #Shape bottom row arr_2d[2] array([35, 40, 45]) #Shape bottom row arr_2d[2,:] array([35, 40, 45]) 20.8.1 Fancy Indexing Fancy indexing allows you to select entire rows or columns out of order,to show this, let’s quickly build out a numpy array: #Set up matrix arr2d = np.zeros((10,10)) #Length of array arr_length = arr2d.shape[1] #Set up array for i in range(arr_length): arr2d[i] = i arr2d array([[ 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [ 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [ 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.], [ 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.], [ 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.], [ 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.], [ 6., 6., 6., 6., 6., 6., 6., 6., 6., 6.], [ 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.], [ 8., 8., 8., 8., 8., 8., 8., 8., 8., 8.], [ 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.]]) 20.8.2 Fancy Indexing Fancy indexing allows the following arr2d[[2,4,6,8]] array([[ 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.], [ 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.], [ 6., 6., 6., 6., 6., 6., 6., 6., 6., 6.], [ 8., 8., 8., 8., 8., 8., 8., 8., 8., 8.]]) #Allows in any order arr2d[[6,4,2,7]] array([[ 6., 6., 6., 6., 6., 6., 6., 6., 6., 6.], [ 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.], [ 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.], [ 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.]]) 20.9 More Indexing Help Indexing a 2d matrix can be a bit confusing at first, especially when you start to add in step size. 20.10 Selection Let’s briefly go over how to use brackets for selection based off of comparison operators. arr = np.arange(1,11) arr array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) arr &gt; 4 array([False, False, False, False, True, True, True, True, True, True]) 20.11 Selection bool_arr = arr&gt;4 # Sometimes called a mask bool_arr array([False, False, False, False, True, True, True, True, True, True]) 20.12 Selection arr[bool_arr] array([ 5, 6, 7, 8, 9, 10]) arr[arr&gt;2] array([ 3, 4, 5, 6, 7, 8, 9, 10]) x = 2 arr[arr&gt;x] array([ 3, 4, 5, 6, 7, 8, 9, 10]) "],["numpy-operations.html", "Chapter 21 NumPy Operations 21.1 Arithmetic 21.2 Arithmetic 21.3 Arithmetic 21.4 Arithmetic 21.5 Arithmetic 21.6 Universal Array Functions 21.7 Universal Array Functions 21.8 Universal Array Functions 21.9 Great Job!", " Chapter 21 NumPy Operations 21.1 Arithmetic You can easily perform array with array arithmetic, or scalar with array arithmetic. Let’s see some examples: arr = np.arange(0,10) 21.2 Arithmetic arr + arr array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]) arr * arr array([ 0, 1, 4, 9, 16, 25, 36, 49, 64, 81]) arr - arr array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) 21.3 Arithmetic # Warning on division by zero, but not an error! # Just replaced with nan arr/arr /home/markumreed/anaconda3/envs/ds4b/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide This is separate from the ipykernel package so we can avoid doing imports until array([nan, 1., 1., 1., 1., 1., 1., 1., 1., 1.]) 21.4 Arithmetic # Also warning, but not an error instead infinity 1/arr /home/markumreed/anaconda3/envs/ds4b/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in true_divide array([ inf, 1. , 0.5 , 0.33333333, 0.25 , 0.2 , 0.16666667, 0.14285714, 0.125 , 0.11111111]) 21.5 Arithmetic arr**3 array([ 0, 1, 8, 27, 64, 125, 216, 343, 512, 729]) 21.6 Universal Array Functions Numpy comes with many universal array functions, which are essentially just mathematical operations you can use to perform the operation across the array. Let’s show some common ones: #Taking Square Roots np.sqrt(arr) array([ 0. , 1. , 1.41421356, 1.73205081, 2. , 2.23606798, 2.44948974, 2.64575131, 2.82842712, 3. ]) 21.7 Universal Array Functions #Calcualting exponential (e^) np.exp(arr) array([ 1.00000000e+00, 2.71828183e+00, 7.38905610e+00, 2.00855369e+01, 5.45981500e+01, 1.48413159e+02, 4.03428793e+02, 1.09663316e+03, 2.98095799e+03, 8.10308393e+03]) np.max(arr) #same as arr.max() 9 21.8 Universal Array Functions np.sin(arr) array([ 0. , 0.84147098, 0.90929743, 0.14112001, -0.7568025 , -0.95892427, -0.2794155 , 0.6569866 , 0.98935825, 0.41211849]) np.log(arr) /home/markumreed/anaconda3/envs/ds4b/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log &quot;&quot;&quot;Entry point for launching an IPython kernel. array([ -inf, 0. , 0.69314718, 1.09861229, 1.38629436, 1.60943791, 1.79175947, 1.94591015, 2.07944154, 2.19722458]) 21.9 Great Job! That’s all we need to know for now! "],["introduction.html", "Chapter 22 Introduction", " Chapter 22 Introduction In this part of the book, you’ll learn about data wrangling, the art of getting your data into R in a useful form for visualisation and modelling. Data wrangling is very important: without it you can’t work with your own data! There are three main parts to data wrangling: Data Scince Wrangling Workflow: Import, Tidy, Transform This part of the book proceeds as follows: In DataFrames, you’ll learn about the variant of the data frame that we use in this book: the tibble. You’ll learn what makes them different from regular data frames, and how you can construct them “by hand.” In data import, you’ll learn how to get your data from disk and into R. We’ll focus on plain-text rectangular formats, but will give you pointers to packages that help with other types of data. In tidy data, you’ll learn about tidy data, a consistent way of storing your data that makes transformation, visualisation, and modelling easier. You’ll learn the underlying principles, and how to get your data into a tidy form. Data wrangling also encompasses data transformation, which you’ve already learned a little about. Now we’ll focus on new skills for three specific types of data you will frequently encounter in practice: Relational data will give you tools for working with multiple interrelated datasets. Strings will introduce regular expressions, a powerful tool for manipulating strings. Categoricals are how Python stores categorical data. They are used when a variable has a fixed set of possible values, or when you want to use a non-alphabetical ordering of a string. Time series will give you the key tools for working with dates and date-times. "],["dataframes.html", "Chapter 23 DataFrames 23.1 Introduction 23.2 Creating a dataframe using List: 23.3 Creating DataFrame from dict of ndarray/lists: 23.4 Dealing with Rows and Columns 23.5 Select Multiple Columns 23.6 Row Selection: 23.7 Indexing and Selecting Data", " Chapter 23 DataFrames 23.1 Introduction Throughout this book we work with DataFrames. A DataFrame is a two-dimensional data structure, i.e., data is aligned in a tabular fashion in rows and columns. Pandas DataFrame consists of three principal components, the data, rows, and columns. DataFrame Anatomy We will get a brief insight on all these basic operation which can be performed on Pandas DataFrame : Creating a DataFrame Dealing with Rows and Columns Indexing and Selecting Data Working with Missing Data Iterating over rows and columns In the real world, a Pandas DataFrame will be created by loading the datasets from existing storage, storage can be SQL Database, CSV file, and Excel file. Pandas DataFrame can be created from the lists, dictionary, and from a list of dictionary etc. Dataframe can be created in different ways here are some ways by which we create a dataframe: 23.2 Creating a dataframe using List: DataFrame can be created using a single list or a list of lists. import pandas as pd Create a list of strings #@title Introducing Lists { display-mode: &quot;form&quot; } #@markdown This 4-minute video gives an overview of the key features of Booleans: from IPython.display import YouTubeVideo YouTubeVideo(&#39;BCN4PRoQnI4&#39;, width=600, height=400) state_lst = [&#39;California&#39;,&#39;Texas&#39;,&#39;New York&#39;,&#39;Florida&#39;,&#39;Illinois&#39;] Call the DataFrame constructor on the list. df = pd.DataFrame(state_lst) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 0 California 1 Texas 2 New York 3 Florida 4 Illinois 23.3 Creating DataFrame from dict of ndarray/lists: To create DataFrame from dict of narray/list, all the narray must be of same length. If index is passed then the length index should be equal to the length of arrays. If no index is passed, then by default, index will be range(n) where n is the array length. Intialise dictionary of lists. data = {&#39;state&#39;:[&#39;California&#39;,&#39;Texas&#39;,&#39;New York&#39;,&#39;Florida&#39;,&#39;Illinois&#39;], &#39;pop&#39;:[3833252,26448193,19651127,19552860,12882135], &#39;area&#39;: [423967,695662,141297,170312,149995]} #@title Introducing Dictionaries { display-mode: &quot;form&quot; } #@markdown This 3-minute video gives an overview of the key features of Booleans: from IPython.display import YouTubeVideo YouTubeVideo(&#39;1LRepvqzXzM&#39;, width=600, height=400) df = pd.DataFrame(data) df.set_index(keys=&#39;state&#39;, inplace=True) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pop area state California 3833252 423967 Texas 26448193 695662 New York 19651127 141297 Florida 19552860 170312 Illinois 12882135 149995 23.4 Dealing with Rows and Columns A Data frame is a two-dimensional data structure, i.e., data is aligned in a tabular fashion in rows and columns. We can perform basic operations on rows/columns like selecting, deleting, adding, and renaming. 23.4.1 Column Selection: The individual Series that make up the columns of the DataFrame can be accessed via dictionary-style indexing of the column name: df[&#39;area&#39;] state_name California 423967 Texas 695662 New York 141297 Florida 170312 Illinois 149995 Name: area, dtype: int64 Equivalently, we can use attribute-style access with column names that are strings: df.area state_name California 423967 Texas 695662 New York 141297 Florida 170312 Illinois 149995 Name: area, dtype: int64 Though this is a useful shorthand, keep in mind that it does not work for all cases! For example, if the column names are not strings, or if the column names conflict with methods of the DataFrame, this attribute-style access is not possible. For example, the DataFrame has a pop() method, so data.pop will point to this rather than the “pop” column: df.pop is df[&#39;pop&#39;] False In particular, you should avoid the temptation to try column assignment via attribute (i.e., use data['pop'] = z rather than data.pop = z). Like with the Series objects discussed earlier, this dictionary-style syntax can also be used to modify the object, in this case adding a new column: df[&#39;density&#39;] = df[&#39;pop&#39;] / df[&#39;area&#39;] df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pop area density state California 3833252 423967 9.041392 Texas 26448193 695662 38.018740 New York 19651127 141297 139.076746 Florida 19552860 170312 114.806121 Illinois 12882135 149995 85.883763 23.5 Select Multiple Columns df[[&#39;density&#39;,&#39;area&#39;]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } density area state California 9.041392 423967 Texas 38.018740 695662 New York 139.076746 141297 Florida 114.806121 170312 Illinois 85.883763 149995 23.6 Row Selection: Pandas provide a unique method to retrieve rows from a Data frame. DataFrame.loc[] method is used to retrieve rows from Pandas DataFrame. Rows can also be selected by passing integer location to an iloc[] function. df.loc[&quot;California&quot;] pop 3.833252e+06 area 4.239670e+05 density 9.041392e+00 Name: California, dtype: float64 df.loc[&quot;Texas&quot;] pop 2.644819e+07 area 6.956620e+05 density 3.801874e+01 Name: Texas, dtype: float64 23.7 Indexing and Selecting Data Indexing in pandas means simply selecting particular rows and columns of data from a DataFrame. Indexing could mean selecting all the rows and some of the columns, some of the rows and all of the columns, or some of each of the rows and columns. Indexing can also be known as Subset Selection. 23.7.1 Indexing a Dataframe using indexing operator [] : Indexing operator is used to refer to the square brackets following an object. The .loc and .iloc indexers also use the indexing operator to make selections. In this indexing operator to refer to df[]. 23.7.1.1 Selecting Single Columns In order to select a single column, we simply put the name of the column in-between the brackets df[&#39;density&#39;] state California 9.041392 Texas 38.018740 New York 139.076746 Florida 114.806121 Illinois 85.883763 Name: density, dtype: float64 23.7.1.2 Selecting Multiple Columns df[[&#39;area&#39;,&#39;density&#39;]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area density state California 423967 9.041392 Texas 695662 38.018740 New York 141297 139.076746 Florida 170312 114.806121 Illinois 149995 85.883763 23.7.2 Indexing a DataFrame using .loc[ ] : This function selects data by the label of the rows and columns. The df.loc indexer selects data in a different way than just the indexing operator. It can select subsets of rows or columns. It can also simultaneously select subsets of rows and columns. 23.7.2.1 Selecting a single row In order to select a single row using .loc[], we put a single row label in a .loc function. df.loc[&quot;Florida&quot;] pop 1.955286e+07 area 1.703120e+05 density 1.148061e+02 Name: Florida, dtype: float64 23.7.2.2 Selecting multiple rows df.loc[[&#39;Florida&#39;,&#39;Illinois&#39;]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pop area density state Florida 19552860 170312 114.806121 Illinois 12882135 149995 85.883763 23.7.3 Indexing a DataFrame using .iloc[ ] : This function allows us to retrieve rows and columns by position. In order to do that, we’ll need to specify the positions of the rows that we want, and the positions of the columns that we want as well. The df.iloc indexer is very similar to df.loc but only uses integer locations to make its selections. 23.7.3.1 Selecting a single row In order to select a single row using .iloc[], we can pass a single integer to .iloc[] function. df.iloc[2] pop 1.965113e+07 area 1.412970e+05 density 1.390767e+02 Name: New York, dtype: float64 23.7.3.2 Selecting multiple rows df.iloc[[0,2]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pop area density state California 3833252 423967 9.041392 New York 19651127 141297 139.076746 Next Section: Data Import "],["introduction-to-pandas.html", "Chapter 24 Introduction to Pandas 24.1 Overview of Pandas Capabilities", " Chapter 24 Introduction to Pandas Pandas is an open source library built on top of NumPy Allows fast analysis, cleaning and preparation of data High performance and productivity Built-in visualization capability Work with data from many sources 24.1 Overview of Pandas Capabilities Series DataFrames Selection Missing Data Operations Merging, Joining, and Concatenating GroupBy Reshaping Data and Pivot Tables Time Series Data Input/Output "],["pandas.html", "Chapter 25 Pandas", " Chapter 25 Pandas This is a basic introduction to the pandas module. First we start off with the customary imports. import numpy as np import pandas as pd np.random.seed(42) "],["object-creation.html", "Chapter 26 Object Creation 26.1 Series 26.2 Series Creation 26.3 Series 26.4 Series Examples 26.5 Series Examples 26.6 Series Example 26.7 Series", " Chapter 26 Object Creation 26.1 Series Similar to NumPy array Built on top of it Can have axis labels 26.2 Series Creation Here we will show a few ways to create series Throughout the course we will be primarily dealing with DataFrames DataFrames will be discussed shortly ## Series Series can hold a variety of object types Numbers, strings, etc Create a Series by passing a list, letting pandas create a default index value. s = pd.Series([1,2,3,np.nan, 4,5]) s 0 1.0 1 2.0 2 3.0 3 NaN 4 4.0 5 5.0 dtype: float64 26.3 Series Key to using a series is understanding its index Pandas makes use of these index names/numbers Allows fast lookups of information Works like a hash table or dictionary 26.4 Series Examples s_1 = pd.Series([1,2,3,4], [&#39;USA&#39;, &#39;Germany&#39;, &#39;China&#39;, &#39;Japan&#39;]) s_1 USA 1 Germany 2 China 3 Japan 4 dtype: int64 26.5 Series Examples s_2 = pd.Series([1,2,5,6],[&#39;USA&#39;, &#39;Germany&#39;, &#39;Italy&#39;, &#39;China&#39;]) s_2 USA 1 Germany 2 Italy 5 China 6 dtype: int64 s_2[&#39;China&#39;] # Indexing is type dependent 6 s_2.index Index([&#39;USA&#39;, &#39;Germany&#39;, &#39;Italy&#39;, &#39;China&#39;], dtype=&#39;object&#39;) 26.6 Series Example labels = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;] s_3 = pd.Series(data=labels) s_3 0 a 1 b 2 c dtype: object s_3[2] &#39;c&#39; 26.7 Series Matches operation off of the index Creates NaN object where missing matches Integers convert to floats s_1 USA 1 Germany 2 China 3 Japan 4 dtype: int64 s_2 USA 1 Germany 2 Italy 5 China 6 dtype: int64 s_1 + s_2 China 9.0 Germany 4.0 Italy NaN Japan NaN USA 2.0 dtype: float64 "],["dataframe-creation.html", "Chapter 27 DataFrame Creation", " Chapter 27 DataFrame Creation Creating a DataFrame by passing a NumPy array, with a datetime index and labeled columns. dates = pd.date_range(&#39;20190101&#39;,periods=10) dates DatetimeIndex([&#39;2019-01-01&#39;, &#39;2019-01-02&#39;, &#39;2019-01-03&#39;, &#39;2019-01-04&#39;, &#39;2019-01-05&#39;, &#39;2019-01-06&#39;, &#39;2019-01-07&#39;, &#39;2019-01-08&#39;, &#39;2019-01-09&#39;, &#39;2019-01-10&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) df = pd.DataFrame(np.random.randn(10,4), index=dates, columns=list(&#39;ABCD&#39;)) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2019-01-01 0.496714 -0.138264 0.647689 1.523030 2019-01-02 -0.234153 -0.234137 1.579213 0.767435 2019-01-03 -0.469474 0.542560 -0.463418 -0.465730 2019-01-04 0.241962 -1.913280 -1.724918 -0.562288 2019-01-05 -1.012831 0.314247 -0.908024 -1.412304 2019-01-06 1.465649 -0.225776 0.067528 -1.424748 2019-01-07 -0.544383 0.110923 -1.150994 0.375698 2019-01-08 -0.600639 -0.291694 -0.601707 1.852278 2019-01-09 -0.013497 -1.057711 0.822545 -1.220844 2019-01-10 0.208864 -1.959670 -1.328186 0.196861 Creating a DataFrame by passing a dictionary that can be converted to a series: df2 = pd.DataFrame({&#39;A&#39;:1., &#39;B&#39;: pd.Timestamp(&#39;20190101&#39;), &#39;C&#39;:pd.Series(1, index=list(range(4)),dtype=&#39;float32&#39;), &#39;D&#39;:np.array([3] * 4, dtype=&#39;int32&#39;), &#39;E&#39;: pd.Categorical([&#39;test&#39;,&#39;train&#39;,&#39;test&#39;,&#39;train&#39;]), &#39;F&#39;:&#39;foo&#39;}) df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D E F 0 1.0 2019-01-01 1.0 3 test foo 1 1.0 2019-01-01 1.0 3 train foo 2 1.0 2019-01-01 1.0 3 test foo 3 1.0 2019-01-01 1.0 3 train foo The columns of the DataFrame have different dtypes. df2.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 4 entries, 0 to 3 Data columns (total 6 columns): A 4 non-null float64 B 4 non-null datetime64[ns] C 4 non-null float32 D 4 non-null int32 E 4 non-null category F 4 non-null object dtypes: category(1), datetime64[ns](1), float32(1), float64(1), int32(1), object(1) memory usage: 260.0+ bytes "],["viewing-data.html", "Chapter 28 Viewing Data", " Chapter 28 Viewing Data Here is how to view the top and bottom rows of the frame: df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2019-01-01 0.496714 -0.138264 0.647689 1.523030 2019-01-02 -0.234153 -0.234137 1.579213 0.767435 2019-01-03 -0.469474 0.542560 -0.463418 -0.465730 2019-01-04 0.241962 -1.913280 -1.724918 -0.562288 2019-01-05 -1.012831 0.314247 -0.908024 -1.412304 df.tail(2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2019-01-09 -0.013497 -1.057711 0.822545 -1.220844 2019-01-10 0.208864 -1.959670 -1.328186 0.196861 Display the index, columns: df.index DatetimeIndex([&#39;2019-01-01&#39;, &#39;2019-01-02&#39;, &#39;2019-01-03&#39;, &#39;2019-01-04&#39;, &#39;2019-01-05&#39;, &#39;2019-01-06&#39;, &#39;2019-01-07&#39;, &#39;2019-01-08&#39;, &#39;2019-01-09&#39;, &#39;2019-01-10&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) df.columns Index([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;], dtype=&#39;object&#39;) df.shape (10, 4) We can convert our DataFrame (of floating-points) to a NumPy array. df.to_numpy() # This can be a taxing operation in not all floats # df.values array([[ 0.49671415, -0.1382643 , 0.64768854, 1.52302986], [-0.23415337, -0.23413696, 1.57921282, 0.76743473], [-0.46947439, 0.54256004, -0.46341769, -0.46572975], [ 0.24196227, -1.91328024, -1.72491783, -0.56228753], [-1.01283112, 0.31424733, -0.90802408, -1.4123037 ], [ 1.46564877, -0.2257763 , 0.0675282 , -1.42474819], [-0.54438272, 0.11092259, -1.15099358, 0.37569802], [-0.60063869, -0.29169375, -0.60170661, 1.85227818], [-0.01349722, -1.05771093, 0.82254491, -1.22084365], [ 0.2088636 , -1.95967012, -1.32818605, 0.19686124]]) df2.to_numpy() # This has multiple dtypes array([[1.0, Timestamp(&#39;2019-01-01 00:00:00&#39;), 1.0, 3, &#39;test&#39;, &#39;foo&#39;], [1.0, Timestamp(&#39;2019-01-01 00:00:00&#39;), 1.0, 3, &#39;train&#39;, &#39;foo&#39;], [1.0, Timestamp(&#39;2019-01-01 00:00:00&#39;), 1.0, 3, &#39;test&#39;, &#39;foo&#39;], [1.0, Timestamp(&#39;2019-01-01 00:00:00&#39;), 1.0, 3, &#39;train&#39;, &#39;foo&#39;]], dtype=object) describe() shows a quick statistic summary of your data: df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D count 10.000000 10.000000 10.000000 10.000000 mean -0.046179 -0.485280 -0.306027 -0.037061 std 0.701907 0.874335 1.060584 1.181041 min -1.012831 -1.959670 -1.724918 -1.424748 25% -0.525656 -0.866207 -1.090251 -1.056205 50% -0.123825 -0.229957 -0.532562 -0.134434 75% 0.233688 0.048626 0.502648 0.669501 max 1.465649 0.542560 1.579213 1.852278 Transposing your data: df.T .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2019-01-01 2019-01-02 2019-01-03 2019-01-04 2019-01-05 2019-01-06 2019-01-07 2019-01-08 2019-01-09 2019-01-10 A 0.496714 -0.234153 -0.469474 0.241962 -1.012831 1.465649 -0.544383 -0.600639 -0.013497 0.208864 B -0.138264 -0.234137 0.542560 -1.913280 0.314247 -0.225776 0.110923 -0.291694 -1.057711 -1.959670 C 0.647689 1.579213 -0.463418 -1.724918 -0.908024 0.067528 -1.150994 -0.601707 0.822545 -1.328186 D 1.523030 0.767435 -0.465730 -0.562288 -1.412304 -1.424748 0.375698 1.852278 -1.220844 0.196861 Sorting by an axis: df.sort_index(axis=0, ascending=False) # axis = 1 Columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2019-01-10 0.208864 -1.959670 -1.328186 0.196861 2019-01-09 -0.013497 -1.057711 0.822545 -1.220844 2019-01-08 -0.600639 -0.291694 -0.601707 1.852278 2019-01-07 -0.544383 0.110923 -1.150994 0.375698 2019-01-06 1.465649 -0.225776 0.067528 -1.424748 2019-01-05 -1.012831 0.314247 -0.908024 -1.412304 2019-01-04 0.241962 -1.913280 -1.724918 -0.562288 2019-01-03 -0.469474 0.542560 -0.463418 -0.465730 2019-01-02 -0.234153 -0.234137 1.579213 0.767435 2019-01-01 0.496714 -0.138264 0.647689 1.523030 Sorting by values: df.sort_values(by=&#39;B&#39;, ascending=False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2019-01-03 -0.469474 0.542560 -0.463418 -0.465730 2019-01-05 -1.012831 0.314247 -0.908024 -1.412304 2019-01-07 -0.544383 0.110923 -1.150994 0.375698 2019-01-01 0.496714 -0.138264 0.647689 1.523030 2019-01-06 1.465649 -0.225776 0.067528 -1.424748 2019-01-02 -0.234153 -0.234137 1.579213 0.767435 2019-01-08 -0.600639 -0.291694 -0.601707 1.852278 2019-01-09 -0.013497 -1.057711 0.822545 -1.220844 2019-01-04 0.241962 -1.913280 -1.724918 -0.562288 2019-01-10 0.208864 -1.959670 -1.328186 0.196861 "],["selection-3.html", "Chapter 29 Selection", " Chapter 29 Selection Selecting a single columns yields a Series df[&#39;A&#39;] 2019-01-01 0.496714 2019-01-02 -0.234153 2019-01-03 -0.469474 2019-01-04 0.241962 2019-01-05 -1.012831 2019-01-06 1.465649 2019-01-07 -0.544383 2019-01-08 -0.600639 2019-01-09 -0.013497 2019-01-10 0.208864 Freq: D, Name: A, dtype: float64 Selecting with [] slices the rows df[0:3] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2019-01-01 0.496714 -0.138264 0.647689 1.523030 2019-01-02 -0.234153 -0.234137 1.579213 0.767435 2019-01-03 -0.469474 0.542560 -0.463418 -0.465730 df[&#39;20190101&#39;:&#39;20190103&#39;] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2019-01-01 0.496714 -0.138264 0.647689 1.523030 2019-01-02 -0.234153 -0.234137 1.579213 0.767435 2019-01-03 -0.469474 0.542560 -0.463418 -0.465730 "],["data-import.html", "Chapter 30 Data Import 30.1 Introduction 30.2 Getting started 30.3 Parsing a list 30.4 Numbers 30.5 Strings 30.6 Categoricals in Pandas 30.7 Dates, date-times, and times 30.8 Parsing a file 30.9 Writing to a file 30.10 Other types of data 30.11 Selection by label 30.12 Selection by position", " Chapter 30 Data Import 30.1 Introduction Working with data provided by Python packages is a great way to learn the tools of data science, but at some point you want to stop learning and start working with your own data. In this chapter, you’ll learn how to read plain-text rectangular files into Python. Here, we’ll only scratch the surface of data import, but many of the principles will translate to other forms of data. We’ll finish with a few pointers to packages that are useful for other types of data. Importing data is one of the most essential and very first steps in any data related problem. The ability to import the data correctly is a must-have skill for every aspiring data scientist. Data exists in many different forms, and not only should you know how to import various data formats but also how to analyze and manipulate the data to gain useful insights. pandas is an open source Python library which is easy-to-use, provides high-performance, and a data analysis tool for various data formats. It gives you the capability to read various types of data formats like CSV, JSON, Excel, Pickle, etc. It allows you to represent your data in a row and column tabular fashion, which makes the data readable and presentable. pandas represent the data in a DataFrame form and provide you with extensive usage for data analysis and data manipulation. Once you start making sense out of the data using the various functionalities in pandas, you can then use this data for analyzing, forecasting, classifying, and more. pandas has an input and output API which has a set of top-level reader and writer functions. The reader function is accessed with pandas.read_json() that returns a pandas object, and the writer function is accessed with pandas.to_json() which is an object method. DataFrame has a Reader and a Writer function. The Reader function allows you to read the different data formats, while the Writer function enables you to save the data in a particular format. To get other types of data into Python, keep following the pd.read_ syntax. Below are data formats that DataFrame supports, which means if your data is in any of the below forms, you can use pandas to load that data format and even write into a particular format. Format Type Data Description Reader Writer text CSV read_csv to_csv text JSON read_json to_json text HTML read_html to_html text Local clipboard read_clipboard to_clipboard binary MS Excel read_excel to_excel binary OpenDocument read_excel binary HDF5 Format read_hdf to_hdf binary Feather Format read_feather to_feather binary Parquet Format read_parquet to_parquet binary Msgpack read_msgpack to_msgpack binary Stata read_stata to_stata binary SAS read_sas binary Python Pickle Format read_pickle to_pickle SQL SQL read_sql to_sql SQL Google Big Query read_gbq to_gbq 30.1.1 Prerequisites In this chapter, you’ll learn how to load flat files in Python with the pandas package. import pandas as pd 30.2 Getting started Most of pandas.read_ functions are concerned with turning files into dataframes. These functions all have similar syntax: once you’ve mastered one, you can use the others with ease. For the rest of this chapter we’ll focus on pandas.read_csv(). Not only are csv files one of the most common forms of data storage, but once you understand pandas.read_csv(), you can easily apply your knowledge to all the other functions pd.read_csv() reads comma delimited files, by changes the sep argument it also reads semicolon separated files (common in countries where , is used as the decimal place), reads tab delimited files, and reads in files with any delimiter. The first argument to read_csv() is the most important: it’s the path to the file to read. df = pd.read_csv(&quot;heights.csv&quot;,) df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } earn height sex ed age race 0 50000.0 74.424439 male 16 45 white 1 60000.0 65.537543 female 16 58 white 2 30000.0 63.629198 female 16 29 white 3 50000.0 63.108562 female 16 91 other 4 51000.0 63.402484 female 17 39 white read_csv() uses the first line of the data for the column names, which is a very common convention. There are two cases where you might want to tweak this behaviour: Sometimes there are a few lines of metadata at the top of the file. You can use skiprows = n to skip the first n lines; or use comment = \"#\" to drop all lines that start with (e.g.) # The data might not have column names. You can use header = None to tell read_csv() not to treat the first row as headings, and instead label them sequentially from 0 to n: pd.read_csv(&quot;heights.csv&quot;, header=None) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 0 earn height sex ed age race 1 50000.0 74.4244387818035 male 16 45 white 2 60000.0 65.5375428255647 female 16 58 white 3 30000.0 63.6291977374349 female 16 29 white 4 50000.0 63.108561675297096 female 16 91 other … … … … … … … 1188 19000.0 72.1657330563758 male 12 29 white 1189 15000.0 61.135799531126395 female 18 82 white 1190 8000.0 63.6641635315027 female 12 33 white 1191 60000.0 71.9258358024526 male 12 50 white 1192 6000.0 68.3684862144291 male 12 27 white 1193 rows × 6 columns You can pass names a list of strings which will be used as the column names: Another option that commonly needs tweaking is na_values: this specifies the value (or values) that are used to represent missing values in your file: pd.read_csv() 30.2.1 Exercises What function would you use to read a file where fields were separated with “|?” Apart from those mentioned in this chapter, what other arguments does read_csv() have? Import the following files using the correct pd.read_ syntax. 30.3 Parsing a list 30.4 Numbers 30.5 Strings 30.6 Categoricals in Pandas 30.7 Dates, date-times, and times 30.7.1 Exercises 30.8 Parsing a file 30.8.1 Strategy 30.8.2 Problems 30.8.3 Other Strategies 30.9 Writing to a file 30.10 Other types of data Next Section: Tidy Data 30.11 Selection by label For getting a cross section using a label: import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline dates = pd.date_range(&#39;20190101&#39;,periods=10) df = pd.DataFrame(np.random.randn(10,4), index=dates, columns=list(&#39;ABCD&#39;)) dates[0] Timestamp(&#39;2019-01-01 00:00:00&#39;, freq=&#39;D&#39;) df.loc[dates[0]] A -1.158573 B -0.941688 C 2.568583 D 0.494481 Name: 2019-01-01 00:00:00, dtype: float64 Selecting on a mutli-axis by label df.loc[:, [&#39;A&#39;,&#39;D&#39;]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A D 2019-01-01 -1.158573 0.494481 2019-01-02 -0.683202 -0.187010 2019-01-03 0.564102 -0.146018 2019-01-04 -0.939398 0.034436 2019-01-05 1.135158 -2.690404 2019-01-06 -0.953653 2.677651 2019-01-07 1.677311 0.462660 2019-01-08 -0.612544 0.027003 2019-01-09 1.106210 -0.057844 2019-01-10 0.165680 -0.706584 Showing label slicing, both endpoints are included df.loc[&#39;20190102&#39;:&#39;20190104&#39;, [&#39;A&#39;, &#39;B&#39;]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 2019-01-02 -0.683202 0.056144 2019-01-03 0.564102 1.448998 2019-01-04 -0.939398 2.276844 Reduction in the dimensions of the returns object df.loc[&#39;20190102&#39;,[&#39;A&#39;,&#39;B&#39;]] A -0.683202 B 0.056144 Name: 2019-01-02 00:00:00, dtype: float64 For getting a scalar value: df.loc[dates[0], &#39;A&#39;] -1.1585727488744095 30.12 Selection by position Select with the position of the passed integers: df.iloc[2] A 0.564102 B 1.448998 C 1.036779 D -0.146018 Name: 2019-01-03 00:00:00, dtype: float64 By integer slices, acting similar to numpy/python: df.iloc[2:4, 0:2] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 2019-01-03 0.564102 1.448998 2019-01-04 -0.939398 2.276844 By lists of integer position locations, similar to the numpy/python style: df.iloc[[1,2,4],[0,2]] For slicing rows explicitly: df.iloc[1:3, :] For slicing columns explicitly: df.iloc[:,1:3] For getting a value explicitly: df.iloc[1,1] "],["boolean-indexing.html", "Chapter 31 Boolean indexing", " Chapter 31 Boolean indexing Using a single column’s values to select data. mask = df[&#39;A&#39;] &gt; 0 mask.head() 2019-01-01 False 2019-01-02 False 2019-01-03 True 2019-01-04 False 2019-01-05 True Freq: D, Name: A, dtype: bool df[mask] # ~ negates the mask .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2019-01-03 0.564102 1.448998 1.036779 -0.146018 2019-01-05 1.135158 0.720135 -0.771679 -2.690404 2019-01-07 1.677311 -0.572894 0.405523 0.462660 2019-01-09 1.106210 -0.385144 1.194672 -0.057844 2019-01-10 0.165680 0.786776 -1.079196 -0.706584 Selecting values from a DataFrame where a boolean condition is met. df[df &gt; 0] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2019-01-01 NaN NaN 2.568583 0.494481 2019-01-02 NaN 0.056144 0.677926 NaN 2019-01-03 0.564102 1.448998 1.036779 NaN 2019-01-04 NaN 2.276844 NaN 0.034436 2019-01-05 1.135158 0.720135 NaN NaN 2019-01-06 NaN 0.296899 NaN 2.677651 2019-01-07 1.677311 NaN 0.405523 0.462660 2019-01-08 NaN NaN NaN 0.027003 2019-01-09 1.106210 NaN 1.194672 NaN 2019-01-10 0.165680 0.786776 NaN NaN Using the isin() method for filtering: df2 = df.copy() df2[&#39;E&#39;] = [&#39;one&#39;,&#39;one&#39;,&#39;two&#39;,&#39;three&#39;,&#39;four&#39;,&#39;three&#39;,&#39;two&#39;,&#39;three&#39;,&#39;four&#39;,&#39;four&#39;] df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D E 2019-01-01 -1.158573 -0.941688 2.568583 0.494481 one 2019-01-02 -0.683202 0.056144 0.677926 -0.187010 one 2019-01-03 0.564102 1.448998 1.036779 -0.146018 two 2019-01-04 -0.939398 2.276844 -0.400286 0.034436 three 2019-01-05 1.135158 0.720135 -0.771679 -2.690404 four 2019-01-06 -0.953653 0.296899 -1.022055 2.677651 three 2019-01-07 1.677311 -0.572894 0.405523 0.462660 two 2019-01-08 -0.612544 -1.102798 -0.829374 0.027003 three 2019-01-09 1.106210 -0.385144 1.194672 -0.057844 four 2019-01-10 0.165680 0.786776 -1.079196 -0.706584 four df2[df2[&#39;E&#39;].isin([&#39;two&#39;, &#39;four&#39;])] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D E 2019-01-03 0.564102 1.448998 1.036779 -0.146018 two 2019-01-05 1.135158 0.720135 -0.771679 -2.690404 four 2019-01-07 1.677311 -0.572894 0.405523 0.462660 two 2019-01-09 1.106210 -0.385144 1.194672 -0.057844 four 2019-01-10 0.165680 0.786776 -1.079196 -0.706584 four "],["setting.html", "Chapter 32 Setting", " Chapter 32 Setting Setting a new column automatically aligns the data by the indexes. s1 = pd.Series([1, 2, 3, 4, 5, 6,7,8,9,10], index=pd.date_range(&#39;20190101&#39;, periods=10)) s1 2019-01-01 1 2019-01-02 2 2019-01-03 3 2019-01-04 4 2019-01-05 5 2019-01-06 6 2019-01-07 7 2019-01-08 8 2019-01-09 9 2019-01-10 10 Freq: D, dtype: int64 df[&#39;F&#39;] = s1 df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D F 2019-01-01 -1.158573 -0.941688 2.568583 0.494481 1 2019-01-02 -0.683202 0.056144 0.677926 -0.187010 2 2019-01-03 0.564102 1.448998 1.036779 -0.146018 3 2019-01-04 -0.939398 2.276844 -0.400286 0.034436 4 2019-01-05 1.135158 0.720135 -0.771679 -2.690404 5 2019-01-06 -0.953653 0.296899 -1.022055 2.677651 6 2019-01-07 1.677311 -0.572894 0.405523 0.462660 7 2019-01-08 -0.612544 -1.102798 -0.829374 0.027003 8 2019-01-09 1.106210 -0.385144 1.194672 -0.057844 9 2019-01-10 0.165680 0.786776 -1.079196 -0.706584 10 Setting values by label: df.loc[dates[0]] = 0 Setting values by position df.iloc[0,1] = 0 Setting by assigning with a NumPy array: df.loc[:, &#39;D&#39;] = np.array([5] * len(df)) The result of the prior setting operations. df A where operation with setting. df2 = df.copy() df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D F 2019-01-01 0.000000 0.000000 0.000000 5 0 2019-01-02 -0.683202 0.056144 0.677926 5 2 2019-01-03 0.564102 1.448998 1.036779 5 3 2019-01-04 -0.939398 2.276844 -0.400286 5 4 2019-01-05 1.135158 0.720135 -0.771679 5 5 2019-01-06 -0.953653 0.296899 -1.022055 5 6 2019-01-07 1.677311 -0.572894 0.405523 5 7 2019-01-08 -0.612544 -1.102798 -0.829374 5 8 2019-01-09 1.106210 -0.385144 1.194672 5 9 2019-01-10 0.165680 0.786776 -1.079196 5 10 df2[df2 &gt; 0] = -df2 df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D F 2019-01-01 0.000000 0.000000 0.000000 -5 0 2019-01-02 -0.683202 -0.056144 -0.677926 -5 -2 2019-01-03 -0.564102 -1.448998 -1.036779 -5 -3 2019-01-04 -0.939398 -2.276844 -0.400286 -5 -4 2019-01-05 -1.135158 -0.720135 -0.771679 -5 -5 2019-01-06 -0.953653 -0.296899 -1.022055 -5 -6 2019-01-07 -1.677311 -0.572894 -0.405523 -5 -7 2019-01-08 -0.612544 -1.102798 -0.829374 -5 -8 2019-01-09 -1.106210 -0.385144 -1.194672 -5 -9 2019-01-10 -0.165680 -0.786776 -1.079196 -5 -10 "],["missing-data.html", "Chapter 33 Missing Data 33.1 Dealing with Missing Data 33.2 Creating Missing Data 33.3 Drop Missing Data 33.4 Keep rows at Threshold 33.5 Fill Missing Values", " Chapter 33 Missing Data pandas primarily uses the value np.nan to represent missing data. Reindexing allows you to change/add/delete the index on a specified axis. This returns a copy of the data. import pandas as pd import numpy as np dates = pd.date_range(&#39;20190101&#39;,periods=10) df = pd.DataFrame(np.random.randn(10,4), index=dates, columns=list(&#39;ABCD&#39;)) df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + [&#39;E&#39;]) df1.loc[dates[0]:dates[1], &#39;E&#39;] = 1 df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D E 2019-01-01 0.199886 0.312388 1.394258 -0.311931 1.0 2019-01-02 0.259445 -0.377668 -1.481911 1.805175 1.0 2019-01-03 1.452134 -2.576209 -0.246738 -1.127367 NaN 2019-01-04 2.026428 0.183045 1.275433 -0.834084 NaN To drop any rows that have missing data. df1.dropna(how=&#39;any&#39;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D E 2019-01-01 0.199886 0.312388 1.394258 -0.311931 1.0 2019-01-02 0.259445 -0.377668 -1.481911 1.805175 1.0 Filling missing data. df1.fillna(value=&#39;FILL VALUE&#39;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D E 2019-01-01 0.199886 0.312388 1.394258 -0.311931 1 2019-01-02 0.259445 -0.377668 -1.481911 1.805175 1 2019-01-03 1.452134 -2.576209 -0.246738 -1.127367 FILL VALUE 2019-01-04 2.026428 0.183045 1.275433 -0.834084 FILL VALUE To get the boolean mask where values are nan. pd.isna(df1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D E 2019-01-01 False False False False False 2019-01-02 False False False False False 2019-01-03 False False False False True 2019-01-04 False False False False True 33.1 Dealing with Missing Data import numpy as np import pandas as pd np.random.seed(42) df = pd.DataFrame(np.random.randn(5,3), index=[&#39;a&#39;,&#39;c&#39;,&#39;d&#39;,&#39;f&#39;,&#39;g&#39;], columns=[&#39;one&#39;,&#39;two&#39;,&#39;three&#39;]) df[&#39;four&#39;] = &#39;blah&#39; df[&#39;five&#39;] = df[&#39;one&#39;] &gt; 0 33.2 Creating Missing Data df.iloc[2,2] = np.nan df.iloc[3,4] = np.nan # Notice what happens here! df.iloc[3,3] = np.nan df.iloc[4,4] = np.nan df.iloc[1,1] = np.nan df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } one two three four five a 0.496714 -0.138264 0.647689 blah 1.0 c 1.523030 NaN -0.234137 blah 1.0 d 1.579213 0.767435 NaN blah 1.0 f 0.542560 -0.463418 -0.465730 NaN NaN g 0.241962 -1.913280 -1.724918 blah NaN 33.3 Drop Missing Data df.dropna() # Keeps only complete rows .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } one two three four five a 0.496714 -0.138264 0.647689 blah 1.0 df.dropna(axis=1) # Keeps only complete columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } one a -0.719844 c 0.343618 d -0.385082 f 1.031000 g -0.309212 33.4 Keep rows at Threshold df.dropna(thresh=4) # Keeps rows that have AT LEAST 4 non-na values .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } one two three four five a 0.496714 -0.138264 0.647689 blah 1.0 c 1.523030 NaN -0.234137 blah 1.0 d 1.579213 0.767435 NaN blah 1.0 g 0.241962 -1.913280 -1.724918 blah NaN 33.5 Fill Missing Values df.fillna(value=&quot;PINK FLUFFY UNICORN&quot;) # Fill with whatever you want .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } one two three four five a 0.496714 -0.138264 0.647689 blah 1 c 1.523030 PINK FLUFFY UNICORN -0.234137 blah 1 d 1.579213 0.767435 PINK FLUFFY UNICORN blah 1 f 0.542560 -0.463418 -0.46573 PINK FLUFFY UNICORN PINK FLUFFY UNICORN g 0.241962 -1.91328 -1.72492 blah PINK FLUFFY UNICORN df[&#39;two&#39;].fillna(value=df[&#39;two&#39;].mean()) a -0.460639 c 0.031246 d -0.676922 f 0.931280 g 0.331263 Name: two, dtype: float64 "],["done.html", "Chapter 34 DONE!", " Chapter 34 DONE! "],["tidy-data.html", "Chapter 35 Tidy Data 35.1 Introduction 35.2 Spreading and gathering 35.3 Pivot (Spreading) 35.4 Separating and uniting 35.5 Separate 35.6 Missing Values 35.7 Case Study 35.8 Non-tidy data", " Chapter 35 Tidy Data 35.1 Introduction In this chapter, you will learn a consistent way to organise your data in Python, an organisation called tidy data. Getting your data into this format requires some upfront work, but that work pays off in the long term. Once you have tidy data and the tidy tools provided by pandas, you will spend much less time munging data from one representation to another, allowing you to spend more time on the analytic questions at hand. This chapter will give you a practical introduction to tidy data and the accompanying tools in pandas. If you’d like to learn more about the underlying theory, you might enjoy the Tidy Data paper published in the Journal of Statistical Software, http://www.jstatsoft.org/v59/i10/paper. 35.1.1 Prerequisties In this chapter we’ll focus on pandas, a package that provides tools to help tidy up your messy datasets. import pandas as pd import numpy as np You can represent the same underlying data in multiple ways. The example below shows the same data organised in four different ways. Each dataset shows the same values of four variables country, year, population, and cases, but each dataset organises the values in a different way. Table 1 country year cases population Afghanistan 1999 745 19987071 Afghanistan 2000 2666 20595360 Brazil 1999 37737 172006362 Brazil 2000 80488 174504898 China 1999 212258 1272915272 China 2000 213766 1280428583 Table 2 country year type count Afghanistan 1999 cases 745 Afghanistan 1999 population 19987071 Afghanistan 2000 cases 2666 Afghanistan 2000 population 20595360 Brazil 1999 cases 37737 Brazil 1999 population 172006362 Table 3 country year rate Afghanistan 1999 745/19987071 Afghanistan 2000 2666/20595360 Brazil 1999 37737/172006362 Brazil 2000 80488/174504898 China 1999 212258/1272915272 China 2000 213766/1280428583 Data Spread accross two tables. Table 4a: Cases country 1999 2000 Afghanistan 745 2666 Brazil 37737 80488 China 212258 213766 Table 4b: Population country 1999 2000 Afghanistan 19987071 20595360 Brazil 172006362 174504898 China 1272915272 1280428583 These are all representations of the same underlying data, but they are not equally easy to use. One dataset, the tidy dataset, will be much easier to work with inside the tidyverse. There are three interrelated rules which make a dataset tidy: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. Visual Representation of the rules Following three rules makes a dataset tidy: variables are in columns, observations are in rows, and values are in cells. These three rules are interrelated because it’s impossible to only satisfy two of the three. That interrelationship leads to an even simpler set of practical instructions: Put each dataset in a DataFrame. Put each variable in a column. In this example, only Table 1 is tidy. It’s the only representation where each column is a variable. Why ensure that your data is tidy? There are two main advantages: There’s a general advantage to picking one consistent way of storing data. If you have a consistent data structure, it’s easier to learn the tools that work with it because they have an underlying uniformity. There’s a specific advantage to placing variables in columns because it allows pandas’s vectorised nature to shine. As you learned, most built-in pandas functions work with vectors of values. That makes transforming tidy data feel particularly natural. !ll the other functions/methods in pandas are designed to work with tidy data. Here are a couple of small examples showing how you might work with from io import StringIO table1 = StringIO(&quot;&quot;&quot;country | year | cases | population Afghanistan | 1999 | 745 | 19987071 Afghanistan | 2000 | 2666 | 20595360 Brazil | 1999 | 37737 | 172006362 Brazil | 2000 | 80488 | 174504898 China | 1999 | 212258 | 1272915272 China | 2000 | 213766 | 1280428583&quot;&quot;&quot;) table1 = pd.read_csv(table1, sep=&quot;|&quot;, ) table1.columns = table1.columns.str.strip() Compute rate per 10,000 table1.assign(rate = lambda x: x[&quot;cases&quot;] / x[&quot;population&quot;] * 10000) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year cases population rate 0 Afghanistan 1999 745 19987071 0.372741 1 Afghanistan 2000 2666 20595360 1.294466 2 Brazil 1999 37737 172006362 2.193930 3 Brazil 2000 80488 174504898 4.612363 4 China 1999 212258 1272915272 1.667495 5 China 2000 213766 1280428583 1.669488 Compute cases per year table1.groupby(&#39;year&#39;)[&#39;cases&#39;].count() year 1999 3 2000 3 Name: cases, dtype: int64 Visualise changes over time import seaborn as sns sns.lineplot(x=&#39;year&#39;,y=&#39;cases&#39;,hue=&#39;country&#39;,data=table1); png 35.1.2 Exercises Using prose, describe how the variables and observations are organised in each of the sample tables. Compute the rate for table2, and table4a + table4b. You will need to perform four operations: Extract the number of TB cases per country per year. Extract the matching population per country per year. Divide cases by population, and multiply by 10000. Store back in the appropriate place. Which representation is easiest to work with? Which is hardest? Why? Recreate the plot showing change in cases over time using table2 instead of table1. What do you need to do first? 35.2 Spreading and gathering The principles of tidy data seem so obvious that you might wonder if you’ll ever encounter a dataset that isn’t tidy. Unfortunately, however, most data that you will encounter will be untidy. There are two main reasons: Most people aren’t familiar with the principles of tidy data, and it’s hard to derive them yourself unless you spend a lot of time working with data. Data is often organised to facilitate some use other than analysis. For example, data is often organised to make entry as easy as possible. This means for most real analyses, you’ll need to do some tidying. The first step is always to figure out what the variables and observations are. Sometimes this is easy; other times you’ll need to consult with the people who originally generated the data. The second step is to resolve one of two common problems: One variable might be spread across multiple columns. One observation might be scattered across multiple rows. Typically a dataset will only suffer from one of these problems; it’ll only suffer from both if you’re really unlucky! To fix these problems, you’ll need the two most important functions in pandas: melt() and pivot_table(). 35.2.1 Melt (Gathering) A common problem is a dataset where some of the column names are not names of variables, but values of a variable. Take table4a: the column names 1999 and 2000 represent values of the year variable, and each row represents two observations, not one. E.g., we are unpivoting a DataFrame from wide format to long format, optionally leaving identifier variables set. This function is useful to massage a DataFrame into a format where one or more columns are identifier variables (id_vars), while all other columns, considered measured variables (value_vars), are “unpivoted” to the row axis, leaving just two non-identifier columns, ‘variable’ and ‘value.’ table4a = StringIO(&quot;&quot;&quot;country|1999|2000 Afghanistan | 745 | 2666 Brazil | 37737 | 80488 China | 212258 | 213766 &quot;&quot;&quot;) table4a = pd.read_csv(table4a, sep=&quot;|&quot;) table4a .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country 1999 2000 0 Afghanistan 745 2666 1 Brazil 37737 80488 2 China 212258 213766 To tidy a dataset like this, we need to melt (gather) those columns into a new pair of variables. To describe that operation we need three parameters: The set of columns that represent values, not variables. In this example, those are the columns 1999 and 2000. The name of the identifier variable(s) is the id_vars, and here it is country. The name of the variable whose values form the column names. We call that the var_name, and here it is year. The name of the variable whose values are spread over the cells. We call that value_name, and here it’s the number of cases. Together those parameters generate the call to pd.melt() table4a = pd.melt(table4a, id_vars=[&quot;country&quot;], value_vars=[&quot;1999&quot;,&quot;2000&quot;], var_name = &quot;year&quot;, value_name=&quot;cases&quot; ) table4a .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year cases 0 Afghanistan 1999 745 1 Brazil 1999 37737 2 China 1999 212258 3 Afghanistan 2000 2666 4 Brazil 2000 80488 5 China 2000 213766 Here there are only two columns, so we list them individually. Note that “1999” and “2000” are non-syntactic names (because they don’t start with a letter) so we have to surround them in backticks. To refresh your memory of the other ways to select columns. Gathering table4 into a tidy form. In the final result, the gathered columns are dropped, and we get new key and value columns. Otherwise, the relationships between the original variables are preserved. We can use melt() to tidy table4b in a similar fashion. table4b = StringIO(&quot;&quot;&quot;country|1999|2000 Afghanistan | 19987071 | 20595360 Brazil | 172006362 | 174504898 China | 1272915272 | 1280428583&quot;&quot;&quot;) table4b = pd.read_csv(table4b, sep=&quot;|&quot;) table4b = pd.melt(table4b, id_vars=[&quot;country&quot;], value_vars=[&quot;1999&quot;,&quot;2000&quot;], var_name = &quot;year&quot;, value_name=&quot;population&quot; ) table4b .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year population 0 Afghanistan 1999 19987071 1 Brazil 1999 172006362 2 China 1999 1272915272 3 Afghanistan 2000 20595360 4 Brazil 2000 174504898 5 China 2000 1280428583 To combine the tidied versions of table4a and table4b into a single tibble, we need to use merge(), which you’ll learn about in relational data. pd.merge(table4a, table4b, how=&quot;left&quot;, on=[&quot;country&quot;,&quot;year&quot;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year cases population 0 Afghanistan 1999 745 19987071 1 Brazil 1999 37737 172006362 2 China 1999 212258 1272915272 3 Afghanistan 2000 2666 20595360 4 Brazil 2000 80488 174504898 5 China 2000 213766 1280428583 35.3 Pivot (Spreading) Pivoting (spreading) is the opposite of melting (gathering). You use it when an observation is scattered across multiple rows. For example, take table2: an observation is a country in a year, but each observation is spread across two rows. table2 = StringIO(&quot;&quot;&quot;country|year|type|count Afghanistan | 1999 | cases | 745 Afghanistan | 1999 | population | 19987071 Afghanistan | 2000 | cases | 2666 Afghanistan | 2000 | population | 20595360 Brazil | 1999 | cases | 37737 Brazil | 1999 | population | 172006362 &quot;&quot;&quot;) table2 = pd.read_csv(table2, sep=&quot;|&quot;) table2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year type count 0 Afghanistan 1999 cases 745 1 Afghanistan 1999 population 19987071 2 Afghanistan 2000 cases 2666 3 Afghanistan 2000 population 20595360 4 Brazil 1999 cases 37737 5 Brazil 1999 population 172006362 To tidy this up, we first analyse the representation in similar way to pivot_table(). pd.pivot_table(table2, index=[&quot;country&quot;,&quot;year&quot;],values=&quot;count&quot;, columns=&quot;type&quot;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type cases population country year Afghanistan 1999 745 19987071 2000 2666 20595360 Brazil 1999 37737 172006362 Spreading table2 makes it tidy. As you might have guessed that melt() and pivot_table() are complements. melt() makes wide tables narrower and longer; pivot_table() makes long tables shorter and wider. 35.3.1 Exercises 35.4 Separating and uniting So far you’ve learned how to tidy table2 and table4, but not table3. table3 has a different problem: we have one column (rate) that contains two variables (cases and population). To fix this problem, we’ll need the split() function. You’ll also learn about the complement of split(): cat(), which you use if a single variable is spread across multiple columns. 35.5 Separate str.split() pulls apart one column into multiple columns, by splitting whatever a separator character appears. The expand=True parameter converts the separated values into new columns. The astype(int) converts the string into integer columns. Take table3: table3 = StringIO(&quot;&quot;&quot;country|year|rate Afghanistan | 1999 | 745/19987071 Afghanistan | 2000 | 2666/20595360 Brazil | 1999 | 37737/172006362 Brazil | 2000 | 80488/174504898 China | 1999 | 212258/1272915272 China | 2000 | 213766/1280428583 &quot;&quot;&quot;) table3 = pd.read_csv(table3, sep=&quot;|&quot;) table3[[&#39;cases&#39;,&#39;population&#39;]] = table3[&#39;rate&#39;].str.split(&quot;/&quot;, expand=True).astype(int) Separating table3 makes it tidy By default, split() will split values by ,. If you wish to use a specific character to separate a column, you can pass the character to the sep argument. Formally, sep is a regular expression, which you’ll learn more about in strings table3[&quot;century&quot;] = table3[&quot;year&quot;].astype(str).str[:2] table3[&quot;year&quot;] = table3[&quot;year&quot;].astype(str).str[2:] table3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year rate cases population century 0 Afghanistan 99 745/19987071 745 19987071 19 1 Afghanistan 00 2666/20595360 2666 20595360 20 2 Brazil 99 37737/172006362 37737 172006362 19 3 Brazil 00 80488/174504898 80488 174504898 20 4 China 99 212258/1272915272 212258 1272915272 19 5 China 00 213766/1280428583 213766 1280428583 20 35.5.1 Unite cat() is the inverse of split()1: it combines multiple columns into a single column. You’ll need it much less frequently thansplit()`, but it’s still a useful tool to have in your back pocket. table3[&quot;new&quot;] = table3[&quot;century&quot;].str.cat(table3[&quot;year&quot;]) table3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country year rate cases population century new 0 Afghanistan 99 745/19987071 745 19987071 19 1999 1 Afghanistan 00 2666/20595360 2666 20595360 20 2000 2 Brazil 99 37737/172006362 37737 172006362 19 1999 3 Brazil 00 80488/174504898 80488 174504898 20 2000 4 China 99 212258/1272915272 212258 1272915272 19 1999 5 China 00 213766/1280428583 213766 1280428583 20 2000 35.5.2 Exercises 35.6 Missing Values Changing the representation of a dataset brings up an important subtlety of missing values. Surprisingly, a value can be missing in one of two possible ways: 1. Explicitly, i.e. flagged with NaN 1. Implicitly, i.e. simply not present in the data. Let’s illustrate this idea with a very simple dataset: stocks = pd.DataFrame( { &quot;year&quot;:[2015, 2015, 2015, 2015, 2016, 2016, 2016], &quot;qtr&quot;:[1, 2, 3, 4, 2, 3, 4], &quot;return&quot;:[1.88, 0.59, 0.35, np.nan, 0.92, 0.17, 2.66] } ) stocks .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year qtr return 0 2015 1 1.88 1 2015 2 0.59 2 2015 3 0.35 3 2015 4 NaN 4 2016 2 0.92 5 2016 3 0.17 6 2016 4 2.66 There are two missing values in this dataset: The return for the fourth quarter of 2015 is explicitly missing, because the cell where its value should be instead contains NaN. The return for the first quarter of 2016 is implicitly missing, because it simply does not appear in the dataset. One way to think about the difference is with this Zen-like koan: An explicit missing value is the presence of an absence; an implicit missing value is the absence of a presence. The way that a dataset is represented can make implicit values explicit. For example, we can make the implicit missing value explicit by putting years in the columns: stocks_pivot = stocks.pivot(index=&quot;qtr&quot;, columns=&quot;year&quot;,values=&quot;return&quot;) stocks_pivot .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year 2015 2016 qtr 1 1.88 NaN 2 0.59 0.92 3 0.35 0.17 4 NaN 2.66 Because these explicit missing values may not be important in other representations of the data, you can use .dropna() in melt() to turn explicit missing values implicit: stocks_explicit = pd.melt(stocks_pivot.reset_index(), id_vars=[&#39;qtr&#39;], value_vars=[2015,2016]) stocks_explicit .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } qtr year value 0 1 2015 1.88 1 2 2015 0.59 2 3 2015 0.35 3 4 2015 NaN 4 1 2016 NaN 5 2 2016 0.92 6 3 2016 0.17 7 4 2016 2.66 stocks_implicit = pd.melt(stocks_pivot.reset_index(), id_vars=[&#39;qtr&#39;], value_vars=[2015,2016]).dropna() stocks_implicit .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } qtr year value 0 1 2015 1.88 1 2 2015 0.59 2 3 2015 0.35 5 2 2016 0.92 6 3 2016 0.17 7 4 2016 2.66 There’s one other important tool that you should know for working with missing values. To drop any rows that have missing data. stocks_explicit.dropna() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } qtr year value 0 1 2015 1.88 1 2 2015 0.59 2 3 2015 0.35 5 2 2016 0.92 6 3 2016 0.17 7 4 2016 2.66 Filling missing data. stocks_explicit.fillna(&quot;FILL VALUE&quot;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } qtr year value 0 1 2015 1.88 1 2 2015 0.59 2 3 2015 0.35 3 4 2015 FILL VALUE 4 1 2016 FILL VALUE 5 2 2016 0.92 6 3 2016 0.17 7 4 2016 2.66 stocks_explicit.fillna(0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } qtr year value 0 1 2015 1.88 1 2 2015 0.59 2 3 2015 0.35 3 4 2015 0.00 4 1 2016 0.00 5 2 2016 0.92 6 3 2016 0.17 7 4 2016 2.66 stocks_explicit.fillna(method=&quot;bfill&quot;) # backfill .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } qtr year value 0 1 2015 1.88 1 2 2015 0.59 2 3 2015 0.35 3 4 2015 0.92 4 1 2016 0.92 5 2 2016 0.92 6 3 2016 0.17 7 4 2016 2.66 stocks_explicit.fillna(method=&quot;ffill&quot;) # forward fill .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } qtr year value 0 1 2015 1.88 1 2 2015 0.59 2 3 2015 0.35 3 4 2015 0.35 4 1 2016 0.35 5 2 2016 0.92 6 3 2016 0.17 7 4 2016 2.66 There are a variety of other methods but we’ll leave those for another time. 35.6.1 Exercise 35.7 Case Study To finish off the chapter, let’s pull together everything you’ve learned to tackle a realistic data tidying problem. WHO dataset contains tuberculosis (TB) cases broken down by year, country, age, gender, and diagnosis method. The data comes from the 2014 World Health Organization Global Tuberculosis Report, available at http://www.who.int/tb/country/data/download/en/. There’s a wealth of epidemiological information in this dataset, but it’s challenging to work with the data in the form that it’s provided: url = &quot;https://extranet.who.int/tme/generateCSV.asp?ds=notifications&quot; who = pd.read_csv(url) This is a very typical real-life example dataset. It contains redundant columns, odd variable codes, and many missing values. In short, who is messy, and we’ll need multiple steps to tidy it. That means in real-life situations you’ll usually need to string together multiple functions into a pipeline. The best place to start is almost always to gather together the columns that are not variables. Let’s have a look at what we’ve got: It looks like country, iso2, and iso3 are three variables that redundantly specify the country. year is clearly also a variable. We don’t know what all the other columns are yet, but given the structure in the variable names (e.g. new_sp_m014, new_ep_m014, new_ep_f014) these are likely to be values, not variables. So we need to gather together all the columns from new_sp_m014 to newrel_f65. We don’t know what those values represent yet, so we’ll give them the generic name “key.” We know the cells represent the count of cases, so we’ll use the variable cases. There are a lot of missing values in the current representation, so for now we’ll use .dropna() just so we can focus on the values that are present. who1 = pd.melt(who, id_vars=[&quot;country&quot;,&quot;iso2&quot;,&quot;iso3&quot;,&quot;year&quot;], var_name=&quot;key&quot;, value_name=&quot;cases&quot;).dropna() We can get some hint of the structure of the values in the new key column by counting them: who1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country iso2 iso3 year key cases 0 Afghanistan AF AFG 1980 iso_numeric 4 1 Afghanistan AF AFG 1981 iso_numeric 4 2 Afghanistan AF AFG 1982 iso_numeric 4 3 Afghanistan AF AFG 1983 iso_numeric 4 4 Afghanistan AF AFG 1984 iso_numeric 4 … … … … … … … 1325717 Zambia ZM ZMB 2015 hiv_reg_new2 3888 1325719 Zambia ZM ZMB 2017 hiv_reg_new2 199278 1325755 Zimbabwe ZW ZWE 2014 hiv_reg_new2 215379 1325757 Zimbabwe ZW ZWE 2016 hiv_reg_new2 168968 1325758 Zimbabwe ZW ZWE 2017 hiv_reg_new2 164963 214489 rows × 6 columns You might be able to parse this out by yourself with a little thought and some experimentation, but luckily we have the data dictionary handy. It tells us: The first three letters of each column denote whether the column contains new or old cases of TB. In this dataset, each column contains new cases. The next two letters describe the type of TB: rel stands for cases of relapse ep stands for cases of extrapulmonary TB sn stands for cases of pulmonary TB that could not be diagnosed by a pulmonary smear (smear negative) sp stands for cases of pulmonary TB that could be diagnosed be a pulmonary smear (smear positive) The sixth letter gives the sex of TB patients. The dataset groups cases by males (m) and females (f). The remaining numbers gives the age group. The dataset groups cases into seven age groups: 014 = 0 – 14 years old 1524 = 15 – 24 years old 2534 = 25 – 34 years old 3544 = 35 – 44 years old 4554 = 45 – 54 years old 5564 = 55 – 64 years old 65 = 65 or older We need to make a minor fix to the format of the column names: unfortunately the names are slightly inconsistent because instead of new_rel we have newrel (it’s hard to spot this here but if you don’t fix it we’ll get errors in subsequent steps). You’ll learn aboutreplace() in strings, but the basic idea is pretty simple: replace the characters “newrel” with “new_rel.” This makes all variable names consistent. who2 = who1.assign(key= who1[&quot;key&quot;].str.replace(&quot;newrel&quot;,&quot;new_rel&quot;).str.replace(&quot;newret&quot;,&quot;new_ret&quot;)) We can separate the values in each code with two passes. The first pass will split the codes at each underscore. who2.key.str.split(&quot;_&quot;, expand=True).rename({0:&quot;new&quot;, 1:&quot;type&quot;,2:&quot;sexage&quot;,3:&quot;unk&quot;}, axis=1).unk.unique() array([None, &#39;flg&#39;, &#39;events&#39;, &#39;all&#39;], dtype=object) 35.7.1 Exercises 35.8 Non-tidy data "],["operations.html", "Chapter 36 Operations 36.1 Stats", " Chapter 36 Operations 36.1 Stats Operations in general exclude missing data. Performing a descriptive statistic: import pandas as pd import numpy as np dates = pd.date_range(&#39;20190101&#39;,periods=10) df = pd.DataFrame(np.random.randn(10,4), index=dates, columns=list(&#39;ABCD&#39;)) df = df.reindex(index=dates[0:4], columns=list(df.columns) + [&#39;E&#39;]) df.loc[dates[0]:dates[1], &#39;E&#39;] = 1 df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D E 2019-01-01 -0.914745 -1.169413 0.445567 -1.612202 1.0 2019-01-02 1.382350 1.083761 -1.132513 -0.282442 1.0 2019-01-03 -1.022387 2.451375 1.505668 0.344774 NaN 2019-01-04 0.907028 -0.532535 -0.730059 -0.608755 NaN df.mean() A 0.088062 B 0.458297 C 0.022166 D -0.539656 E 1.000000 dtype: float64 Same operation on the other axis: df.mean(axis=1) 2019-01-01 -0.450158 2019-01-02 0.410231 2019-01-03 0.819858 2019-01-04 -0.241081 Freq: D, dtype: float64 "],["apply.html", "Chapter 37 Apply", " Chapter 37 Apply Applying functions to the data: df.apply(np.cumsum) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D E 2019-01-01 -0.914745 -1.169413 0.445567 -1.612202 1.0 2019-01-02 0.467606 -0.085651 -0.686946 -1.894644 2.0 2019-01-03 -0.554781 2.365724 0.818722 -1.549870 NaN 2019-01-04 0.352247 1.833188 0.088663 -2.158625 NaN df.apply(lambda x: x.max() - x.min()) A 2.404737 B 3.620788 C 2.638181 D 1.956975 E 0.000000 dtype: float64 "],["histogramming.html", "Chapter 38 Histogramming", " Chapter 38 Histogramming s = pd.Series(np.random.randint(0, 7, size=10)) s 0 4 1 0 2 5 3 5 4 3 5 2 6 2 7 5 8 4 9 2 dtype: int64 s.value_counts() 5 3 2 3 4 2 3 1 0 1 dtype: int64 "],["string-methods-1.html", "Chapter 39 String Methods", " Chapter 39 String Methods Series is equipped with a set of string processing methods in the str attribute that make it easy to operate on each element of the array, as in the code snippet below. Note that pattern-matching in str generally uses regular expressions by default. s = pd.Series([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;Aaba&#39;, &#39;Baca&#39;, np.nan, &#39;CABA&#39;, &#39;dog&#39;, &#39;cat&#39;]) s 0 A 1 B 2 C 3 Aaba 4 Baca 5 NaN 6 CABA 7 dog 8 cat dtype: object s.str.title() 0 A 1 B 2 C 3 Aaba 4 Baca 5 NaN 6 Caba 7 Dog 8 Cat dtype: object "],["merge.html", "Chapter 40 Merge 40.1 Concat", " Chapter 40 Merge 40.1 Concat pandas provides various facilities for easily combining together Series and DataFrame objects with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations. Concatenating pandas objects together with concat(): import pandas as pd import numpy as np np.random.seed() df = pd.DataFrame(np.random.randn(10, 4)) df # DataFrame .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 0 -0.493947 1.625480 0.244771 -0.185388 1 0.198916 -1.376590 -1.215013 -1.194330 2 -0.119116 0.843964 -0.320494 -2.380751 3 0.024455 1.921114 -0.142516 0.262443 4 -0.871334 -0.456046 -0.653551 1.771860 5 1.058511 -2.091128 1.171792 1.563637 6 -0.192952 1.377423 0.552583 0.288304 7 1.480328 -0.757365 -0.272379 -0.362830 8 -0.033634 -0.371534 1.738937 -1.010350 9 -0.206187 -1.458660 0.538586 -1.532622 df[:3] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 0 -0.493947 1.625480 0.244771 -0.185388 1 0.198916 -1.376590 -1.215013 -1.194330 2 -0.119116 0.843964 -0.320494 -2.380751 pieces = [df[:3], df[3:7], df[7:]] # Break it into pieces pieces [ 0 1 2 3 0 -0.493947 1.625480 0.244771 -0.185388 1 0.198916 -1.376590 -1.215013 -1.194330 2 -0.119116 0.843964 -0.320494 -2.380751, 0 1 2 3 3 0.024455 1.921114 -0.142516 0.262443 4 -0.871334 -0.456046 -0.653551 1.771860 5 1.058511 -2.091128 1.171792 1.563637 6 -0.192952 1.377423 0.552583 0.288304, 0 1 2 3 7 1.480328 -0.757365 -0.272379 -0.362830 8 -0.033634 -0.371534 1.738937 -1.010350 9 -0.206187 -1.458660 0.538586 -1.532622] pd.concat(pieces) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 0 -0.493947 1.625480 0.244771 -0.185388 1 0.198916 -1.376590 -1.215013 -1.194330 2 -0.119116 0.843964 -0.320494 -2.380751 3 0.024455 1.921114 -0.142516 0.262443 4 -0.871334 -0.456046 -0.653551 1.771860 5 1.058511 -2.091128 1.171792 1.563637 6 -0.192952 1.377423 0.552583 0.288304 7 1.480328 -0.757365 -0.272379 -0.362830 8 -0.033634 -0.371534 1.738937 -1.010350 9 -0.206187 -1.458660 0.538586 -1.532622 "],["join.html", "Chapter 41 Join", " Chapter 41 Join SQL style merges. left = pd.DataFrame({&#39;key&#39;: [&#39;foo&#39;, &#39;foo&#39;], &#39;lval&#39;: [1, 2]}) right = pd.DataFrame({&#39;key&#39;: [&#39;foo&#39;, &#39;foo&#39;], &#39;rval&#39;: [4, 5]}) left .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key lval 0 foo 1 1 foo 2 right .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key rval 0 foo 4 1 foo 5 pd.merge(left, right, on=&#39;key&#39;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key lval rval 0 foo 1 4 1 foo 1 5 2 foo 2 4 3 foo 2 5 Another Example left = pd.DataFrame({&#39;key&#39;: [&#39;foo&#39;, &#39;bar&#39;], &#39;lval&#39;: [1, 2]}) right = pd.DataFrame({&#39;key&#39;: [&#39;foo&#39;, &#39;bar&#39;], &#39;rval&#39;: [4, 5]}) left .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key lval 0 foo 1 1 bar 2 right .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key rval 0 foo 4 1 bar 5 pd.merge(left, right, on=&#39;key&#39;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key lval rval 0 foo 1 4 1 bar 2 5 "],["append.html", "Chapter 42 Append", " Chapter 42 Append Append rows to a dataframe. df = pd.DataFrame(np.random.randn(8, 4), columns=[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 -0.784375 0.511805 0.323780 -0.065874 1 -1.777933 0.149912 -1.727348 0.197836 2 2.382810 0.501593 1.274527 -0.456674 3 0.003459 0.793957 1.188773 0.999236 4 -0.874190 0.413551 0.020734 -0.300775 5 -1.520646 1.291041 0.561644 -0.705396 6 0.487781 0.191630 0.788804 1.912556 7 0.612815 0.366643 -1.120432 -1.543861 s = df.iloc[3] s A 0.003459 B 0.793957 C 1.188773 D 0.999236 Name: 3, dtype: float64 df.append(s, ignore_index=True) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 -0.784375 0.511805 0.323780 -0.065874 1 -1.777933 0.149912 -1.727348 0.197836 2 2.382810 0.501593 1.274527 -0.456674 3 0.003459 0.793957 1.188773 0.999236 4 -0.874190 0.413551 0.020734 -0.300775 5 -1.520646 1.291041 0.561644 -0.705396 6 0.487781 0.191630 0.788804 1.912556 7 0.612815 0.366643 -1.120432 -1.543861 8 0.003459 0.793957 1.188773 0.999236 "],["merging-joining-and-concatenating.html", "Chapter 43 Merging, Joining, and Concatenating 43.1 Create DataFrame Examples Using List Comprehension 43.2 Concatenation 43.3 Example DataFrames with Keys 43.4 Merging 43.5 Joining", " Chapter 43 Merging, Joining, and Concatenating There are 3 methods to combine DataFrames: 1. Merging (`pd.merge()`) 2. Joining (`df_left.join(df_right)`) 3. Concatenating (`pd.concat()`) 43.1 Create DataFrame Examples Using List Comprehension import numpy as np import pandas as pd l = [let + str(num) for let in [&#39;A&#39;,&#39;B&#39;,&#39;C&#39;, &#39;D&#39;] for num in range(12)] l [&#39;A0&#39;, &#39;A1&#39;, &#39;A2&#39;, &#39;A3&#39;, &#39;A4&#39;, &#39;A5&#39;, &#39;A6&#39;, &#39;A7&#39;, &#39;A8&#39;, &#39;A9&#39;, &#39;A10&#39;, &#39;A11&#39;, &#39;B0&#39;, &#39;B1&#39;, &#39;B2&#39;, &#39;B3&#39;, &#39;B4&#39;, &#39;B5&#39;, &#39;B6&#39;, &#39;B7&#39;, &#39;B8&#39;, &#39;B9&#39;, &#39;B10&#39;, &#39;B11&#39;, &#39;C0&#39;, &#39;C1&#39;, &#39;C2&#39;, &#39;C3&#39;, &#39;C4&#39;, &#39;C5&#39;, &#39;C6&#39;, &#39;C7&#39;, &#39;C8&#39;, &#39;C9&#39;, &#39;C10&#39;, &#39;C11&#39;, &#39;D0&#39;, &#39;D1&#39;, &#39;D2&#39;, &#39;D3&#39;, &#39;D4&#39;, &#39;D5&#39;, &#39;D6&#39;, &#39;D7&#39;, &#39;D8&#39;, &#39;D9&#39;, &#39;D10&#39;, &#39;D11&#39;] n = 4 d = [l[i:i + n] for i in range(0, len(l), n)] df_1 = pd.DataFrame({&#39;A&#39;:d[0],&#39;B&#39;:d[3], &#39;C&#39;:d[6],&#39;D&#39;:d[9]}, index=[0,1,2,3]) df_2 = pd.DataFrame({&#39;A&#39;:d[1],&#39;B&#39;:d[4], &#39;C&#39;:d[7],&#39;D&#39;:d[10]}, index=[4,5,6,7]) df_3 = pd.DataFrame({&#39;A&#39;:d[2],&#39;B&#39;:d[5], &#39;C&#39;:d[8],&#39;D&#39;:d[11]}, index=[8,9,10,11]) df_1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 df_2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 4 A4 B4 C4 D4 5 A5 B5 C5 D5 6 A6 B6 C6 D6 7 A7 B7 C7 D7 df_3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 8 A8 B8 C8 D8 9 A9 B9 C9 D9 10 A10 B10 C10 D10 11 A11 B11 C11 D11 43.2 Concatenation Concatenation glues together DataFrames The dimensions of each series should match Use pd.concat() with list of DataFrames pd.concat([df_1,df_2,df_3]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 4 A4 B4 C4 D4 5 A5 B5 C5 D5 6 A6 B6 C6 D6 7 A7 B7 C7 D7 8 A8 B8 C8 D8 9 A9 B9 C9 D9 10 A10 B10 C10 D10 11 A11 B11 C11 D11 pd.concat([df_1,df_2,df_3],axis=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D A B C D A B C D 0 A0 B0 C0 D0 NaN NaN NaN NaN NaN NaN NaN NaN 1 A1 B1 C1 D1 NaN NaN NaN NaN NaN NaN NaN NaN 2 A2 B2 C2 D2 NaN NaN NaN NaN NaN NaN NaN NaN 3 A3 B3 C3 D3 NaN NaN NaN NaN NaN NaN NaN NaN 4 NaN NaN NaN NaN A4 B4 C4 D4 NaN NaN NaN NaN 5 NaN NaN NaN NaN A5 B5 C5 D5 NaN NaN NaN NaN 6 NaN NaN NaN NaN A6 B6 C6 D6 NaN NaN NaN NaN 7 NaN NaN NaN NaN A7 B7 C7 D7 NaN NaN NaN NaN 8 NaN NaN NaN NaN NaN NaN NaN NaN A8 B8 C8 D8 9 NaN NaN NaN NaN NaN NaN NaN NaN A9 B9 C9 D9 10 NaN NaN NaN NaN NaN NaN NaN NaN A10 B10 C10 D10 11 NaN NaN NaN NaN NaN NaN NaN NaN A11 B11 C11 D11 43.3 Example DataFrames with Keys left = pd.DataFrame({&#39;key&#39;: [&#39;K0&#39;, &#39;K1&#39;, &#39;K2&#39;, &#39;K3&#39;], &#39;A&#39;: d[0], &#39;B&#39;: d[3]}) right = pd.DataFrame({&#39;key&#39;: [&#39;K0&#39;, &#39;K1&#39;, &#39;K2&#39;, &#39;K3&#39;], &#39;C&#39;: d[6], &#39;D&#39;: d[9]}) left .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key A B 0 K0 A0 B0 1 K1 A1 B1 2 K2 A2 B2 3 K3 A3 B3 right .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key C D 0 K0 C0 D0 1 K1 C1 D1 2 K2 C2 D2 3 K3 C3 D3 43.4 Merging pd.merge() allows you to merge DataFrames together This is similar to merging SQL Tables pd.merge(left,right,how=&#39;inner&#39;,on=&#39;key&#39;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key A B C D 0 K0 A0 B0 C0 D0 1 K1 A1 B1 C1 D1 2 K2 A2 B2 C2 D2 3 K3 A3 B3 C3 D3 Or to show a more complicated example: left = pd.DataFrame({&#39;key1&#39;: [&#39;K0&#39;, &#39;K0&#39;, &#39;K1&#39;, &#39;K2&#39;], &#39;key2&#39;: [&#39;K0&#39;, &#39;K1&#39;, &#39;K0&#39;, &#39;K1&#39;], &#39;A&#39;: d[0], &#39;B&#39;: d[3]}) right = pd.DataFrame({&#39;key1&#39;: [&#39;K0&#39;, &#39;K1&#39;, &#39;K1&#39;, &#39;K2&#39;], &#39;key2&#39;: [&#39;K0&#39;, &#39;K0&#39;, &#39;K0&#39;, &#39;K0&#39;], &#39;C&#39;: d[6], &#39;D&#39;: d[9]}) pd.merge(left, right, on=[&#39;key1&#39;, &#39;key2&#39;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key1 key2 A B C D 0 K0 K0 A0 B0 C0 D0 1 K1 K0 A2 B2 C1 D1 2 K1 K0 A2 B2 C2 D2 pd.merge(left, right, how=&#39;outer&#39;, on=[&#39;key1&#39;, &#39;key2&#39;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key1 key2 A B C D 0 K0 K0 A0 B0 C0 D0 1 K0 K1 A1 B1 NaN NaN 2 K1 K0 A2 B2 C1 D1 3 K1 K0 A2 B2 C2 D2 4 K2 K1 A3 B3 NaN NaN 5 K2 K0 NaN NaN C3 D3 pd.merge(left, right, how=&#39;right&#39;, on=[&#39;key1&#39;, &#39;key2&#39;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key1 key2 A B C D 0 K0 K0 A0 B0 C0 D0 1 K1 K0 A2 B2 C1 D1 2 K1 K0 A2 B2 C2 D2 3 K2 K0 NaN NaN C3 D3 pd.merge(left, right, how=&#39;left&#39;, on=[&#39;key1&#39;, &#39;key2&#39;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key1 key2 A B C D 0 K0 K0 A0 B0 C0 D0 1 K0 K1 A1 B1 NaN NaN 2 K1 K0 A2 B2 C1 D1 3 K1 K0 A2 B2 C2 D2 4 K2 K1 A3 B3 NaN NaN 43.5 Joining left.join(right) combines the columns of two (potentially differently) indexed DataFrames into a single DataFrame left = pd.DataFrame({&#39;A&#39;: d[0], &#39;B&#39;: d[3]}, index=[&#39;K0&#39;, &#39;K1&#39;, &#39;K2&#39;, &#39;K4&#39;]) right = pd.DataFrame({&#39;C&#39;: d[6], &#39;D&#39;: d[9]}, index=[&#39;K0&#39;, &#39;K2&#39;, &#39;K3&#39;, &#39;K5&#39;]) left.join(right) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D K0 A0 B0 C0 D0 K1 A1 B1 NaN NaN K2 A2 B2 C1 D1 K4 A3 B3 NaN NaN left.join(right, how=&#39;outer&#39;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D K0 A0 B0 C0 D0 K1 A1 B1 NaN NaN K2 A2 B2 C1 D1 K3 NaN NaN C2 D2 K4 A3 B3 NaN NaN K5 NaN NaN C3 D3 "],["done-1.html", "Chapter 44 DONE", " Chapter 44 DONE "],["grouping.html", "Chapter 45 Grouping", " Chapter 45 Grouping By “group by” we are referring to a process involving one or more of the following steps: Splitting the data into groups based on some criteria Applying a function to each group independently Combining the results into a data structure df = pd.DataFrame({&#39;A&#39;: [&#39;foo&#39;, &#39;bar&#39;, &#39;foo&#39;, &#39;bar&#39;, &#39;foo&#39;, &#39;bar&#39;, &#39;foo&#39;, &#39;foo&#39;], &#39;B&#39;: [&#39;one&#39;, &#39;one&#39;, &#39;two&#39;, &#39;three&#39;, &#39;two&#39;, &#39;two&#39;, &#39;one&#39;, &#39;three&#39;], &#39;C&#39;: np.random.randn(8), &#39;D&#39;: np.random.randn(8)}) df Grouping and then applying the sum() function to the resulting groups. df.groupby(&#39;A&#39;).sum() Grouping by multiple columns forms a hierarchical index, and again we can apply the sum function. df.groupby([&#39;A&#39;, &#39;B&#39;]).sum() "],["groupby.html", "Chapter 46 Groupby 46.1 groupby() 46.2 Groupby and Aggregation Methods 46.3 Groupby + describe method 46.4 Groupby + describe method 46.5 Groupby + describe method (multiple numeric values)", " Chapter 46 Groupby .groupby method allow syou to group rows of data together and call aggregate functions import numpy as np import pandas as pd data = {&#39;school&#39;: [&#39;HSU&#39;, &#39;HSU&#39;,&#39;HSU&#39;, &#39;OBU&#39;,&#39;OBU&#39;, &#39;SIU&#39;,&#39;SIU&#39;, &#39;SEU&#39;, &#39;SEU&#39;], &#39;professor&#39;: [&#39;Bob&#39;,&#39;Jeff&#39;,&#39;Angela&#39;,&#39;Susan&#39;, &#39;Albert&#39;,&#39;Zelda&#39;,&#39;Alexa&#39;, &#39;Heather&#39;,&#39;Rebecca&#39;], &#39;publication&#39;: [10,2,30,25,0,80,4,30,15]} df = pd.DataFrame(data) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } school professor publication 0 HSU Bob 10 1 HSU Jeff 2 2 HSU Angela 30 3 OBU Susan 25 4 OBU Albert 0 5 SIU Zelda 80 6 SIU Alexa 4 7 SEU Heather 30 8 SEU Rebecca 15 46.1 groupby() use the .groupby() method to group rows together based off of a column name Create a group based off of school df.groupby(&#39;school&#39;) #creates a DataFrameGroupBy object &lt;pandas.core.groupby.groupby.DataFrameGroupBy object at 0x119d4db00&gt; by_school = df.groupby(&#39;school&#39;) 46.2 Groupby and Aggregation Methods Use the aggregation methods on the grouped object by_school.mean() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } publication school HSU 14.0 OBU 12.5 SEU 22.5 SIU 42.0 df.groupby(&#39;school&#39;).mean() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } publication school HSU 14.0 OBU 12.5 SEU 22.5 SIU 42.0 by_school.std() # standard deviation .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } publication school HSU 14.422205 OBU 17.677670 SEU 10.606602 SIU 53.740115 by_school.min() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } professor publication school HSU Angela 2 OBU Albert 0 SEU Heather 15 SIU Alexa 4 by_school.max() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } professor publication school HSU Jeff 30 OBU Susan 25 SEU Rebecca 30 SIU Zelda 80 by_school.count() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } professor publication school HSU 3 3 OBU 2 2 SEU 2 2 SIU 2 2 46.3 Groupby + describe method by_school.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } publication count mean std min 25% 50% 75% max school HSU 3.0 14.0 14.422205 2.0 6.00 10.0 20.00 30.0 OBU 2.0 12.5 17.677670 0.0 6.25 12.5 18.75 25.0 SEU 2.0 22.5 10.606602 15.0 18.75 22.5 26.25 30.0 SIU 2.0 42.0 53.740115 4.0 23.00 42.0 61.00 80.0 46.4 Groupby + describe method type(by_school.describe().transpose()) pandas.core.frame.DataFrame 46.5 Groupby + describe method (multiple numeric values) data = {&#39;school&#39;: [&#39;HSU&#39;, &#39;HSU&#39;,&#39;HSU&#39;, &#39;OBU&#39;,&#39;OBU&#39;,&#39;SIU&#39;,&#39;SIU&#39;, &#39;SEU&#39;, &#39;SEU&#39;], &#39;professor&#39;: [&#39;Bob&#39;,&#39;Jeff&#39;,&#39;Angela&#39;,&#39;Susan&#39;,&#39;Albert&#39;,&#39;Zelda&#39;,&#39;Alexa&#39;,&#39;Heather&#39;,&#39;Rebecca&#39;], &#39;publication&#39;: [10,2,30,25,0,80,4,30,15], &#39;years&#39;:[2,8,14,3,4,25,7,2,5]} df = pd.DataFrame(data) df.groupby(&#39;school&#39;).describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } publication years count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max school HSU 3.0 14.0 14.422205 2.0 6.00 10.0 20.00 30.0 3.0 8.0 6.000000 2.0 5.00 8.0 11.00 14.0 OBU 2.0 12.5 17.677670 0.0 6.25 12.5 18.75 25.0 2.0 3.5 0.707107 3.0 3.25 3.5 3.75 4.0 SEU 2.0 22.5 10.606602 15.0 18.75 22.5 26.25 30.0 2.0 3.5 2.121320 2.0 2.75 3.5 4.25 5.0 SIU 2.0 42.0 53.740115 4.0 23.00 42.0 61.00 80.0 2.0 16.0 12.727922 7.0 11.50 16.0 20.50 25.0 df.groupby(&#39;school&#39;).describe().transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } school HSU OBU SEU SIU publication count 3.000000 2.000000 2.000000 2.000000 mean 14.000000 12.500000 22.500000 42.000000 std 14.422205 17.677670 10.606602 53.740115 min 2.000000 0.000000 15.000000 4.000000 25% 6.000000 6.250000 18.750000 23.000000 50% 10.000000 12.500000 22.500000 42.000000 75% 20.000000 18.750000 26.250000 61.000000 max 30.000000 25.000000 30.000000 80.000000 years count 3.000000 2.000000 2.000000 2.000000 mean 8.000000 3.500000 3.500000 16.000000 std 6.000000 0.707107 2.121320 12.727922 min 2.000000 3.000000 2.000000 7.000000 25% 5.000000 3.250000 2.750000 11.500000 50% 8.000000 3.500000 3.500000 16.000000 75% 11.000000 3.750000 4.250000 20.500000 max 14.000000 4.000000 5.000000 25.000000 "],["done-2.html", "Chapter 47 DONE", " Chapter 47 DONE "],["reshaping.html", "Chapter 48 Reshaping 48.1 Stack 48.2 Removing Columns 48.3 Remove/Drop data (For real) 48.4 Dropping rows 48.5 Why 0 for row; 1 for column", " Chapter 48 Reshaping 48.1 Stack tuples = list(zip(*[[&#39;bar&#39;, &#39;bar&#39;, &#39;baz&#39;, &#39;baz&#39;, &#39;foo&#39;, &#39;foo&#39;, &#39;qux&#39;, &#39;qux&#39;], [&#39;one&#39;, &#39;two&#39;, &#39;one&#39;, &#39;two&#39;, &#39;one&#39;, &#39;two&#39;, &#39;one&#39;, &#39;two&#39;]])) index = pd.MultiIndex.from_tuples(tuples, names=[&#39;first&#39;, &#39;second&#39;]) df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=[&#39;A&#39;, &#39;B&#39;]) df2 = df[:4] df2 The stack() method “compresses” a level in the DataFrame’s columns. stacked = df2.stack() stacked With a “stacked” DataFrame or Series (having a MultiIndex as the index), the inverse operation of stack() is unstack(), which by default unstacks the last level: stacked.unstack() stacked.unstack(1) stacked.unstack(0) ## Creating New Columns df[&#39;new&#39;] = df[&#39;A&#39;] + df[&#39;B&#39;] df 48.2 Removing Columns df.drop(&#39;new&#39;, axis=&#39;columns&#39;) # could also use axis = 1 df # new is still there!! 48.3 Remove/Drop data (For real) Use inplace to remove PERMENANTLY df.drop(&#39;new&#39;, axis=&#39;columns&#39;, inplace=True) 48.4 Dropping rows Default drop is row (or use axis = 0) df.drop(&#39;second&#39;, axis=&#39;rows&#39;) 48.5 Why 0 for row; 1 for column df.shape # location 0 for row; 1 for column "],["pivot-tables.html", "Chapter 49 Pivot tables", " Chapter 49 Pivot tables df = pd.DataFrame({&#39;A&#39;: [&#39;one&#39;, &#39;one&#39;, &#39;two&#39;, &#39;three&#39;] * 3, &#39;B&#39;: [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;] * 4, &#39;C&#39;: [&#39;foo&#39;, &#39;foo&#39;, &#39;foo&#39;, &#39;bar&#39;, &#39;bar&#39;, &#39;bar&#39;] * 2, &#39;D&#39;: np.random.randn(12), &#39;E&#39;: np.random.randn(12)}) df We can produce pivot tables from this data very easily: pd.pivot_table(df, values=&#39;D&#39;, index=[&#39;A&#39;, &#39;B&#39;], columns=[&#39;C&#39;]) "],["time-series.html", "Chapter 50 Time Series", " Chapter 50 Time Series pandas has simple, powerful, and efficient functionality for performing resampling operations during frequency conversion (e.g., converting secondly data into 5-minutely data). This is extremely common in, but not limited to, financial applications. rng = pd.date_range(&#39;1/1/2012&#39;, periods=100, freq=&#39;S&#39;) ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng) ts.resample(&#39;5Min&#39;).sum() Time zone representation: rng = pd.date_range(&#39;3/6/2012 00:00&#39;, periods=5, freq=&#39;D&#39;) ts = pd.Series(np.random.randn(len(rng)), rng) ts ts_utc = ts.tz_localize(&#39;UTC&#39;) ts_utc Converting to another time zone: ts_utc.tz_convert(&#39;US/Eastern&#39;) Converting between time span representations: rng = pd.date_range(&#39;1/1/2012&#39;, periods=5, freq=&#39;M&#39;) ts = pd.Series(np.random.randn(len(rng)), index=rng) ts ps = ts.to_period() ps ps.to_timestamp() Converting between period and timestamp enables some convenient arithmetic functions to be used. In the following example, we convert a quarterly frequency with year ending in November to 9am of the end of the month following the quarter end: prng = pd.period_range(&#39;1990Q1&#39;, &#39;2000Q4&#39;, freq=&#39;Q-NOV&#39;) ts = pd.Series(np.random.randn(len(prng)), prng) ts.index = (prng.asfreq(&#39;M&#39;, &#39;e&#39;) + 1).asfreq(&#39;H&#39;, &#39;s&#39;) + 9 ts.head() "],["categoricals.html", "Chapter 51 Categoricals", " Chapter 51 Categoricals pandas can include categorical data in a DataFrame. df = pd.DataFrame({&quot;id&quot;: [1, 2, 3, 4, 5, 6], &quot;raw_grade&quot;: [&#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;a&#39;, &#39;a&#39;, &#39;e&#39;]}) Convert the raw grades to a categorical data type. df[&quot;grade&quot;] = df[&quot;raw_grade&quot;].astype(&quot;category&quot;) df[&#39;grade&#39;] Rename the categories to more meaningful names (assigning to Series.cat.categories is inplace!). df[&quot;grade&quot;].cat.categories = [&quot;very good&quot;, &quot;good&quot;, &quot;very bad&quot;] df[&#39;grade&#39;] Reorder the categories and simultaneously add the missing categories (methods under Series .cat return a new Series by default). df[&quot;grade&quot;] = df[&quot;grade&quot;].cat.set_categories([&quot;very bad&quot;, &quot;bad&quot;, &quot;medium&quot;, &quot;good&quot;, &quot;very good&quot;]) df[&#39;grade&#39;] df.sort_values(by=&quot;grade&quot;) Grouping by a categorical column also shows empty categories. df.groupby(&quot;grade&quot;).size() "],["plotting.html", "Chapter 52 Plotting", " Chapter 52 Plotting ts = pd.Series(np.random.randn(1000), index=pd.date_range(&#39;1/1/2000&#39;, periods=1000)) ts = ts.cumsum() ts.plot(); On a DataFrame, the plot() method is a convenience to plot all of the columns with labels: df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;]) df = df.cumsum() df.plot(); "],["getting-data-inout.html", "Chapter 53 Getting data in/out 53.1 CSV 53.2 HDF5 53.3 Excel", " Chapter 53 Getting data in/out 53.1 CSV Writing a csv file. df.to_csv(&#39;foo.csv&#39;) Reading from a csvs file. pd.read_csv(&#39;foo.csv&#39;) 53.2 HDF5 Writing to a HDF5 Store. df.to_hdf(&#39;foo.h5&#39;,&#39;df&#39;) Reading a HDF5 Store pd.read_hdf(&#39;foo.h5&#39;, &#39;df&#39;) 53.3 Excel Writing an excel file df.to_excel(&#39;foo.xlsx&#39;, sheet_name=&#39;Sheet1&#39;) Reading from an excel file pd.read_excel(&#39;foo.xlsx&#39;,&#39;Sheet1&#39;,index_col=None, na_values=[&#39;NA&#39;]) "],["data-input-and-output.html", "Chapter 54 Data Input and Output 54.1 CSV 54.2 Excel", " Chapter 54 Data Input and Output Let’s take a look at importing and exporting data with Pandas import numpy as np import pandas as pd 54.1 CSV 54.1.1 CSV Input df = pd.read_csv(&#39;../../../data/diabetes.csv&#39;) df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id chol stab.glu hdl ratio glyhb location age gender height weight frame bp.1s bp.1d bp.2s bp.2d waist hip time.ppn 0 1000 203.0 82 56.0 3.6 4.31 Buckingham 46 female 62.0 121.0 medium 118.0 59.0 NaN NaN 29.0 38.0 720.0 1 1001 165.0 97 24.0 6.9 4.44 Buckingham 29 female 64.0 218.0 large 112.0 68.0 NaN NaN 46.0 48.0 360.0 2 1002 228.0 92 37.0 6.2 4.64 Buckingham 58 female 61.0 256.0 large 190.0 92.0 185.0 92.0 49.0 57.0 180.0 3 1003 78.0 93 12.0 6.5 4.63 Buckingham 67 male 67.0 119.0 large 110.0 50.0 NaN NaN 33.0 38.0 480.0 4 1005 249.0 90 28.0 8.9 7.72 Buckingham 64 male 68.0 183.0 medium 138.0 80.0 NaN NaN 44.0 41.0 300.0 54.1.2 CSV Output df.to_csv(&#39;diabetes_copy.csv&#39;,index=False) # Index False is important 54.2 Excel Pandas can read and write excel files, keep in mind, this only imports data. Not formulas or images, having images or macros may cause this read_excel method to crash. pd.read_excel(&#39;../../../data/diabetes.xls&#39;) 54.2.1 Excel Output df.to_excel(&#39;diabetes_copy.xlsx&#39;,sheet_name=&#39;new_name&#39;) 54.2.2 HTML Input Pandas read_html function will read tables off of a webpage and return a list of DataFrame objects: df = pd.read_html(&#39;http://www.fdic.gov/bank/individual/failed/banklist.html&#39;) df[0] # since read_html creates list of dataframes "],["operations-1.html", "Chapter 55 Operations 55.1 Unique 55.2 Select Data 55.3 Applying Functions 55.4 Columns 55.5 Sort and Order DataFrames 55.6 Null Values", " Chapter 55 Operations There are many operations you can use with pandas We’ll cover the most common ones bank = df[0] bank.info() bank.tail(10) 55.1 Unique bank[&#39;ST&#39;].unique() # unique obs bank[&#39;ST&#39;].nunique() # Number of unique obs bank[&#39;ST&#39;].value_counts() # counts each of the unique obs 55.2 Select Data bank.head() bank[(bank[&#39;CERT&#39;] &lt; 2000) &amp; (bank[&#39;ST&#39;] == &#39;IL&#39;)] 55.3 Applying Functions def divide_1000(x): return x / 1000 bank[&#39;CERT&#39;].apply(lambda x: x / 1000) bank[&#39;CERT&#39;].apply(np.log) bank[&#39;CERT&#39;].sum() 55.4 Columns bank.columns # list columns del bank[&#39;Updated Date&#39;] # permanently remove column bank.columns bank.index 55.5 Sort and Order DataFrames bank.head() bank.sort_values(by=&#39;ST&#39;) # inplace = False by default 55.6 Null Values bank.isnull() bank.dropna() # Seen this before "],["sql.html", "Chapter 56 SQL", " Chapter 56 SQL The pandas.io.sql module provides a collection of query wrappers to both facilitate data retrieval and to reduce dependency on DB-specific API. Database abstraction is provided by SQLAlchemy if installed. In addition you will need a driver library for your database. Examples of such drivers are psycopg2 for PostgreSQL or pymysql for MySQL. For SQLite this is included in Python’s standard library by default. You can find an overview of supported drivers for each SQL dialect in the SQLAlchemy docs. If SQLAlchemy is not installed, a fallback is only provided for sqlite (and for mysql for backwards compatibility, but this is deprecated and will be removed in a future version). This mode requires a Python database adapter which respect the Python DB-API. See also some cookbook examples for some advanced strategies. The key functions are: read_sql_table(table_name, con[, schema, ...]) Read SQL database table into a DataFrame. read_sql_query(sql, con[, index_col, ...]) Read SQL query into a DataFrame. read_sql(sql, con[, index_col, ...]) Read SQL query or database table into a DataFrame. DataFrame.to_sql(name, con[, flavor, ...]) Write records stored in a DataFrame to a SQL database. from sqlalchemy import create_engine engine = create_engine(&#39;sqlite:///:memory:&#39;) df.to_sql(&#39;temp&#39;, engine) sql_df = pd.read_sql(&#39;data&#39;,con=engine) sql_df "],["pandas-basics.html", "Chapter 57 Pandas Basics", " Chapter 57 Pandas Basics NumPy Wrangling Pandas Introduction DataFrames Importing Data Data Selection Missing Data: Part 1 Missing Data: Part 2 Tidy Data Operations Merging Data: Part 1 Merging Data: Part 2 Groupby: Part 1 Groupby: Part 2 Reshaping Time Series Categorical Data Ploting Data with Pandas Data Input/Output: Part 1 Data Input/Output: Part 2 Table of Contents 2 Statistical Learning 2.1 What is Statistical Learning? 2.1.1 Why Estimate \\(f\\)? 2.1.2 How to Estimate \\(f\\)? 2.1.3 Accuracy vs. Interpretability 2.1.4 Supervised vs. Unsupervised Learning 2.1.5 Regression vs. Classification 2.2 Assessing Model Accuracy 2.2.1 Measuring Quality of Fit 2.2.2 The Bias-Variance Tradeoff 2.2.3 The Classification Setting 2.3 Footnotes "],["statistical-learning.html", "Chapter 58 Statistical Learning 58.1 What is Statistical Learning? 58.2 Assessing Model Accuracy 58.3 Footnotes", " Chapter 58 Statistical Learning 58.1 What is Statistical Learning? Given paired data \\((X, Y)\\), assume a relationship between \\(X\\) and \\(Y\\) modeled by \\[ Y = f(X) + \\epsilon \\] where \\(f:\\mathbb{R}^p \\rightarrow \\mathbb{R}\\) is a function and \\(\\epsilon\\) is a random error term with \\(\\mathbb{E}(\\epsilon) = 0\\). Statistical learning is a set of approaches for estimating \\(f\\)0 58.1.1 Why Estimate \\(f\\)? 58.1.1.0.1 Prediction We may want to predict the output \\(Y\\) from an estimate \\(\\hat{f}\\) of \\(f\\). The predicted value for a given \\(Y\\) is then \\[ \\hat{Y} = \\hat{f}(X)\\]. In prediction, we often treat \\(f\\) as a black-box The mean squared-error2 \\(\\mathbf{mse}(\\hat{Y})=\\mathbb{E}(Y-\\hat{Y})^2\\) is a good measure of the accuracy of \\(\\hat{Y}\\) as a predictor for \\(Y\\). One can write \\[ \\mathbf{mse}(\\hat{Y}) = \\left(f(X) - \\hat{f}(X)\\right)^2 + \\mathbb{V}(\\epsilon) \\] These two terms are known as the reducible error and irreducible error, respectively3 58.1.1.0.2 Inference Instead of predicting \\(Y\\) from \\(X\\), we may be more interested how \\(Y\\) changes as a function of \\(X\\). In inference, we usually do not treat \\(f\\) as a black box. Examples of important inference questions: Which predictors have the largest influence on the response? What is the relationship between the response and each predictor? *Is f linear or non-linear? 58.1.2 How to Estimate \\(f\\)? 58.1.2.0.1 Parametric methods Steps for parametric method: Assume a parametric model for \\(f\\), that is assume a specific functional form4 \\[f = f(X, \\boldsymbol{\\beta}) \\] for some vector of parameters \\(\\boldsymbol{\\beta} = (\\beta_1,\\dots,\\beta_p)^T\\) Use the training data to fit or train the model, that is to choose \\(\\beta_i\\) such that \\[Y \\approx f(X, \\boldsymbol{\\beta})\\] 58.1.2.0.2 Non-parametric methods These methods make no assumptions about the functional form of \\(f\\). 58.1.3 Accuracy vs. Interpretability In inference, generally speaking the more flexible the method, the less interpretable. In prediction, generally speaking the more flexible the method, the less accurate 58.1.4 Supervised vs. Unsupervised Learning In supervised learning, training data consists of pairs \\((X, Y)\\) where \\(X\\) is a vector of predictors and \\(Y\\) a response. Prediction and inference are supervised learning problems, and the response variable (or the relationship between the response and the predictors) supervises the analysis of model In unsupervised learning, training data lacks a response variable. 58.1.5 Regression vs. Classification Problems with a quantitative response (\\(Y\\in S \\subseteq \\mathbb{R}\\)) tend to be called regression problems Problems with a qualitative, or categorical response (\\(Y \\in \\{y_1, \\dots, y_n\\})\\) tend to be called classification problems 58.2 Assessing Model Accuracy There is no free lunch in statistics 58.2.1 Measuring Quality of Fit To evaluate the performance of a method on a data set, we need measure model accuracy (how well predictions match observed data). In regression, the most common measure is the mean-squared error \\[MSE = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2\\] where \\(y_i\\) and \\(\\hat{f}(x_i)\\) are the \\(i\\) true and predicting responses, respectively. We are usually not interested in minimizing MSE with respect to training data but rather to test data. There is no guarantee low training MSE will translate to low test MSE. Having low training MSE but high test MSE is called overfitting 58.2.2 The Bias-Variance Tradeoff For a given \\(x_0\\), the expected 5 MSE can be written \\[\\begin{align*} \\mathbb{E}\\left[\\left(y_0 - \\hat{f}(x_0)\\right)^2\\right] &amp;= \\left(\\mathbb{E}\\left[\\hat{f}(x) \\right] - f(x)\\right)^2 + \\mathbb{E}\\left[\\left(\\hat{f}(x_0) - \\mathbb{E}\\left[\\hat{f}(x_0)\\right]\\right)^2\\right] + \\mathbb{E}\\left[\\left(\\epsilon - \\mathbb{E}[\\epsilon]\\right)^2\\right]\\\\ &amp;= \\mathbf{bias}^2\\left(\\hat{f}(x_0))\\right) + \\mathbb{V}\\left(\\hat{f}(x_0)\\right) + \\mathbb{V}(\\epsilon) \\end{align*}\\] A good method minimizes variance and bias simultaneously. As a general rule, these quantities are inversely proportional. More flexible methods have lower bias but higher variance, while less flexible methods have the opposite. This is the bias-variance tradeoff In practice the mse, variance and bias cannot be calculated exactly but one must keep the bias-variance tradeoff in mind. 58.2.3 The Classification Setting In the classification setting, the most common measure of model accuracy is the error rate 6 \\[\\frac{1}{n}\\sum_{i=1}^n I(y_i \\neq \\hat{y}_i)\\] As with the regression, we are interested in minimizing the test error rate, not the training error rate. 58.2.3.0.1 The Bayes Classifier Given \\(K\\) classes, the Bayes Classifier predicts \\[ \\hat{y_0} = \\underset{1\\leqslant j \\leqslant K}{\\text{argmax}\\,} \\mathbb{P}\\left(Y=j\\ |\\ X = x_0\\right)\\] The set of points \\[\\{x_0\\in\\mathbb{R}^p\\ |\\ \\mathbb{P}\\left(Y=j\\ |\\ X = x_0\\right) = \\mathbb{P}\\left(Y=k\\ |\\ X = x_0\\right)\\ \\text{for all}\\ 1\\leqslant j,k \\leqslant K\\}\\] is called the Bayes decision boundary The test error rate of the Bayes classifier is the Bayes error rate, which is minimal among classifiers. It is given by \\[ 1 - \\mathbb{E}\\left(\\underset{j}{\\max} \\mathbb{P}\\left(Y=j\\ |\\ X\\right)\\right)\\] The Bayes classifier is optimal, but in practice we don’t know \\(\\mathbb{P}\\left(Y\\ |\\ X\\right)\\). 58.2.3.0.2 K-Nearest Neighbors The K-nearest neighbors classifier works by estimating \\(\\mathbb{P}\\left(Y\\ |\\ X\\right)\\) as follows. Given \\(K\\geqslant 1\\) and \\(x_0\\), find the set of points \\[ \\mathcal{N}_0 = \\{K\\ \\text{nearest points to}\\ x_0\\}\\subseteq\\mathbb{R}^p \\] For each class \\(j\\) set \\[ \\mathbb{P}\\left(Y=j\\ |\\ X\\right) = \\frac{1}{K}\\sum_{x_i\\in\\mathcal{N}_0}I(y_i = j)\\] Predict \\[ \\hat{y_0} = \\underset{1\\leqslant j \\leqslant K}{\\text{argmax}\\,} \\mathbb{P}\\left(Y=j\\ |\\ X = x_0\\right)\\] 58.3 Footnotes Reading the rest of the chapter, one realized this is the situation for supervised learning, which is the vast majority of this book is concerned with. ↩︎ Here \\(X=(X_1,\\dots, X_p)^T\\) is a vector. ↩︎ This is usual definition of the mean squared-error of \\(\\hat{Y}\\) as an estimator of the (non-parametric) quantity \\(Y=f(X)\\). ↩︎ We can in principle control the reducible error by improving the estimate \\(\\hat{f}\\), but we cannot control the irreducible error. ↩︎ For example, a simple but popular assumption is that f is linear in both the parameters and the features, that is: \\[f(X) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\\] This is linear regression. ↩︎ Here the random variable is \\(\\hat{f}(x_0)\\), so the average is taken over all data sets ↩︎ This is just the proportion of misclassified observations. ↩︎ "],["introduction-to-machine-learning.html", "Chapter 59 Introduction to Machine Learning", " Chapter 59 Introduction to Machine Learning "],["chapter-3-linear-regression.html", "Chapter 60 Chapter 3 - Linear Regression 60.1 3.1 Simple Linear Regression 60.2 3.2 Multiple Linear Regression 60.3 3.3 Other Considerations in the Regression Model", " Chapter 60 Chapter 3 - Linear Regression Load Datasets 3.1 Simple Linear Regression 3.2 Multiple Linear Regression 3.3 Other Considerations in the Regression Model import pandas as pd import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import axes3d import seaborn as sns from sklearn.preprocessing import scale import sklearn.linear_model as skl_lm from sklearn.metrics import mean_squared_error, r2_score import statsmodels.api as sm import statsmodels.formula.api as smf %matplotlib inline plt.style.use(&#39;seaborn-white&#39;) 60.0.1 Load Datasets Datasets available on https://statlearning.com/data.html !../data_grabber.sh data/ ../data_urls.txt Error: data/ not found. Will make directory and place files in data/. advertising = pd.read_csv(&#39;data/Advertising.csv&#39;, usecols=[1,2,3,4]) advertising.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 4 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 TV 200 non-null float64 1 radio 200 non-null float64 2 newspaper 200 non-null float64 3 sales 200 non-null float64 dtypes: float64(4) memory usage: 6.4 KB credit = pd.read_csv(&#39;data/Credit.csv&#39;, usecols=list(range(1,12))) credit[&#39;Student2&#39;] = credit.Student.map({&#39;No&#39;:0, &#39;Yes&#39;:1}) credit.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Income Limit Rating Cards Age Education Gender Student Married Ethnicity Balance Student2 0 14.891 3606 283 2 34 11 Male No Yes Caucasian 333 0 1 106.025 6645 483 3 82 15 Female Yes Yes Asian 903 1 2 104.593 7075 514 4 71 11 Male No No Asian 580 0 auto = pd.read_table(&#39;data/Auto.data&#39;, na_values=&#39;?&#39;).dropna() auto.info() auto.head() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 0 entries Data columns (total 14 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 mpg 0 non-null object 1 cylinders 0 non-null object 2 displacement 0 non-null float64 3 horsepower weight 0 non-null float64 4 acceleration 0 non-null float64 5 year 0 non-null float64 6 origin 0 non-null float64 7 name 0 non-null float64 8 Unnamed: 8 0 non-null float64 9 Unnamed: 9 0 non-null float64 10 Unnamed: 10 0 non-null float64 11 Unnamed: 11 0 non-null float64 12 Unnamed: 12 0 non-null float64 13 Unnamed: 13 0 non-null float64 dtypes: float64(12), object(2) memory usage: 0.0+ bytes .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mpg cylinders displacement horsepower weight acceleration year origin name Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13 60.1 3.1 Simple Linear Regression 60.1.1 Figure 3.1 - Least squares fit sns.regplot(advertising.TV, advertising.Sales, order=1, ci=None, scatter_kws={&#39;color&#39;:&#39;r&#39;, &#39;s&#39;:9}) plt.xlim(-10,310) plt.ylim(ymin=0); png 60.1.2 Figure 3.2 - Regression coefficients - RSS Note that the text in the book describes the coefficients based on uncentered data, whereas the plot shows the model based on centered data. The latter is visually more appealing for explaining the concept of a minimum RSS. I think that, in order not to confuse the reader, the values on the axis of the B0 coefficients have been changed to correspond with the text. The axes on the plots below are unaltered. # Regression coefficients (Ordinary Least Squares) regr = skl_lm.LinearRegression() X = scale(advertising.TV, with_mean=True, with_std=False).reshape(-1,1) y = advertising.Sales regr.fit(X,y) print(regr.intercept_) print(regr.coef_) 14.0225 [ 0.04753664] # Create grid coordinates for plotting B0 = np.linspace(regr.intercept_-2, regr.intercept_+2, 50) B1 = np.linspace(regr.coef_-0.02, regr.coef_+0.02, 50) xx, yy = np.meshgrid(B0, B1, indexing=&#39;xy&#39;) Z = np.zeros((B0.size,B1.size)) # Calculate Z-values (RSS) based on grid of coefficients for (i,j),v in np.ndenumerate(Z): Z[i,j] =((y - (xx[i,j]+X.ravel()*yy[i,j]))**2).sum()/1000 # Minimized RSS min_RSS = r&#39;$\\beta_0$, $\\beta_1$ for minimized RSS&#39; min_rss = np.sum((regr.intercept_+regr.coef_*X - y.values.reshape(-1,1))**2)/1000 min_rss 2.1025305831313514 fig = plt.figure(figsize=(15,6)) fig.suptitle(&#39;RSS - Regression coefficients&#39;, fontsize=20) ax1 = fig.add_subplot(121) ax2 = fig.add_subplot(122, projection=&#39;3d&#39;) # Left plot CS = ax1.contour(xx, yy, Z, cmap=plt.cm.Set1, levels=[2.15, 2.2, 2.3, 2.5, 3]) ax1.scatter(regr.intercept_, regr.coef_[0], c=&#39;r&#39;, label=min_RSS) ax1.clabel(CS, inline=True, fontsize=10, fmt=&#39;%1.1f&#39;) # Right plot ax2.plot_surface(xx, yy, Z, rstride=3, cstride=3, alpha=0.3) ax2.contour(xx, yy, Z, zdir=&#39;z&#39;, offset=Z.min(), cmap=plt.cm.Set1, alpha=0.4, levels=[2.15, 2.2, 2.3, 2.5, 3]) ax2.scatter3D(regr.intercept_, regr.coef_[0], min_rss, c=&#39;r&#39;, label=min_RSS) ax2.set_zlabel(&#39;RSS&#39;) ax2.set_zlim(Z.min(),Z.max()) ax2.set_ylim(0.02,0.07) # settings common to both plots for ax in fig.axes: ax.set_xlabel(r&#39;$\\beta_0$&#39;, fontsize=17) ax.set_ylabel(r&#39;$\\beta_1$&#39;, fontsize=17) ax.set_yticks([0.03,0.04,0.05,0.06]) ax.legend() png 60.1.3 Confidence interval on page 67 &amp; Table 3.1 &amp; 3.2 - Statsmodels est = smf.ols(&#39;Sales ~ TV&#39;, advertising).fit() est.summary().tables[1] coef std err t P&gt;|t| [0.025 0.975] Intercept 7.0326 0.458 15.360 0.000 6.130 7.935 TV &lt;td&gt; 0.0475&lt;/td&gt; &lt;td&gt; 0.003&lt;/td&gt; &lt;td&gt; 17.668&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 0.042&lt;/td&gt; &lt;td&gt; 0.053&lt;/td&gt; # RSS with regression coefficients ((advertising.Sales - (est.params[0] + est.params[1]*advertising.TV))**2).sum()/1000 2.1025305831313514 60.1.4 Table 3.1 &amp; 3.2 - Scikit-learn regr = skl_lm.LinearRegression() X = advertising.TV.values.reshape(-1,1) y = advertising.Sales regr.fit(X,y) print(regr.intercept_) print(regr.coef_) 7.03259354913 [ 0.04753664] Sales_pred = regr.predict(X) r2_score(y, Sales_pred) 0.61187505085007099 60.2 3.2 Multiple Linear Regression 60.2.1 Table 3.3 - Statsmodels est = smf.ols(&#39;Sales ~ Radio&#39;, advertising).fit() est.summary().tables[1] coef std err t P&gt;|t| [0.025 0.975] Intercept 9.3116 0.563 16.542 0.000 8.202 10.422 Radio 0.2025 0.020 9.921 0.000 0.162 0.243 est = smf.ols(&#39;Sales ~ Newspaper&#39;, advertising).fit() est.summary().tables[1] coef std err t P&gt;|t| [0.025 0.975] Intercept 12.3514 0.621 19.876 0.000 11.126 13.577 Newspaper 0.0547 0.017 3.300 0.001 0.022 0.087 60.2.2 Table 3.4 &amp; 3.6 - Statsmodels est = smf.ols(&#39;Sales ~ TV + Radio + Newspaper&#39;, advertising).fit() est.summary() OLS Regression Results Dep. Variable: &lt;td&gt;Sales&lt;/td&gt; &lt;th&gt; R-squared: &lt;/th&gt; &lt;td&gt; 0.897&lt;/td&gt; Model: &lt;td&gt;OLS&lt;/td&gt; &lt;th&gt; Adj. R-squared: &lt;/th&gt; &lt;td&gt; 0.896&lt;/td&gt; Method: &lt;td&gt;Least Squares&lt;/td&gt; &lt;th&gt; F-statistic: &lt;/th&gt; &lt;td&gt; 570.3&lt;/td&gt; Date: &lt;td&gt;Tue, 09 Jan 2018&lt;/td&gt; &lt;th&gt; Prob (F-statistic):&lt;/th&gt; &lt;td&gt;1.58e-96&lt;/td&gt; Time: &lt;td&gt;23:14:15&lt;/td&gt; &lt;th&gt; Log-Likelihood: &lt;/th&gt; &lt;td&gt; -386.18&lt;/td&gt; No. Observations: &lt;td&gt; 200&lt;/td&gt; &lt;th&gt; AIC: &lt;/th&gt; &lt;td&gt; 780.4&lt;/td&gt; Df Residuals: &lt;td&gt; 196&lt;/td&gt; &lt;th&gt; BIC: &lt;/th&gt; &lt;td&gt; 793.6&lt;/td&gt; Df Model: &lt;td&gt; 3&lt;/td&gt; &lt;th&gt; &lt;/th&gt; &lt;td&gt; &lt;/td&gt; Covariance Type: &lt;td&gt;nonrobust&lt;/td&gt; &lt;th&gt; &lt;/th&gt; &lt;td&gt; &lt;/td&gt; coef std err t P&gt;|t| [0.025 0.975] Intercept 2.9389 0.312 9.422 0.000 2.324 3.554 TV &lt;td&gt; 0.0458&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt; 32.809&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 0.043&lt;/td&gt; &lt;td&gt; 0.049&lt;/td&gt; Radio 0.1885 0.009 21.893 0.000 0.172 0.206 Newspaper -0.0010 0.006 -0.177 0.860 -0.013 0.011 Omnibus: &lt;td&gt;60.414&lt;/td&gt; &lt;th&gt; Durbin-Watson: &lt;/th&gt; &lt;td&gt; 2.084&lt;/td&gt; Prob(Omnibus): 0.000 Jarque-Bera (JB): 151.241 Skew: &lt;td&gt;-1.327&lt;/td&gt; &lt;th&gt; Prob(JB): &lt;/th&gt; &lt;td&gt;1.44e-33&lt;/td&gt; Kurtosis: &lt;td&gt; 6.332&lt;/td&gt; &lt;th&gt; Cond. No. &lt;/th&gt; &lt;td&gt; 454.&lt;/td&gt; 60.2.3 Table 3.5 - Correlation Matrix advertising.corr() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TV Radio Newspaper Sales TV 1.000000 0.054809 0.056648 0.782224 Radio 0.054809 1.000000 0.354104 0.576223 Newspaper 0.056648 0.354104 1.000000 0.228299 Sales 0.782224 0.576223 0.228299 1.000000 60.2.4 Figure 3.5 - Multiple Linear Regression regr = skl_lm.LinearRegression() X = advertising[[&#39;Radio&#39;, &#39;TV&#39;]].as_matrix() y = advertising.Sales regr.fit(X,y) print(regr.coef_) print(regr.intercept_) [ 0.18799423 0.04575482] 2.92109991241 # What are the min/max values of Radio &amp; TV? # Use these values to set up the grid for plotting. advertising[[&#39;Radio&#39;, &#39;TV&#39;]].describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Radio TV count 200.000000 200.000000 mean 23.264000 147.042500 std 14.846809 85.854236 min 0.000000 0.700000 25% 9.975000 74.375000 50% 22.900000 149.750000 75% 36.525000 218.825000 max 49.600000 296.400000 # Create a coordinate grid Radio = np.arange(0,50) TV = np.arange(0,300) B1, B2 = np.meshgrid(Radio, TV, indexing=&#39;xy&#39;) Z = np.zeros((TV.size, Radio.size)) for (i,j),v in np.ndenumerate(Z): Z[i,j] =(regr.intercept_ + B1[i,j]*regr.coef_[0] + B2[i,j]*regr.coef_[1]) # Create plot fig = plt.figure(figsize=(10,6)) fig.suptitle(&#39;Regression: Sales ~ Radio + TV Advertising&#39;, fontsize=20) ax = axes3d.Axes3D(fig) ax.plot_surface(B1, B2, Z, rstride=10, cstride=5, alpha=0.4) ax.scatter3D(advertising.Radio, advertising.TV, advertising.Sales, c=&#39;r&#39;) ax.set_xlabel(&#39;Radio&#39;) ax.set_xlim(0,50) ax.set_ylabel(&#39;TV&#39;) ax.set_ylim(ymin=0) ax.set_zlabel(&#39;Sales&#39;); png 60.3 3.3 Other Considerations in the Regression Model 60.3.1 Figure 3.6 sns.pairplot(credit[[&#39;Balance&#39;,&#39;Age&#39;,&#39;Cards&#39;,&#39;Education&#39;,&#39;Income&#39;,&#39;Limit&#39;,&#39;Rating&#39;]]); png 60.3.2 Table 3.7 est = smf.ols(&#39;Balance ~ Gender&#39;, credit).fit() est.summary().tables[1] coef std err t P&gt;|t| [0.025 0.975] Intercept &lt;td&gt; 509.8031&lt;/td&gt; &lt;td&gt; 33.128&lt;/td&gt; &lt;td&gt; 15.389&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 444.675&lt;/td&gt; &lt;td&gt; 574.931&lt;/td&gt; Gender[T.Female] 19.7331 46.051 0.429 0.669 -70.801 110.267 60.3.3 Table 3.8 est = smf.ols(&#39;Balance ~ Ethnicity&#39;, credit).fit() est.summary().tables[1] coef std err t P&gt;|t| [0.025 0.975] Intercept &lt;td&gt; 531.0000&lt;/td&gt; &lt;td&gt; 46.319&lt;/td&gt; &lt;td&gt; 11.464&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 439.939&lt;/td&gt; &lt;td&gt; 622.061&lt;/td&gt; Ethnicity[T.Asian] -18.6863 65.021 -0.287 0.774 -146.515 109.142 Ethnicity[T.Caucasian] -12.5025 56.681 -0.221 0.826 -123.935 98.930 60.3.4 Table 3.9 - Interaction Variables est = smf.ols(&#39;Sales ~ TV + Radio + TV*Radio&#39;, advertising).fit() est.summary().tables[1] coef std err t P&gt;|t| [0.025 0.975] Intercept 6.7502 0.248 27.233 0.000 6.261 7.239 TV &lt;td&gt; 0.0191&lt;/td&gt; &lt;td&gt; 0.002&lt;/td&gt; &lt;td&gt; 12.699&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 0.016&lt;/td&gt; &lt;td&gt; 0.022&lt;/td&gt; Radio 0.0289 0.009 3.241 0.001 0.011 0.046 TV:Radio 0.0011 5.24e-05 20.727 0.000 0.001 0.001 60.3.5 Figure 3.7 - Interaction between qualitative and quantative variables est1 = smf.ols(&#39;Balance ~ Income + Student2&#39;, credit).fit() regr1 = est1.params est2 = smf.ols(&#39;Balance ~ Income + Income*Student2&#39;, credit).fit() regr2 = est2.params print(&#39;Regression 1 - without interaction term&#39;) print(regr1) print(&#39;\\nRegression 2 - with interaction term&#39;) print(regr2) Regression 1 - without interaction term Intercept 211.142964 Income 5.984336 Student2 382.670539 dtype: float64 Regression 2 - with interaction term Intercept 200.623153 Income 6.218169 Student2 476.675843 Income:Student2 -1.999151 dtype: float64 # Income (x-axis) income = np.linspace(0,150) # Balance without interaction term (y-axis) student1 = np.linspace(regr1[&#39;Intercept&#39;]+regr1[&#39;Student2&#39;], regr1[&#39;Intercept&#39;]+regr1[&#39;Student2&#39;]+150*regr1[&#39;Income&#39;]) non_student1 = np.linspace(regr1[&#39;Intercept&#39;], regr1[&#39;Intercept&#39;]+150*regr1[&#39;Income&#39;]) # Balance with iteraction term (y-axis) student2 = np.linspace(regr2[&#39;Intercept&#39;]+regr2[&#39;Student2&#39;], regr2[&#39;Intercept&#39;]+regr2[&#39;Student2&#39;]+ 150*(regr2[&#39;Income&#39;]+regr2[&#39;Income:Student2&#39;])) non_student2 = np.linspace(regr2[&#39;Intercept&#39;], regr2[&#39;Intercept&#39;]+150*regr2[&#39;Income&#39;]) # Create plot fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) ax1.plot(income, student1, &#39;r&#39;, income, non_student1, &#39;k&#39;) ax2.plot(income, student2, &#39;r&#39;, income, non_student2, &#39;k&#39;) for ax in fig.axes: ax.legend([&#39;student&#39;, &#39;non-student&#39;], loc=2) ax.set_xlabel(&#39;Income&#39;) ax.set_ylabel(&#39;Balance&#39;) ax.set_ylim(ymax=1550) png 60.3.6 Figure 3.8 - Non-linear relationships # With Seaborn&#39;s regplot() you can easily plot higher order polynomials. plt.scatter(auto.horsepower, auto.mpg, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.5) sns.regplot(auto.horsepower, auto.mpg, ci=None, label=&#39;Linear&#39;, scatter=False, color=&#39;orange&#39;) sns.regplot(auto.horsepower, auto.mpg, ci=None, label=&#39;Degree 2&#39;, order=2, scatter=False, color=&#39;lightblue&#39;) sns.regplot(auto.horsepower, auto.mpg, ci=None, label=&#39;Degree 5&#39;, order=5, scatter=False, color=&#39;g&#39;) plt.legend() plt.ylim(5,55) plt.xlim(40,240); png 60.3.7 Table 3.10 auto[&#39;horsepower2&#39;] = auto.horsepower**2 auto.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mpg cylinders displacement horsepower weight acceleration year origin name horsepower2 0 18.0 8 307.0 130.0 3504 12.0 70 1 chevrolet chevelle malibu 16900.0 1 15.0 8 350.0 165.0 3693 11.5 70 1 buick skylark 320 27225.0 2 18.0 8 318.0 150.0 3436 11.0 70 1 plymouth satellite 22500.0 est = smf.ols(&#39;mpg ~ horsepower + horsepower2&#39;, auto).fit() est.summary().tables[1] coef std err t P&gt;|t| [0.025 0.975] Intercept 56.9001 1.800 31.604 0.000 53.360 60.440 horsepower -0.4662 0.031 -14.978 0.000 -0.527 -0.405 horsepower2 0.0012 0.000 10.080 0.000 0.001 0.001 60.3.8 Figure 3.9 regr = skl_lm.LinearRegression() # Linear fit X = auto.horsepower.values.reshape(-1,1) y = auto.mpg regr.fit(X, y) auto[&#39;pred1&#39;] = regr.predict(X) auto[&#39;resid1&#39;] = auto.mpg - auto.pred1 # Quadratic fit X2 = auto[[&#39;horsepower&#39;, &#39;horsepower2&#39;]].as_matrix() regr.fit(X2, y) auto[&#39;pred2&#39;] = regr.predict(X2) auto[&#39;resid2&#39;] = auto.mpg - auto.pred2 fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) # Left plot sns.regplot(auto.pred1, auto.resid1, lowess=True, ax=ax1, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;lw&#39;:1}, scatter_kws={&#39;facecolors&#39;:&#39;None&#39;, &#39;edgecolors&#39;:&#39;k&#39;, &#39;alpha&#39;:0.5}) ax1.hlines(0,xmin=ax1.xaxis.get_data_interval()[0], xmax=ax1.xaxis.get_data_interval()[1], linestyles=&#39;dotted&#39;) ax1.set_title(&#39;Residual Plot for Linear Fit&#39;) # Right plot sns.regplot(auto.pred2, auto.resid2, lowess=True, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;lw&#39;:1}, ax=ax2, scatter_kws={&#39;facecolors&#39;:&#39;None&#39;, &#39;edgecolors&#39;:&#39;k&#39;, &#39;alpha&#39;:0.5}) ax2.hlines(0,xmin=ax2.xaxis.get_data_interval()[0], xmax=ax2.xaxis.get_data_interval()[1], linestyles=&#39;dotted&#39;) ax2.set_title(&#39;Residual Plot for Quadratic Fit&#39;) for ax in fig.axes: ax.set_xlabel(&#39;Fitted values&#39;) ax.set_ylabel(&#39;Residuals&#39;) png 60.3.9 Figure 3.14 fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) # Left plot ax1.scatter(credit.Limit, credit.Age, facecolor=&#39;None&#39;, edgecolor=&#39;r&#39;) ax1.set_ylabel(&#39;Age&#39;) # Right plot ax2.scatter(credit.Limit, credit.Rating, facecolor=&#39;None&#39;, edgecolor=&#39;r&#39;) ax2.set_ylabel(&#39;Rating&#39;) for ax in fig.axes: ax.set_xlabel(&#39;Limit&#39;) ax.set_xticks([2000,4000,6000,8000,12000]) png 60.3.10 Figure 3.15 y = credit.Balance # Regression for left plot X = credit[[&#39;Age&#39;, &#39;Limit&#39;]].as_matrix() regr1 = skl_lm.LinearRegression() regr1.fit(scale(X.astype(&#39;float&#39;), with_std=False), y) print(&#39;Age/Limit\\n&#39;,regr1.intercept_) print(regr1.coef_) # Regression for right plot X2 = credit[[&#39;Rating&#39;, &#39;Limit&#39;]].as_matrix() regr2 = skl_lm.LinearRegression() regr2.fit(scale(X2.astype(&#39;float&#39;), with_std=False), y) print(&#39;\\nRating/Limit\\n&#39;,regr2.intercept_) print(regr2.coef_) Age/Limit 520.015 [-2.29148553 0.17336497] Rating/Limit 520.015 [ 2.20167217 0.02451438] # Create grid coordinates for plotting B_Age = np.linspace(regr1.coef_[0]-3, regr1.coef_[0]+3, 100) B_Limit = np.linspace(regr1.coef_[1]-0.02, regr1.coef_[1]+0.02, 100) B_Rating = np.linspace(regr2.coef_[0]-3, regr2.coef_[0]+3, 100) B_Limit2 = np.linspace(regr2.coef_[1]-0.2, regr2.coef_[1]+0.2, 100) X1, Y1 = np.meshgrid(B_Limit, B_Age, indexing=&#39;xy&#39;) X2, Y2 = np.meshgrid(B_Limit2, B_Rating, indexing=&#39;xy&#39;) Z1 = np.zeros((B_Age.size,B_Limit.size)) Z2 = np.zeros((B_Rating.size,B_Limit2.size)) Limit_scaled = scale(credit.Limit.astype(&#39;float&#39;), with_std=False) Age_scaled = scale(credit.Age.astype(&#39;float&#39;), with_std=False) Rating_scaled = scale(credit.Rating.astype(&#39;float&#39;), with_std=False) # Calculate Z-values (RSS) based on grid of coefficients for (i,j),v in np.ndenumerate(Z1): Z1[i,j] =((y - (regr1.intercept_ + X1[i,j]*Limit_scaled + Y1[i,j]*Age_scaled))**2).sum()/1000000 for (i,j),v in np.ndenumerate(Z2): Z2[i,j] =((y - (regr2.intercept_ + X2[i,j]*Limit_scaled + Y2[i,j]*Rating_scaled))**2).sum()/1000000 fig = plt.figure(figsize=(12,5)) fig.suptitle(&#39;RSS - Regression coefficients&#39;, fontsize=20) ax1 = fig.add_subplot(121) ax2 = fig.add_subplot(122) min_RSS = r&#39;$\\beta_0$, $\\beta_1$ for minimized RSS&#39; # Left plot CS = ax1.contour(X1, Y1, Z1, cmap=plt.cm.Set1, levels=[21.25, 21.5, 21.8]) ax1.scatter(regr1.coef_[1], regr1.coef_[0], c=&#39;r&#39;, label=min_RSS) ax1.clabel(CS, inline=True, fontsize=10, fmt=&#39;%1.1f&#39;) ax1.set_ylabel(r&#39;$\\beta_{Age}$&#39;, fontsize=17) # Right plot CS = ax2.contour(X2, Y2, Z2, cmap=plt.cm.Set1, levels=[21.5, 21.8]) ax2.scatter(regr2.coef_[1], regr2.coef_[0], c=&#39;r&#39;, label=min_RSS) ax2.clabel(CS, inline=True, fontsize=10, fmt=&#39;%1.1f&#39;) ax2.set_ylabel(r&#39;$\\beta_{Rating}$&#39;, fontsize=17) ax2.set_xticks([-0.1, 0, 0.1, 0.2]) for ax in fig.axes: ax.set_xlabel(r&#39;$\\beta_{Limit}$&#39;, fontsize=17) ax.legend() png 60.3.11 Variance Inflation Factor - page 102 est_Age = smf.ols(&#39;Age ~ Rating + Limit&#39;, credit).fit() est_Rating = smf.ols(&#39;Rating ~ Age + Limit&#39;, credit).fit() est_Limit = smf.ols(&#39;Limit ~ Age + Rating&#39;, credit).fit() print(1/(1-est_Age.rsquared)) print(1/(1-est_Rating.rsquared)) print(1/(1-est_Limit.rsquared)) 1.01138468607 160.668300959 160.592879786 Table of Contents 3 Linear Regression 3.1 Simple Linear Regression 3.1.1 Estimating the Coefficients 3.1.2 Assessing the Accuracy of the Coefficent Estimates 3.1.3 Assessing the Accuracy of the Model 3.1.4 Residual Standard Errors 3.1.5 \\(R^2\\) Statistic 3.2 Multiple Linear Regression 3.2.1 Estimating the Regression Coefficients 3.2.2 Important Questions 3.2.2.1 Is There a Relationship Between the Response and Predictors? 3.2.2.2 Deciding on Important Variables 3.2.2.3 Model Fit 3.2.2.4 Predictions 3.3 Other Considerations In the Regression Model 3.3.1 Qualitative Predictors 3.3.2 Extensions of the Linear Model 3.3.2.1 Removing the Additive Assumption 3.3.2.2 Non-linear Relationships 3.3.3 Potential Problems 3.3.3.1 Non-linearity of the Data 3.3.3.2 Correlation of Error Terms 3.3.3.3 Non-constant Variance of Error Terms 3.3.3.4 Outliers 3.3.3.5 High Leverage Points 3.3.3.6 Collinearity 3.4 The Marketing Plan 3.5 Comparison of Linear Regression and K-Nearest Neighbors 3.6 Footnotes "],["linear-regression.html", "Chapter 61 Linear Regression 61.1 Simple Linear Regression 61.2 Multiple Linear Regression 61.3 Other Considerations In the Regression Model 61.4 The Marketing Plan 61.5 Comparison of Linear Regression and K-Nearest Neighbors 61.6 Footnotes", " Chapter 61 Linear Regression 61.1 Simple Linear Regression For data (X, Y), \\(X, Y\\in\\mathbb{R}\\), simple linear regression models \\(Y\\) as a linear function of \\(X\\) \\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\] and predicts \\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X\\] where \\(\\hat{\\beta}_i\\) is the estimate for \\(\\beta_i\\). 61.1.1 Estimating the Coefficients Estimates of the coefficients \\(\\beta_0, \\beta_1\\) arize from minimizing residual sum of squares \\[RSS = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\] using calculus one finds estimates7 \\[\\begin{align*} \\hat{\\beta}_1 &amp;= \\frac{\\sum_i (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_i (x_i - \\overline{x}) ^2}\\\\ \\hat{\\beta}_0 &amp;= \\overline{y}-\\hat{\\beta}_1\\overline{x} \\end{align*}\\] These are sometimes called the least squares estimates. 61.1.2 Assessing the Accuracy of the Coefficent Estimates The population regression line8 is the line given by \\[ Y = \\beta_0 + \\beta_1 X \\] and the least squares regression line is the line given by \\[ Y = \\hat{\\beta}_0 + \\hat{\\beta}_1 X \\] The least squares estimate is an unbiased estimator 9 Assuming errors \\(\\epsilon_i\\) are uncorrelated with common variance \\(\\sigma^2=\\mathbb{V}(\\epsilon)\\), the standard errors of \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\) are \\[ \\mathbf{se}(\\hat{\\beta}_0) = \\sigma\\sqrt{\\left[\\frac{1}{n} + \\frac{\\overline{x}}{\\sum_i (x_i - \\overline{x})^2}\\right]} \\] \\[ \\mathbf{se}(\\hat{\\beta}_1) = \\sigma\\sqrt{\\frac{1}{\\sum_i (x_i - \\overline{x})^2}} \\] The estimated standard errors \\(\\hat{\\mathbf{se}}(\\hat{\\beta}_0), \\hat{\\mathbf{se}}(\\hat{\\beta}_0)\\) are found by estimating \\(\\sigma\\) with the residual standard error 10 \\[ \\hat{\\sigma} = RSE := \\sqrt{\\frac{RSS}{n-2}} \\] Approximate \\(1 - \\alpha\\) confidence intervals 11 for the least squares estimators are \\[ \\hat{\\beta_i} \\pm t_{\\alpha/2}\\hat{\\mathbf{se}}(\\hat{\\beta}_i) \\] Most common hypothesis tests for the least squares estimates are \\[H_0: \\beta_i = 0\\] \\[H_a: \\beta_i \\neq 0\\] the rejection region is \\[\\{ x\\in \\mathbb{R}\\ |\\ T &gt; t \\}\\] where \\(t\\) is the test-statistic 12 \\[ t = \\frac{\\hat{\\beta}_i - \\beta_i}{\\hat{\\mathbf{se}}(\\hat{\\beta_i})} \\] 61.1.3 Assessing the Accuracy of the Model Quality of fit (model accuracy) is commonly assessed using \\(RSE\\) and the \\(R^2\\) statistic. 61.1.4 Residual Standard Errors The RSE is a measure of the overall difference between the observed responses \\(y_i\\) and the predicted responses \\(\\hat{y}_i\\). Thus it provides a measure of lack-of-fit of the model – higher RSE indicates worse fit. RSE is measured in units of \\(Y\\) so it provides an absolute measure of lack of fit, which is sometimes difficult to interpret 61.1.5 \\(R^2\\) Statistic The \\(R^2\\) statistic is \\[ R^2 = \\frac{TSS - RSS}{TSS}\\] where \\(TSS = \\sum_i (y_i - \\overline{y})^2\\) is the total sum of squares. \\(TSS\\) measures the total variability in \\(Y\\), while \\(RSS\\) measures the variability left after modeling \\(Y\\) by \\(f(X)\\). Thus, \\(R^2\\) measures the proportion of variability in \\(Y\\) that can be explained by the model. \\(R^2\\) is dimensionless so it provides a good relative measure of lack-of-fit. As \\(R^2 \\rightarrow 1\\), the model explains more of the variability in \\(Y\\). As \\(R^2 \\rightarrow 0\\), the model explains less 13. What constitutes a good \\(R^2\\) value depends on context We can also think of \\(R^2\\) as a measure of the linear relationship between \\(Y\\) and \\(X\\). Another such measure is the correlation \\(\\text{corr}(X,Y)\\), which is estimated by the sample correlation \\(r\\). In the case of simple linear regression, \\(R^2 = r^2\\). 61.2 Multiple Linear Regression For data (X, Y), \\(X=(X_1,\\dots,X_p)^T\\in\\mathbb{R}^p\\),\\(Y\\in\\mathbb{R}\\), multiple linear regression models \\(Y\\) as a linear function 14 of \\(X\\) \\[Y = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p + \\epsilon\\] and predicts \\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\cdots + \\hat{\\beta}_p X_p + \\epsilon \\] where \\(\\hat{\\beta}_i\\) is the estimate of \\(\\beta_i\\) If we form the \\(n \\times (p + 1)\\) matrix \\(\\mathbf{X}\\) with rows \\((1, X_{i1}, \\dots, X_{ip})\\), response vector \\(Y=(Y_1,\\dots,Y_n)\\), parameter vector \\(\\beta = (\\beta_0, \\dots, \\beta_p)\\) and noise vector \\(\\epsilon = (\\epsilon_1, \\dots, \\epsilon_n)\\) then the model can be written in matrix form \\[ Y = \\mathbf{X}\\beta + \\epsilon \\] 61.2.1 Estimating the Regression Coefficients RSS is defined and estimates \\(\\hat{\\beta}_i\\) for the parameters \\(\\beta_i\\) are chosen to minimize RSS 15 as in the case of simple regression. If the data matrix \\(\\mathbf{X}\\) has full rank, then the estimate 16 \\(\\hat{\\beta}\\) for the parameter vector is \\[ \\hat{\\beta} = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\beta \\] 61.2.2 Important Questions 61.2.2.1 Is There a Relationship Between the Response and Predictors? One way to answer this question is a hypothesis test \\[\\begin{align*} H_0:&amp; \\beta_i = 0 &amp;\\text{for all}\\ 1 \\leqslant i \\leqslant p\\\\ H_a:&amp; \\beta_i \\neq 0&amp;\\text{for some}\\ 1 \\leqslant i \\leqslant p \\end{align*}\\] The test statistic is the \\(F\\)-statistic17 \\[ F = \\frac{(TSS - RSS)/p}{RSS/(n - p - 1)} \\] where \\(TSS, RSS\\) are defined as in simple linear regression. Assuming the model is correct, \\[ \\mathbb{E}\\left(\\frac{RSS}{n-p-1}\\right) = \\sigma^2 \\] where again, \\(\\sigma^2 = \\mathbb{V}(\\epsilon)\\). Further assuming \\(H_0\\) is true, \\[ \\mathbb{E}\\left(\\frac{TSS - TSS}{p}\\right) = \\sigma^2 \\] hence \\(H_0 \\Rightarrow F \\approx 1\\) and \\(H_a \\Rightarrow F &gt; 1\\)18. Another way to answer this question is a hypothesis test on a subset of the predictors of size \\(q\\) \\[\\begin{align*} H_0:&amp; \\beta_{i} = 0 &amp;\\text{for all}\\ p - q + 1 \\leqslant i \\leqslant p\\\\ H_a:&amp; \\beta_i \\neq 0 &amp;\\text{for some}\\ p - q + 1 \\leqslant i \\leqslant p \\end{align*}\\] where \\(RSS_0\\) is the residual sum of squares for a second model ommitting the last \\(q\\) predictors. The \\(F\\)-statistic is \\[ F = \\frac{(RSS_0 - RSS)/p}{RSS/(n - p - 1)} \\] These hypothesis tests help us conclude that at least one of the predictors is related to the response (the second test narrows it down a bit), but don’t indicate which ones. 61.2.2.2 Deciding on Important Variables The task of finding which predictors are related to the response is sometimes known as variable selection.19 Various statistics can be used to judge the quality of models using different subsets of the predictors. Examples are Mallows \\(C_p\\) criterion, Akaike Information Criterion (AIC), Bayesian Information Criterion and adjusted \\(R^2\\). Since the number of distinct linear regression models grows exponentially with \\(p\\) exhaustive search is infeasible unless \\(p\\) is small. Common approaches to consider a smaller set of possible models are Forward Selection Start with the null model \\(M_0\\) (an intercept but no predictors). Fit \\(p\\) simple regressions and add to the null model the one with lowest \\(RSS\\), resulting in a new model \\(M_1\\). Iterate until a stopping rule is reached. Backward Selection Start with a model \\(M_p\\) consisting of all predictors. Remove the variable with largest \\(p\\)-value, resulting in a new model \\(M_{p-1}\\). Iterate until a stopping rule is reached. Mixed Selection Proceed with forward selection, but remove any predictors whose \\(p\\)-value is too large. 61.2.2.3 Model Fit As in simple regression, \\(RSE\\) and \\(R^2\\) are two common measures of model fit In multiple regression, \\(R^2 = Corr(Y, \\hat{Y})^2\\), with the same interpretation as in simple regression. The model \\(\\hat{Y}\\) maximizes \\(R^2\\) among all linear models. \\(R^2\\) increases monotonically in the number of predictors, but small increases indicate the low relative value of the corresponding predictor. In multiple regression \\[ RSS = \\sqrt{\\frac{RSS}{n - p - 1}} \\] Visualization can be helpful in assessing model fit, e.g. by suggesting the inclusion of interaction terms 61.2.2.4 Predictions There are 3 types of uncertainty associated with predicting \\(Y\\) by \\(\\hat{Y}\\) Estimation Error. \\(\\hat{Y} = \\hat{f}(X)\\) is only an estimate \\(f(X)\\). This error is reducible. We can compute confidence intervals to quantify it. Model Bias. A linear form for \\(f(X)\\) may be inappropriate. This error is also reducible Noise. The noise term \\(\\epsilon\\) is a random variable. This error is irreducible. We can compute predeiction intervals to quantify it. 61.3 Other Considerations In the Regression Model 61.3.1 Qualitative Predictors If the \\(i\\)-th predictor \\(X_i\\) is a factor (qualitative) with \\(K\\) levels (that is \\(K\\) possible values) then we model it by \\(K-1\\) indicator variables (sometimes called a dummy variables). Two commons definitions of the dummy variables are \\[ \\tilde{X}_{i} = \\begin{cases} 1 &amp; X_i = k\\\\ 0 &amp; X_i \\neq k \\end{cases}\\] \\[ \\tilde{X}_{i} = \\begin{cases} 1 &amp; X_i = k\\\\ -1 &amp; X_i \\neq k \\end{cases}\\] for \\(1 \\leqslant k \\leqslant K\\). The corresponding regression model is \\[ Y = \\beta_0 + \\sum_{i} \\beta_i\\tilde{X}_i + \\epsilon \\] since we can only have \\(\\tilde{X}_i = 1\\) if \\(\\tilde{X}_j \\neq 1\\) for \\(j \\neq i\\), this model can be seen as \\(K\\) distinct models \\[ Y = \\begin{cases} \\beta_0 &amp; X_i = 1 \\\\ \\beta_0 + \\beta_1 &amp; X_i = 2 \\\\ \\vdots &amp; \\vdots \\\\ \\beta_0 + \\beta_K &amp; X_i = K \\end{cases} \\] 61.3.2 Extensions of the Linear Model The standard linear regression we have been discussing relies on the twin assumptions Additivity: The effect of \\(X_i\\) on \\(Y\\) is independent of the effect of \\(X_j\\) for \\(j\\neq i\\). Linearity: \\(Y\\) is linear in \\(X_i\\) for all \\(i\\). We can extend the model by relaxing these assumptions 61.3.2.1 Removing the Additive Assumption Dropping the assumption of additivity leads to the possible inclusion of interaction or synergy effects among predictors. One way to model an interaction effect between predictors \\(X_i\\) and \\(X_j\\) is to include an interaction term, \\(\\beta_{i + j}X_iX_j\\). The non-interaction terms \\(\\beta_i X_i\\) model the main effects. We can perform hypothesis tests as in the standard linear model to select important terms/variables. However, the hierarchical principle dictates that, if we include an interaction effect, we should include the corresponding main effects, even if the latter aren’t statistically significant. 61.3.2.2 Non-linear Relationships Dropping the assumption of linearity leads to the possible includion of non-linear effects. One common way to model non-linearity is to use polynomial regression 20, that is model \\(f(X)\\) with a polynomial in the predictors. For example in the case of a single predictor \\(X\\) \\[Y = \\beta_0 + \\beta_1 X + \\dots + \\beta_d X^s \\] models \\(Y\\) as a degree \\(d\\) polynomial in \\(X\\) In general one can model a non-linear effect of predictors \\(X_i\\) by including a non-linear function of the \\(X_i\\) in the model 61.3.3 Potential Problems 61.3.3.1 Non-linearity of the Data Residual plots are a useful way of vizualizing non-linearity. The presence of a discernible pattern may indicate a problem with the linearity of the model. 61.3.3.2 Correlation of Error Terms Standard linear regression assumes \\(\\text{Corr}(\\epsilon_i,\\epsilon_j) = 0\\) for \\(i\\neq j\\). Correlated error terms frequently occur in the context of time series. Positively correlated error terms may display tracking behavior (adjacent residuals may have similar values). 61.3.3.3 Non-constant Variance of Error Terms Standard linear regression assumes the variance of errors is constant across observations, i.e. \\(\\mathbb{V}(\\epsilon_i) = \\sigma^2\\) for all \\(1 \\leqslant i \\leqslant n\\) Hetereoscedasticity, or variance which changes across observations can be identified by a funnel shape in the residual plot. One way to reduce hetereoscedasticity is to transform \\(Y\\) by a concave function such as \\(\\log Y\\) or \\(\\sqrt{Y}\\). Another way to do this is weighted least squares. This weights terms in \\(RSS\\) with weights \\(w_i\\) inversely proportional to \\(\\sigma_i^2\\) where \\(\\sigma_i^2 = \\mathbb{V}(\\epsilon_i)\\). 61.3.3.4 Outliers An outlier is an observation for which the value of \\(y_i\\) given \\(x_i\\) is unusual, i.e. such that the squared-error \\((y_i - \\hat{y}_i)^2\\) is large Outliers can have disproportionate effects on statistics e.g. \\(R^2\\), which in turn affect the entire analysis (e.g. confidence intervals, hypothesis tests). Residual plots can identify outliers. In practice, we plot studentized residuals \\[\\frac{\\hat{\\epsilon}_i}{\\hat{\\mathbf{se}}(\\hat{\\epsilon}_i)} \\] If an outlier is due to a data collection error it can be removed, but great care should be taken when doing this. 61.3.3.5 High Leverage Points A high leverage point is a point with an unusual value of \\(x_i\\). High leverage points tend to have a sizable impact on \\(\\hat{f}\\). To quantify the leverage of \\(x_i\\), we use the leverage statistic. In simple linear regression this is \\[ h_i = \\frac{1}{n} + \\frac{(X_j - \\overline{X})^2}{\\sum_{j} (X_{j} - \\overline{X})^2} \\] 61.3.3.6 Collinearity Collinearity is a linear relationship among two or more predictors. Collinearity reduces the accuracy of coefficient estimates 21 Collinearity reduces the power22 of the hypothesis test Collinearity between two variables can be detected by the sample correlation matrix \\(\\hat{\\Sigma}\\). A high value for \\[|(\\hat{\\Sigma})_{ij}| = |\\hat{\\text{corr}(X_i, X_j)}|\\] indicates high correlation between \\(X_i, X_j\\) hence high collinearity in the data23. Multicollinearity is a linear relationship among more than two predictors. Multicollinearity can be detected using the variance inflation factor (VIF)24. \\[ VIF(\\hat{\\beta}_i) = \\frac{1}{1-R^2_{X_i|X_{-i}}}\\] where \\(R^2_{X_i|X_{-i}}\\) is the \\(R^2\\) from regression of \\(X_i\\) onto all other predictors. One solution to the presence of collinearity is to drop one of the problematic variables, which is usually not an issue, since correlation among variables is seen as redundant. Another solution is to combine the problematic variables into a single predictor (e.g. an average) 61.4 The Marketing Plan Skip 61.5 Comparison of Linear Regression and K-Nearest Neighbors Linear regression is a parametric model for regression (with parameter \\(\\beta = (\\beta_0, \\dots, \\beta_p)\\)). KNN regression is a popular non-parametric model, which estimates \\[\\hat{f}(x_0) = \\frac{1}{K}\\sum_{x_i\\in\\mathcal{N}_0} y_i \\] In general, a parametric model will outperform a non-parametric model if the parametric estimation \\(\\hat{f}\\) is close to the true \\(f\\). KNN regression suffers from the curse of dimensionality - as the dimension increases the data become sparse. Effectively this is a reduction in sample size, hence KNN performance commonly decreases as the dimension \\(p\\) increases. In general parametric methods outperform non-parametric methods when there is a small number of observations per predictor. Even if performance of KNN and linear regression is comparable, the latter may be favored for interpretability. 61.6 Footnotes The value \\((\\hat{\\beta}_0, \\hat{\\beta}_1)\\) is the local minimum in \\(\\mathbb{R}^2\\) of the “loss function” given by RSS ↩︎ Here estimate means the same as “estimator,” found elsewhere in the statistics literature. The population regression line is given by the “true” (population) values \\((\\beta_0, \\beta_1)\\) of the parameter, while the least squares line is given by the estimator \\((\\hat{\\beta}_0, \\hat{\\beta}_1)\\) ↩︎ In other words, \\(\\mathbb{E}\\left((\\hat{\\beta}_0, \\hat{\\beta}_1)\\right) = (\\beta_0, \\beta_1)\\) ↩︎ The factor \\(\\frac{1}{n-2}\\) is a correction to make this an unbiased estimator, the quantity \\(n - 2\\) is known as the “degrees of freedom.” Note this is a special case of \\(n - p - 1\\) degrees of freedom for \\(p\\) predictors where \\(p = 1\\). ↩︎ This appears to be based on the assumption (no doubt proved in the literature) that the least squares estimators are asymptotically t-distributed, \\(\\hat{\\beta}_i \\approx Student_{n-2}(\\beta_i, \\hat{\\mathbf{se}}(\\hat{\\beta}_i))\\). ↩︎ This is the Wald test for the statistic \\(T\\), which (by footnote 4) has \\(T \\approx Student_{n - 2}(0, 1)\\). ↩︎ This can happen if either the model is wrong (i.e. a linear form for \\(f(X)\\) isn’t a good choice) or because \\(\\mathbb{V}(\\epsilon)\\) is large. ↩︎ This estimation method is known as Ordinary Least Squares (OLS). The estimate is the solution to the quadratic minimzation problem \\[ \\hat{\\beta} = \\underset{\\beta}{\\text{argmin}\\,} || y - \\mathbf{X}\\beta ||^2 \\] ↩︎ The estimate is any solution to the quadratic minimzation problem \\[ \\hat{\\beta} = \\underset{\\beta}{\\text{argmin}\\,} || y - \\mathbf{X}\\beta ||^2 \\] which can be found by solving the normal equations \\[\\mathbf{X}^\\top\\mathbf{X}\\hat{\\beta} = \\mathbf{X}^\\top y\\] ↩︎ If \\(\\mathbf{X}\\) has full rank then \\(\\mathbf{X}^\\top\\mathbf{X}\\) is invertible and the normal equations have a unique solution ↩︎ Assuming the \\(\\epsilon_i\\) are normally distributed, \\(\\epsilon_i \\sim N(\\mu_i, \\sigma^2)\\) where \\(\\mu = \\beta_0 + \\sum \\beta_i X_i\\)), the \\(F\\)-statistic has an \\(F\\)-distribution with \\(p, n-p\\) degrees of freedom (\\(F\\) has this asymptotic distribution even without the normality assumption). The use of the \\(F\\) statistic arises from ANOVA among the predictors, which is beyond our scope. There is some qualitative discussion of the motivation for the \\(F\\) statistic on page 77 of the text. It is an appropriate statistic in the case \\(p\\) i ↩︎ How much \\(F &gt; 1\\) should be before we rejct \\(H_0\\) depends on \\(n\\) and \\(p\\). If \\(n\\) is large, \\(F\\) need not be much greater than 1, and if it’s small, ↩︎ This is discussed extensively in chapter 6. ↩︎ This is discussed in chapter 7. ↩︎ This is due to issues identifying the global minimum of \\(RSS\\). In the example in the text, in the presence of collinearity, the global minimum is in a long “valley.” The coefficient estimates are very sensitive to the data – small changes in the data yeild large changes in the estimates. ↩︎ The power of the test is the probability of correctly rejecting \\(H_0: \\beta_i = 0\\), i.e. correctly accepting \\(H_a: \\beta_i \\neq 0\\). Since it uncreases uncertainty of the coefficient estimates, it increases \\(\\hat{se}(\\hat{\\beta_i})\\), hence reduces the \\(t\\)-statistic, making it less likely \\(H_0\\) is rejected. ↩︎ However, the converse is not true – absence of such entries in the sample correlation matrix doesn’t indicate absence of collinearity. The matrix only detects pairwise correlation, and a predictor may correlate two or more other predictors. ↩︎ This is defined the ratio of the (sample) variance of \\(\\hat{\\beta_i}\\) when fitting the full model divided by the variance of \\(\\hat{\\beta_i}\\) when fit on it’s own. It can be computed using the given formula. ↩︎ TEST HERE "],["chapter-4-classification.html", "Chapter 62 Chapter 4 - Classification 62.1 4.3 Logistic Regression 62.2 4.4 Linear Discriminant Analysis", " Chapter 62 Chapter 4 - Classification Load dataset The Default data set 4.3 Logistic Regression 4.4 Linear Discriminant Analysis Lab: 4.6.3 Linear Discriminant Analysis Lab: 4.6.4 Quadratic Discriminant Analysis Lab: 4.6.5 K-Nearest Neighbors Lab: 4.6.6 An Application to Caravan Insurance Data # %load ../standard_import.txt import pandas as pd import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns import sklearn.linear_model as skl_lm from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis from sklearn.metrics import confusion_matrix, classification_report, precision_score from sklearn import preprocessing from sklearn import neighbors import statsmodels.api as sm import statsmodels.formula.api as smf %matplotlib inline plt.style.use(&#39;seaborn-white&#39;) 62.0.1 Load dataset # In R, I exported the dataset from package &#39;ISLR&#39; to an Excel file df = pd.read_excel(&#39;Data/Default.xlsx&#39;) # Note: factorize() returns two objects: a label array and an array with the unique values. # We are only interested in the first object. df[&#39;default2&#39;] = df.default.factorize()[0] df[&#39;student2&#39;] = df.student.factorize()[0] df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } default student balance income default2 student2 1 No No 729.526495 44361.625074 0 0 2 No Yes 817.180407 12106.134700 0 1 3 No No 1073.549164 31767.138947 0 0 62.0.2 Figure 4.1 - Default data set fig = plt.figure(figsize=(12,5)) gs = mpl.gridspec.GridSpec(1, 4) ax1 = plt.subplot(gs[0,:-2]) ax2 = plt.subplot(gs[0,-2]) ax3 = plt.subplot(gs[0,-1]) # Take a fraction of the samples where target value (default) is &#39;no&#39; df_no = df[df.default2 == 0].sample(frac=0.15) # Take all samples where target value is &#39;yes&#39; df_yes = df[df.default2 == 1] df_ = df_no.append(df_yes) ax1.scatter(df_[df_.default == &#39;Yes&#39;].balance, df_[df_.default == &#39;Yes&#39;].income, s=40, c=&#39;orange&#39;, marker=&#39;+&#39;, linewidths=1) ax1.scatter(df_[df_.default == &#39;No&#39;].balance, df_[df_.default == &#39;No&#39;].income, s=40, marker=&#39;o&#39;, linewidths=&#39;1&#39;, edgecolors=&#39;lightblue&#39;, facecolors=&#39;white&#39;, alpha=.6) ax1.set_ylim(ymin=0) ax1.set_ylabel(&#39;Income&#39;) ax1.set_xlim(xmin=-100) ax1.set_xlabel(&#39;Balance&#39;) c_palette = {&#39;No&#39;:&#39;lightblue&#39;, &#39;Yes&#39;:&#39;orange&#39;} sns.boxplot(&#39;default&#39;, &#39;balance&#39;, data=df, orient=&#39;v&#39;, ax=ax2, palette=c_palette) sns.boxplot(&#39;default&#39;, &#39;income&#39;, data=df, orient=&#39;v&#39;, ax=ax3, palette=c_palette) gs.tight_layout(plt.gcf()) png 62.1 4.3 Logistic Regression 62.1.1 Figure 4.2 X_train = df.balance.values.reshape(-1,1) y = df.default2 # Create array of test data. Calculate the classification probability # and predicted classification. X_test = np.arange(df.balance.min(), df.balance.max()).reshape(-1,1) clf = skl_lm.LogisticRegression(solver=&#39;newton-cg&#39;) clf.fit(X_train,y) prob = clf.predict_proba(X_test) fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5)) # Left plot sns.regplot(df.balance, df.default2, order=1, ci=None, scatter_kws={&#39;color&#39;:&#39;orange&#39;}, line_kws={&#39;color&#39;:&#39;lightblue&#39;, &#39;lw&#39;:2}, ax=ax1) # Right plot ax2.scatter(X_train, y, color=&#39;orange&#39;) ax2.plot(X_test, prob[:,1], color=&#39;lightblue&#39;) for ax in fig.axes: ax.hlines(1, xmin=ax.xaxis.get_data_interval()[0], xmax=ax.xaxis.get_data_interval()[1], linestyles=&#39;dashed&#39;, lw=1) ax.hlines(0, xmin=ax.xaxis.get_data_interval()[0], xmax=ax.xaxis.get_data_interval()[1], linestyles=&#39;dashed&#39;, lw=1) ax.set_ylabel(&#39;Probability of default&#39;) ax.set_xlabel(&#39;Balance&#39;) ax.set_yticks([0, 0.25, 0.5, 0.75, 1.]) ax.set_xlim(xmin=-100) png 62.1.2 Table 4.1 y = df.default2 62.1.2.0.1 scikit-learn # Using newton-cg solver, the coefficients are equal/closest to the ones in the book. # I do not know the details on the differences between the solvers. clf = skl_lm.LogisticRegression(solver=&#39;newton-cg&#39;) X_train = df.balance.values.reshape(-1,1) clf.fit(X_train,y) print(clf) print(&#39;classes: &#39;,clf.classes_) print(&#39;coefficients: &#39;,clf.coef_) print(&#39;intercept :&#39;, clf.intercept_) LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class=&#39;ovr&#39;, n_jobs=1, penalty=&#39;l2&#39;, random_state=None, solver=&#39;newton-cg&#39;, tol=0.0001, verbose=0, warm_start=False) classes: [0 1] coefficients: [[ 0.00549891]] intercept : [-10.65131761] 62.1.2.0.2 statsmodels X_train = sm.add_constant(df.balance) est = smf.Logit(y.ravel(), X_train).fit() est.summary2().tables[1] Optimization terminated successfully. Current function value: 0.079823 Iterations 10 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Coef. Std.Err. z P&gt;|z| [0.025 0.975] const -10.651331 0.361169 -29.491287 3.723665e-191 -11.359208 -9.943453 balance 0.005499 0.000220 24.952404 2.010855e-137 0.005067 0.005931 62.1.3 Table 4.2 X_train = sm.add_constant(df.student2) y = df.default2 est = smf.Logit(y, X_train).fit() est.summary2().tables[1] Optimization terminated successfully. Current function value: 0.145434 Iterations 7 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Coef. Std.Err. z P&gt;|z| [0.025 0.975] const -3.504128 0.070713 -49.554094 0.000000 -3.642723 -3.365532 student2 0.404887 0.115019 3.520177 0.000431 0.179454 0.630320 62.1.4 Table 4.3 - Multiple Logistic Regression X_train = sm.add_constant(df[[&#39;balance&#39;, &#39;income&#39;, &#39;student2&#39;]]) est = smf.Logit(y, X_train).fit() est.summary2().tables[1] Optimization terminated successfully. Current function value: 0.078577 Iterations 10 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Coef. Std.Err. z P&gt;|z| [0.025 0.975] const -10.869045 0.492273 -22.079320 4.995499e-108 -11.833882 -9.904209 balance 0.005737 0.000232 24.736506 4.331521e-135 0.005282 0.006191 income 0.000003 0.000008 0.369808 7.115254e-01 -0.000013 0.000019 student2 -0.646776 0.236257 -2.737595 6.189022e-03 -1.109831 -0.183721 62.1.5 Figure 4.3 - Confounding # balance and default vectors for students X_train = df[df.student == &#39;Yes&#39;].balance.values.reshape(df[df.student == &#39;Yes&#39;].balance.size,1) y = df[df.student == &#39;Yes&#39;].default2 # balance and default vectors for non-students X_train2 = df[df.student == &#39;No&#39;].balance.values.reshape(df[df.student == &#39;No&#39;].balance.size,1) y2 = df[df.student == &#39;No&#39;].default2 # Vector with balance values for plotting X_test = np.arange(df.balance.min(), df.balance.max()).reshape(-1,1) clf = skl_lm.LogisticRegression(solver=&#39;newton-cg&#39;) clf2 = skl_lm.LogisticRegression(solver=&#39;newton-cg&#39;) clf.fit(X_train,y) clf2.fit(X_train2,y2) prob = clf.predict_proba(X_test) prob2 = clf2.predict_proba(X_test) df.groupby([&#39;student&#39;,&#39;default&#39;]).size().unstack(&#39;default&#39;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } default No Yes student No 6850 206 Yes 2817 127 # creating plot fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5)) # Left plot ax1.plot(X_test, pd.DataFrame(prob)[1], color=&#39;orange&#39;, label=&#39;Student&#39;) ax1.plot(X_test, pd.DataFrame(prob2)[1], color=&#39;lightblue&#39;, label=&#39;Non-student&#39;) ax1.hlines(127/2817, colors=&#39;orange&#39;, label=&#39;Overall Student&#39;, xmin=ax1.xaxis.get_data_interval()[0], xmax=ax1.xaxis.get_data_interval()[1], linestyles=&#39;dashed&#39;) ax1.hlines(206/6850, colors=&#39;lightblue&#39;, label=&#39;Overall Non-Student&#39;, xmin=ax1.xaxis.get_data_interval()[0], xmax=ax1.xaxis.get_data_interval()[1], linestyles=&#39;dashed&#39;) ax1.set_ylabel(&#39;Default Rate&#39;) ax1.set_xlabel(&#39;Credit Card Balance&#39;) ax1.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1.]) ax1.set_xlim(450,2500) ax1.legend(loc=2) # Right plot sns.boxplot(&#39;student&#39;, &#39;balance&#39;, data=df, orient=&#39;v&#39;, ax=ax2, palette=c_palette); png 62.2 4.4 Linear Discriminant Analysis 62.2.1 Table 4.4 X = df[[&#39;balance&#39;, &#39;income&#39;, &#39;student2&#39;]].as_matrix() y = df.default2.as_matrix() lda = LinearDiscriminantAnalysis(solver=&#39;svd&#39;) y_pred = lda.fit(X, y).predict(X) df_ = pd.DataFrame({&#39;True default status&#39;: y, &#39;Predicted default status&#39;: y_pred}) df_.replace(to_replace={0:&#39;No&#39;, 1:&#39;Yes&#39;}, inplace=True) df_.groupby([&#39;Predicted default status&#39;,&#39;True default status&#39;]).size().unstack(&#39;True default status&#39;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } True default status No Yes Predicted default status No 9645 254 Yes 22 79 print(classification_report(y, y_pred, target_names=[&#39;No&#39;, &#39;Yes&#39;])) precision recall f1-score support No 0.97 1.00 0.99 9667 Yes 0.78 0.24 0.36 333 avg / total 0.97 0.97 0.97 10000 62.2.2 Table 4.5 Instead of using the probability of 50% as decision boundary, we say that a probability of default of 20% is to be classified as ‘Yes.’ decision_prob = 0.2 y_prob = lda.fit(X, y).predict_proba(X) df_ = pd.DataFrame({&#39;True default status&#39;: y, &#39;Predicted default status&#39;: y_prob[:,1] &gt; decision_prob}) df_.replace(to_replace={0:&#39;No&#39;, 1:&#39;Yes&#39;, &#39;True&#39;:&#39;Yes&#39;, &#39;False&#39;:&#39;No&#39;}, inplace=True) df_.groupby([&#39;Predicted default status&#39;,&#39;True default status&#39;]).size().unstack(&#39;True default status&#39;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } True default status No Yes Predicted default status No 9435 140 Yes 232 193 "],["lab.html", "Chapter 63 Lab", " Chapter 63 Lab 63.0.1 4.6.3 Linear Discriminant Analysis df = pd.read_csv(&#39;Data/Smarket.csv&#39;, usecols=range(1,10), index_col=0, parse_dates=True) X_train = df[:&#39;2004&#39;][[&#39;Lag1&#39;,&#39;Lag2&#39;]] y_train = df[:&#39;2004&#39;][&#39;Direction&#39;] X_test = df[&#39;2005&#39;:][[&#39;Lag1&#39;,&#39;Lag2&#39;]] y_test = df[&#39;2005&#39;:][&#39;Direction&#39;] lda = LinearDiscriminantAnalysis() pred = lda.fit(X_train, y_train).predict(X_test) lda.priors_ array([ 0.49198397, 0.50801603]) lda.means_ array([[ 0.04279022, 0.03389409], [-0.03954635, -0.03132544]]) # These do not seem to correspond to the values from the R output in the book? lda.coef_ array([[-0.05544078, -0.0443452 ]]) confusion_matrix(y_test, pred).T array([[ 35, 35], [ 76, 106]]) print(classification_report(y_test, pred, digits=3)) precision recall f1-score support Down 0.500 0.315 0.387 111 Up 0.582 0.752 0.656 141 avg / total 0.546 0.560 0.538 252 pred_p = lda.predict_proba(X_test) np.unique(pred_p[:,1]&gt;0.5, return_counts=True) (array([False, True], dtype=bool), array([ 70, 182])) np.unique(pred_p[:,1]&gt;0.9, return_counts=True) (array([False], dtype=bool), array([252])) 63.0.2 4.6.4 Quadratic Discriminant Analysis qda = QuadraticDiscriminantAnalysis() pred = qda.fit(X_train, y_train).predict(X_test) qda.priors_ array([ 0.49198397, 0.50801603]) qda.means_ array([[ 0.04279022, 0.03389409], [-0.03954635, -0.03132544]]) confusion_matrix(y_test, pred).T array([[ 30, 20], [ 81, 121]]) print(classification_report(y_test, pred, digits=3)) precision recall f1-score support Down 0.600 0.270 0.373 111 Up 0.599 0.858 0.706 141 avg / total 0.599 0.599 0.559 252 63.0.3 4.6.5 K-Nearest Neighbors knn = neighbors.KNeighborsClassifier(n_neighbors=1) pred = knn.fit(X_train, y_train).predict(X_test) print(confusion_matrix(y_test, pred).T) print(classification_report(y_test, pred, digits=3)) [[43 58] [68 83]] precision recall f1-score support Down 0.426 0.387 0.406 111 Up 0.550 0.589 0.568 141 avg / total 0.495 0.500 0.497 252 knn = neighbors.KNeighborsClassifier(n_neighbors=3) pred = knn.fit(X_train, y_train).predict(X_test) print(confusion_matrix(y_test, pred).T) print(classification_report(y_test, pred, digits=3)) [[48 55] [63 86]] precision recall f1-score support Down 0.466 0.432 0.449 111 Up 0.577 0.610 0.593 141 avg / total 0.528 0.532 0.529 252 63.0.4 4.6.6 An Application to Caravan Insurance Data 63.0.4.1 K-Nearest Neighbors # In R, I exported the dataset from package &#39;ISLR&#39; to a csv file df = pd.read_csv(&#39;Data/Caravan.csv&#39;) y = df.Purchase X = df.drop(&#39;Purchase&#39;, axis=1).astype(&#39;float64&#39;) X_scaled = preprocessing.scale(X) X_train = X_scaled[1000:,:] y_train = y[1000:] X_test = X_scaled[:1000,:] y_test = y[:1000] def KNN(n_neighbors=1, weights=&#39;uniform&#39;): clf = neighbors.KNeighborsClassifier(n_neighbors, weights) clf.fit(X_train, y_train) pred = clf.predict(X_test) score = clf.score(X_test, y_test) return(pred, score, clf.classes_) def plot_confusion_matrix(cm, classes, n_neighbors, title=&#39;Confusion matrix (Normalized)&#39;, cmap=plt.cm.Blues): plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=plt.cm.Blues) plt.title(&#39;Normalized confusion matrix: KNN-{}&#39;.format(n_neighbors)) plt.colorbar() plt.xticks(np.arange(2), classes) plt.yticks(np.arange(2), classes) plt.tight_layout() plt.xlabel(&#39;True label&#39;,rotation=&#39;horizontal&#39;, ha=&#39;right&#39;) plt.ylabel(&#39;Predicted label&#39;) plt.show() for i in [1,3,5]: pred, score, classes = KNN(i) cm = confusion_matrix(y_test, pred) cm_normalized = cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis] plot_confusion_matrix(cm_normalized.T, classes, n_neighbors=i) cm_df = pd.DataFrame(cm.T, index=classes, columns=classes) cm_df.index.name = &#39;Predicted&#39; cm_df.columns.name = &#39;True&#39; print(cm_df) print(pd.DataFrame(precision_score(y_test, pred, average=None), index=classes, columns=[&#39;Precision&#39;])) png True No Yes Predicted No 882 48 Yes 59 11 Precision No 0.948387 Yes 0.157143 png True No Yes Predicted No 921 53 Yes 20 6 Precision No 0.945585 Yes 0.230769 png True No Yes Predicted No 934 55 Yes 7 4 Precision No 0.944388 Yes 0.363636 63.0.4.2 Logistic Regression regr = skl_lm.LogisticRegression() regr.fit(X_train, y_train) LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class=&#39;ovr&#39;, n_jobs=1, penalty=&#39;l2&#39;, random_state=None, solver=&#39;liblinear&#39;, tol=0.0001, verbose=0, warm_start=False) pred = regr.predict(X_test) cm_df = pd.DataFrame(confusion_matrix(y_test, pred).T, index=regr.classes_, columns=regr.classes_) cm_df.index.name = &#39;Predicted&#39; cm_df.columns.name = &#39;True&#39; print(cm_df) print(classification_report(y_test, pred)) True No Yes Predicted No 935 59 Yes 6 0 precision recall f1-score support No 0.94 0.99 0.97 941 Yes 0.00 0.00 0.00 59 avg / total 0.89 0.94 0.91 1000 pred_p = regr.predict_proba(X_test) cm_df = pd.DataFrame({&#39;True&#39;: y_test, &#39;Pred&#39;: pred_p[:,1] &gt; .25}) cm_df.Pred.replace(to_replace={True:&#39;Yes&#39;, False:&#39;No&#39;}, inplace=True) print(cm_df.groupby([&#39;True&#39;, &#39;Pred&#39;]).size().unstack(&#39;True&#39;).T) print(classification_report(y_test, cm_df.Pred)) Pred No Yes True No 919 22 Yes 48 11 precision recall f1-score support No 0.95 0.98 0.96 941 Yes 0.33 0.19 0.24 59 avg / total 0.91 0.93 0.92 1000 Table of Contents 4 Logistic Regression 4.1 An Overview of Classification 4.2 Why Not Linear Regression? 4.3 Logistic Regression 4.3.1 The Logistic Model 4.3.2 Estimating the Regression Coefficients 4.3.3 Making Predictions 4.3.4 Multiple Logistic Regression 4.3.5 Logistic Regression for more than two response classes 4.4 Linear Discriminant Analysis 4.4.1 Bayes Theorem for Classification 4.4.2 Linear Discriminant Analysis for p=1 -1\" data-toc-modified-id=\"Linear-Discriminant-Analysis-for-p->-1-4.4.3\">4.4.3 Linear Discriminant Analysis for p &gt; 1 4.4.4 Quadratic Discriminant Analysis 4.5 A Comparison of Classification Methods 4.6 Footnotes "],["logistic-regression-2.html", "Chapter 64 Logistic Regression 64.1 An Overview of Classification 64.2 Why Not Linear Regression? 64.3 Logistic Regression 64.4 Linear Discriminant Analysis 64.5 A Comparison of Classification Methods 64.6 Footnotes", " Chapter 64 Logistic Regression 64.1 An Overview of Classification In classification we consider paired data \\((\\mathbf{X}, Y)\\), where \\(Y\\) is a qualitative variable, that is, a finite random variable. The values \\(Y\\) takes are called classes 64.2 Why Not Linear Regression? Because a linear regression model implies an ordering on the values of the response and in general there is no natural ordering on the values of a qualitative variable 64.3 Logistic Regression 64.3.1 The Logistic Model Consider a quantitiative predictor \\(X\\) binary response variable \\(Y \\in {0,1}\\) We want to model the conditional probability of \\(Y=1\\) given \\(X\\) \\[ P(X) := P\\left(Y=1 | X\\right)\\] We model \\(P(X)\\) with the logistic function \\[ P(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}} \\] The logistic model can be considered a linear model for the log-odds or logit \\[\\log\\left(\\frac{P(X)}{1 - P(X)}\\right) = \\beta_0 + \\beta_1 X\\] 64.3.2 Estimating the Regression Coefficients The likelihood function for the logistic regression parameter \\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1)\\) is \\[\\begin{align*} \\ell(\\boldsymbol{\\beta}) &amp;= \\prod_{i = 1}^n p(x_i)\\\\ &amp;= \\prod_{i: y_i = 1}p(x_i) \\prod_{i: y_i = 0} (1 - p(x_i)) \\end{align*}\\] The maximum likelihood estimate (MLE) for the regression parameter is \\[ \\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\beta}\\in \\mathbb{R}^2}{\\text{argmax}\\,} \\ell(\\boldsymbol{\\beta})\\] There isn’t a closed form solution for \\(\\hat{\\boldsymbol{\\beta}}\\) so it must be found using numerical methods 64.3.3 Making Predictions The MLE \\(\\hat{\\boldsymbol{\\beta}}\\) results in an estimate for the conditional probability \\(\\hat{P}(X)\\) which can be used to predict the class \\(Y\\) 64.3.4 Multiple Logistic Regression Multiple logistic regression considers the case of multiple predictors \\(\\mathbf{X} = (X_1,\\dots, X_p)^\\top\\). If we write the predictors as \\(\\mathbf{X} = (1, X_1, \\dots, X_p)^\\top\\), and the parameter \\(\\boldsymbol(\\beta) = (\\beta_0, \\dots, \\beta_p)^\\top\\) then multiple logistic regression models \\[p(X) = \\frac{\\exp(\\boldsymbol{\\beta}^\\top \\mathbf{X})}{1 + \\exp(\\boldsymbol{\\beta}^\\top \\mathbf{X})} \\] 64.3.5 Logistic Regression for more than two response classes This isn’t used often (a softmax is often used) 64.4 Linear Discriminant Analysis This is a method for modeling the conditional probability of a qualitative response \\(Y\\) given quantitative predictors \\(\\mathbf{X}\\) when \\(Y\\) takes more than two values. It is useful because: Parameter estimates for logistic regression are suprisingly unstable when the classes are well separated, but LDA doesn’t have this problem If \\(n\\) is small and the \\(X_i\\) are approximately normal in the classes (i.e. the conditional \\(X_i | Y = k\\) is approximately normal) LDA is more stable LDA can accomodate more than two clases 64.4.1 Bayes Theorem for Classification Consider a quantitiative input \\(\\mathbf{X}\\) and qualitative response \\(Y \\in {1, \\dots K}\\). Let \\(\\pi_k := \\mathbb{P}(Y = k)\\) be the prior probability that \\(Y=k\\), let \\(p_k(x) := \\mathbb{P}(Y = k\\ |\\ X = x)\\) be the posterior probability that \\(Y = k\\), and let \\(f_k(x):= \\mathbb{P}(X = x\\ |\\ Y = k)\\). Then Bayes’ theorem says: \\[ p_k(x) = \\frac{\\pi_k f_k(x)}{\\sum_{l}\\pi_l f_l(x)} \\] We can form an estimate \\(\\hat{p}_k(x)\\) for \\(p_k(x)\\) with estimates of \\(\\pi_k\\) and \\(f_k(x)\\) for each k, and for \\(x\\) predicts 25 \\[\\hat{y} = \\underset{1 \\leqslant k \\leqslant K}{\\text{argmax}\\,} \\hat{p}_k(x) \\] 64.4.2 Linear Discriminant Analysis for p=1 Assume that the conditional $ X| Y = k (_k, _k^2)$ and that the variances are equal across classes \\(\\sigma_1^2 = \\cdots = \\sigma_K^2 = \\sigma^2\\). The Bayes classifier predicts \\(Y = k\\) where \\(p_k(x)\\) is largest or equivalently \\[\\begin{align*} \\hat{y} &amp;= \\underset{1 \\leqslant k \\leqslant K}{\\text{argmax}\\ } \\delta_k(x)\\\\ \\end{align*}\\] where \\[ \\delta_k(x) := \\left(\\frac{\\mu_k}{\\sigma^2}\\right) - \\frac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k)\\] is the discriminant function26. The LDA classifier 27 estimates the parameters \\[\\begin{align*} \\hat{\\mu}_k &amp;= \\frac{1}{n_k}\\sum_{i: y_i = k} x_i\\\\ \\hat{\\sigma}_k &amp;= \\frac{1}{n-K} \\sum_{k = 1}^K \\sum_{i: y_i = k} \\left(x_i - \\hat{\\mu}_k\\right)^2 \\end{align*}\\] Where \\(n_k\\) is the number of observations in class \\(k\\) 28 and predicts \\[\\begin{align*} \\hat{y} &amp;= \\underset{1 \\leqslant k \\leqslant K}{\\text{argmax}\\ } \\hat{\\sigma}_k(x)) \\\\ \\end{align*}\\] 64.4.3 Linear Discriminant Analysis for p &gt; 1 Assume that the conditional \\((\\mathbf{X}| Y = k) \\sim \\text{Normal}(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\\) and that the covariance matrices are equal across classes \\(\\boldsymbol{\\Sigma}_1 = \\cdots = \\boldsymbol{\\Sigma}_K = \\boldsymbol{\\Sigma}\\). The discriminant functions are \\[\\sigma_k(x) = x^\\top\\boldsymbol{\\Sigma}^{-1}\\mu_k - \\frac{1}{2}\\mu_k {\\Sigma}^{-1} \\mu_k + \\log(\\pi_k)\\] LDA estimates \\(\\boldsymbol{\\mu}_k\\) and \\(\\boldsymbol{\\Sigma}_k\\) componentwise \\[\\begin{align*} (\\hat{\\mu}_k)_j &amp;= \\frac{1}{n_k}\\sum_{i: y_i = k} x_{ij}\\\\ (\\hat{\\sigma}_k)_j &amp;= \\frac{1}{n-K} \\sum_{k = 1}^K \\sum_{i: y_i = k} \\left(x_{ij} - (\\hat{\\mu}_k)_j\\right)^2 \\end{align*}\\] for \\(1 \\leqslant j \\leqslant p\\) as above and predicts \\[\\begin{align*} \\hat{y} &amp;= \\underset{1 \\leqslant k \\leqslant K}{\\text{argmax}\\ } \\hat{\\sigma}_k(x)) \\\\ \\end{align*}\\] as above Confusion matrices help analyze misclassifications for an LDA model 29 The Bayes decision boundary may not be agreeable in every context so sometimes a different decsision boundary (threshold) is used. An ROC curve is useful for vizualising true vs false positives over different decision thresholds in the binary response case. 64.4.4 Quadratic Discriminant Analysis Assume that the conditional \\((\\mathbf{X}| Y = k) \\sim \\text{Normal}(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\\) but assume that the covariance matrices \\(\\boldsymbol{\\Sigma}_k\\) are not equal across classes The discriminant functions are now quadratic in \\(x\\) \\[\\sigma_k(x) = x^\\top\\boldsymbol{\\Sigma}_k^{-1}\\mu_k - \\frac{1}{2}\\mu_k {\\Sigma}_k^{-1} \\mu_k + \\log(\\pi_k)\\] QDA has more degrees of freedom than LDA 30 so generally has lower bias but higher variance. 64.5 A Comparison of Classification Methods So far our classification methods are KNN, logistic regression (LogReg), LDA and QDA LDA and LogReg both produce linear decision boundaries. They often give similar performance results, although LDA tends to outperform when the conditionals \\(X | Y = k\\) are normally distributed, and not when they aren’t As a non-parametric approach, KNN produces a non-linear decision boundary, so tends to outperform LDA and LogReg when the true decision boundary is highly non-linear. It doesn’t help with selecting important predictors With a quadratic decision boundary, QDA is a compromise between the non-linear KNN and the linear LDA/LogReg 64.6 Footnotes Recall that that the Bayes classifier predicts \\[\\hat{y} = \\underset{1 \\leqslant k \\leqslant K}{\\text{argmax}\\,} p_k(x) \\] So we can think of LDA as an esimate of the Bayes Classifier ↩︎ The Bayes decision boundary corresponds to the parameter values \\(\\boldsymbol{\\mu} = (\\mu_1, \\dots, \\mu_K)\\), \\(\\boldsymbol{\\sigma} = (\\sigma_1, \\dots, \\sigma_K)\\) such that \\(\\delta_k(x) = \\delta_j(x)\\) for all \\(1 \\leqslant j,k \\leqslant K\\). The Bayes classifier assigns a class \\(y\\) to an input \\(x\\) based on where \\(x\\) falls with respect to this boundary. For the case \\(K=2\\), this is equivalent to assigning \\(x\\) to class 1 if \\(2x(\\mu_1 - \\mu_2) &gt; \\mu_1^2 - \\mu_2^2\\). ↩︎ The functions \\(\\hat{\\delta}_k(x)\\) are called discriminant functions, and since they’re linear in \\(x\\), the method is called linear discriminant analysis. ↩︎ \\(\\hat{\\mu}_k\\) is the average of all observation inputs in the \\(k\\)-th class, and \\(\\hat{\\sigma}_k\\) is a weighted average of the samples variances over the \\(K\\) classes, ↩︎ False positives and false negatives in the binary case. ↩︎ \\(K\\binom{p}{2}\\) for QDA versus \\(\\binom{p}{2}\\) for LDA. ↩︎ "],["chapter-6-linear-model-selection-and-regularization.html", "Chapter 65 Chapter 6 - Linear Model Selection and Regularization", " Chapter 65 Chapter 6 - Linear Model Selection and Regularization Lab 2: Ridge Regression Lab 2: The Lasso Lab 3: Principal Components Regression Lab 3: Partial Least Squares # %load ../standard_import.txt import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import glmnet as gln from sklearn.preprocessing import scale from sklearn import model_selection from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV from sklearn.decomposition import PCA from sklearn.cross_decomposition import PLSRegression from sklearn.model_selection import KFold, cross_val_score from sklearn.metrics import mean_squared_error %matplotlib inline plt.style.use(&#39;seaborn-white&#39;) "],["lab-2.html", "Chapter 66 Lab 2", " Chapter 66 Lab 2 # In R, I exported the dataset from package &#39;ISLR&#39; to a csv file. df = pd.read_csv(&#39;Data/Hitters.csv&#39;, index_col=0).dropna() df.index.name = &#39;Player&#39; df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 263 entries, -Alan Ashby to -Willie Wilson Data columns (total 20 columns): AtBat 263 non-null int64 Hits 263 non-null int64 HmRun 263 non-null int64 Runs 263 non-null int64 RBI 263 non-null int64 Walks 263 non-null int64 Years 263 non-null int64 CAtBat 263 non-null int64 CHits 263 non-null int64 CHmRun 263 non-null int64 CRuns 263 non-null int64 CRBI 263 non-null int64 CWalks 263 non-null int64 League 263 non-null object Division 263 non-null object PutOuts 263 non-null int64 Assists 263 non-null int64 Errors 263 non-null int64 Salary 263 non-null float64 NewLeague 263 non-null object dtypes: float64(1), int64(16), object(3) memory usage: 43.1+ KB df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI CWalks League Division PutOuts Assists Errors Salary NewLeague Player -Alan Ashby 315 81 7 24 38 39 14 3449 835 69 321 414 375 N W 632 43 10 475.0 N -Alvin Davis 479 130 18 66 72 76 3 1624 457 63 224 266 263 A W 880 82 14 480.0 A -Andre Dawson 496 141 20 65 78 37 11 5628 1575 225 828 838 354 N E 200 11 3 500.0 N -Andres Galarraga 321 87 10 39 42 30 2 396 101 12 48 46 33 N E 805 40 4 91.5 N -Alfredo Griffin 594 169 4 74 51 35 11 4408 1133 19 501 336 194 A W 282 421 25 750.0 A dummies = pd.get_dummies(df[[&#39;League&#39;, &#39;Division&#39;, &#39;NewLeague&#39;]]) dummies.info() print(dummies.head()) &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 263 entries, -Alan Ashby to -Willie Wilson Data columns (total 6 columns): League_A 263 non-null uint8 League_N 263 non-null uint8 Division_E 263 non-null uint8 Division_W 263 non-null uint8 NewLeague_A 263 non-null uint8 NewLeague_N 263 non-null uint8 dtypes: uint8(6) memory usage: 3.6+ KB League_A League_N Division_E Division_W NewLeague_A \\ Player -Alan Ashby 0 1 0 1 0 -Alvin Davis 1 0 0 1 1 -Andre Dawson 0 1 1 0 0 -Andres Galarraga 0 1 1 0 0 -Alfredo Griffin 1 0 0 1 1 NewLeague_N Player -Alan Ashby 1 -Alvin Davis 0 -Andre Dawson 1 -Andres Galarraga 1 -Alfredo Griffin 0 y = df.Salary # Drop the column with the independent variable (Salary), and columns for which we created dummy variables X_ = df.drop([&#39;Salary&#39;, &#39;League&#39;, &#39;Division&#39;, &#39;NewLeague&#39;], axis=1).astype(&#39;float64&#39;) # Define the feature set X. X = pd.concat([X_, dummies[[&#39;League_N&#39;, &#39;Division_W&#39;, &#39;NewLeague_N&#39;]]], axis=1) X.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 263 entries, -Alan Ashby to -Willie Wilson Data columns (total 19 columns): AtBat 263 non-null float64 Hits 263 non-null float64 HmRun 263 non-null float64 Runs 263 non-null float64 RBI 263 non-null float64 Walks 263 non-null float64 Years 263 non-null float64 CAtBat 263 non-null float64 CHits 263 non-null float64 CHmRun 263 non-null float64 CRuns 263 non-null float64 CRBI 263 non-null float64 CWalks 263 non-null float64 PutOuts 263 non-null float64 Assists 263 non-null float64 Errors 263 non-null float64 League_N 263 non-null uint8 Division_W 263 non-null uint8 NewLeague_N 263 non-null uint8 dtypes: float64(16), uint8(3) memory usage: 35.7+ KB X.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI CWalks PutOuts Assists Errors League_N Division_W NewLeague_N Player -Alan Ashby 315.0 81.0 7.0 24.0 38.0 39.0 14.0 3449.0 835.0 69.0 321.0 414.0 375.0 632.0 43.0 10.0 1 1 1 -Alvin Davis 479.0 130.0 18.0 66.0 72.0 76.0 3.0 1624.0 457.0 63.0 224.0 266.0 263.0 880.0 82.0 14.0 0 1 0 -Andre Dawson 496.0 141.0 20.0 65.0 78.0 37.0 11.0 5628.0 1575.0 225.0 828.0 838.0 354.0 200.0 11.0 3.0 1 0 1 -Andres Galarraga 321.0 87.0 10.0 39.0 42.0 30.0 2.0 396.0 101.0 12.0 48.0 46.0 33.0 805.0 40.0 4.0 1 0 1 -Alfredo Griffin 594.0 169.0 4.0 74.0 51.0 35.0 11.0 4408.0 1133.0 19.0 501.0 336.0 194.0 282.0 421.0 25.0 0 1 0 66.0.0.1 I executed the R code and downloaded the exact same training/test sets used in the book. X_train = pd.read_csv(&#39;Data/Hitters_X_train.csv&#39;, index_col=0) y_train = pd.read_csv(&#39;Data/Hitters_y_train.csv&#39;, index_col=0) X_test = pd.read_csv(&#39;Data/Hitters_X_test.csv&#39;, index_col=0) y_test = pd.read_csv(&#39;Data/Hitters_y_test.csv&#39;, index_col=0) 66.0.1 6.6.1 Ridge Regression 66.0.2 Scikit-learn The glmnet algorithms in R optimize the objective function using cyclical coordinate descent, while scikit-learn Ridge regression uses linear least squares with L2 regularization. They are rather different implementations, but the general principles are the same. The glmnet() function in R optimizes: ### \\[ \\frac{1}{N}|| X\\beta-y||^2_2+\\lambda\\bigg(\\frac{1}{2}(1−\\alpha)||\\beta||^2_2 \\ +\\ \\alpha||\\beta||_1\\bigg) \\] (See R documentation and https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet_beta.pdf) The function supports L1 and L2 regularization. For just Ridge regression we need to use $= 0 $. This reduces the above cost function to ### \\[ \\frac{1}{N}|| X\\beta-y||^2_2+\\frac{1}{2}\\lambda ||\\beta||^2_2 \\] The sklearn Ridge() function optimizes: ### \\[ ||X\\beta - y||^2_2 + \\alpha ||\\beta||^2_2 \\] which is equivalent to optimizing ### \\[ \\frac{1}{N}||X\\beta - y||^2_2 + \\frac{\\alpha}{N} ||\\beta||^2_2 \\] alphas = 10**np.linspace(10,-2,100)*0.5 ridge = Ridge() coefs = [] for a in alphas: ridge.set_params(alpha=a) ridge.fit(scale(X), y) coefs.append(ridge.coef_) ax = plt.gca() ax.plot(alphas, coefs) ax.set_xscale(&#39;log&#39;) ax.set_xlim(ax.get_xlim()[::-1]) # reverse axis plt.axis(&#39;tight&#39;) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;weights&#39;) plt.title(&#39;Ridge coefficients as a function of the regularization&#39;); png The above plot shows that the Ridge coefficients get larger when we decrease alpha. 66.0.2.1 Alpha = 4 from sklearn.preprocessing import StandardScaler scaler = StandardScaler().fit(X_train) ridge2 = Ridge(alpha=len(X_)*11498/2) ridge2.fit(scaler.transform(X_train), y_train) pred = ridge2.predict(scaler.transform(X_test)) mean_squared_error(y_test, pred) 193147.46143016344 pd.Series(ridge2.coef_.flatten(), index=X.columns) AtBat 0.015146 Hits 0.016050 HmRun 0.013561 Runs 0.015681 RBI 0.016782 Walks 0.019662 Years 0.010390 CAtBat 0.016570 CHits 0.017627 CHmRun 0.015072 CRuns 0.018771 CRBI 0.016697 CWalks 0.016821 PutOuts 0.003228 Assists -0.007600 Errors 0.013672 League_N 0.003519 Division_W 0.003339 NewLeague_N 0.003499 dtype: float64 66.0.2.2 Alpha = \\(10^{10}\\) This big penalty shrinks the coefficients to a very large degree and makes the model more biased, resulting in a higher MSE. ridge2.set_params(alpha=10**10) ridge2.fit(scale(X_train), y_train) pred = ridge2.predict(scale(X_test)) mean_squared_error(y_test, pred) 193253.09741651407 66.0.2.3 Compute the regularization path using RidgeCV ridgecv = RidgeCV(alphas=alphas, scoring=&#39;neg_mean_squared_error&#39;) ridgecv.fit(scale(X_train), y_train) RidgeCV(alphas=array([5.00000e+09, 3.78232e+09, ..., 6.60971e-03, 5.00000e-03]), cv=None, fit_intercept=True, gcv_mode=None, normalize=False, scoring=&#39;neg_mean_squared_error&#39;, store_cv_values=False) ridgecv.alpha_ 115.5064850041579 ridge2.set_params(alpha=ridgecv.alpha_) ridge2.fit(scale(X_train), y_train) mean_squared_error(y_test, ridge2.predict(scale(X_test))) 97384.92959172592 pd.Series(ridge2.coef_.flatten(), index=X.columns) AtBat 7.576771 Hits 22.596030 HmRun 18.971990 Runs 20.193945 RBI 21.063875 Walks 55.713281 Years -4.687149 CAtBat 20.496892 CHits 29.230247 CHmRun 14.293124 CRuns 35.881788 CRBI 20.212172 CWalks 24.419768 PutOuts 16.128910 Assists -44.102264 Errors 54.624503 League_N 5.771464 Division_W -0.293713 NewLeague_N 11.137518 dtype: float64 66.0.3 python-glmnet (update 2016-08-29) This relatively new module is a wrapper for the fortran library used in the R package glmnet. It gives mostly the exact same results as described in the book. However, the predict() method does not give you the regression coefficients for lambda values not in the lambda_path. It only returns the predicted values. https://github.com/civisanalytics/python-glmnet grid = 10**np.linspace(10,-2,100) ridge3 = gln.ElasticNet(alpha=0, lambda_path=grid) ridge3.fit(X, y) ElasticNet(alpha=0, cut_point=1.0, fit_intercept=True, lambda_path=array([1.00000e+10, 7.56463e+09, ..., 1.32194e-02, 1.00000e-02]), max_iter=100000, min_lambda_ratio=0.0001, n_jobs=1, n_lambda=100, n_splits=3, random_state=None, scoring=None, standardize=True, tol=1e-07, verbose=False) 66.0.3.1 Lambda 11498 ridge3.lambda_path_[49] 11497.569953977356 print(&#39;Intercept: {:.3f}&#39;.format(ridge3.intercept_path_[49])) Intercept: 407.356 pd.Series(np.round(ridge3.coef_path_[:,49], decimals=3), index=X.columns) AtBat 0.037 Hits 0.138 HmRun 0.525 Runs 0.231 RBI 0.240 Walks 0.290 Years 1.108 CAtBat 0.003 CHits 0.012 CHmRun 0.088 CRuns 0.023 CRBI 0.024 CWalks 0.025 PutOuts 0.016 Assists 0.003 Errors -0.021 League_N 0.085 Division_W -6.215 NewLeague_N 0.301 dtype: float64 np.sqrt(np.sum(ridge3.coef_path_[:,49]**2)) 6.3606122865384505 66.0.3.2 Lambda 705 ridge3.lambda_path_[59] 705.4802310718645 print(&#39;Intercept: {:.3f}&#39;.format(ridge3.intercept_path_[59])) Intercept: 54.325 pd.Series(np.round(ridge3.coef_path_[:,59], decimals=3), index=X.columns) AtBat 0.112 Hits 0.656 HmRun 1.180 Runs 0.938 RBI 0.847 Walks 1.320 Years 2.596 CAtBat 0.011 CHits 0.047 CHmRun 0.338 CRuns 0.094 CRBI 0.098 CWalks 0.072 PutOuts 0.119 Assists 0.016 Errors -0.704 League_N 13.684 Division_W -54.659 NewLeague_N 8.612 dtype: float64 np.sqrt(np.sum(ridge3.coef_path_[:,59]**2)) 57.11003436702412 66.0.3.3 Fit model using just the training set. ridge4 = gln.ElasticNet(alpha=0, lambda_path=grid, scoring=&#39;mean_squared_error&#39;, tol=1e-12) ridge4.fit(X_train, y_train.values.ravel()) ElasticNet(alpha=0, cut_point=1.0, fit_intercept=True, lambda_path=array([1.00000e+10, 7.56463e+09, ..., 1.32194e-02, 1.00000e-02]), max_iter=100000, min_lambda_ratio=0.0001, n_jobs=1, n_lambda=100, n_splits=3, random_state=None, scoring=&#39;mean_squared_error&#39;, standardize=True, tol=1e-12, verbose=False) # prediction using lambda = 4 pred = ridge4.predict(X_test, lamb=4) mean_squared_error(y_test.values.ravel(), pred) 101036.83230892917 66.0.3.4 Lambda chosen by cross validation ridge5 = gln.ElasticNet(alpha=0, scoring=&#39;mean_squared_error&#39;) ridge5.fit(X_train, y_train.values.ravel()) ElasticNet(alpha=0, cut_point=1.0, fit_intercept=True, lambda_path=None, max_iter=100000, min_lambda_ratio=0.0001, n_jobs=1, n_lambda=100, n_splits=3, random_state=None, scoring=&#39;mean_squared_error&#39;, standardize=True, tol=1e-07, verbose=False) # Lambda with best CV performance ridge5.lambda_max_ 255.04348848905948 # Lambda larger than lambda_max_, but with a CV score that is within 1 standard deviation away from lambda_max_ ridge5.lambda_best_ array([1974.70910641]) plt.figure(figsize=(15,6)) plt.errorbar(np.log(ridge5.lambda_path_), -ridge5.cv_mean_score_, color=&#39;r&#39;, linestyle=&#39;None&#39;, marker=&#39;o&#39;, markersize=5, yerr=ridge5.cv_standard_error_, ecolor=&#39;lightgrey&#39;, capsize=4) for ref, txt in zip([ridge5.lambda_best_, ridge5.lambda_max_], [&#39;Lambda best&#39;, &#39;Lambda max&#39;]): plt.axvline(x=np.log(ref), linestyle=&#39;dashed&#39;, color=&#39;lightgrey&#39;) plt.text(np.log(ref), .95*plt.gca().get_ylim()[1], txt, ha=&#39;center&#39;) plt.xlabel(&#39;log(Lambda)&#39;) plt.ylabel(&#39;Mean-Squared Error&#39;); png # MSE for lambda with best CV performance pred = ridge5.predict(X_test, lamb=ridge5.lambda_max_) mean_squared_error(y_test, pred) 96006.84514850576 66.0.3.5 Fit model to full data set ridge6= gln.ElasticNet(alpha=0, scoring=&#39;mean_squared_error&#39;, n_splits=10) ridge6.fit(X, y) ElasticNet(alpha=0, cut_point=1.0, fit_intercept=True, lambda_path=None, max_iter=100000, min_lambda_ratio=0.0001, n_jobs=1, n_lambda=100, n_splits=10, random_state=None, scoring=&#39;mean_squared_error&#39;, standardize=True, tol=1e-07, verbose=False) # These are not really close to the ones in the book. pd.Series(ridge6.coef_path_[:,ridge6.lambda_max_inx_], index=X.columns) AtBat -0.681594 Hits 2.772311 HmRun -1.365704 Runs 1.014812 RBI 0.713030 Walks 3.378558 Years -9.066826 CAtBat -0.001200 CHits 0.136102 CHmRun 0.697992 CRuns 0.295890 CRBI 0.257072 CWalks -0.278966 PutOuts 0.263887 Assists 0.169878 Errors -3.685656 League_N 53.209503 Division_W -122.834334 NewLeague_N -18.102528 dtype: float64 66.0.4 6.6.2 The Lasso 66.0.5 Scikit-learn For both glmnet in R and sklearn Lasso() function the standard L1 penalty is: ### \\[ \\lambda |\\beta|_1 \\] lasso = Lasso(max_iter=10000) coefs = [] for a in alphas*2: lasso.set_params(alpha=a) lasso.fit(scale(X_train), y_train) coefs.append(lasso.coef_) ax = plt.gca() ax.plot(alphas*2, coefs) ax.set_xscale(&#39;log&#39;) ax.set_xlim(ax.get_xlim()[::-1]) # reverse axis plt.axis(&#39;tight&#39;) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;weights&#39;) plt.title(&#39;Lasso coefficients as a function of the regularization&#39;); png lassocv = LassoCV(alphas=None, cv=10, max_iter=10000) lassocv.fit(scale(X_train), y_train.values.ravel()) LassoCV(alphas=None, copy_X=True, cv=10, eps=0.001, fit_intercept=True, max_iter=10000, n_alphas=100, n_jobs=1, normalize=False, positive=False, precompute=&#39;auto&#39;, random_state=None, selection=&#39;cyclic&#39;, tol=0.0001, verbose=False) lassocv.alpha_ 30.013822564464284 lasso.set_params(alpha=lassocv.alpha_) lasso.fit(scale(X_train), y_train) mean_squared_error(y_test, lasso.predict(scale(X_test))) 102924.90954696963 # Some of the coefficients are now reduced to exactly zero. pd.Series(lasso.coef_, index=X.columns) AtBat 0.000000 Hits 0.000000 HmRun 2.154219 Runs 0.000000 RBI 30.835560 Walks 104.071528 Years -0.000000 CAtBat 0.000000 CHits 0.000000 CHmRun 0.000000 CRuns 132.858095 CRBI 0.000000 CWalks 0.000000 PutOuts 1.896185 Assists -51.058752 Errors 76.779641 League_N 0.000000 Division_W 0.000000 NewLeague_N 0.000000 dtype: float64 66.0.6 python-glmnet lasso2 = gln.ElasticNet(alpha=1, lambda_path=grid, scoring=&#39;mean_squared_error&#39;, n_splits=10) lasso2.fit(X_train, y_train.values.ravel()) ElasticNet(alpha=1, cut_point=1.0, fit_intercept=True, lambda_path=array([1.00000e+10, 7.56463e+09, ..., 1.32194e-02, 1.00000e-02]), max_iter=100000, min_lambda_ratio=0.0001, n_jobs=1, n_lambda=100, n_splits=10, random_state=None, scoring=&#39;mean_squared_error&#39;, standardize=True, tol=1e-07, verbose=False) l1_norm = np.sum(np.abs(lasso2.coef_path_), axis=0) plt.figure(figsize=(10,6)) plt.plot(l1_norm, lasso2.coef_path_.T) plt.xlabel(&#39;L1 norm&#39;) plt.ylabel(&#39;Coefficients&#39;); png 66.0.6.1 Let glmnet() create a grid to use in CV lasso3 = gln.ElasticNet(alpha=1, scoring=&#39;mean_squared_error&#39;, n_splits=10) lasso3.fit(X_train, y_train.values.ravel()) ElasticNet(alpha=1, cut_point=1.0, fit_intercept=True, lambda_path=None, max_iter=100000, min_lambda_ratio=0.0001, n_jobs=1, n_lambda=100, n_splits=10, random_state=None, scoring=&#39;mean_squared_error&#39;, standardize=True, tol=1e-07, verbose=False) plt.figure(figsize=(15,6)) plt.errorbar(np.log(lasso3.lambda_path_), -lasso3.cv_mean_score_, color=&#39;r&#39;, linestyle=&#39;None&#39;, marker=&#39;o&#39;, markersize=5, yerr=lasso3.cv_standard_error_, ecolor=&#39;lightgrey&#39;, capsize=4) for ref, txt in zip([lasso3.lambda_best_, lasso3.lambda_max_], [&#39;Lambda best&#39;, &#39;Lambda max&#39;]): plt.axvline(x=np.log(ref), linestyle=&#39;dashed&#39;, color=&#39;lightgrey&#39;) plt.text(np.log(ref), .95*plt.gca().get_ylim()[1], txt, ha=&#39;center&#39;) plt.xlabel(&#39;log(Lambda)&#39;) plt.ylabel(&#39;Mean-Squared Error&#39;); png pred = lasso3.predict(X_test, lamb=lasso3.lambda_max_) mean_squared_error(y_test, pred) 101294.32852317697 66.0.6.2 Fit model on full dataset lasso4 = gln.ElasticNet(alpha=1, lambda_path=grid, scoring=&#39;mean_squared_error&#39;, n_splits=10) lasso4.fit(X, y) ElasticNet(alpha=1, cut_point=1.0, fit_intercept=True, lambda_path=array([1.00000e+10, 7.56463e+09, ..., 1.32194e-02, 1.00000e-02]), max_iter=100000, min_lambda_ratio=0.0001, n_jobs=1, n_lambda=100, n_splits=10, random_state=None, scoring=&#39;mean_squared_error&#39;, standardize=True, tol=1e-07, verbose=False) # These are not really close to the ones in the book. pd.Series(lasso4.coef_path_[:,lasso4.lambda_max_inx_], index=X.columns) AtBat -1.560098 Hits 5.693168 HmRun 0.000000 Runs 0.000000 RBI 0.000000 Walks 4.750540 Years -9.518024 CAtBat 0.000000 CHits 0.000000 CHmRun 0.519161 CRuns 0.660407 CRBI 0.391541 CWalks -0.532687 PutOuts 0.272620 Assists 0.174816 Errors -2.056721 League_N 32.109569 Division_W -119.258342 NewLeague_N 0.000000 dtype: float64 "],["lab-3.html", "Chapter 67 Lab 3", " Chapter 67 Lab 3 67.0.1 6.7.1 Principal Components Regression Scikit-klearn does not have an implementation of PCA and regression combined like the ‘pls’ package in R. https://cran.r-project.org/web/packages/pls/vignettes/pls-manual.pdf pca = PCA() X_reduced = pca.fit_transform(scale(X)) print(pca.components_.shape) pd.DataFrame(pca.components_.T).loc[:4,:5] (19, 19) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 0 0.198290 -0.383784 0.088626 0.031967 0.028117 -0.070646 1 0.195861 -0.377271 0.074032 0.017982 -0.004652 -0.082240 2 0.204369 -0.237136 -0.216186 -0.235831 0.077660 -0.149646 3 0.198337 -0.377721 -0.017166 -0.049942 -0.038536 -0.136660 4 0.235174 -0.314531 -0.073085 -0.138985 0.024299 -0.111675 The above loadings are the same as in R. print(X_reduced.shape) pd.DataFrame(X_reduced).loc[:4,:5] (263, 19) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 0 -0.009649 1.870522 1.265145 -0.935481 1.109636 1.211972 1 0.411434 -2.429422 -0.909193 -0.264212 1.232031 1.826617 2 3.466822 0.825947 0.555469 -1.616726 -0.857488 -1.028712 3 -2.558317 -0.230984 0.519642 -2.176251 -0.820301 1.491696 4 1.027702 -1.573537 1.331382 3.494004 0.983427 0.513675 The above principal components are the same as in R. # Variance explained by the principal components np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100) array([38.31, 60.15, 70.84, 79.03, 84.29, 88.63, 92.26, 94.96, 96.28, 97.25, 97.97, 98.64, 99.14, 99.46, 99.73, 99.88, 99.95, 99.98, 99.99]) # 10-fold CV, with shuffle n = len(X_reduced) kf_10 = KFold(n_splits=10, shuffle=True, random_state=1) regr = LinearRegression() mse = [] # Calculate MSE with only the intercept (no principal components in regression) score = -1*cross_val_score(regr, np.ones((n,1)), y.ravel(), cv=kf_10, scoring=&#39;neg_mean_squared_error&#39;).mean() mse.append(score) # Calculate MSE using CV for the 19 principle components, adding one component at the time. for i in np.arange(1, 20): score = -1*cross_val_score(regr, X_reduced[:,:i], y.ravel(), cv=kf_10, scoring=&#39;neg_mean_squared_error&#39;).mean() mse.append(score) plt.plot(mse, &#39;-v&#39;) plt.xlabel(&#39;Number of principal components in regression&#39;) plt.ylabel(&#39;MSE&#39;) plt.title(&#39;Salary&#39;) plt.xlim(xmin=-1); png The above plot indicates that the lowest training MSE is reached when doing regression on 18 components. regr_test = LinearRegression() regr_test.fit(X_reduced, y) regr_test.coef_ array([ 106.36859204, -21.60350456, 24.2942534 , -36.9858579 , -58.41402748, 62.20632652, 24.63862038, 15.82817701, 29.57680773, 99.64801199, -30.11209105, 20.99269291, 72.40210574, -276.68551696, -74.17098665, 422.72580227, -347.05662353, -561.59691587, -83.25441536]) 67.0.1.1 Fitting PCA with training data pca2 = PCA() X_reduced_train = pca2.fit_transform(scale(X_train)) n = len(X_reduced_train) # 10-fold CV, with shuffle kf_10 = KFold(n_splits=10, shuffle=False, random_state=1) mse = [] # Calculate MSE with only the intercept (no principal components in regression) score = -1*cross_val_score(regr, np.ones((n,1)), y_train, cv=kf_10, scoring=&#39;neg_mean_squared_error&#39;).mean() mse.append(score) # Calculate MSE using CV for the 19 principle components, adding one component at the time. for i in np.arange(1, 20): score = -1*cross_val_score(regr, X_reduced_train[:,:i], y_train, cv=kf_10, scoring=&#39;neg_mean_squared_error&#39;).mean() mse.append(score) plt.plot(np.array(mse), &#39;-v&#39;) plt.xlabel(&#39;Number of principal components in regression&#39;) plt.ylabel(&#39;MSE&#39;) plt.title(&#39;Salary&#39;) plt.xlim(xmin=-1); png The above plot indicates that the lowest training MSE is reached when doing regression on 6 components. 67.0.1.2 Transform test data with PCA loadings and fit regression on 6 principal components X_reduced_test = pca2.transform(scale(X_test))[:,:7] # Train regression model on training data regr = LinearRegression() regr.fit(X_reduced_train[:,:7], y_train) # Prediction with test data pred = regr.predict(X_reduced_test) mean_squared_error(y_test, pred) 96320.02078250324 67.0.2 6.7.2 Partial Least Squares Scikit-learn PLSRegression gives same results as the pls package in R when using ‘method=’oscorespls.’ In the LAB excercise, the standard method is used which is ‘kernelpls.’ When doing a slightly different fitting in R, the result is close to the one obtained using scikit-learn. pls.fit=plsr(Salary~., data=Hitters, subset=train, scale=TRUE, validation=&quot;CV&quot;, method=&#39;oscorespls&#39;) validationplot(pls.fit,val.type=&quot;MSEP&quot;, intercept = FALSE) See documentation: http://scikit-learn.org/dev/modules/generated/sklearn.cross_decomposition.PLSRegression.html#sklearn.cross_decomposition.PLSRegression n = len(X_train) # 10-fold CV, with shuffle kf_10 = KFold(n_splits=10, shuffle=False, random_state=1) mse = [] for i in np.arange(1, 20): pls = PLSRegression(n_components=i) score = cross_val_score(pls, scale(X_train), y_train, cv=kf_10, scoring=&#39;neg_mean_squared_error&#39;).mean() mse.append(-score) plt.plot(np.arange(1, 20), np.array(mse), &#39;-v&#39;) plt.xlabel(&#39;Number of principal components in regression&#39;) plt.ylabel(&#39;MSE&#39;) plt.title(&#39;Salary&#39;) plt.xlim(xmin=-1); png pls = PLSRegression(n_components=2) pls.fit(scale(X_train), y_train) mean_squared_error(y_test, pls.predict(scale(X_test))) 102234.27995999217 Table of Contents 6 Linear Model Selection and Regularization 6.1 Subset Selection 6.1.1 Best Subset Selection 6.1.1.1 Advantages 6.1.1.2 Disadvantages 6.1.2 Stepwise Selection 6.1.2.1 Forward Stepwise Selection 6.1.2.2 Backward Stepwise Selection 6.1.2.3 Hybrid Approaches 6.1.3 Choosing the Optimal Model 6.1.3.1 \\(C_p\\), AIC, BIC and Adjusted \\(R^2\\) 6.1.3.2 Validation and Cross-Validation 6.2 Shrinkage Methods 6.2.1 Ridge Regression 6.2.2 The Lasso 6.2.3 Selecting the Tuning Parameter 6.3 Dimension Reduction Methods 6.3.1 Principal Components Regression 6.3.2 Partial Least Squares 6.4 Considerations in High Dimensions 6.4.1 High-Dimensional Data 6.4.2 What Goes Wrong in High Dimensions? 6.4.3 Regression in High Dimensions 6.4.4 Interpreting Results in High Dimensions 6.5 Footnotes 6.5.1 blah "],["linear-model-selection-and-regularization.html", "Chapter 68 Linear Model Selection and Regularization 68.1 Subset Selection 68.2 Shrinkage Methods 68.3 Dimension Reduction Methods 68.4 Considerations in High Dimensions 68.5 Footnotes", " Chapter 68 Linear Model Selection and Regularization Alternatives to the least squares fitting procedures can yield better prediction accuracy model interpretability 68.1 Subset Selection Methods for selecting a subset of the predictors to improve test performance 68.1.1 Best Subset Selection 68.1.1.0.0.1 Algorithm: Best Subset Selection (BSS) for linear regression Let \\(\\mathcal{M}_0\\) denote the null model35 For \\(1 \\leqslant k \\leqslant p\\): Fit all \\(\\binom{p}{k}\\) linear regression models with \\(k\\) predictors Let \\(\\mathcal{M}_k = \\underset{\\text{models}}{\\text{argmin}}\\ RSS\\) Choose the best model \\(\\mathcal{M}_i, 1 \\leqslant i \\leqslant p\\) based on estimated test error 36 For logistic regression, in step 2.A., let \\(\\mathcal{M}_k = \\underset{\\text{models}}{\\text{argmin}}\\ D(y, \\hat{y})\\) where \\(D(y, \\hat{y})\\) is the deviance37 of the model 68.1.1.1 Advantages Slightly faster than brute force. Model evaluation is \\(O(p)\\) as opposed to \\(O(2^p)\\) for brute force. Conceptually simple 68.1.1.2 Disadvantages Still very slow. Fitting is \\(O(2^p)\\) as for brute force Overfitting and high variance of coefficient estimates when \\(p\\) is large 68.1.2 Stepwise Selection 68.1.2.1 Forward Stepwise Selection 68.1.2.1.1 Algorithm: Forward Stepwise Selection (FSS) for linear regression38 Let \\(\\mathcal{M}_0\\) denote the null model For \\(0 \\leqslant k \\leqslant p - 1\\): Fit all \\(p-k\\) linear regression models that augment model \\(\\mathcal{M}_k\\) with one additional predictor Let \\(\\mathcal{M}_{k+1} = \\underset{\\text{models}}{\\text{argmin}}\\ RSS\\) Choose the best model \\(\\mathcal{M}_i, 1 \\leqslant i \\leqslant p\\) based on estimated test error 68.1.2.1.2 Advantages Faster than BSS. Fitting is \\(O(p^2)\\) and evaluation is \\(O(p)\\) Can be applied in the high-dimensional setting \\(n &lt; p\\) 68.1.2.1.3 Disadvantages Evaluation is more challenging since it compares models with different numbers of predictors. Searches less of the parameter space, hence may be suboptimal 68.1.2.2 Backward Stepwise Selection 68.1.2.2.1 Algorithm: Backward Stepwise Selection (BKSS) for linear regression 39 is Let \\(\\mathcal{M}_p\\) denote the full model 40 For \\(k = p, p-1, \\dots, 1\\): Fit all \\(k\\) linear regression models of \\(k-1\\) predictors that contain all but one of the predictors in \\(\\mathcal{M}_k\\). Let \\(\\mathcal{M}_{k-1} = \\underset{\\text{models}}{\\text{argmin}}\\ RSS\\) Choose the best model \\(\\mathcal{M}_i, 1 \\leqslant i \\leqslant p\\) based on estimated test error 68.1.2.2.2 Advantages As fast as FSS 68.1.2.2.3 Disadvantages Same disadvantages as FSS Cannot be used when \\(n &lt; p\\) 68.1.2.3 Hybrid Approaches Other approaches exist which may add variables sequentially (as with FSS) but may also remove variables (as with BSS). These methods strike a balance between optimality (e.g. BSS) and speed (FSS/BSS) 68.1.3 Choosing the Optimal Model Two common approaches to estimating the test error: Estimate indirectly by adjusting the training error to account for overfitting bias Estimate directly using a validation approach 68.1.3.1 \\(C_p\\), AIC, BIC and Adjusted \\(R^2\\) Train MSE underestimates test MSE and decreases as \\(p\\) increases, so it cannot be used to select from models with different numbers of predictors. However we may adjust the training error to account for the model size, and use this to estimate the test MSE For least squares models, the \\(C_p\\) estimate41 of the test MSE for a model with \\(d\\) predictors is \\[ C_p = \\frac{1}{n}(RSS + 2d\\hat{\\sigma}^2) \\] where \\(\\hat{\\sigma} = \\hat{\\mathbb{V}}(\\epsilon)\\). For maximum likelihood models42, the Akaike Information Criterion (AIC) estimate of the test MSE is \\[ AIC = \\frac{1}{n\\hat{\\sigma}^2}(RSS + 2d\\hat{\\sigma}^2) \\] For least squares models, the Bayes Information Criterion (BIC) estimate43 of the test MSE is \\[ BIC = \\frac{1}{n}(RSS + \\log(n)d\\hat{\\sigma}^2) \\] For least squares models, the adjusted \\(R^2\\) statistic44 is \\[AdjR^2 = 1 - \\frac{RSS/(n - d - 1)}{TSS/(n - 1)}\\] 68.1.3.2 Validation and Cross-Validation Instead using adjusted training error to estimate test error indirectly, we can directly estimate using validation or cross-validation In the past this was computationally prohibitive but advances in computation have made this method very attractive. In this approach, we can select a model using the one-standard-error rule, i.e. selecting the model for which the estimated standard error is within one standard error of the \\(p\\) vs. error curve. 68.2 Shrinkage Methods Methods for constraining or regularizing the coefficient estimates, i.e. shrinking them towards zero. This can significantly reduce their variance. 68.2.1 Ridge Regression Ridge regression introduces an \\(L^2\\)-penalty45 for the training error and estimates \\[\\hat{\\beta}^R = RSS+\\lambda\\|\\tilde{\\beta}\\|_2^2\\] where \\(\\lambda\\) is a tuning parameter46 and \\(\\tilde{\\beta} = (\\beta_1, \\dots, \\beta_p)\\)47. The term \\(\\lambda\\|\\beta\\|_2^2\\) is called a shrinkage penalty Selecting a good value for \\(\\lambda\\) is critical, see section 6.2.3 Standardizing the predictors \\(X_i \\mapsto \\frac{X_i - \\mu_i}{s_i}\\) is advised. 68.2.1.0.1 Advantages Takes advantage of bias-variance tradeoff by decreasing flexibility 48 thus decreasing variance. Preferable to least squares in situations when the latter has high variance (close to linear relationship, \\(p \\lesssim n\\) In contrast to least squares, works when \\(p &gt; n\\) 68.2.1.0.2 Disadvantages Lower variance means higher bias. Will not eliminate any predictors which can be an issue for interpretation when \\(p\\) is large. 68.2.2 The Lasso Lasso regression introduces an \\(L^1\\)-penalty 49 for the training error and estimates \\[\\hat{\\beta}^R = RSS+\\lambda\\|\\tilde{\\beta}\\|^2_1\\] 68.2.2.0.1 Advantages Same advantages as ridge regression. Improves over ridge regression by yielding sparse models (i.e. performs variable selection) when \\(\\lambda\\) is sufficiently large 68.2.2.0.2 Disadvantages Lower variance means higher bias. 68.2.2.0.3 Another Formulation for Ridge Regression and the Lasso Ridge Regression is equivalent to the quadratic optimization problem: \\[\\begin{align*} \\min&amp;\\ RSS + \\|\\tilde{\\beta}\\|_2\\\\ \\text{s.t.}&amp;\\ \\| \\tilde{\\beta} \\|_2^2 \\leqslant s \\end{align*}\\] Lasso Regression is equivalent to the quadratic optimization problem: \\[\\begin{align*} \\min&amp;\\ RSS + \\|\\tilde{\\beta}\\|_1\\\\ \\text{s.t.}&amp;\\ \\| \\tilde{\\beta} \\|_1 \\leqslant s \\end{align*}\\] 68.2.2.0.4 Bayesian Interpretation for Ridge and Lasso Regression Given Gaussian errors, and simple assumptions on the prior \\(p(\\beta)\\), ridge and lasso regression emerge as solutions If the \\(\\beta_i \\sim \\text{Normal}(0, h(\\lambda))\\) iid for some function \\(h=h(\\lambda)\\) then the posterior mode for \\(\\beta\\) (i.e. \\(\\underset{\\beta}{\\text{argmax}} p(\\beta| X, Y)\\)) is the ridge regression solution If the \\(\\beta_i \\sim \\text{Laplace}(0, h(\\lambda))\\) iid then the posterior mode is the lasso regression solution. 68.2.3 Selecting the Tuning Parameter Compute the cross-validation error \\(CV_{(n),i}\\) for for a “grid” (evenly-spaced discrete set) of values \\(\\lambda_i\\), and choose \\[ \\lambda = \\underset{i}{\\text{argmin}\\ CV_{(n),i}}\\] 68.3 Dimension Reduction Methods Dimension reduction methods transform the predictors \\(X_1, \\dots, X_p\\) into a smaller set of predictors \\(Z_1, \\dots, Z_M\\), \\(M &lt; p\\). When \\(p &gt;&gt; n\\), \\(M &lt;&lt; p\\) can greatly reduce the variance of the coefficient estimates. In this section we consider linear transformations \\[Z_m = \\sum_{j = 1}^p \\phi_{jm}X_j\\] and a least squares regression model \\[ Y = \\mathbf{Z}\\theta + \\epsilon \\] where \\(\\mathbf{Z} = (1,Z_1, \\dots, Z_M)\\) 68.3.1 Principal Components Regression Principal Components Analysis is a popular unsupervised approach 50 that can be used for dimensional reduction 68.3.1.0.1 An Overview of Principal Components Analysis The principal components of a data matrix \\(n\\times p\\) matrix \\(\\mathbf{X}\\) can be seen (among many different perspectives) as the right singular eigenvectors \\(v_1, \\dots, v_p\\) of the \\(p\\times p\\) sample covariance matrix \\(C\\), i.e. the eigenvectors of \\(C^{\\top}C\\)) ordered by decreasing absolute value of the corresponding eigenvalues. Let \\(\\sigma_1^2,\\dots, \\sigma_k^2\\) be the singular values of \\(C\\) (the squares of the eigenvalues of \\(C^{\\top}C\\)) and let \\(v_1, \\dots, v_p\\) be the corresponding eigenvectors of \\(C\\). Then \\(\\sigma_i^2\\) is the variance of the data along the direction \\(v_i\\), and \\(\\sigma_1^2\\) is the direction of maximal variance. 68.3.1.0.2 The Principal Components Regression Approach Principal Components Regression takes \\(Z_1,\\dots, Z_M\\) to be the first \\(M\\) principal components of \\(\\mathbf{X}\\) and then fits a least squares model on these components. The assumption is that, since the principal components correspond to the directions of greatest variation of the data, they show the most association with \\(Y\\). Furthermore, they are ordered by decreasing magnitude of association. Typically \\(M\\) is chosen by cross-validation. 68.3.1.0.2.1 Advantages If the assumption holds then the least squares model on \\(Z_1, \\dots, Z_M\\) will perform better than \\(X_1, \\dots, X_p\\), since it will contain most of the information related to the response 51, and by choosing \\(M&lt;&lt;p\\) we can mitigate overfitting. Decreased variance of coefficient estimates relative to OLS regression 68.3.1.0.2.2 Disadvantages Is not a feature selection method, since each \\(Z_i\\) is a linear function of the predictors 68.3.1.0.2.3 Recommendations Data should usually be standarized prior to finding the principal components. 68.3.2 Partial Least Squares A supervised dimension reduction method which proceeds roughly as follows Standardize the variables Compute \\(Z_1\\) by setting \\(\\phi_{j1} = \\hat{\\beta_j}\\) the ordinary least squares estimate 52 For $ 1 &lt; m &lt; M$, \\(Z_m\\) is determined by Adjust the data \\(X_j = \\epsilon_j\\) where \\(\\epsilon_j\\) is the residual from regression of \\(Z_{m - 1}\\) onto \\(X_j\\) Compute \\(Z_m\\) in the same fashion as \\(Z_1\\) on the adjusted data As with PCR, \\(M\\) is chosen by cross-validation 68.3.2.0.1 Advantages Decreased variance of coefficient estimates relative to OLS regression Supervised dimension reduction may reduce bias 68.3.2.0.2 Disadvantages May increase variance relative to PCR (which is unsupervised). May be no better than PCR in practice 68.4 Considerations in High Dimensions 68.4.1 High-Dimensional Data Low dimensional means \\(p &lt;&lt; n\\), high dimensional is \\(p \\gtrsim n\\) 68.4.2 What Goes Wrong in High Dimensions? If \\(p \\gtrsim n\\), then linear models will create a perfect fit, hence overfit (usually badly) \\(C_p\\), \\(AIC\\), \\(BIC\\), and \\(R^2\\) approaches don’t work in well in this setting 68.4.3 Regression in High Dimensions Regularization or shrinkage plays a key role in high-dimensional problems. Appropriate tuning parameter selection is crucial for good predictive performance. The test error tends to increase as the dimensionality of the problem increases if the additional features aren’t truly associated with the response (the curse of dimensionality) 68.4.4 Interpreting Results in High Dimensions Multicollinearity problem is maximal in high dimensional setting This makes interpretation difficult, since models obtained from highly multicollinear data fail to identify which features are “preferred” Care must be taken to measure performance 53 68.5 Footnotes This is the model that predicts \\(\\hat{y} = \\overline{y}\\), i.e. \\(\\hat{\\beta_i} = 0\\) for \\(i &gt; 1\\) and \\(\\hat{\\beta_0} = \\overline{y}\\). ↩︎ Estimates of test error can come from CV, \\(C_p (AIC)\\), \\(BIC\\) or adjusted \\(R^2\\) ↩︎ Here \\(D(y, \\hat{y}) = -2\\log(p(y\\ |\\ \\hat{\\beta})\\) where \\(\\hat{\\beta}\\) is the MLE for \\(\\beta\\).The author’s definition of deviance can be found in the comment on the Wikipedia entry if \\(\\hat{\\theta}_0\\) ↩︎ As with BSS, we can use FSS for logistic regression by replacing \\(RSS\\) with the deviance in step 2B. ↩︎ As with BSS, we can use BackSS for logistic regression by replacing \\(RSS\\) with the deviance in step 2B. ↩︎ Here full means contains all \\(p\\) predictors. ↩︎ Thus \\(C_p\\) is RSS plus a penalty which depends on the number of predictors and the estimate of the error variance. One can show that if \\(\\hat{\\sigma}^2\\) is unbiased then then \\(C_p\\) is unbiased. ↩︎ For Gaussian errors, the least squares estimate is the maximumlikelihood estimate so in that case \\(C_p\\) and \\(AIC\\) are proportional. ↩︎ The BIC places a heavier penalty than \\(C_p\\) when \\(n &gt; 7\\) due to the \\(\\log(n)d\\hat{\\sigma}^2\\) term. The book says this means BIC places a heavier penalty than \\(C_p\\) on models with many variables although this isn’t clear. It would seem it places a penalty on large numbers of observation (unless somehow larger numbers of observations are correlated with larger numbers of predictors). ↩︎ \\(C_p, AIC\\) and \\(BIC\\) are all estimates of the test \\(MSE\\) so smaller values are better. By contrast, larger values of adjusted \\(R^2\\), but this is equivalent to minimizing \\(RSS/(n - d - 1)\\) which likely can be thought of as a test MSE estimate. Note that curiously, adjusted \\(R^2\\) is not defined when \\(d = n - 1\\). ↩︎ Here \\(L^2\\) is a reference to the \\(L^p\\) norm (denoted \\(\\| \\cdot \\|_2\\)) when \\(p=2\\) (see also p216), which is just the standard Euclidean norm. ↩︎ The tuning parameter \\(\\lambda\\) is actually a Lagrange multiplier used to turn the constrained optimization problem \\[ \\begin{align*} \\min&amp;\\ RSS\\\\ \\text{s.t.}&amp;\\ \\| \\tilde{\\beta} \\|_2^2 \\leqslant s \\end{align*} \\] into the unconstrained optimization problem \\[ \\begin{align*} \\min&amp;\\ RSS + \\lambda\\| \\tilde{\\beta} \\|_2^2 \\end{align*} \\] see this section ↩︎ We use \\(\\tilde{\\beta}\\) instead of \\(\\beta\\) because we don’t want to shrink the intercept \\(\\beta_0\\). If the data have been centered about their mean then \\(\\hat{\\beta}_0 = \\overline{y}\\) ↩︎ Flexibility decreases because the shrinkage penalty effectively decreases the size of the parameter space? ↩︎ The \\(L^1\\) norm is \\(\\|\\tilde{\\beta}\\|_1 = \\sum_{i=1}^p |\\beta_p|\\) ↩︎ Unsupervised since it only takes the predictors \\(\\mathbf{X}\\) and not the response \\(\\mathbf{Y}\\) as input. ↩︎ Under certain assumptions, PCA is an optimal dimension reduction method from an information theoretic perspective ↩︎ One can show that \\(\\hat{\\beta}_j \\sim \\text{corr}(Y, X_j)\\), so \\(Z_1\\) effectively weights the variables by correlation. The intuition is, that at each iteration, the residuals (hence the variable \\(Z_m\\)) contain information not accounted for by the previous variable \\(Z_{m - 1}\\). ↩︎ For example SSE, \\(p\\)-values, and \\(R^2\\) statistics from the training data are useless in this setting. Thus it is important to e.g. evaluate performance on an independent test set or use resampling methods ↩︎ 68.5.1 blah "],["chapter-7-moving-beyond-linearity.html", "Chapter 69 Chapter 7 - Moving Beyond Linearity 69.1 Lab", " Chapter 69 Chapter 7 - Moving Beyond Linearity Lab: 7.8.1 Polynomial Regression and Step Functions Lab: 7.8.2 Splines # %load ../standard_import.txt import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.preprocessing import PolynomialFeatures import statsmodels.api as sm import statsmodels.formula.api as smf from patsy import dmatrix %matplotlib inline plt.style.use(&#39;seaborn-white&#39;) 69.0.1 Load dataset Using write.csv in R, I exported the dataset from package ‘ISLR’ to a csv file. df = pd.read_csv(&#39;Data/Wage.csv&#39;) df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Unnamed: 0 year age sex maritl race education region jobclass health health_ins logwage wage 0 231655 2006 18 Male Never Married White &lt; HS Grad Middle Atlantic Industrial &lt;=Good No 4.318063 75.043154 1 86582 2004 24 Male Never Married White College Grad Middle Atlantic Information &gt;=Very Good No 4.255273 70.476020 2 161300 2003 45 Male Married White Some College Middle Atlantic Industrial &lt;=Good Yes 4.875061 130.982177 df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 3000 entries, 0 to 2999 Data columns (total 13 columns): Unnamed: 0 3000 non-null int64 year 3000 non-null int64 age 3000 non-null int64 sex 3000 non-null object maritl 3000 non-null object race 3000 non-null object education 3000 non-null object region 3000 non-null object jobclass 3000 non-null object health 3000 non-null object health_ins 3000 non-null object logwage 3000 non-null float64 wage 3000 non-null float64 dtypes: float64(2), int64(3), object(8) memory usage: 304.8+ KB 69.1 Lab 69.1.1 7.8.1 Polynomial Regression and Step Functions Create polynomials for ‘age.’ These correspond to those in R, when using raw=TRUE in poly() function. X1 = PolynomialFeatures(1).fit_transform(df.age.values.reshape(-1,1)) X2 = PolynomialFeatures(2).fit_transform(df.age.values.reshape(-1,1)) X3 = PolynomialFeatures(3).fit_transform(df.age.values.reshape(-1,1)) X4 = PolynomialFeatures(4).fit_transform(df.age.values.reshape(-1,1)) X5 = PolynomialFeatures(5).fit_transform(df.age.values.reshape(-1,1)) y = (df.wage &gt; 250).map({False:0, True:1}).values print(&#39;X4:\\n&#39;, X4[:5]) print(&#39;y:\\n&#39;, y[:5]) X4: [[1.000000e+00 1.800000e+01 3.240000e+02 5.832000e+03 1.049760e+05] [1.000000e+00 2.400000e+01 5.760000e+02 1.382400e+04 3.317760e+05] [1.000000e+00 4.500000e+01 2.025000e+03 9.112500e+04 4.100625e+06] [1.000000e+00 4.300000e+01 1.849000e+03 7.950700e+04 3.418801e+06] [1.000000e+00 5.000000e+01 2.500000e+03 1.250000e+05 6.250000e+06]] y: [0 0 0 0 0] 69.1.1.1 Linear regression model. (Degree 4) fit2 = sm.GLS(df.wage, X4).fit() fit2.summary().tables[1] coef std err t P&gt;|t| [0.025 0.975] const -184.1542 60.040 -3.067 0.002 -301.879 -66.430 x1 21.2455 5.887 3.609 0.000 9.703 32.788 x2 -0.5639 0.206 -2.736 0.006 -0.968 -0.160 x3 0.0068 0.003 2.221 0.026 0.001 0.013 x4 -3.204e-05 1.64e-05 -1.952 0.051 -6.42e-05 1.45e-07 Selecting a suitable degree for the polynomial of age. fit_1 = sm.GLS(df.wage, X1).fit() fit_2 = sm.GLS(df.wage, X2).fit() fit_3 = sm.GLS(df.wage, X3).fit() fit_4 = sm.GLS(df.wage, X4).fit() fit_5 = sm.GLS(df.wage, X5).fit() sm.stats.anova_lm(fit_1, fit_2, fit_3, fit_4, fit_5, typ=1) /Users/jordi/anaconda3/envs/jwv/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater return (self.a &lt; x) &amp; (x &lt; self.b) /Users/jordi/anaconda3/envs/jwv/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less return (self.a &lt; x) &amp; (x &lt; self.b) /Users/jordi/anaconda3/envs/jwv/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1821: RuntimeWarning: invalid value encountered in less_equal cond2 = cond0 &amp; (x &lt;= self.a) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } df_resid ssr df_diff ss_diff F Pr(&gt;F) 0 2998.0 5.022216e+06 0.0 NaN NaN NaN 1 2997.0 4.793430e+06 1.0 228786.010128 143.593107 2.363850e-32 2 2996.0 4.777674e+06 1.0 15755.693664 9.888756 1.679202e-03 3 2995.0 4.771604e+06 1.0 6070.152124 3.809813 5.104620e-02 4 2994.0 4.770322e+06 1.0 1282.563017 0.804976 3.696820e-01 The polynomial degree 4 seems best. X = X4 Scikit-learn implements a regularized logistic regression model particularly suitable for high dimensional data. Since we just have one feature (age) we use the GLM model from statsmodels. clf = sm.GLM(y, X, family=sm.families.Binomial(sm.families.links.logit)) res = clf.fit() Create array of test data. Transform to polynomial degree 4 and run prediction. age_grid = np.arange(df.age.min(), df.age.max()).reshape(-1,1) X_test = PolynomialFeatures(4).fit_transform(age_grid) pred = res.predict(X_test) 69.1.2 Figure 7.1 # creating plots fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5)) fig.suptitle(&#39;Degree-4 Polynomial&#39;, fontsize=14) # Scatter plot with polynomial regression line ax1.scatter(df.age, df.wage, facecolor=&#39;None&#39;, edgecolor=&#39;k&#39;, alpha=0.3) sns.regplot(df.age, df.wage, order = 4, truncate=True, scatter=False, ax=ax1) ax1.set_ylim(ymin=0) # Logistic regression showing Pr(wage&gt;250) for the age range. ax2.plot(age_grid, pred, color=&#39;b&#39;) # Rug plot showing the distribution of wage&gt;250 in the training data. # &#39;True&#39; on the top, &#39;False&#39; on the bottom. ax2.scatter(df.age, y/5, s=30, c=&#39;grey&#39;, marker=&#39;|&#39;, alpha=0.7) ax2.set_ylim(-0.01,0.21) ax2.set_xlabel(&#39;age&#39;) ax2.set_ylabel(&#39;Pr(wage&gt;250|age)&#39;); png 69.1.2.1 Step function df_cut, bins = pd.cut(df.age, 4, retbins=True, right=True) df_cut.value_counts(sort=False) (17.938, 33.5] 750 (33.5, 49.0] 1399 (49.0, 64.5] 779 (64.5, 80.0] 72 Name: age, dtype: int64 df_steps = pd.concat([df.age, df_cut, df.wage], keys=[&#39;age&#39;,&#39;age_cuts&#39;,&#39;wage&#39;], axis=1) df_steps.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age age_cuts wage 0 18 (17.938, 33.5] 75.043154 1 24 (17.938, 33.5] 70.476020 2 45 (33.5, 49.0] 130.982177 3 43 (33.5, 49.0] 154.685293 4 50 (49.0, 64.5] 75.043154 # Create dummy variables for the age groups df_steps_dummies = pd.get_dummies(df_steps[&#39;age_cuts&#39;]) # Statsmodels requires explicit adding of a constant (intercept) df_steps_dummies = sm.add_constant(df_steps_dummies) df_steps_dummies.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } const (17.938, 33.5] (33.5, 49.0] (49.0, 64.5] (64.5, 80.0] 0 1.0 1 0 0 0 1 1.0 1 0 0 0 2 1.0 0 1 0 0 3 1.0 0 1 0 0 4 1.0 0 0 1 0 # Using statsmodels because it has a more complete output for coefficients fit3 = sm.GLM(df_steps.wage, df_steps_dummies.drop(df_steps_dummies.columns[1], axis=1)).fit() fit3.summary().tables[1] coef std err z P&gt;|z| [0.025 0.975] const &lt;td&gt; 94.1584&lt;/td&gt; &lt;td&gt; 1.476&lt;/td&gt; &lt;td&gt; 63.790&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 91.265&lt;/td&gt; &lt;td&gt; 97.051&lt;/td&gt; (33.5, 49.0] 24.0535 1.829 13.148 0.000 20.468 27.639 (49.0, 64.5] 23.6646 2.068 11.443 0.000 19.611 27.718 (64.5, 80.0] 7.6406 4.987 1.532 0.126 -2.135 17.416 # Put the test data in the same bins as the training data. bin_mapping = np.digitize(age_grid.ravel(), bins) bin_mapping array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]) # Get dummies, drop first dummy category, add constant X_test2 = sm.add_constant(pd.get_dummies(bin_mapping).drop(1, axis=1)) X_test2.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } const 2 3 4 0 1.0 0 0 0 1 1.0 0 0 0 2 1.0 0 0 0 3 1.0 0 0 0 4 1.0 0 0 0 69.1.2.2 Linear Regression pred2 = fit3.predict(X_test2) 69.1.2.3 Logistic Regression clf2 = sm.GLM(y, df_steps_dummies.drop(df_steps_dummies.columns[1], axis=1), family=sm.families.Binomial(sm.families.links.logit)) res2 = clf2.fit() pred3 = res2.predict(X_test2) 69.1.3 Figure 7.2 # creating plots fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5)) fig.suptitle(&#39;Piecewise Constant&#39;, fontsize=14) # Scatter plot with polynomial regression line ax1.scatter(df.age, df.wage, facecolor=&#39;None&#39;, edgecolor=&#39;k&#39;, alpha=0.3) ax1.plot(age_grid, pred2, c=&#39;b&#39;) ax1.set_xlabel(&#39;age&#39;) ax1.set_ylabel(&#39;wage&#39;) ax1.set_ylim(ymin=0) # Logistic regression showing Pr(wage&gt;250) for the age range. ax2.plot(np.arange(df.age.min(), df.age.max()).reshape(-1,1), pred3, color=&#39;b&#39;) # Rug plot showing the distribution of wage&gt;250 in the training data. # &#39;True&#39; on the top, &#39;False&#39; on the bottom. ax2.scatter(df.age, y/5, s=30, c=&#39;grey&#39;, marker=&#39;|&#39;, alpha=0.7) ax2.set_ylim(-0.01,0.21) ax2.set_xlabel(&#39;age&#39;) ax2.set_ylabel(&#39;Pr(wage&gt;250|age)&#39;); png 69.1.4 7.8.2 Splines Using patsy to create non-linear transformations of the input data. See http://patsy.readthedocs.org/en/latest/ I have not found functions to create smoothing splines or GAMs or do local regression. 69.1.4.1 Cubic splines # Specifying 3 knots transformed_x = dmatrix(&quot;bs(df.age, knots=(25,40,60), degree=3, include_intercept=False)&quot;, {&quot;df.age&quot;: df.age}, return_type=&#39;dataframe&#39;) fit4 = sm.GLM(df.wage, transformed_x).fit() pred4 = fit4.predict(dmatrix(&quot;bs(age_grid, knots=(25,40,60), degree=3, include_intercept=False)&quot;, {&quot;age_grid&quot;: age_grid}, return_type=&#39;dataframe&#39;)) fit4.params Intercept 60.493714 bs(df.age, knots=(25, 40, 60), degree=3, include_intercept=False)[0] 3.980500 bs(df.age, knots=(25, 40, 60), degree=3, include_intercept=False)[1] 44.630980 bs(df.age, knots=(25, 40, 60), degree=3, include_intercept=False)[2] 62.838788 bs(df.age, knots=(25, 40, 60), degree=3, include_intercept=False)[3] 55.990830 bs(df.age, knots=(25, 40, 60), degree=3, include_intercept=False)[4] 50.688098 bs(df.age, knots=(25, 40, 60), degree=3, include_intercept=False)[5] 16.606142 dtype: float64 # Specifying 6 degrees of freedom transformed_x2 = dmatrix(&quot;bs(df.age, df=6, degree=3, include_intercept=False)&quot;, {&quot;df.age&quot;: df.age}, return_type=&#39;dataframe&#39;) fit5 = sm.GLM(df.wage, transformed_x2).fit() pred5 = fit5.predict(dmatrix(&quot;bs(age_grid, df=6, degree=3, include_intercept=False)&quot;, {&quot;age_grid&quot;: age_grid}, return_type=&#39;dataframe&#39;)) fit5.params Intercept 56.313841 bs(df.age, df=6, degree=3, include_intercept=False)[0] 27.824002 bs(df.age, df=6, degree=3, include_intercept=False)[1] 54.062546 bs(df.age, df=6, degree=3, include_intercept=False)[2] 65.828391 bs(df.age, df=6, degree=3, include_intercept=False)[3] 55.812734 bs(df.age, df=6, degree=3, include_intercept=False)[4] 72.131473 bs(df.age, df=6, degree=3, include_intercept=False)[5] 14.750876 dtype: float64 69.1.4.2 Natural splines # Specifying 4 degrees of freedom transformed_x3 = dmatrix(&quot;cr(df.age, df=4)&quot;, {&quot;df.age&quot;: df.age}, return_type=&#39;dataframe&#39;) fit6 = sm.GLM(df.wage, transformed_x3).fit() pred6 = fit6.predict(dmatrix(&quot;cr(age_grid, df=4)&quot;, {&quot;age_grid&quot;: age_grid}, return_type=&#39;dataframe&#39;)) fit6.params Intercept 79.642095 cr(df.age, df=4)[0] -14.667784 cr(df.age, df=4)[1] 36.811142 cr(df.age, df=4)[2] 35.934874 cr(df.age, df=4)[3] 21.563863 dtype: float64 plt.scatter(df.age, df.wage, facecolor=&#39;None&#39;, edgecolor=&#39;k&#39;, alpha=0.3) plt.plot(age_grid, pred4, color=&#39;b&#39;, label=&#39;Specifying three knots&#39;) plt.plot(age_grid, pred5, color=&#39;r&#39;, label=&#39;Specifying df=6&#39;) plt.plot(age_grid, pred6, color=&#39;g&#39;, label=&#39;Natural spline df=4&#39;) [plt.vlines(i , 0, 350, linestyles=&#39;dashed&#39;, lw=2, colors=&#39;b&#39;) for i in [25,40,60]] plt.legend(bbox_to_anchor=(1.5, 1.0)) plt.xlim(15,85) plt.ylim(0,350) plt.xlabel(&#39;age&#39;) plt.ylabel(&#39;wage&#39;); png Table of Contents 7 Moving Beyond Linearity 7.1 Polynomial Regression 7.2 Step Functions 7.3 Basis Functions 7.4 Regression Splines 7.4.1 Piecewise Polynomials 7.4.2 Constraints and Splines 7.4.3 The Spline Basis Representation 7.4.4 Choosing the Number and the Locations of the Knots 7.4.5 Comparison to Polynomial Regression 7.5 Smoothing Splines 7.5.1 An Overview of Smoothing Splines 7.5.2 Choosing the Smoothing Parameter \\(\\lambda\\) 7.6 Local Regression 7.7 Generalized Additive Models 7.7.1 GAMs for Regression Problems 7.7.2 GAMs for Classification Problems 7.8 Footnotes "],["moving-beyond-linearity.html", "Chapter 70 Moving Beyond Linearity 70.1 Polynomial Regression 70.2 Step Functions 70.3 Basis Functions 70.4 Regression Splines 70.5 Smoothing Splines 70.6 Local Regression 70.7 Generalized Additive Models 70.8 Footnotes", " Chapter 70 Moving Beyond Linearity 70.1 Polynomial Regression Simple polynomial regression is a regression model which is polynomial54 in the feature variable X \\[Y = \\beta_0 + \\sum_{i = 1}^d \\beta_iX^d\\] - The model can be fit as a simple linear regression model with predictors \\(X_1, \\dots, X_d = X, \\dots X^d\\). - It is rare to take \\(d \\geqslant 4\\) because it lead strange curves 70.1.0.0.1 Advantages Interpretability More flexibility than linear regression, can better model non-linear relationships 70.1.0.0.2 Disadvantages Greater flexibility can lead to overfitting (can be mitigating by keeping \\(d\\) low) Imposes global structure on target function (as does linear regression) 70.2 Step Functions Step functions model the target function as locally constant by converting the continuous variable \\(X\\) into an ordered categorical variable.as follows Choose \\(K\\) points \\(c_1, \\dots, c_K \\in [\\min(X), \\max(X)]\\) Define \\(K + 1\\) “dummy” variables \\[\\begin{align*} C_0(X) &amp;= I(X &lt; c_1)\\\\ C_i(X) &amp;= I(c_i \\leqslant X &lt; c_{i+1})\\qquad 1 \\leqslant i \\leqslant K - 1\\\\ C_K(X) &amp;= I(c_K \\leqslant X) \\end{align*}\\] Fit a linear regression model to the predictors \\(C_1, \\dots, C_K\\)55 70.2.0.0.1 Advantages Flexibility to model non-linear relationships Can model local behavior better than global models (e.g. linear and polynomial regression) 70.2.0.0.2 Disadvantages Locally constant assumption is strong, breakpoints in data may not be realized. 70.3 Basis Functions In general, we can fit a regression model \\[Y = \\beta_0 + \\sum_{i=1}^Kb_i(X)\\] where the \\(b_i(X)\\) are called basis functions 56 70.3.0.0.1 Advantages Different choices of basis functions are useful for modeling different types of relationships (for example, Fourier basis functions can model periodic behavior). 70.3.0.0.2 Disadvantages As usual, greater flexibility can lead to overfitting Some choices of basis functions (i.e. basis functions which are not suited to the assumed true functional relationship) will likely have poor performance. 70.4 Regression Splines Regression splines are a flexible (and common choice of) class of basis functions which extend both polynomial and piecewise constant basis functions. 70.4.1 Piecewise Polynomials Piecewise polynomials fit separate low-degree polynomials over different regions of \\(X\\). The points where the coefficients change are called knots. 70.4.1.0.1 Advantages Flexibility to model non-linear relationships (as with all non-linear methods discussed in this chapter) Sensitivity to local behavior (less rigid than global model). 70.4.1.0.2 Disadvantages Overly flexible - each piece has independent degrees of freedom Can have unnatural breaks at knots without appropriate constraints Possibility of overfitting (as with all non-linear methods discussed in this chapter) 70.4.2 Constraints and Splines To remedy overflexibility of piecewise polynomials, we can impose constraints at the knots, e.g. continuity, differentiability of various orders (smoothness). A spline is a piecewise degree \\(d\\) polynomial that has continuous derivatives up to order \\(d-1\\) at each knot (hence everywhere). 70.4.2.0.1 Advantages Same advantages to piecewise polynomials, while improving on the disadvantages 70.4.2.0.2 Disadvantages Overfitting Poor match to the true relationship 70.4.3 The Spline Basis Representation Regression splines can be modeled using an appropriate basis, of which there are many choices. For example, we can model a \\(d\\) degree spline with \\(K\\) knots using truncated power basis \\[b_1(X), \\dots, b_{K+d}(X) = x, \\dots, x^d, h(X, \\xi_1), \\dots, h(X, \\xi_K)\\] where \\(\\xi_i\\) is the \\(i-th\\) knot and \\[h(X - \\xi_i) = \\begin{cases} (X-\\xi_i)^d &amp; X &gt; \\xi_i\\\\ 0 &amp; X \\leqslant \\xi_i \\end{cases}\\] is the truncated power function of degree \\(d\\). 70.4.3.0.1 Advantages Ibid. 70.4.3.0.2 Disadvantages Beyond those mentioned above, splines can have a high variance near \\(\\min(X), \\max(X)\\) (this can be overcome by using natural splines which impose boundary constraints, i.e constraints on the form of the model on \\([\\min(X), \\xi_1]\\), \\([\\max(X), \\xi_K]\\) (e.g. linearity) 70.4.4 Choosing the Number and the Locations of the Knots In practice, we place knots in uniform fashion, e.g. by specifying the desired degrees of freedom and using software to place the knots at uniform quantiles of the data. The desired degrees of freedom (hence number of knots) can be obtained using cross-validation. 70.4.5 Comparison to Polynomial Regression Often gives superior results to polynomial regression – the latter must use higher degrees (imposing global structure) while the former can increase the number of knots while leaving the degree fixed (sensitivity to local behavior) as well as varying the density of knots (i.e. placing more where the response varies rapidly, less where it is more stable) 70.5 Smoothing Splines 70.5.1 An Overview of Smoothing Splines A smoothing spline 57 is a function \\[\\hat{g}_\\lambda = \\underset{g}{\\text{argmin}\\,}\\sum_{i=1}^n(y_i - g(x_i))^2 + \\lambda \\int g&#39;&#39;(t)^2\\,dt\\] where \\(\\lambda = 0\\) is a tuning parameter58 - \\(\\lambda\\) controls the bias-variance tradeoff. \\(\\lambda = 0\\) corresponds to the interpolation spline which fits all the data points exactly and will be thus woefull overfit. In the limit \\(\\lambda \\rightarrow \\infty\\), \\(\\hat{g}_\\lambda\\) approaches the least squares line - It can be show that the function \\(\\hat{g}_\\lambda\\) is a piecewise cubic polynomial with knots at the unique \\(x_i\\) and continuous first and second derivatives at the knots 59 70.5.2 Choosing the Smoothing Parameter \\(\\lambda\\) The parameter \\(\\lambda\\) controls the effective degrees of freedom \\(df_{\\lambda}\\). As \\(\\lambda\\) goes from $0 $ to \\(\\infty\\), \\(df_\\lambda\\) goes from \\(n\\) to \\(2\\). The effective degress of freedom is defined to be \\[df_\\lambda = \\text{trace}(S_\\lambda)\\] where \\(S_\\lambda\\) is the matrix such that \\(\\mathbf{\\hat{g}}_\\lambda = S_\\lambda \\mathbf{y}\\) where \\(\\mathbf{\\hat{g}}\\) is the vector of fitted values. \\(\\lambda\\) can be chosen by cross-validation. LOOCV is particularly efficient to compute 60 \\[RSS_{cv}(\\lambda) = \\sum_{i=1}^n (y_i - \\hat{g}_\\lambda^{(-i)}(x_i))^2 = \\sum_{i=1}^n\\left(\\frac{y_i - \\hat{g}_\\lambda(x_i)}{1-tr(S_{\\lambda})}\\right)^2 \\] 70.5.2.0.1 Advantages Flexibility/nonlinearity As a shrinkage method, effective degrees of freedom are reduced, helping to balance bias-variance tradeoff and avoid overfitting. 70.5.2.0.2 Disadvantages As usual, flexibility can lead to overfitting 70.6 Local Regression Computes the fit at a target point by regressing on nearby training observations Is memory-based - all the training data is necessary for computing a prediction In multiple linear regression, variable coefficient models fit global regression to some variables and local to others 70.6.0.0.1 Algorithm: \\(K\\)-nearest neighbors regression Fix the parameter61 \\(1 \\leqslant k \\leqslant n\\). For each \\(X_=x_0\\): 1. Get the neighborhood \\(N_{i0}= \\{k\\ \\text{closest}\\ x_i\\}\\). 2. Assign a weight \\(K_{i0} = K(x_i, x_0)\\) to each point \\(x_i\\) such that such that - each point outside \\(x_i\\notin N_{i0}\\) has \\(K_{i0}(x_i)=0\\). - the furthest point \\(x_i\\in N_{i0}\\) has weight zero - the closest point \\(x_i\\in N_{i0}\\) has the highest weight. 3. Fit a weighted least squares regression \\[ (\\hat{\\beta_0}, \\hat{\\beta_1}) = \\sum_{i=1}^nK_{i0}(y_i - \\beta_0 - \\beta_1 x_i)^2\\] 4. Predict \\(\\hat{f}(x_0) = \\hat{\\beta_0} + \\hat{\\beta_1}x_0\\). 70.7 Generalized Additive Models A Generalized additive model is a model which is a sum of nonlinear functions of the individual predictors. 70.7.1 GAMs for Regression Problems A GAM for regression 62 is a model \\[Y =\\beta_0 + \\sum_{j=1}^p f_j(X_j) + \\epsilon\\] where the functions \\(f_j\\) are smooth non-linear functions. GAMs can be used to combine methods from this chapter – one can fit different nonlinear functions \\(f_j\\) to the predictors \\(X_j\\) 63 Standard software can fit GAMs with smoothing splines via backfitting 70.7.1.0.1 Advantages Nonlinearity hence flexibility Automatically introduces nonlinearity - obviates the need to experiment with different nonlinear transformations Interpretability/inference - the \\(f_j\\) allow to consider the effect of each feature \\(X_j\\) independently of the others. Smoothness of individual \\(f_j\\) can be summarized via degrees of freedom. Represents a nice compromise betwee linear and fully non-parametric models (see §8). 70.7.1.0.2 Disadvantages Usual disadvantages of nonlinearity Doesn’t allow for interactions between features (this can be overcome by including nonlinear functios of the interaction terms \\(f(X_j,X_k)\\) The additive constraint is strong, restricts flexibility. 70.7.2 GAMs for Classification Problems GAMs can be used for classification. For example, a GAM for logistic regression is \\[\\log\\left(\\frac{p_k(X)}{1 - p_k(X)}\\right) =\\beta_0 + \\sum_{j=1}^p f_j(X_j) + \\epsilon\\] where \\(p_k(X) =\\text{Pr}(Y = k\\ |\\ X)\\). 70.8 Footnotes In statistical literature, polynomial regression is sometimes referred to as linear regression. This is because the model is linear in the population parameters \\(\\beta_i\\). ↩︎ The variable \\(C_0(X)\\) accounts for an intercept. Alternatively fit a linear model to \\(C_0, \\dots, C_K\\) with no intercept. ↩︎ Such a model amounts to the assumption that the target function lives in a finite-dimensional subspace of the vector space of all functions \\(f:X\\rightarrow Y\\). ↩︎ The function \\(g\\) is not guaranteed to be smooth in the sense of infinitely differentiable. The penalty on the second derivative (curvature) penalizes the “roughness” or “wiggliness” of \\(g\\), hence “smoothes out” noise in the data. Other penalties have been used ↩︎ A tuning parameter is also called a hyperparameter ↩︎ Thus \\(\\hat{g}\\) is a natural cubic spline with knots at the \\(x_i\\). However, it is not the spline one obtains in §7.4.3. It is a “shrunken” version, where \\(\\lambda\\) controls the shrinkage. ↩︎ Compare to a similar formula in §5.1.2 ↩︎ Our description of the algorithm deviates a bit from the book, but it’s equivalent. ↩︎ “Additive” because we are summing the \\(f_i\\). “Generalized” because it generalizes from the linear functions \\(\\beta_jX_j\\) in ordinary linear regression. ↩︎ It’s not hard to see that (with the exception of local regression), all the models discussed in this chapter can be seen as special cases of GAM. ↩︎ Table of Contents 8 Tree-Based Methods 8.1 The Basics of Decision Trees 8.1.1 Regression Trees 8.1.2 Classification Trees 8.1.3 Trees Versus Linear Models 8.1.4 Advantages and Disadvantages of Trees 8.2 Bagging, Random Forests, Boosting 8.2.1 Bagging 8.2.2 Random Forests 8.2.3 Boosting 8.3 Footnotes 8.3.1 blah "],["tree-based-methods.html", "Chapter 71 Tree-Based Methods 71.1 The Basics of Decision Trees 71.2 Bagging, Random Forests, Boosting 71.3 Footnotes", " Chapter 71 Tree-Based Methods 71.1 The Basics of Decision Trees 71.1.1 Regression Trees 71.1.1.0.1 Overview There are two main steps: Partition predictor space \\(\\mathbb{R}^p\\) into regions \\(R_1, \\dots, R_M\\). For all \\(X = (X_1, \\dots, X_p) \\in R_m\\), predict the average over the responses in \\(R_m\\) \\[\\hat{f}(X) = \\hat{y}_{R_m} := \\frac{1}{N_m}\\sum_{i: y_i\\in R_m} y_i\\] where \\(N_m = |\\{y_i\\ |\\ y_i \\in R_m\\}|\\) In practice, we take the regions of the partition to be rectangular for simplicity and ease of interpretation. We choose the partition to minimize the RSS \\[ \\sum_{m = 1}^M \\sum_{i: y_i \\in R_m} (y_i - \\hat{y}_{R_m})^2 \\] We search the space of partitions using a recursive binary splitting64 strategy. 71.1.1.0.2 Algorithm: Recursive Binary Decision Tree for Linear Regression Start with top node \\(\\mathbb{R}^p\\) While a stopping criterion is unsatisfied: Let \\[(\\hat{i}, \\hat{j}) = \\underset{(i, j)}{\\text{argmin}}\\left( \\sum_{i: x_i\\in R_1} (y_i - \\hat{y}_{R_1})^ 2 + \\sum_{i: x_i\\in R_2} (y_i - \\hat{y}_{R_2})^ 2\\right)\\] where \\[R_{1} = \\{X| X_j &lt; x_{i,j}\\}\\] \\[R_{2} = \\{X| X_j \\geqslant x_{i,j}\\}\\] Add nodes \\[\\hat{R}_{1} = \\{X| X_\\hat{j} &lt; x_{\\hat{i},\\hat{j}}\\}\\] \\[\\hat{R}_{2} = \\{X| X_\\hat{j} \\geqslant x_{\\hat{i},\\hat{j}}\\}\\] to the partition, and recurse on one of the nodes 71.1.1.0.3 Tree-pruning Complex trees can overfit, but simpler trees may avoid it 65. To get a simpler tree, we can grow a large tree \\(T_0\\) and prune it to obtain a subtree. Cost complexity or weakest link pruning is a method for finding an optimal subtree 66. For \\(\\alpha &gt; 0\\), we obtain a subtree \\[ T_\\alpha = \\underset{T\\ \\subset T_0}{\\text{argmin}} \\left(\\sum_{m=1}^{|T|} \\sum_{i: x_i \\in R_m} \\left(y_i - \\hat{y}_{R_m}\\right)^2 + \\alpha|T|\\right)\\] where \\(|T|\\) is the number of terminal nodes of \\(T\\), \\(R_m\\) is the rectangle corresponding to the \\(m\\)-th terminal node, and \\(\\hat{y}_{R_m}\\) is the predicted response (average of \\(y_i\\in R_m\\)) 67. 71.1.1.0.4 Algorithm: Weakest link regression tree with \\(K\\)-fold cross-validation For each68 \\(\\alpha &gt; 0\\): For \\(k = 1, \\dots K\\): Let \\(\\mathcal{D}_k = \\mathcal{D} \\backslash \\{k-\\text{th fold}\\}\\) Use recursive binary splitting to grow a tree \\(T_{k}\\), stopping when each node has fewer than some minimum number of observations \\(M\\) is reached 69 Use weakest link pruning to find a subtree \\(T_{k, \\alpha}\\) Let \\(CV_{(k)}(\\alpha)\\) be the \\(K\\)-fold cross-validation estimate of the mean squared test error Choose \\[ \\hat{\\alpha} = \\underset{\\alpha}{\\text{argmin}}\\ CV_{(k)}(\\alpha) \\] Return \\(\\hat{T} = T_{\\hat{\\alpha}}\\) 71.1.2 Classification Trees Classification trees are very similar to regression trees, but they predict qualitative responses. The predicted class for an observation \\((x_i, y_i)\\) in \\(R_m\\) is 70 is \\[ \\hat{k}_m = \\underset{k}{\\text{argmax}}\\ \\hat{p}_{m,k} \\] where \\(\\hat{p}_{m,k}\\) is the fraction of observations \\((x_i, y_i)\\) in the region \\(R_m\\) such that \\(y_i = k\\). One performance measure is the Classification error rate71 for the region \\(R_m\\) is \\[ E_m = 1 - \\hat{p}_{m, \\hat{k}} \\] A better performance measure is the Gini index for the region \\(R_m\\), a measure of total variance 72 across classes \\[ G_m = \\sum_{k = 1}^K \\hat{p}_{m,k}(1 - \\hat{p}_{m,k})\\] Another better performance measure is the entropy for the region \\(R_m\\)73 \\[ D_m = \\sum_{k = 1}^K - \\hat{p}_{m,k}\\log(\\hat{p}_{m,k}) \\] Typically the Gini index or entropy is used to prune, due to their sensitivity to node purity. However, if prediction accuracy is the goal then classification error rate is preferable. 71.1.3 Trees Versus Linear Models A linear regression model is of the form \\[f(X) = \\beta_0 + \\sum_{j = 1}^p \\beta_j X_j\\] while a regression tree model is of the form \\[ f(X) = \\sum_{m = 1}^M c_m I(X \\in R_m)\\] Linear regression will tend to perform better if the relationship between features and response is well-approximated by a linear function, whereas the regression tree will tend perform better if the relationship is non-linear or complex. 71.1.4 Advantages and Disadvantages of Trees 71.1.4.0.1 Advantages Conceptual simplicity May mirror human decision-making better than previous regression and classification methods Readily visualizable and easily interpreted Can handle qualitative predictors without the need for dummy variables 71.1.4.0.2 Disadvantages Less accurate prediction than previous regression and classification methods Non-robust to changes in data – small changes in data lead to large changes in estimated tree. 71.2 Bagging, Random Forests, Boosting These are methods for improving the prediction accuracy of decision trees. 71.2.1 Bagging The decision trees in § 8.1 suffer from high variance. Bagging is a method of reducing the variance of a statistical learning process 74. The bagging estimate of the target function of the process with dataset \\(\\mathcal{D}\\) is \\[\\hat{f}_{\\text{bag}}(x) = \\frac{1}{B} \\sum_{b = 1}^B \\hat{f}^{*b}(x) \\] where \\(\\hat{f^*}^b(x)\\) is the estimate of target function on the boostrap dataset \\(\\mathcal{D}_b\\) 75. Bagging can be used for any statistical learning method but it is particularly useful for decision trees.76 71.2.1.0.1 Out-of-bag Error Estimation On average, a bagged tree uses about 2/3 of the data – the remaining 1/3 is the out-of-bag (OOB) data. We can predict the response for each observation using the trees for which it was OOB, yielding about B/3 prediction. If we’re doing regression, we can average these predicted responses, or if we’re doing classification, take a majority vote, to get a single OOB prediction for each observation. Test error can be estimated using these predictions. 71.2.1.0.2 Variable Importance Measures Bagging typically results in improved prediction accuracy over single trees, at the expense of interpretability The RSS (for bagging regression trees) and Gini index (for bagging classification trees) can provide measures of variable importance. For both loss functions (RSS/Gini) the amount the loss is decreases due to a split over a given predictor, averaged over the B bagged trees. The greater the decrease, the more important the predictor 71.2.2 Random Forests Random forests works as follows: at each split in the tree, choose a predictor from among a new random sample of \\(1 \\leqslant m \\leqslant p\\) predictors. The random predictor sampling overcomes the tendency of bagged trees to look similar given strong predictors (e.g. the strongest predictor will be at the top of most or all of the bagged trees). On average, \\(\\frac{p-m}{p}\\) of the splits will not consider a given predictor, giving other predictors a chance to be chosen. This decorrelation of the trees improves the reduction in variance achieved by bagged. \\(m=p\\) corresponds to bagging. \\(m &lt;&lt; p\\) is useful when there is a large number of correlated predictors. Typically we choose \\(m \\approx \\sqrt{p}\\) 71.2.3 Boosting Boosting is another method of improving prediction accuracy that can be applied to many statistical learning methods. In decision trees, each tree is build using information from the previous trees. Instead of bootstrapped datasets, the datasets are modified based on the previously grown trees. The boosting approach learns slowly, by slowly improving in areas where it underperforms. It has 3 parameters: Number of trees \\(B\\). Unlike bagging and random forests, boosting can overfit if \\(B\\) is too big, although this happens slowly. The shrinkage parameter \\(\\lambda &gt; 0\\). Typical values are \\(\\lambda = 0.01, 0.001\\). Very small \\(\\lambda\\) can require very large \\(B\\) to get good performance. The number of tree splits \\(d\\), which controls the complexity of the boosted ensemble. Often \\(d\\) works well (the resulting tree is called a stump). 77 71.2.3.0.1 Algorithm: Boosting for Regression Trees Set \\(\\hat{f}(x) = 0\\) and \\(r_i = y_i\\), \\(1 \\leqslant i \\leqslant n\\) For \\(b = 1, 2, \\dots, B\\): Fit a tree \\(\\hat{f}^b\\) with \\(d\\) splits to \\((X, r)\\) Update the model \\(\\hat{f}\\) by adding a shrunk version of the new tree: \\[ \\hat{f}(x) \\leftarrow \\hat{f}(x) + \\lambda \\hat{f}^b(x)\\] Update the residuals: \\[ r_i \\leftarrow r_i - \\lambda \\hat{f}^b(x)\\] Output the boosted model \\[ \\hat{f}(x) = \\sum_{b = 1}^B \\lambda \\hat{f}^b(x)\\] 71.3 Footnotes This strategy results in a binary tree with the partition regions as leaves, and binary splits as nodes. It is “top-down” because it starts at the top of the partition tree (with a single region), “binary” because it splits the predictor space into two regions at each node in the tree, “recursive” because it calls itself at each node, and “greedy” because at each node, it chooses the optimal split at that node. ↩︎ That is, it may lead to lower variance and better interpretation at the cost of a higher bias ↩︎ We want a subtree with minimal estimated test error but it’s infeasible to compute this for all subtrees. ↩︎ This is the RSS for the partition given by the nodes of the tree \\(T\\), with a weighted penalty \\(\\alpha|T|\\) for the number of nodes (hence the complexity). ↩︎ Even though \\(\\alpha \\in [0, \\infty)\\) is a continuous parameter here, in practice it will be selected from a finite set of values. In fact (cf. comment on pg 309 of the text), as \\(\\alpha\\) increases,“branches get pruned in a nested and predictable fashion,” resulting in a sequence of subtrees as a function of \\(\\alpha\\). One can then find a sequence \\(\\alpha_1, \\dots, \\alpha_N\\) such that at each \\(\\alpha_i\\), a branch is removed, and since the tree is finite, the algorithm is guaranteed to terminate. ↩︎ The smallest possible number of observations per node is \\(M=1\\), which results in a partition with only one point in each region. This is clearly a maximal complexity tree, so we probably take \\(M &gt;&gt; 1\\) in practice. ↩︎ That is, the predicted class for observations in \\(R_m\\) is the most frequently occuring class in \\(R_m\\). ↩︎ The classification error rate isn’t sufficiently sensitive to “node purity,” that is degree to which a node contains observations from a single class. ↩︎ The Gini index is a measure of “node purity” – it is minimized when all \\(\\hat{p}_{m, k} \\in \\{0, 1\\}\\), that is, when all nodes contain observations from a single class. ↩︎ The \\(\\hat{p}_{m,k}\\) are the empirical pmf estimates of the conditional probabilities \\(p_{m, k} = P(Y = k | X \\in R_m)\\), so \\(D\\) is an estimate of the conditional entropy, i.e. the entropy of \\(Y\\ |\\ X \\in R_m\\). Thus \\(D\\) is a measure of information that the empirical pmf, and hence the corresponding tree provides, that is, of its average suprisal. As with the Gini index, \\(D\\) is minimized when all \\(\\hat{p}_{m, k} \\in \\{0, 1\\}\\). An average surprisal of zero means the tree provides all information, that is, it perfectly separates the classes. ↩︎ Bagging is another name for bootstrapping. It appears that the latter is usually used in the context of estimating the standard error of a statistic, while the former is used in the context of a statistical learning process (even though these are essentially the same). ↩︎ Really this is the bootstrap estimate of the average of the target function estimate over many datasets. For a given dataset \\(\\mathcal{D}\\), the function \\(\\hat{f}(x)\\) produced by the learning process is an estimate of the target function \\(f(x)\\). Repeating the process \\(1 \\leqslant b \\leqslant B\\) times over datasets \\(\\mathcal{D}_b\\), we get estimates \\(\\hat{f}^b(x)\\). Assuming these are iid, they have common variance \\(\\sigma^2\\), but their average \\[\\hat{f}_{\\text{avg}} = \\frac{1}{B} \\sum_{b = 1}^B \\hat{f}^{b}(x)\\] has variance \\(\\frac{\\sigma^2}{B}\\). Given \\(B\\) large enough, this variance is low. Bagging/bootstrapping gets around the lack of separate datasets \\(\\mathcal{D}_b\\) in practice by repeated sampling with replacement from a single dataset \\(\\mathcal{D}\\). ↩︎ For regression, one grows \\(B\\) deep (unpruned) regression trees on \\(B\\) bootstrapped datasets, each of which has low bias but high variance, then averages them to get a bootstrap estimate which has the same low bias, but much lower variance. For classification (since we can’t average over the classes of the bootstrapped trees) a simple approach is to predict the majority class over the bootstrapped trees. ↩︎ In the case of a stump, the boosted ensemble is fitting an additive model, since each term is a single variable. More generally, \\(d\\) is the interaction depth – since \\(d\\) splits can involve at most \\(d\\) variables, this controls the interaction order of the boosted model (i.e. the model can fit interaction terms up to degree \\(d\\)). ↩︎ 71.3.1 blah "],["chapter-8-tree-based-methods.html", "Chapter 72 Chapter 8 - Tree-based Methods 72.1 Lab", " Chapter 72 Chapter 8 - Tree-based Methods 8.1.1 Regression Trees 8.1.2 Classification Trees Lab: 8.3.1 Fitting Classification Trees Lab: 8.3.2 Fitting Regression Trees Lab: 8.3.3 Bagging and Random Forests Lab: 8.3.4 Boosting # %load ../standard_import.txt import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import pydot from IPython.display import Image from sklearn.model_selection import train_test_split, cross_val_score from sklearn.externals.six import StringIO from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, export_graphviz from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor from sklearn.metrics import mean_squared_error,confusion_matrix, classification_report %matplotlib inline plt.style.use(&#39;seaborn-white&#39;) # This function creates images of tree models using pydot def print_tree(estimator, features, class_names=None, filled=True): tree = estimator names = features color = filled classn = class_names dot_data = StringIO() export_graphviz(estimator, out_file=dot_data, feature_names=features, class_names=classn, filled=filled) graph = pydot.graph_from_dot_data(dot_data.getvalue()) return(graph) 72.0.1 8.1.1 Regression Trees In R, I exported the dataset from package ‘ISLR’ to a csv file. df = pd.read_csv(&#39;Data/Hitters.csv&#39;).dropna() df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 263 entries, 1 to 321 Data columns (total 21 columns): Unnamed: 0 263 non-null object AtBat 263 non-null int64 Hits 263 non-null int64 HmRun 263 non-null int64 Runs 263 non-null int64 RBI 263 non-null int64 Walks 263 non-null int64 Years 263 non-null int64 CAtBat 263 non-null int64 CHits 263 non-null int64 CHmRun 263 non-null int64 CRuns 263 non-null int64 CRBI 263 non-null int64 CWalks 263 non-null int64 League 263 non-null object Division 263 non-null object PutOuts 263 non-null int64 Assists 263 non-null int64 Errors 263 non-null int64 Salary 263 non-null float64 NewLeague 263 non-null object dtypes: float64(1), int64(16), object(4) memory usage: 45.2+ KB X = df[[&#39;Years&#39;, &#39;Hits&#39;]].as_matrix() y = np.log(df.Salary.as_matrix()) fig, (ax1, ax2) = plt.subplots(1,2, figsize=(11,4)) ax1.hist(df.Salary.as_matrix()) ax1.set_xlabel(&#39;Salary&#39;) ax2.hist(y) ax2.set_xlabel(&#39;Log(Salary)&#39;); png regr = DecisionTreeRegressor(max_leaf_nodes=3) regr.fit(X, y) DecisionTreeRegressor(criterion=&#39;mse&#39;, max_depth=None, max_features=None, max_leaf_nodes=3, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter=&#39;best&#39;) 72.0.2 Figure 8.1 graph, = print_tree(regr, features=[&#39;Years&#39;, &#39;Hits&#39;]) Image(graph.create_png()) png 72.0.3 Figure 8.2 df.plot(&#39;Years&#39;, &#39;Hits&#39;, kind=&#39;scatter&#39;, color=&#39;orange&#39;, figsize=(7,6)) plt.xlim(0,25) plt.ylim(ymin=-5) plt.xticks([1, 4.5, 24]) plt.yticks([1, 117.5, 238]) plt.vlines(4.5, ymin=-5, ymax=250) plt.hlines(117.5, xmin=4.5, xmax=25) plt.annotate(&#39;R1&#39;, xy=(2,117.5), fontsize=&#39;xx-large&#39;) plt.annotate(&#39;R2&#39;, xy=(11,60), fontsize=&#39;xx-large&#39;) plt.annotate(&#39;R3&#39;, xy=(11,170), fontsize=&#39;xx-large&#39;); png 72.0.4 Pruning This is currently not supported in scikit-learn. See first point under ’disadvantages of decision trees in the documentation. Implementation has been discussed but Random Forests have better predictive qualities than a single pruned tree anyway if I understand correctly. 72.0.5 8.1.2 Classification Trees Dataset available on http://www-bcf.usc.edu/~gareth/ISL/data.html df2 = pd.read_csv(&#39;Data/Heart.csv&#39;).drop(&#39;Unnamed: 0&#39;, axis=1).dropna() df2.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 297 entries, 0 to 301 Data columns (total 14 columns): Age 297 non-null int64 Sex 297 non-null int64 ChestPain 297 non-null object RestBP 297 non-null int64 Chol 297 non-null int64 Fbs 297 non-null int64 RestECG 297 non-null int64 MaxHR 297 non-null int64 ExAng 297 non-null int64 Oldpeak 297 non-null float64 Slope 297 non-null int64 Ca 297 non-null float64 Thal 297 non-null object AHD 297 non-null object dtypes: float64(2), int64(9), object(3) memory usage: 34.8+ KB df2.ChestPain = pd.factorize(df2.ChestPain)[0] df2.Thal = pd.factorize(df2.Thal)[0] X2 = df2.drop(&#39;AHD&#39;, axis=1) y2 = pd.factorize(df2.AHD)[0] clf = DecisionTreeClassifier(max_depth=None, max_leaf_nodes=6, max_features=3) clf.fit(X2,y2) DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=3, max_leaf_nodes=6, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter=&#39;best&#39;) clf.score(X2,y2) 0.82491582491582494 graph2, = print_tree(clf, features=X2.columns, class_names=[&#39;No&#39;, &#39;Yes&#39;]) Image(graph2.create_png()) png 72.1 Lab 72.1.1 8.3.1 Fitting Classification Trees In R, I exported the dataset from package ‘ISLR’ to a csv file. df3 = pd.read_csv(&#39;Data/Carseats.csv&#39;).drop(&#39;Unnamed: 0&#39;, axis=1) df3.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Sales CompPrice Income Advertising Population Price ShelveLoc Age Education Urban US 0 9.50 138 73 11 276 120 Bad 42 17 Yes Yes 1 11.22 111 48 16 260 83 Good 65 10 Yes Yes 2 10.06 113 35 10 269 80 Medium 59 12 Yes Yes 3 7.40 117 100 4 466 97 Medium 55 14 Yes Yes 4 4.15 141 64 3 340 128 Bad 38 13 Yes No df3[&#39;High&#39;] = df3.Sales.map(lambda x: 1 if x&gt;8 else 0) df3.ShelveLoc = pd.factorize(df3.ShelveLoc)[0] df3.Urban = df3.Urban.map({&#39;No&#39;:0, &#39;Yes&#39;:1}) df3.US = df3.US.map({&#39;No&#39;:0, &#39;Yes&#39;:1}) df3.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 400 entries, 0 to 399 Data columns (total 12 columns): Sales 400 non-null float64 CompPrice 400 non-null int64 Income 400 non-null int64 Advertising 400 non-null int64 Population 400 non-null int64 Price 400 non-null int64 ShelveLoc 400 non-null int64 Age 400 non-null int64 Education 400 non-null int64 Urban 400 non-null int64 US 400 non-null int64 High 400 non-null int64 dtypes: float64(1), int64(11) memory usage: 37.6 KB df3.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Sales CompPrice Income Advertising Population Price ShelveLoc Age Education Urban US High 0 9.50 138 73 11 276 120 0 42 17 1 1 1 1 11.22 111 48 16 260 83 1 65 10 1 1 1 2 10.06 113 35 10 269 80 2 59 12 1 1 1 3 7.40 117 100 4 466 97 2 55 14 1 1 0 4 4.15 141 64 3 340 128 0 38 13 1 0 0 X = df3.drop([&#39;Sales&#39;, &#39;High&#39;], axis=1) y = df3.High X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0) clf = DecisionTreeClassifier(max_depth=6) clf.fit(X, y) DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=6, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter=&#39;best&#39;) print(classification_report(y, clf.predict(X))) precision recall f1-score support 0 0.89 0.99 0.93 236 1 0.98 0.82 0.89 164 avg / total 0.92 0.92 0.92 400 graph3, = print_tree(clf, features=X.columns, class_names=[&#39;No&#39;, &#39;Yes&#39;]) Image(graph3.create_png()) png clf.fit(X_train, y_train) pred = clf.predict(X_test) cm = pd.DataFrame(confusion_matrix(y_test, pred).T, index=[&#39;No&#39;, &#39;Yes&#39;], columns=[&#39;No&#39;, &#39;Yes&#39;]) cm.index.name = &#39;Predicted&#39; cm.columns.name = &#39;True&#39; cm .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } True No Yes Predicted No 100 31 Yes 18 51 # Precision of the model using test data is 74% print(classification_report(y_test, pred)) precision recall f1-score support 0 0.76 0.85 0.80 118 1 0.74 0.62 0.68 82 avg / total 0.75 0.76 0.75 200 Pruning not implemented in scikit-learn. 72.1.2 8.3.2 Fitting Regression Trees In R, I exported the dataset from package ‘MASS’ to a csv file. boston_df = pd.read_csv(&#39;Data/Boston.csv&#39;) boston_df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 506 entries, 0 to 505 Data columns (total 14 columns): crim 506 non-null float64 zn 506 non-null float64 indus 506 non-null float64 chas 506 non-null int64 nox 506 non-null float64 rm 506 non-null float64 age 506 non-null float64 dis 506 non-null float64 rad 506 non-null int64 tax 506 non-null int64 ptratio 506 non-null float64 black 506 non-null float64 lstat 506 non-null float64 medv 506 non-null float64 dtypes: float64(11), int64(3) memory usage: 55.4 KB X = boston_df.drop(&#39;medv&#39;, axis=1) y = boston_df.medv X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0) # Pruning not supported. Choosing max depth 3) regr2 = DecisionTreeRegressor(max_depth=3) regr2.fit(X_train, y_train) pred = regr2.predict(X_test) graph, = print_tree(regr2, features=X.columns) Image(graph.create_png()) png plt.scatter(pred, y_test, label=&#39;medv&#39;) plt.plot([0, 1], [0, 1], &#39;--k&#39;, transform=plt.gca().transAxes) plt.xlabel(&#39;pred&#39;) plt.ylabel(&#39;y_test&#39;) Text(0,0.5,&#39;y_test&#39;) png mean_squared_error(y_test, pred) 26.023230850097445 72.1.3 8.3.3 Bagging and Random Forests # There are 13 features in the dataset X.shape (506, 13) # Bagging: using all features regr1 = RandomForestRegressor(max_features=13, random_state=1) regr1.fit(X_train, y_train) RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=13, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1, oob_score=False, random_state=1, verbose=0, warm_start=False) pred = regr1.predict(X_test) plt.scatter(pred, y_test, label=&#39;medv&#39;) plt.plot([0, 1], [0, 1], &#39;--k&#39;, transform=plt.gca().transAxes) plt.xlabel(&#39;pred&#39;) plt.ylabel(&#39;y_test&#39;) Text(0,0.5,&#39;y_test&#39;) png mean_squared_error(y_test, pred) 18.301366007905138 # Random forests: using 6 features regr2 = RandomForestRegressor(max_features=6, random_state=1) regr2.fit(X_train, y_train) RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=6, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1, oob_score=False, random_state=1, verbose=0, warm_start=False) pred = regr2.predict(X_test) mean_squared_error(y_test, pred) 16.469374703557314 Importance = pd.DataFrame({&#39;Importance&#39;:regr2.feature_importances_*100}, index=X.columns) Importance.sort_values(&#39;Importance&#39;, axis=0, ascending=True).plot(kind=&#39;barh&#39;, color=&#39;r&#39;, ) plt.xlabel(&#39;Variable Importance&#39;) plt.gca().legend_ = None png 72.1.4 8.3.4 Boosting regr = GradientBoostingRegressor(n_estimators=500, learning_rate=0.01, random_state=1) regr.fit(X_train, y_train) GradientBoostingRegressor(alpha=0.9, criterion=&#39;friedman_mse&#39;, init=None, learning_rate=0.01, loss=&#39;ls&#39;, max_depth=3, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=500, presort=&#39;auto&#39;, random_state=1, subsample=1.0, verbose=0, warm_start=False) feature_importance = regr.feature_importances_*100 rel_imp = pd.Series(feature_importance, index=X.columns).sort_values(inplace=False) print(rel_imp) rel_imp.T.plot(kind=&#39;barh&#39;, color=&#39;r&#39;, ) plt.xlabel(&#39;Variable Importance&#39;) plt.gca().legend_ = None zn 0.170382 rad 1.593909 chas 1.844703 indus 3.045285 nox 3.284683 tax 5.007437 black 5.082208 age 5.587239 crim 6.750284 ptratio 8.226473 dis 10.248698 rm 22.134290 lstat 27.024410 dtype: float64 png mean_squared_error(y_test, regr.predict(X_test)) 15.529710264059759 "],["chapter-9-support-vector-machines.html", "Chapter 73 Chapter 9 - Support Vector Machines 73.1 LAB", " Chapter 73 Chapter 9 - Support Vector Machines Lab: 9.6.1 Support Vector Classifier Lab: 9.6.2 Support Vector Machine Lab: 9.6.3 ROC Curves Lab: 9.6.4 SVM with Multiple Classes Lab: 9.6.5 Application to Gene Expression Data # %load ../standard_import.txt import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.preprocessing import label_binarize from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.svm import SVC, LinearSVC from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report %matplotlib inline plt.style.use(&#39;seaborn-white&#39;) 73.1 LAB 73.1.1 9.6.1 Support Vector Classifier Define a function to plot a classifier with support vectors. def plot_svc(svc, X, y, h=0.02, pad=0.25): x_min, x_max = X[:, 0].min()-pad, X[:, 0].max()+pad y_min, y_max = X[:, 1].min()-pad, X[:, 1].max()+pad xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = svc.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.2) plt.scatter(X[:,0], X[:,1], s=70, c=y, cmap=plt.cm.Paired) # Support vectors indicated in plot by vertical lines sv = svc.support_vectors_ plt.scatter(sv[:,0], sv[:,1], c=&#39;k&#39;, marker=&#39;|&#39;, s=100, linewidths=&#39;1&#39;) plt.xlim(x_min, x_max) plt.ylim(y_min, y_max) plt.xlabel(&#39;X1&#39;) plt.ylabel(&#39;X2&#39;) plt.show() print(&#39;Number of support vectors: &#39;, svc.support_.size) # Generating random data: 20 observations of 2 features and divide into two classes. np.random.seed(5) X = np.random.randn(20,2) y = np.repeat([1,-1], 10) X[y == -1] = X[y == -1] +1 plt.scatter(X[:,0], X[:,1], s=70, c=y, cmap=plt.cm.Paired) plt.xlabel(&#39;X1&#39;) plt.ylabel(&#39;X2&#39;); png # Support Vector Classifier with linear kernel. svc = SVC(C= 1.0, kernel=&#39;linear&#39;) svc.fit(X, y) plot_svc(svc, X, y) png Number of support vectors: 13 # When using a smaller cost parameter (C=0.1) the margin is wider, resulting in more support vectors. svc2 = SVC(C=0.1, kernel=&#39;linear&#39;) svc2.fit(X, y) plot_svc(svc2, X, y) png Number of support vectors: 16 # Select the optimal C parameter by cross-validation tuned_parameters = [{&#39;C&#39;: [0.001, 0.01, 0.1, 1, 5, 10, 100]}] clf = GridSearchCV(SVC(kernel=&#39;linear&#39;), tuned_parameters, cv=10, scoring=&#39;accuracy&#39;, return_train_score=True) clf.fit(X, y) clf.cv_results_ {&#39;mean_fit_time&#39;: array([ 0.00041504, 0.00026286, 0.00026855, 0.00027678, 0.00026004, 0.00027893, 0.00039349]), &#39;mean_score_time&#39;: array([ 0.00026352, 0.00018048, 0.0001838 , 0.0001822 , 0.00018001, 0.00018055, 0.00017881]), &#39;mean_test_score&#39;: array([ 0.8 , 0.8 , 0.8 , 0.75, 0.75, 0.75, 0.75]), &#39;mean_train_score&#39;: array([ 0.79444444, 0.79444444, 0.75 , 0.77777778, 0.76666667, 0.76666667, 0.76666667]), &#39;param_C&#39;: masked_array(data = [0.001 0.01 0.1 1 5 10 100], mask = [False False False False False False False], fill_value = ?), &#39;params&#39;: [{&#39;C&#39;: 0.001}, {&#39;C&#39;: 0.01}, {&#39;C&#39;: 0.1}, {&#39;C&#39;: 1}, {&#39;C&#39;: 5}, {&#39;C&#39;: 10}, {&#39;C&#39;: 100}], &#39;rank_test_score&#39;: array([1, 1, 1, 4, 4, 4, 4], dtype=int32), &#39;split0_test_score&#39;: array([ 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]), &#39;split0_train_score&#39;: array([ 0.83333333, 0.83333333, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778]), &#39;split1_test_score&#39;: array([ 0.5, 0.5, 0.5, 0. , 0. , 0. , 0. ]), &#39;split1_train_score&#39;: array([ 0.83333333, 0.83333333, 0.83333333, 0.88888889, 0.88888889, 0.88888889, 0.88888889]), &#39;split2_test_score&#39;: array([ 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]), &#39;split2_train_score&#39;: array([ 0.83333333, 0.83333333, 0.77777778, 0.83333333, 0.83333333, 0.83333333, 0.83333333]), &#39;split3_test_score&#39;: array([ 1., 1., 1., 1., 1., 1., 1.]), &#39;split3_train_score&#39;: array([ 0.77777778, 0.77777778, 0.72222222, 0.72222222, 0.72222222, 0.72222222, 0.72222222]), &#39;split4_test_score&#39;: array([ 1., 1., 1., 1., 1., 1., 1.]), &#39;split4_train_score&#39;: array([ 0.77777778, 0.77777778, 0.72222222, 0.77777778, 0.77777778, 0.77777778, 0.77777778]), &#39;split5_test_score&#39;: array([ 1., 1., 1., 1., 1., 1., 1.]), &#39;split5_train_score&#39;: array([ 0.77777778, 0.77777778, 0.72222222, 0.72222222, 0.72222222, 0.72222222, 0.72222222]), &#39;split6_test_score&#39;: array([ 1., 1., 1., 1., 1., 1., 1.]), &#39;split6_train_score&#39;: array([ 0.77777778, 0.77777778, 0.72222222, 0.77777778, 0.72222222, 0.72222222, 0.72222222]), &#39;split7_test_score&#39;: array([ 1., 1., 1., 1., 1., 1., 1.]), &#39;split7_train_score&#39;: array([ 0.72222222, 0.72222222, 0.72222222, 0.77777778, 0.72222222, 0.72222222, 0.72222222]), &#39;split8_test_score&#39;: array([ 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]), &#39;split8_train_score&#39;: array([ 0.83333333, 0.83333333, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778]), &#39;split9_test_score&#39;: array([ 1., 1., 1., 1., 1., 1., 1.]), &#39;split9_train_score&#39;: array([ 0.77777778, 0.77777778, 0.72222222, 0.72222222, 0.72222222, 0.72222222, 0.72222222]), &#39;std_fit_time&#39;: array([ 1.29045113e-04, 2.34619509e-05, 4.18230226e-05, 3.99072322e-05, 4.80760882e-06, 2.40586557e-05, 4.76230235e-05]), &#39;std_score_time&#39;: array([ 1.05007676e-04, 1.09293532e-05, 1.94490093e-05, 1.08948682e-05, 1.90122934e-05, 1.49680733e-05, 2.26685797e-06]), &#39;std_test_score&#39;: array([ 0.24494897, 0.24494897, 0.24494897, 0.3354102 , 0.3354102 , 0.3354102 , 0.3354102 ]), &#39;std_train_score&#39;: array([ 0.03557291, 0.03557291, 0.0372678 , 0.0496904 , 0.05443311, 0.05443311, 0.05443311])} # 0.001 is best according to GridSearchCV. clf.best_params_ {&#39;C&#39;: 0.001} # Generating test data np.random.seed(1) X_test = np.random.randn(20,2) y_test = np.random.choice([-1,1], 20) X_test[y_test == 1] = X_test[y_test == 1] -1 plt.scatter(X_test[:,0], X_test[:,1], s=70, c=y_test, cmap=plt.cm.Paired) plt.xlabel(&#39;X1&#39;) plt.ylabel(&#39;X2&#39;); png # svc2 : C = 0.1 y_pred = svc2.predict(X_test) pd.DataFrame(confusion_matrix(y_test, y_pred),index=svc.classes_, columns=svc.classes_) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } -1 1 -1 2 6 1 0 12 svc3 = SVC(C=0.001, kernel=&#39;linear&#39;) svc3.fit(X, y) # svc3 : C = 0.001 y_pred = svc3.predict(X_test) pd.DataFrame(confusion_matrix(y_test, y_pred), index=svc3.classes_, columns=svc3.classes_) # The misclassification is the same .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } -1 1 -1 2 6 1 0 12 # Changing the test data so that the classes are really seperable with a hyperplane. X_test[y_test == 1] = X_test[y_test == 1] -1 plt.scatter(X_test[:,0], X_test[:,1], s=70, c=y_test, cmap=plt.cm.Paired) plt.xlabel(&#39;X1&#39;) plt.ylabel(&#39;X2&#39;); png svc4 = SVC(C=10.0, kernel=&#39;linear&#39;) svc4.fit(X_test, y_test) SVC(C=10.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto&#39;, kernel=&#39;linear&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) plot_svc(svc4, X_test, y_test) png Number of support vectors: 4 # Increase the margin. Now there is one misclassification: increased bias, lower variance. svc5 = SVC(C=1, kernel=&#39;linear&#39;) svc5.fit(X_test, y_test) SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto&#39;, kernel=&#39;linear&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) plot_svc(svc5, X_test, y_test) png Number of support vectors: 5 73.1.2 9.6.2 Support Vector Machine # Generating test data np.random.seed(8) X = np.random.randn(200,2) X[:100] = X[:100] +2 X[101:150] = X[101:150] -2 y = np.concatenate([np.repeat(-1, 150), np.repeat(1,50)]) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=2) plt.scatter(X[:,0], X[:,1], s=70, c=y, cmap=plt.cm.Paired) plt.xlabel(&#39;X1&#39;) plt.ylabel(&#39;X2&#39;); png svm = SVC(C=1.0, kernel=&#39;rbf&#39;, gamma=1) svm.fit(X_train, y_train) SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=1, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) plot_svc(svm, X_train, y_train) png Number of support vectors: 51 # Increasing C parameter, allowing more flexibility svm2 = SVC(C=100, kernel=&#39;rbf&#39;, gamma=1.0) svm2.fit(X_train, y_train) SVC(C=100, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=1.0, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) plot_svc(svm2, X_train, y_train) png Number of support vectors: 36 # Set the parameters by cross-validation tuned_parameters = [{&#39;C&#39;: [0.01, 0.1, 1, 10, 100], &#39;gamma&#39;: [0.5, 1,2,3,4]}] clf = GridSearchCV(SVC(kernel=&#39;rbf&#39;), tuned_parameters, cv=10, scoring=&#39;accuracy&#39;, return_train_score=True) clf.fit(X_train, y_train) clf.cv_results_ {&#39;mean_fit_time&#39;: array([ 0.00057094, 0.00040917, 0.00044692, 0.00046566, 0.0004539 , 0.00037632, 0.00042117, 0.00052576, 0.00053477, 0.00054688, 0.00039585, 0.00043306, 0.00058219, 0.00061436, 0.00059385, 0.00039713, 0.00043871, 0.00058134, 0.0006536 , 0.00065069, 0.00043354, 0.00047519, 0.0005713 , 0.00062847, 0.00062041]), &#39;mean_score_time&#39;: array([ 0.00028653, 0.00020535, 0.00020931, 0.00020568, 0.00019453, 0.00019224, 0.00019584, 0.0002043 , 0.00020387, 0.00020037, 0.0001987 , 0.00019855, 0.00020149, 0.00020573, 0.00021515, 0.0001905 , 0.00019574, 0.00020428, 0.00020165, 0.00020831, 0.00020461, 0.00019076, 0.0001929 , 0.00019503, 0.00019488]), &#39;mean_test_score&#39;: array([ 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.92, 0.92, 0.9 , 0.88, 0.85, 0.92, 0.89, 0.86, 0.86, 0.87, 0.84, 0.83, 0.86, 0.87, 0.87]), &#39;mean_train_score&#39;: array([ 0.77002017, 0.77002017, 0.77002017, 0.77002017, 0.77002017, 0.77002017, 0.77002017, 0.77002017, 0.77002017, 0.77002017, 0.93557504, 0.94777366, 0.95998477, 0.96333059, 0.98112195, 0.95113224, 0.9666642 , 0.99112277, 0.99112277, 0.99112277, 0.9688989 , 0.99112277, 0.99112277, 0.99112277, 0.99112277]), &#39;param_C&#39;: masked_array(data = [0.01 0.01 0.01 0.01 0.01 0.1 0.1 0.1 0.1 0.1 1 1 1 1 1 10 10 10 10 10 100 100 100 100 100], mask = [False False False False False False False False False False False False False False False False False False False False False False False False False], fill_value = ?), &#39;param_gamma&#39;: masked_array(data = [0.5 1 2 3 4 0.5 1 2 3 4 0.5 1 2 3 4 0.5 1 2 3 4 0.5 1 2 3 4], mask = [False False False False False False False False False False False False False False False False False False False False False False False False False], fill_value = ?), &#39;params&#39;: [{&#39;C&#39;: 0.01, &#39;gamma&#39;: 0.5}, {&#39;C&#39;: 0.01, &#39;gamma&#39;: 1}, {&#39;C&#39;: 0.01, &#39;gamma&#39;: 2}, {&#39;C&#39;: 0.01, &#39;gamma&#39;: 3}, {&#39;C&#39;: 0.01, &#39;gamma&#39;: 4}, {&#39;C&#39;: 0.1, &#39;gamma&#39;: 0.5}, {&#39;C&#39;: 0.1, &#39;gamma&#39;: 1}, {&#39;C&#39;: 0.1, &#39;gamma&#39;: 2}, {&#39;C&#39;: 0.1, &#39;gamma&#39;: 3}, {&#39;C&#39;: 0.1, &#39;gamma&#39;: 4}, {&#39;C&#39;: 1, &#39;gamma&#39;: 0.5}, {&#39;C&#39;: 1, &#39;gamma&#39;: 1}, {&#39;C&#39;: 1, &#39;gamma&#39;: 2}, {&#39;C&#39;: 1, &#39;gamma&#39;: 3}, {&#39;C&#39;: 1, &#39;gamma&#39;: 4}, {&#39;C&#39;: 10, &#39;gamma&#39;: 0.5}, {&#39;C&#39;: 10, &#39;gamma&#39;: 1}, {&#39;C&#39;: 10, &#39;gamma&#39;: 2}, {&#39;C&#39;: 10, &#39;gamma&#39;: 3}, {&#39;C&#39;: 10, &#39;gamma&#39;: 4}, {&#39;C&#39;: 100, &#39;gamma&#39;: 0.5}, {&#39;C&#39;: 100, &#39;gamma&#39;: 1}, {&#39;C&#39;: 100, &#39;gamma&#39;: 2}, {&#39;C&#39;: 100, &#39;gamma&#39;: 3}, {&#39;C&#39;: 100, &#39;gamma&#39;: 4}], &#39;rank_test_score&#39;: array([16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 1, 1, 4, 6, 13, 1, 5, 10, 10, 7, 14, 15, 10, 7, 7], dtype=int32), &#39;split0_test_score&#39;: array([ 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.90909091, 0.90909091, 0.81818182, 0.81818182, 0.81818182, 0.90909091, 0.81818182, 0.72727273, 0.72727273, 0.72727273, 0.81818182, 0.81818182, 0.72727273, 0.72727273, 0.72727273]), &#39;split0_train_score&#39;: array([ 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.94382022, 0.95505618, 0.96629213, 0.96629213, 0.98876404, 0.95505618, 0.97752809, 1. , 1. , 1. , 0.97752809, 1. , 1. , 1. , 1. ]), &#39;split1_test_score&#39;: array([ 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 1. , 0.90909091, 0.90909091, 0.90909091, 0.81818182, 0.90909091, 0.81818182, 0.90909091, 0.90909091, 0.90909091, 0.81818182, 0.81818182, 0.90909091, 1. , 1. ]), &#39;split1_train_score&#39;: array([ 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.92134831, 0.93258427, 0.95505618, 0.96629213, 0.97752809, 0.96629213, 0.96629213, 0.98876404, 0.98876404, 0.98876404, 0.97752809, 0.98876404, 0.98876404, 0.98876404, 0.98876404]), &#39;split2_test_score&#39;: array([ 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273, 1. , 1. , 1. , 0.90909091, 0.81818182, 1. , 1. , 1. , 0.90909091, 0.90909091, 1. , 1. , 1. , 0.90909091, 0.90909091]), &#39;split2_train_score&#39;: array([ 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.7752809 , 0.93258427, 0.94382022, 0.95505618, 0.95505618, 0.97752809, 0.94382022, 0.95505618, 0.98876404, 0.98876404, 0.98876404, 0.95505618, 0.98876404, 0.98876404, 0.98876404, 0.98876404]), &#39;split3_test_score&#39;: array([ 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 1. , 1. , 1. , 1. , 1. , 0.9, 0.9, 1. , 1. , 1. , 0.9, 0.9, 1. , 1. , 1. ]), &#39;split3_train_score&#39;: array([ 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.94444444, 0.95555556, 0.95555556, 0.95555556, 0.97777778, 0.94444444, 0.95555556, 0.98888889, 0.98888889, 0.98888889, 0.95555556, 0.98888889, 0.98888889, 0.98888889, 0.98888889]), &#39;split4_test_score&#39;: array([ 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.9, 0.7, 0.7, 0.7, 0.8, 0.7, 0.7, 0.7, 0.7, 0.7]), &#39;split4_train_score&#39;: array([ 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.93333333, 0.94444444, 0.95555556, 0.97777778, 0.98888889, 0.95555556, 0.97777778, 0.98888889, 0.98888889, 0.98888889, 0.98888889, 0.98888889, 0.98888889, 0.98888889, 0.98888889]), &#39;split5_test_score&#39;: array([ 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 1. , 1. , 1. , 1. , 0.9, 1. , 1. , 1. , 1. , 1. , 0.9, 1. , 1. , 1. , 1. ]), &#39;split5_train_score&#39;: array([ 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.94444444, 0.94444444, 0.95555556, 0.95555556, 0.97777778, 0.94444444, 0.96666667, 0.98888889, 0.98888889, 0.98888889, 0.96666667, 0.98888889, 0.98888889, 0.98888889, 0.98888889]), &#39;split6_test_score&#39;: array([ 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.7, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.7, 0.7, 0.7, 0.7, 0.6, 0.7, 0.7, 0.7]), &#39;split6_train_score&#39;: array([ 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.76666667, 0.95555556, 0.96666667, 0.96666667, 0.96666667, 0.98888889, 0.95555556, 0.96666667, 1. , 1. , 1. , 0.96666667, 1. , 1. , 1. , 1. ]), &#39;split7_test_score&#39;: array([ 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.88888889, 0.77777778, 0.88888889, 0.88888889, 0.77777778, 0.77777778, 0.77777778, 0.88888889, 0.88888889]), &#39;split7_train_score&#39;: array([ 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.94505495, 0.95604396, 0.96703297, 0.96703297, 0.97802198, 0.95604396, 0.95604396, 0.98901099, 0.98901099, 0.98901099, 0.95604396, 0.98901099, 0.98901099, 0.98901099, 0.98901099]), &#39;split8_test_score&#39;: array([ 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 1. , 1. , 0.88888889, 0.88888889, 0.88888889, 1. , 1. , 0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.88888889]), &#39;split8_train_score&#39;: array([ 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.91208791, 0.93406593, 0.96703297, 0.96703297, 0.97802198, 0.94505495, 0.96703297, 0.98901099, 0.98901099, 0.98901099, 0.96703297, 0.98901099, 0.98901099, 0.98901099, 0.98901099]), &#39;split9_test_score&#39;: array([ 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 0.77777778, 1. , 1. , 1. , 0.88888889, 0.88888889, 1. , 1. , 0.88888889, 0.88888889, 0.88888889, 0.88888889, 0.77777778, 0.88888889, 0.88888889, 0.88888889]), &#39;split9_train_score&#39;: array([ 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.92307692, 0.94505495, 0.95604396, 0.95604396, 0.97802198, 0.94505495, 0.97802198, 0.98901099, 0.98901099, 0.98901099, 0.97802198, 0.98901099, 0.98901099, 0.98901099, 0.98901099]), &#39;std_fit_time&#39;: array([ 1.29946046e-04, 2.37428996e-05, 3.44466995e-05, 4.06270366e-05, 1.08891275e-05, 9.16545024e-06, 3.26168903e-05, 4.05703804e-05, 3.52196780e-05, 5.12081594e-05, 4.58430076e-05, 5.77187892e-05, 6.30817400e-05, 5.33768882e-05, 3.02056298e-05, 1.66381495e-05, 2.50651415e-05, 3.66048308e-05, 4.32769062e-05, 1.73285536e-05, 2.26485603e-05, 3.73201545e-05, 1.64487395e-05, 2.56888689e-05, 2.57250885e-05]), &#39;std_score_time&#39;: array([ 5.44625275e-05, 1.39864760e-05, 2.87118636e-05, 1.88631289e-05, 2.26798611e-06, 3.38697068e-06, 9.65003433e-06, 1.13169492e-05, 1.33860726e-05, 8.99264819e-06, 1.90952892e-05, 1.06008097e-05, 9.35649405e-06, 1.96293797e-05, 3.91622674e-05, 4.30343884e-06, 8.26594154e-06, 2.02857086e-05, 1.05884695e-05, 2.38947401e-05, 4.54962125e-05, 2.53767431e-06, 1.47145025e-06, 2.89460186e-06, 1.58220864e-06]), &#39;std_test_score&#39;: array([ 0.03128559, 0.03128559, 0.03128559, 0.03128559, 0.03128559, 0.03128559, 0.03128559, 0.03128559, 0.03128559, 0.03128559, 0.10933222, 0.08867145, 0.09000561, 0.07563869, 0.06384166, 0.07656779, 0.10140926, 0.11898561, 0.10832051, 0.09712535, 0.09132028, 0.11988631, 0.11898561, 0.11629378, 0.11629378]), &#39;std_train_score&#39;: array([ 0.00360388, 0.00360388, 0.00360388, 0.00360388, 0.00360388, 0.00360388, 0.00360388, 0.00360388, 0.00360388, 0.00360388, 0.01280087, 0.01001371, 0.00553851, 0.00712229, 0.00506044, 0.0072475 , 0.00861247, 0.00443945, 0.00443945, 0.00443945, 0.01086333, 0.00443945, 0.00443945, 0.00443945, 0.00443945])} clf.best_params_ {&#39;C&#39;: 1, &#39;gamma&#39;: 0.5} confusion_matrix(y_test, clf.best_estimator_.predict(X_test)) array([[67, 6], [ 9, 18]]) # 15% of test observations misclassified clf.best_estimator_.score(X_test, y_test) 0.84999999999999998 73.1.3 9.6.3 ROC Curves Comparing the ROC curves of two models on train/test data. One model is more flexible than the other. svm3 = SVC(C=1, kernel=&#39;rbf&#39;, gamma=2) svm3.fit(X_train, y_train) SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=2, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) # More flexible model svm4 = SVC(C=1, kernel=&#39;rbf&#39;, gamma=50) svm4.fit(X_train, y_train) SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=50, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) y_train_score3 = svm3.decision_function(X_train) y_train_score4 = svm4.decision_function(X_train) false_pos_rate3, true_pos_rate3, _ = roc_curve(y_train, y_train_score3) roc_auc3 = auc(false_pos_rate3, true_pos_rate3) false_pos_rate4, true_pos_rate4, _ = roc_curve(y_train, y_train_score4) roc_auc4 = auc(false_pos_rate4, true_pos_rate4) fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(14,6)) ax1.plot(false_pos_rate3, true_pos_rate3, label=&#39;SVM $\\gamma = 1$ ROC curve (area = %0.2f)&#39; % roc_auc3, color=&#39;b&#39;) ax1.plot(false_pos_rate4, true_pos_rate4, label=&#39;SVM $\\gamma = 50$ ROC curve (area = %0.2f)&#39; % roc_auc4, color=&#39;r&#39;) ax1.set_title(&#39;Training Data&#39;) y_test_score3 = svm3.decision_function(X_test) y_test_score4 = svm4.decision_function(X_test) false_pos_rate3, true_pos_rate3, _ = roc_curve(y_test, y_test_score3) roc_auc3 = auc(false_pos_rate3, true_pos_rate3) false_pos_rate4, true_pos_rate4, _ = roc_curve(y_test, y_test_score4) roc_auc4 = auc(false_pos_rate4, true_pos_rate4) ax2.plot(false_pos_rate3, true_pos_rate3, label=&#39;SVM $\\gamma = 1$ ROC curve (area = %0.2f)&#39; % roc_auc3, color=&#39;b&#39;) ax2.plot(false_pos_rate4, true_pos_rate4, label=&#39;SVM $\\gamma = 50$ ROC curve (area = %0.2f)&#39; % roc_auc4, color=&#39;r&#39;) ax2.set_title(&#39;Test Data&#39;) for ax in fig.axes: ax.plot([0, 1], [0, 1], &#39;k--&#39;) ax.set_xlim([-0.05, 1.0]) ax.set_ylim([0.0, 1.05]) ax.set_xlabel(&#39;False Positive Rate&#39;) ax.set_ylabel(&#39;True Positive Rate&#39;) ax.legend(loc=&quot;lower right&quot;) png As expected, the more flexible model scores better on training data but worse on the test data. 73.1.4 9.6.4 SVM with Multiple Classes # Adding a third class of observations np.random.seed(8) XX = np.vstack([X, np.random.randn(50,2)]) yy = np.hstack([y, np.repeat(0,50)]) XX[yy ==0] = XX[yy == 0] +4 plt.scatter(XX[:,0], XX[:,1], s=70, c=yy, cmap=plt.cm.prism) plt.xlabel(&#39;XX1&#39;) plt.ylabel(&#39;XX2&#39;); png svm5 = SVC(C=1, kernel=&#39;rbf&#39;) svm5.fit(XX, yy) SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto&#39;, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) plot_svc(svm5, XX, yy) png Number of support vectors: 133 73.1.5 9.6.5 Application to Gene Expression Data In R, I exported the dataset from package ‘ISLR’ to csv files. X_train = pd.read_csv(&#39;Data/Khan_xtrain.csv&#39;).drop(&#39;Unnamed: 0&#39;, axis=1) y_train = pd.read_csv(&#39;Data/Khan_ytrain.csv&#39;).drop(&#39;Unnamed: 0&#39;, axis=1).as_matrix().ravel() X_test = pd.read_csv(&#39;Data/Khan_xtest.csv&#39;).drop(&#39;Unnamed: 0&#39;, axis=1) y_test = pd.read_csv(&#39;Data/Khan_ytest.csv&#39;).drop(&#39;Unnamed: 0&#39;, axis=1).as_matrix().ravel() # y_train counts pd.Series(y_train).value_counts(sort=False) 1 8 2 23 3 12 4 20 dtype: int64 # y_test counts pd.Series(y_test).value_counts(sort=False) 1 3 2 6 3 6 4 5 dtype: int64 # This model gives identical results to the svm() of the R package e1071, also based on libsvm library. svc = SVC(kernel=&#39;linear&#39;) # This model is based on liblinear library and gives 100 score on the test data. #svc = LinearSVC() svc.fit(X_train, y_train) SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto&#39;, kernel=&#39;linear&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) cm = confusion_matrix(y_train, svc.predict(X_train)) cm_df = pd.DataFrame(cm.T, index=svc.classes_, columns=svc.classes_) cm_df.index.name = &#39;Predicted&#39; cm_df.columns.name = &#39;True&#39; print(cm_df) True 1 2 3 4 Predicted 1 8 0 0 0 2 0 23 0 0 3 0 0 12 0 4 0 0 0 20 cm = confusion_matrix(y_test, svc.predict(X_test)) cm_df = pd.DataFrame(cm.T, index=svc.classes_, columns=svc.classes_) cm_df.index.name = &#39;Predicted&#39; cm_df.columns.name = &#39;True&#39; print(cm_df) True 1 2 3 4 Predicted 1 3 0 0 0 2 0 6 2 0 3 0 0 4 0 4 0 0 0 5 Table of Contents 9 Support Vector Machines 9.1 Maximal Margin Classifier 9.1.1 What Is a Hyperplane? 9.1.2 Classification Using a Separating Hyperplane 9.1.3 The Maximal Margin Classifier 9.1.4 Construction of the Maximal Margin Classifier 9.1.5 The Non-separable Case 9.2 Support Vector Classifiers 9.2.1 Overview of the Support Vector Classifier 9.2.2 Details of the Support Vector Classifier 9.3 Support Vector Machines 9.3.1 Classification with Non-Linear Decision Boundaries 9.3.2 The Support Vector Machine 9.4 SVMs with More than Two Classes 9.4.1 One-Versus-One Classification 9.4.2 One-Versus-All Classification 9.5 Relationship to Logistic Regression 9.6 Footnotes "],["support-vector-machines.html", "Chapter 74 Support Vector Machines 74.1 Maximal Margin Classifier 74.2 Support Vector Classifiers 74.3 Support Vector Machines 74.4 SVMs with More than Two Classes 74.5 Relationship to Logistic Regression 74.6 Footnotes", " Chapter 74 Support Vector Machines 74.1 Maximal Margin Classifier 74.1.1 What Is a Hyperplane? A hyperplane in \\(\\mathbb{R}^p\\) is an affine subspace of dimension \\(p-1\\). Every hyperplane is the set of solutions \\(X\\) to \\(\\beta^\\top X = 0\\) for some \\(\\beta\\in\\mathbb{R}^p\\). A hyperplane \\(\\beta^\\top X = 0\\) partitions \\(\\mathbb{R}^p\\) into two halfspaces: \\[H_+ = \\{X\\in\\mathbb{R}^p\\ |\\ \\beta^\\top X &gt; 0\\}\\] \\[H_- = \\{X\\in\\mathbb{R}^p\\ |\\ \\beta^\\top X &gt; 0\\}\\] corresponding to either side of the plane, or equivalently, \\[H_+ = \\{X\\in\\mathbb{R}^p\\ |\\ \\text{sgn}(\\beta^\\top X) = 1\\}\\] \\[H_- = \\{X\\in\\mathbb{R}^p\\ |\\ \\text{sgn}(\\beta^\\top X) = -1\\}\\] 74.1.2 Classification Using a Separating Hyperplane Given data \\((x_i, y_i)\\), \\(i = 1,\\dots n\\) with response classes \\(y_i \\in \\{ \\pm 1\\}\\), a hyperplane \\(\\beta^\\top X = 0\\) is separating if \\[\\text{sgn}(\\beta^\\top x_i) = y_i\\] for all \\(i\\). - Given a separating hyperplane, we may predict \\[\\hat{y}_i = \\text{sgn}(\\beta^\\top x_i)\\] 74.1.3 The Maximal Margin Classifier Separating hyperplanes are not unique (if one exists then uncountably many exist). A natural choice is the maximal margin hyperplane (or optimal separating hyperplane) The margin is the minimal perpendicular distance to the hyperplane over the sample points \\[ M = \\underset{i}{\\min}\\{\\ ||x_i - P x_i||\\ \\}\\] where \\(P\\) is the projection matrix onto the hyperplane. The points \\((x_i, y_i)\\) “on the margin” (where \\(||x_i - P x_i|| = M\\)) are called support vectors 74.1.4 Construction of the Maximal Margin Classifier The maximal margin classifier is the solution to the optimization problem: \\[\\begin{align*} \\underset{\\boldsymbol{\\beta}}{\\text{argmax}}&amp;\\ M\\\\ \\text{subject to}&amp;\\ ||\\,\\boldsymbol{\\beta}\\,|| = 1\\\\ &amp; \\mathbf{y}^\\top(X\\boldsymbol{\\beta}) \\geqslant \\mathbf{M}\\\\ \\end{align*}\\] where \\(\\mathbf{M} = (M, \\dots, M) \\in \\mathbb{R}^n\\) 78 74.1.5 The Non-separable Case The maximal margin classifier is a natural classifier, but a separating hyperplane is not guaranteed to exist If a separating hyperplane doesn’t exist, we can choose an “almost” separating hyperplane by using a “soft” margin. 74.2 Support Vector Classifiers 74.2.1 Overview of the Support Vector Classifier Separating hyperplanes don’t always exist, and even if they do, they may be undesirable. The distance to the hyperplane can be thought of as a measure of confidence in the classification. For very small margins, the separating hyperplane is very sensitive to individual observations – we have low confidence in the classification of nearby observations. In these situations, we may prefer a hyperplane that doesn’t perfectly separate in the interest of: Greater robustness to individual observations Better classification of most of the training observations This is achieved by the support vector classifier or soft margin classifier 79 74.2.2 Details of the Support Vector Classifier The support vector classifier is the solution to the optimization problem: \\[\\begin{align*} \\underset{\\boldsymbol{\\beta}}{\\text{argmax}}&amp;\\ M\\\\ \\text{subject to}&amp;\\ ||\\,\\boldsymbol{\\beta}\\,|| = 1\\\\ &amp; y_i(\\boldsymbol{\\beta}^\\top x_i) \\geqslant M(1-\\epsilon_i)\\\\ &amp; \\epsilon_i \\geqslant 0\\\\ &amp; \\sum_i \\epsilon_i \\leqslant C \\end{align*}\\] where \\(C \\geqslant 0\\) is a tuning parameter, \\(M\\) is the margin, and the \\(\\epsilon_i\\) are slack variables. 80 Observations on the margin or on the wrong side of the margin are called support vectors 74.3 Support Vector Machines 74.3.1 Classification with Non-Linear Decision Boundaries The support vector classifier is a natural choice for two response classes when the class boundary is linear, but may perform poorly when the boundary is non-linear. Non-linear transformations of the features will lead to a non-linear class boundary, but enlarging the feature space too much can lead to intractable computations. The support vector machine enlarges the feature space in a way which is computationally efficient. 74.3.2 The Support Vector Machine It can be shown that: the linear support vector classifier is a model of the form \\[f(x) = \\beta_0 + \\sum_{i = 1}^n \\alpha_i \\langle x, x_i\\rangle \\] the parameter estimates \\(\\hat{\\alpha}_i, \\hat{\\beta}_0\\) can be computed from the \\(\\binom{n}{2}\\) inner products \\(\\langle x, x_i \\rangle\\) The support vector machine is a model of the form \\[f(x) = \\beta_0 + \\sum_{i = 1}^n \\alpha_i K(x, x_i) \\] where \\(K\\) is a kernel function 81 Popular kernels 82 are The polynomial kernel \\[K(x_i, x_i&#39;) = (1 + x_i^\\top x_i&#39;)^d\\] The radial kernel \\[K(x_i, x_i&#39;) = \\exp(-\\gamma\\,||x_i - x_i&#39;||^2)\\] 74.4 SVMs with More than Two Classes 74.4.1 One-Versus-One Classification This approach works as follows: 1. Fit \\(\\binom{K}{2}\\) SVMs, one for each pair of classes \\(k,k&#39;\\) encoded as \\(\\pm 1\\), respectively. 2. For each observation \\(x\\), classify using each of the predictors in 1, and let \\(N_k\\) be the number of times \\(x\\) was assigned to class \\(k\\). 3. Predict \\[ \\hat{f}(x) = \\underset{k}{\\text{argmax}}\\, N_k\\] 74.4.2 One-Versus-All Classification This approach works as follows: 1. Fit \\(K\\) SVMs, comparing each class \\(k\\) to other \\(K-1\\) classes, encoded as \\(\\pm 1\\), respectively. Let \\(\\beta_k (\\beta_{0k}, \\dots, \\beta_{pk})\\) be resulting parameters. 2. Predict \\[\\hat{f}(x) = \\underset{k}{\\text{argmax}}\\, \\beta_k^\\top x\\] 74.5 Relationship to Logistic Regression The optimization problem leading to the support vector classifier can be rewritten as \\[\\underset{\\beta}{\\text{argmin}}\\left(\\sum_{i = 1}^n \\max\\{0, 1 - y_i(\\beta^\\top x_i)\\} + \\lambda\\,||\\beta||^2\\right)\\] where $ $ is a tuning parameter 83 . The hinge loss 84 is very similar to the logistic regression loss, so both methods tend to give similar results. However, SVMs tend to perform better when the classes are well separated, while logistic regression tends to perform better when they are not. 74.6 Footnotes The constraint \\(|| \\boldsymbol{\\beta} || = 1\\) ensures that the perpendicular distance \\(||x_i - P x_i||\\) is given by \\(y_i(\\beta^\\top x_i)\\). ↩︎ Sometimes the maximal margin and support vector classifiers are called “hard margin” and “soft margin” support vector classifiers, respectively. ↩︎ For each \\(i\\), if \\(\\epsilon_i = 0\\) the \\(i\\)-th observation is on the correct side of the margin. If \\(\\epsilon_i &gt; 0\\) then it is on the wrong side of the margin, and if \\(\\epsilon_i &gt; 1\\) then it is on the wrong side of the hyperplane. The parameter \\(C\\) is a “margin violation tolerance” – it bounds the \\(\\epsilon_i\\) and thus the number/size of margin violations. Greater \\(C\\) implies greater tolerance. The case \\(C = 0\\) is the maximal margin hyperplane. ↩︎ 81.In this context a kernel function is a positive-definite kernel . Among other things, it is a generalization of an inner product (every inner product \\(\\langle x, y \\rangle\\) is a kernel function), and is one way of quantifying similarity between points.\\ In the context of statistical and machine learning, a kernel method is one which makes use of the “kernel trick.” The kernel function \\(K(x_i, x_i&#39;)\\) encodes the similarity of the observations \\(x_i, x_i&#39;\\) in a transformed feature space, but it is more computationally efficient to compute the \\(\\binom{n}{k}\\) kernels themselves than to transform the data. The kernel fits a support vector classifier (hence a linear classification boundary) in the transformed feature space, which corresponds to a non-linear boundary in the original feature space. The polynomial kernel is effectively the inner product on the space of \\(d\\)-degree polynomials in the features \\(X_j\\). The radial kernel is a similarity measure in an infinite dimensional feature space. This is another instance of a general form of a “regularized loss” or “loss + penalty” \\[\\underset{\\beta}{\\text{argmin}}L(\\mathbf{X}, \\mathbf{y}, \\beta) + \\lambda P(\\beta)\\] where the loss function \\(L(\\mathbf{X}, \\mathbf{y}, \\beta)\\) quantifies how well the parameter model with parameter \\(\\beta\\) fits the data \\((\\mathbf{X}, \\mathbf{y})\\), and \\(P(\\beta)\\) is a penalty function controlled by \\(\\lambda\\). In this case \\[L(\\mathbf{X}, \\mathbf{y}, \\beta) = \\sum_{i = 1}^n \\{0, 1 - y_i(\\beta^\\top x_i\\}\\] is called the hinge loss. "],["chapter-10-unsupervised-learning.html", "Chapter 75 Chapter 10 - Unsupervised Learning 75.1 Lab 1: Principal Component Analysis 75.2 Lab 2: Clustering 75.3 Lab 3: NCI60 Data Example", " Chapter 75 Chapter 10 - Unsupervised Learning Lab 1: Principal Component Analysis Lab 2: K-Means Clustering Lab 2: Hierarchical Clustering Lab 3: NCI60 Data Example # %load ../standard_import.txt import pandas as pd import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns from sklearn.preprocessing import scale from sklearn.decomposition import PCA from sklearn.cluster import KMeans from scipy.cluster import hierarchy %matplotlib inline plt.style.use(&#39;seaborn-white&#39;) 75.1 Lab 1: Principal Component Analysis # In R, I exported the dataset to a csv file. It is part of the base R distribution. df = pd.read_csv(&#39;Data/USArrests.csv&#39;, index_col=0) df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 50 entries, Alabama to Wyoming Data columns (total 4 columns): Murder 50 non-null float64 Assault 50 non-null int64 UrbanPop 50 non-null int64 Rape 50 non-null float64 dtypes: float64(2), int64(2) memory usage: 2.0+ KB df.mean() Murder 7.788 Assault 170.760 UrbanPop 65.540 Rape 21.232 dtype: float64 df.var() Murder 18.970465 Assault 6945.165714 UrbanPop 209.518776 Rape 87.729159 dtype: float64 X = pd.DataFrame(scale(df), index=df.index, columns=df.columns) # The loading vectors pca_loadings = pd.DataFrame(PCA().fit(X).components_.T, index=df.columns, columns=[&#39;V1&#39;, &#39;V2&#39;, &#39;V3&#39;, &#39;V4&#39;]) pca_loadings .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } V1 V2 V3 V4 Murder 0.535899 0.418181 -0.341233 0.649228 Assault 0.583184 0.187986 -0.268148 -0.743407 UrbanPop 0.278191 -0.872806 -0.378016 0.133878 Rape 0.543432 -0.167319 0.817778 0.089024 # Fit the PCA model and transform X to get the principal components pca = PCA() df_plot = pd.DataFrame(pca.fit_transform(X), columns=[&#39;PC1&#39;, &#39;PC2&#39;, &#39;PC3&#39;, &#39;PC4&#39;], index=X.index) df_plot .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PC1 PC2 PC3 PC4 Alabama 0.985566 1.133392 -0.444269 0.156267 Alaska 1.950138 1.073213 2.040003 -0.438583 Arizona 1.763164 -0.745957 0.054781 -0.834653 Arkansas -0.141420 1.119797 0.114574 -0.182811 California 2.523980 -1.542934 0.598557 -0.341996 Colorado 1.514563 -0.987555 1.095007 0.001465 Connecticut -1.358647 -1.088928 -0.643258 -0.118469 Delaware 0.047709 -0.325359 -0.718633 -0.881978 Florida 3.013042 0.039229 -0.576829 -0.096285 Georgia 1.639283 1.278942 -0.342460 1.076797 Hawaii -0.912657 -1.570460 0.050782 0.902807 Idaho -1.639800 0.210973 0.259801 -0.499104 Illinois 1.378911 -0.681841 -0.677496 -0.122021 Indiana -0.505461 -0.151563 0.228055 0.424666 Iowa -2.253646 -0.104054 0.164564 0.017556 Kansas -0.796881 -0.270165 0.025553 0.206496 Kentucky -0.750859 0.958440 -0.028369 0.670557 Louisiana 1.564818 0.871055 -0.783480 0.454728 Maine -2.396829 0.376392 -0.065682 -0.330460 Maryland 1.763369 0.427655 -0.157250 -0.559070 Massachusetts -0.486166 -1.474496 -0.609497 -0.179599 Michigan 2.108441 -0.155397 0.384869 0.102372 Minnesota -1.692682 -0.632261 0.153070 0.067317 Mississippi 0.996494 2.393796 -0.740808 0.215508 Missouri 0.696787 -0.263355 0.377444 0.225824 Montana -1.185452 0.536874 0.246889 0.123742 Nebraska -1.265637 -0.193954 0.175574 0.015893 Nevada 2.874395 -0.775600 1.163380 0.314515 New Hampshire -2.383915 -0.018082 0.036855 -0.033137 New Jersey 0.181566 -1.449506 -0.764454 0.243383 New Mexico 1.980024 0.142849 0.183692 -0.339534 New York 1.682577 -0.823184 -0.643075 -0.013484 North Carolina 1.123379 2.228003 -0.863572 -0.954382 North Dakota -2.992226 0.599119 0.301277 -0.253987 Ohio -0.225965 -0.742238 -0.031139 0.473916 Oklahoma -0.311783 -0.287854 -0.015310 0.010332 Oregon 0.059122 -0.541411 0.939833 -0.237781 Pennsylvania -0.888416 -0.571100 -0.400629 0.359061 Rhode Island -0.863772 -1.491978 -1.369946 -0.613569 South Carolina 1.320724 1.933405 -0.300538 -0.131467 South Dakota -1.987775 0.823343 0.389293 -0.109572 Tennessee 0.999742 0.860251 0.188083 0.652864 Texas 1.355138 -0.412481 -0.492069 0.643195 Utah -0.550565 -1.471505 0.293728 -0.082314 Vermont -2.801412 1.402288 0.841263 -0.144890 Virginia -0.096335 0.199735 0.011713 0.211371 Washington -0.216903 -0.970124 0.624871 -0.220848 West Virginia -2.108585 1.424847 0.104775 0.131909 Wisconsin -2.079714 -0.611269 -0.138865 0.184104 Wyoming -0.629427 0.321013 -0.240659 -0.166652 fig , ax1 = plt.subplots(figsize=(9,7)) ax1.set_xlim(-3.5,3.5) ax1.set_ylim(-3.5,3.5) # Plot Principal Components 1 and 2 for i in df_plot.index: ax1.annotate(i, (df_plot.PC1.loc[i], -df_plot.PC2.loc[i]), ha=&#39;center&#39;) # Plot reference lines ax1.hlines(0,-3.5,3.5, linestyles=&#39;dotted&#39;, colors=&#39;grey&#39;) ax1.vlines(0,-3.5,3.5, linestyles=&#39;dotted&#39;, colors=&#39;grey&#39;) ax1.set_xlabel(&#39;First Principal Component&#39;) ax1.set_ylabel(&#39;Second Principal Component&#39;) # Plot Principal Component loading vectors, using a second y-axis. ax2 = ax1.twinx().twiny() ax2.set_ylim(-1,1) ax2.set_xlim(-1,1) ax2.tick_params(axis=&#39;y&#39;, colors=&#39;orange&#39;) ax2.set_xlabel(&#39;Principal Component loading vectors&#39;, color=&#39;orange&#39;) # Plot labels for vectors. Variable &#39;a&#39; is a small offset parameter to separate arrow tip and text. a = 1.07 for i in pca_loadings[[&#39;V1&#39;, &#39;V2&#39;]].index: ax2.annotate(i, (pca_loadings.V1.loc[i]*a, -pca_loadings.V2.loc[i]*a), color=&#39;orange&#39;) # Plot vectors ax2.arrow(0,0,pca_loadings.V1[0], -pca_loadings.V2[0]) ax2.arrow(0,0,pca_loadings.V1[1], -pca_loadings.V2[1]) ax2.arrow(0,0,pca_loadings.V1[2], -pca_loadings.V2[2]) ax2.arrow(0,0,pca_loadings.V1[3], -pca_loadings.V2[3]); png # Standard deviation of the four principal components np.sqrt(pca.explained_variance_) array([ 1.5908673 , 1.00496987, 0.6031915 , 0.4206774 ]) pca.explained_variance_ array([ 2.53085875, 1.00996444, 0.36383998, 0.17696948]) pca.explained_variance_ratio_ array([ 0.62006039, 0.24744129, 0.0891408 , 0.04335752]) plt.figure(figsize=(7,5)) plt.plot([1,2,3,4], pca.explained_variance_ratio_, &#39;-o&#39;, label=&#39;Individual component&#39;) plt.plot([1,2,3,4], np.cumsum(pca.explained_variance_ratio_), &#39;-s&#39;, label=&#39;Cumulative&#39;) plt.ylabel(&#39;Proportion of Variance Explained&#39;) plt.xlabel(&#39;Principal Component&#39;) plt.xlim(0.75,4.25) plt.ylim(0,1.05) plt.xticks([1,2,3,4]) plt.legend(loc=2); png 75.2 Lab 2: Clustering 75.2.1 10.5.1 K-Means Clustering # Generate data np.random.seed(2) X = np.random.standard_normal((50,2)) X[:25,0] = X[:25,0]+3 X[:25,1] = X[:25,1]-4 75.2.1.1 K = 2 km1 = KMeans(n_clusters=2, n_init=20) km1.fit(X) KMeans(algorithm=&#39;auto&#39;, copy_x=True, init=&#39;k-means++&#39;, max_iter=300, n_clusters=2, n_init=20, n_jobs=1, precompute_distances=&#39;auto&#39;, random_state=None, tol=0.0001, verbose=0) km1.labels_ array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], dtype=int32) See plot for K=2 below. 75.2.1.2 K = 3 np.random.seed(4) km2 = KMeans(n_clusters=3, n_init=20) km2.fit(X) KMeans(algorithm=&#39;auto&#39;, copy_x=True, init=&#39;k-means++&#39;, max_iter=300, n_clusters=3, n_init=20, n_jobs=1, precompute_distances=&#39;auto&#39;, random_state=None, tol=0.0001, verbose=0) pd.Series(km2.labels_).value_counts() 1 21 0 20 2 9 dtype: int64 km2.cluster_centers_ array([[-0.27876523, 0.51224152], [ 2.82805911, -4.11351797], [ 0.69945422, -2.14934345]]) km2.labels_ array([1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2], dtype=int32) # Sum of distances of samples to their closest cluster center. km2.inertia_ 68.973792009397258 fig, (ax1, ax2) = plt.subplots(1,2, figsize=(14,5)) ax1.scatter(X[:,0], X[:,1], s=40, c=km1.labels_, cmap=plt.cm.prism) ax1.set_title(&#39;K-Means Clustering Results with K=2&#39;) ax1.scatter(km1.cluster_centers_[:,0], km1.cluster_centers_[:,1], marker=&#39;+&#39;, s=100, c=&#39;k&#39;, linewidth=2) ax2.scatter(X[:,0], X[:,1], s=40, c=km2.labels_, cmap=plt.cm.prism) ax2.set_title(&#39;K-Means Clustering Results with K=3&#39;) ax2.scatter(km2.cluster_centers_[:,0], km2.cluster_centers_[:,1], marker=&#39;+&#39;, s=100, c=&#39;k&#39;, linewidth=2); png 75.2.2 10.5.3 Hierarchical Clustering 75.2.2.1 scipy fig, (ax1,ax2,ax3) = plt.subplots(3,1, figsize=(15,18)) for linkage, cluster, ax in zip([hierarchy.complete(X), hierarchy.average(X), hierarchy.single(X)], [&#39;c1&#39;,&#39;c2&#39;,&#39;c3&#39;], [ax1,ax2,ax3]): cluster = hierarchy.dendrogram(linkage, ax=ax, color_threshold=0) ax1.set_title(&#39;Complete Linkage&#39;) ax2.set_title(&#39;Average Linkage&#39;) ax3.set_title(&#39;Single Linkage&#39;); png 75.3 Lab 3: NCI60 Data Example 75.3.1 § 10.6.1 PCA # In R, I exported the two elements of this ISLR dataset to csv files. # There is one file for the features and another file for the classes/types. df2 = pd.read_csv(&#39;Data/NCI60_X.csv&#39;).drop(&#39;Unnamed: 0&#39;, axis=1) df2.columns = np.arange(df2.columns.size) df2.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 64 entries, 0 to 63 Columns: 6830 entries, 0 to 6829 dtypes: float64(6830) memory usage: 3.3 MB X = pd.DataFrame(scale(df2)) X.shape (64, 6830) y = pd.read_csv(&#39;Data/NCI60_y.csv&#39;, usecols=[1], skiprows=1, names=[&#39;type&#39;]) y.shape (64, 1) y.type.value_counts() RENAL 9 NSCLC 9 MELANOMA 8 BREAST 7 COLON 7 OVARIAN 6 LEUKEMIA 6 CNS 5 PROSTATE 2 MCF7D-repro 1 K562B-repro 1 K562A-repro 1 MCF7A-repro 1 UNKNOWN 1 Name: type, dtype: int64 # Fit the PCA model and transform X to get the principal components pca2 = PCA() df2_plot = pd.DataFrame(pca2.fit_transform(X)) fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6)) color_idx = pd.factorize(y.type)[0] cmap = plt.cm.hsv # Left plot ax1.scatter(df2_plot.iloc[:,0], -df2_plot.iloc[:,1], c=color_idx, cmap=cmap, alpha=0.5, s=50) ax1.set_ylabel(&#39;Principal Component 2&#39;) # Right plot ax2.scatter(df2_plot.iloc[:,0], df2_plot.iloc[:,2], c=color_idx, cmap=cmap, alpha=0.5, s=50) ax2.set_ylabel(&#39;Principal Component 3&#39;) # Custom legend for the classes (y) since we do not create scatter plots per class (which could have their own labels). handles = [] labels = pd.factorize(y.type.unique()) norm = mpl.colors.Normalize(vmin=0.0, vmax=14.0) for i, v in zip(labels[0], labels[1]): handles.append(mpl.patches.Patch(color=cmap(norm(i)), label=v, alpha=0.5)) ax2.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.) # xlabel for both plots for ax in fig.axes: ax.set_xlabel(&#39;Principal Component 1&#39;) png pd.DataFrame([df2_plot.iloc[:,:5].std(axis=0, ddof=0).as_matrix(), pca2.explained_variance_ratio_[:5], np.cumsum(pca2.explained_variance_ratio_[:5])], index=[&#39;Standard Deviation&#39;, &#39;Proportion of Variance&#39;, &#39;Cumulative Proportion&#39;], columns=[&#39;PC1&#39;, &#39;PC2&#39;, &#39;PC3&#39;, &#39;PC4&#39;, &#39;PC5&#39;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PC1 PC2 PC3 PC4 PC5 Standard Deviation 27.853469 21.481355 19.820465 17.032556 15.971807 Proportion of Variance 0.113589 0.067562 0.057518 0.042476 0.037350 Cumulative Proportion 0.113589 0.181151 0.238670 0.281145 0.318495 df2_plot.iloc[:,:10].var(axis=0, ddof=0).plot(kind=&#39;bar&#39;, rot=0) plt.ylabel(&#39;Variances&#39;); png fig , (ax1,ax2) = plt.subplots(1,2, figsize=(15,5)) # Left plot ax1.plot(pca2.explained_variance_ratio_, &#39;-o&#39;) ax1.set_ylabel(&#39;Proportion of Variance Explained&#39;) ax1.set_ylim(ymin=-0.01) # Right plot ax2.plot(np.cumsum(pca2.explained_variance_ratio_), &#39;-ro&#39;) ax2.set_ylabel(&#39;Cumulative Proportion of Variance Explained&#39;) ax2.set_ylim(ymax=1.05) for ax in fig.axes: ax.set_xlabel(&#39;Principal Component&#39;) ax.set_xlim(-1,65) png 75.3.2 § 10.6.2 Clustering X= pd.DataFrame(scale(df2), index=y.type, columns=df2.columns) fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(20,20)) for linkage, cluster, ax in zip([hierarchy.complete(X), hierarchy.average(X), hierarchy.single(X)], [&#39;c1&#39;,&#39;c2&#39;,&#39;c3&#39;], [ax1,ax2,ax3]): cluster = hierarchy.dendrogram(linkage, labels=X.index, orientation=&#39;right&#39;, color_threshold=0, leaf_font_size=10, ax=ax) ax1.set_title(&#39;Complete Linkage&#39;) ax2.set_title(&#39;Average Linkage&#39;) ax3.set_title(&#39;Single Linkage&#39;); png plt.figure(figsize=(10,20)) cut4 = hierarchy.dendrogram(hierarchy.complete(X), labels=X.index, orientation=&#39;right&#39;, color_threshold=140, leaf_font_size=10) plt.vlines(140,0,plt.gca().yaxis.get_data_interval()[1], colors=&#39;r&#39;, linestyles=&#39;dashed&#39;); png 75.3.2.0.1 KMeans np.random.seed(2) km4 = KMeans(n_clusters=4, n_init=50) km4.fit(X) KMeans(algorithm=&#39;auto&#39;, copy_x=True, init=&#39;k-means++&#39;, max_iter=300, n_clusters=4, n_init=50, n_jobs=1, precompute_distances=&#39;auto&#39;, random_state=None, tol=0.0001, verbose=0) km4.labels_ array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3], dtype=int32) # Observations per KMeans cluster pd.Series(km4.labels_).value_counts().sort_index() 0 8 1 23 2 24 3 9 dtype: int64 75.3.2.0.2 Hierarchical # Observations per Hierarchical cluster cut4b = hierarchy.dendrogram(hierarchy.complete(X), truncate_mode=&#39;lastp&#39;, p=4, show_leaf_counts=True) png # Hierarchy based on Principal Components 1 to 5 plt.figure(figsize=(10,20)) pca_cluster = hierarchy.dendrogram(hierarchy.complete(df2_plot.iloc[:,:5]), labels=y.type.values, orientation=&#39;right&#39;, color_threshold=100, leaf_font_size=10) png cut4c = hierarchy.dendrogram(hierarchy.complete(df2_plot), truncate_mode=&#39;lastp&#39;, p=4, show_leaf_counts=True) # See also color coding in plot above. png Table of Contents 10 Unsupervised Learning 10.1 The Challenge of Unsupervised Learning 10.2 Principal Components Analysis 10.2.1 What Are Principal Components? 10.2.2 Another Interpretation of Principal Components 10.2.3 More on PCA 10.2.4 Other Uses for Principal Components 10.3 Clustering Methods 10.3.1 \\(K\\)-Means Clustering 10.3.2 Hierarchical Clustering 10.3.3 Practical Issues in Clustering 10.4 Footnotes "],["unsupervised-learning.html", "Chapter 76 Unsupervised Learning 76.1 The Challenge of Unsupervised Learning 76.2 Principal Components Analysis 76.3 Clustering Methods 76.4 Footnotes", " Chapter 76 Unsupervised Learning 76.1 The Challenge of Unsupervised Learning Unsupervised learning is learning in the absence of a response. It is often part of exploratory data analysis (EDA). Without a response, we aren’t intested in prediction or classification, rather we are interested in discovering interesting things about the data. This can be difficult because such a goal is somewhat subjective. Objective performance critera for unsupervised learning can also be challenging. 76.2 Principal Components Analysis Principal components were discussed earlier as a dimensional reduction methof in the context of regression. They provide a low-dimensional representation of the data that contains as much variation as possible. Principal Components Analysis is the process of computing principal components and using them in data analysis. 76.2.1 What Are Principal Components? The first principal component of features \\(X_1, \\dots, X_p\\) is the normalized linear combination \\[ Z_1 = \\hat{\\phi}_1^\\top X\\] where \\(X = (X_1, \\dots, X_p), \\hat{\\phi}_1\\in \\mathbb{R}^p\\) and \\(|| \\hat{\\phi} || = 1\\). The vector \\(\\hat{\\phi}_1\\) is called the loading vector (its entries are called the loadings) and \\[ \\hat{\\phi}_1 = \\underset{\\underset{||\\phi|| = 1}{\\phi \\in \\mathbb{R}^p}}{\\text{argmax}}\\left(\\frac{1}{n}\\sum_{i=1}^n \\left(\\phi^\\top x_i\\right)^2\\right)\\] Assume we have data \\(X_i\\) with features \\(X_1, \\dots, X_p\\) which is centered in the features (each feature has mean zero). The objective function in the above optimization problem can be rewritten \\[ \\hat{\\phi}_1 = \\underset{\\phi \\in \\mathbb{R}^p}{\\text{argmax}}\\left(\\frac{1}{n}\\sum_{i=1}^n ||z_i ||^2\\right)\\] which is just the sample variance. The \\(z_{i1}\\) are called the scores of the first principal component \\(Z_1\\). The first principal component has a nice geometric interpretation 85. The loading vector \\(\\phi_{1}\\) defines a direction in \\(\\mathbb{R}^p\\) along which the variation is maximized. The principal component scores \\(z_{i1}\\) are the projections of the data \\(x_i\\) onto \\(\\phi_1\\) – that is, the components of the \\(x_i\\) along this direction. For \\(j = 2,...,p\\) we can compute the \\(j\\)-th principal component \\(\\phi_j\\) recursively \\[ \\hat{\\phi}_j = \\underset{\\phi \\in \\mathbb{R}^p}{\\text{argmax}}\\left(\\frac{1}{n}\\sum_{i=1}^n \\left(\\phi^\\top x_i\\right)^2\\right)\\] subject to 86 \\[\\phi_j^\\top \\phi_{j - 1} = 0\\]. - We can plot the principal components against each other for a low-dimensional visualization of the data. For example a biplot plots both the scores and the loading vectors 87. 76.2.2 Another Interpretation of Principal Components Principal components can also be seen as providing low-dimensional surfaces that are “closest” to the observations. The span of the first \\(M\\) loading vectors \\(\\phi_1, \\dots, \\phi_M\\) can be seen as the \\(M\\)-dimensional linear subspaces of \\(\\mathbb{R}^p\\) which is closest to the observations \\(x_i\\) 88 Together the principal components \\(Z_1, \\dots, Z_M\\) and loading vectors \\(\\phi_1, \\dots, \\phi_M\\) can be seen as an \\(M\\)-dimensional approximation89 of each observation \\[x_{ij} \\approx \\sum_{m = 1}^M z_{im}\\phi_{jm}\\] 76.2.3 More on PCA PCA requires that the variables are centered to have mean zero PCA is sensitive to scaling, so we usually scale each variable to have standard deviation 1. Scaling to standard deviation 1 is particularly important when variables are measured in different units, however if they are measured in the same units we may not wish to do this. 76.2.3.0.1 Uniqueness of the Principal Components The loading vectors and score vectors are unique up to sign flips. 76.2.3.0.2 The Proportion of Variance Explained How much of the information in a given data set is lost by projecting onto the principal components? More precisely, what is the proportion of variance explained (PVE) by each principal component? Assuming centered data, the total variance 90 is \\[\\text{var}_{total} := \\sum_{j = 1}^p \\mathbb{V}(X_j) = \\sum_{i = 1}^p \\left(\\frac{1}{n} \\sum_{i = 1}^n x_{ij}^2 \\right)\\] while the variance explained by the \\(m\\)-th principal component is \\[\\text{var}_{m} := \\frac{1}{n} \\sum_{i=1}^n z_{im}^2 = \\frac{1}{n} \\sum_{i = 1}^n \\left(\\sum_{i = 1}^p \\phi_{jm}x_{ij} \\right)^2 \\]. The PVE of the \\(m\\)-th component is then \\[\\text{PVE}_m := \\frac{\\text{var}_{m}}{\\text{var}_{total}}\\] and the cumulative PVE of the first \\(M\\) components 91 is \\[\\sum_{m = 1}^M \\text{PVE}_m \\] 76.2.3.0.3 Deciding How Many Principal Components to Use In general choose we may not be interested in using all principal components, but just enough to get a “good” understanding of the data 92. A scree plot, which plots \\(\\text{PVM}_m\\) vs. \\(m\\), can help identify a good number of principal components to use, is one visual method for identifying a good number of principal components. We look for an elbow - a value of \\(m\\) such that \\(\\text{PVM}_m\\) drops off thereafter. In general, the question of how many principal components are “enough” is ill-defined, and depends on the application and the dataset. We maybe look at the first few principal components in order to find interesting patterns. If none are evident, then we conclude further components are unlikely to be of use. If some are evident, we continue looking at components until no more interesting patterns are found. In an unpervised setting, these methods are all ad hoc, and reflect the fact that PCA is generally used in EDA 93. 76.2.4 Other Uses for Principal Components Many statistical techniques (regression, classification, clustering) can be adapted to the \\(n \\times M\\) PCA matrix with columns the first \\(M &lt;&lt; p\\) principal component score vectors. The PCA matrix can be seen as a “de-noising” 94 of the original data, since the signal (as opposed to the noise) is weighted towards the earlier principal components 76.3 Clustering Methods This is a broad set of techniques for finding clusters (or subgroups) of the data set. Observations should be “similar” within clusters and dissimilar across clusters. The definition of “similar” is context dependent. Clustering is popular in many fields, so there exist a great number of methods. 76.3.1 \\(K\\)-Means Clustering \\(K\\)-means clustering seeks to partition the data into a pre-specified number \\(K\\) of distinct, non-overlapping clusters. More precisely, we seek a partition \\(\\hat{C}_1, \\dots \\hat{C}_K\\) of the set of indices \\(\\{1, \\dots n\\}\\) \\[\\hat{C}_1, \\dots \\hat{C}_K = \\underset{C_1, \\dots, C_k}{\\text{argmin}}\\left(\\sum_{k = 1}^K W(C_k)\\right)\\] where \\(W(C_k)\\) is some measure of the variation within cluster \\(C_k\\). - A typical choice of \\(W(C_k)\\) is the average 95 squared Euclidean distance between points in \\(C_k\\): \\[W(C)_k = \\frac{1}{|C_k|}\\sum_{i, i&#39; \\in C_k} ||x_i - x_i&#39;||^2\\] - A brute force algorithm for finding the global minimum is \\(O(K^n)\\) but there is a much faster algorithm which is guaranteed to find a local minimum. It uses a random initialization so it should be performed several times. 76.3.1.0.1 Algorithm: \\(K\\)-Means Clustering Initialize by randomly assigning a cluster number \\(1,\\dots K\\) to each observation. While the cluster assignments change: For each \\(k = 1, \\dots K\\), compute the centroid of the \\(k\\)-th cluster (the vector of feature means for the observations in the cluster). Assign to each observation the number of the cluster whose centroid is closest. 76.3.1.0.2 Advantages 76.3.1.0.3 Disadvantages 76.3.2 Hierarchical Clustering Hierarchical clustering is an alternative clustering method which doesn’t require a specified number of clusters and results in an attractive tree-based representation of the data called a dendrogram. Bottom-up or agglomerative hierarchical clustering builds a dendrogram from the leaves up to the trunk. 76.3.2.0.1 Interpreting a Dendrogram A dendrogram is a tree (visualized as upside down) with leaves corresponding to observations. As we move up the tree, similar observations fuse into branches, and similar branches again fuse. The earlier fusions occur, the more similar the corresponding groups of observations. The height at which two observations are joined by this fusing is a measure of this similarity. At each height in the dendrogram, a horizontal cut splits the observations into \\(k\\) clusters (corresponding to each of the branches cut) where \\(1 \\leqslant k \\leqslant n\\). The best choice of cut (hence number \\(k\\) of clusters) is often obtained by inspecting the diagram. 76.3.2.0.2 The Hierarchical Clustering Algorithm This algorithm uses a notion of dissimilarity defined for clusters, called a linkage. Let \\(A, B\\) be clusters, and let \\(d(a, b)\\) be a dissimilarity measure 95 for observations \\(a, b\\). A linkage defines a dissimilarity measure \\(d(A,B)\\) between the clusters \\(A, B\\). The four most common types of linkage are complete: \\[d_{comp}(A, B) = \\underset{(a, b) \\in A \\times B}{\\max} d(a, b)\\] single: \\[d_{sing}(A, B) = \\underset{(a, b) \\in A \\times B}{\\min} d(a, b)\\] average \\[d_{avg}(A, B) = \\frac{1}{|A||B|} \\underset{(a, b) \\in A \\times B}{\\sum} d(a, b)\\] centroid \\[d_{cent}(A, B) = d(x_a, x_b)\\], where \\(x_a\\) (resp. \\(x_b\\)) is the centroid of \\(A\\) (resp. \\(B\\)). Average, complete, and single linkages are preferred by statisticians. Average and complete linkages are generally preferred as they result in more balanced dendrograms. Centroid linkage is often used in genomics, but suffers from the possibility of an inversion, in which two clusters are fused at a height below the individual clusters, which makes interpretation difficult. 76.3.2.0.2.1 Choice of Dissimilarity Measure The squared Euclidean distance is often used as a dissimilarity measure. An alternative is the correlation-based distance The choice of dissimilarity measure is very important and has a strong effect on the resulting dendrogram. The choice of measure should be determined by context. One should consider scaling the data before choosing the dissimilarity measure. 76.3.2.0.2.2 Algorithm: Hierarchical Clustering Initialize with \\(n\\) clusters, one for each observation, and compute the dissimilarities \\(d(x_i, x_j)\\) for each pair. For \\(i = n, \\dots, 2\\): Compute all dissimilarites among the \\(i\\) clusters, and fuse the two clusters which are the least dissimilar. This dissimilarity is the height in the dendrogram where the fusion is placed. Compute the dissimilarities among the new \\(i -1\\) clusters. 76.3.2.0.2.3 Advantages 76.3.2.0.2.4 Disadvantages 76.3.3 Practical Issues in Clustering 76.3.3.0.1 Small Decisions with Big Consequences Should observations or features be standardized in some way? For hierarchical clustering: What dissimilarity measure should be used? What type of linkage should be used? Where should we cut the dendrogram to determine the number of clusters? For \\(K\\)-means clustering, what is the choice of \\(K\\)? 76.3.3.0.2 Validating the Clusters Obtained It is important to decide whether the clusters obtained reflect true subgroups in the data or are a result of “clustering the noise.” There exist techniques for making this decision, such as obtaining \\(p\\)-values for each cluster. 76.3.3.0.3 Other Considerations in Clustering Both \\(K\\)-means and hierarchical clustering assign all observations to some cluster. This can be problematic, for example in the presence of outliers that don’t clearly belong to any cluster. “Mixture models” are an attractive approach to accommodating outliers (they amount to a “soft” clustering approach). Clustering methods are not robust to perturbations. 76.3.3.0.4 A Tempered Approach to Interpreting the Results of Clustering Clustering can be a very useful and valid statistical tool if used properly. To overcome the sensitivity to hyperparameters, is recommended to try hyperparameter optimization. To overcome the sensitivity to perturbations, it is recommended to cluster on subsets of the data. Finally, results of cluster analysis should be considered a part of EDA and not taken too seriously 76.4 Footnotes The linear algebra interpretation is also nice ↩︎ This constraint is equivalent to \\[\\text{corr}(Z_j, Z_{j-1}) = 0\\] ↩︎ See book figure 10.1 and corresponding discussion. ↩︎ That is, the linear subspace in \\(\\mathbb{R}^p\\) which minimizes the sum of the squared euclidean distances to the points \\(x_i\\). ↩︎ When \\(M = \\min\\{n - 1, p\\}\\), the approximation is exact. ↩︎ More accurately, the sum on the right is an estimate of the sum on the left. In general there are \\(\\min\\{n-1, p\\}\\) principal components and \\[\\sum_{m = 1}^{\\min\\{n-1, p\\}} \\text{PVE}_m = 1\\] Indeed, if we take \\(M &lt; p\\) principal components, then we are truly doing dimensional reduction. In a supervised setting, however, we can treat the number of components as a tuning parameter. There is a nice information-theoretic interpretation of this statement. That we are taking an average is probably the reason for the “means” in “\\(K\\)-means.” For example, the commonly used squared Euclidean distance. See Choice of Dissimilarity Measure "]]
