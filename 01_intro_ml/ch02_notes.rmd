<h1>Table of Contents<span class="tocSkip"></span></h1>
<div class="toc"><ul class="toc-item"><li><span><a href="#Statistical-Learning" data-toc-modified-id="Statistical-Learning-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>Statistical Learning</a></span><ul class="toc-item"><li><span><a href="#What-is-Statistical-Learning?" data-toc-modified-id="What-is-Statistical-Learning?-2.1"><span class="toc-item-num">2.1&nbsp;&nbsp;</span>What is Statistical Learning?</a></span><ul class="toc-item"><li><span><a href="#Why-Estimate-$f$?" data-toc-modified-id="Why-Estimate-$f$?-2.1.1"><span class="toc-item-num">2.1.1&nbsp;&nbsp;</span>Why Estimate $f$?</a></span></li><li><span><a href="#How-to-Estimate-$f$?" data-toc-modified-id="How-to-Estimate-$f$?-2.1.2"><span class="toc-item-num">2.1.2&nbsp;&nbsp;</span>How to Estimate $f$?</a></span></li><li><span><a href="#Accuracy-vs.-Interpretability" data-toc-modified-id="Accuracy-vs.-Interpretability-2.1.3"><span class="toc-item-num">2.1.3&nbsp;&nbsp;</span>Accuracy vs. Interpretability</a></span></li><li><span><a href="#Supervised-vs.-Unsupervised-Learning" data-toc-modified-id="Supervised-vs.-Unsupervised-Learning-2.1.4"><span class="toc-item-num">2.1.4&nbsp;&nbsp;</span>Supervised vs. Unsupervised Learning</a></span></li><li><span><a href="#Regression-vs.-Classification" data-toc-modified-id="Regression-vs.-Classification-2.1.5"><span class="toc-item-num">2.1.5&nbsp;&nbsp;</span>Regression vs. Classification</a></span></li></ul></li><li><span><a href="#Assessing-Model-Accuracy" data-toc-modified-id="Assessing-Model-Accuracy-2.2"><span class="toc-item-num">2.2&nbsp;&nbsp;</span>Assessing Model Accuracy</a></span><ul class="toc-item"><li><span><a href="#Measuring-Quality-of-Fit" data-toc-modified-id="Measuring-Quality-of-Fit-2.2.1"><span class="toc-item-num">2.2.1&nbsp;&nbsp;</span>Measuring Quality of Fit</a></span></li><li><span><a href="#The-Bias-Variance-Tradeoff" data-toc-modified-id="The-Bias-Variance-Tradeoff-2.2.2"><span class="toc-item-num">2.2.2&nbsp;&nbsp;</span>The Bias-Variance Tradeoff</a></span></li><li><span><a href="#The-Classification-Setting" data-toc-modified-id="The-Classification-Setting-2.2.3"><span class="toc-item-num">2.2.3&nbsp;&nbsp;</span>The Classification Setting</a></span></li></ul></li><li><span><a href="#Footnotes" data-toc-modified-id="Footnotes-2.3"><span class="toc-item-num">2.3&nbsp;&nbsp;</span>Footnotes</a></span></li></ul></li></ul></div>

___
# Statistical Learning
____

## What is Statistical Learning?

Given paired data  $(X, Y)$, assume a relationship between $X$ and $Y$ modeled by

$$ Y = f(X) + \epsilon $$

where $f:\mathbb{R}^p \rightarrow \mathbb{R}$ is a function and $\epsilon$ is a random error term with $\mathbb{E}(\epsilon) = 0$.

***Statistical learning*** is a set of  approaches for estimating $f$<sup><a href='#foot0' id='ref0'>0</a></sup>

### Why Estimate $f$?

##### Prediction

- We may want to ***predict*** the output $Y$ from an estimate $\hat{f}$ of $f$. The predicted value for a given $Y$ is then $$ \hat{Y} = \hat{f}(X)$$. In prediction, we often treat $f$ as a ***black-box***

- The mean squared-error<sup><a href='#foot2' id='ref2'>2</a></sup> $\mathbf{mse}(\hat{Y})=\mathbb{E}(Y-\hat{Y})^2$ is a good measure of the accuracy of $\hat{Y}$ as a predictor for $Y$.

- One can write

$$ \mathbf{mse}(\hat{Y}) = \left(f(X) - \hat{f}(X)\right)^2 + \mathbb{V}(\epsilon) $$

These two terms are known as the ***reducible error*** and ***irreducible error***, respectively<sup><a href='#foot3' id='ref3'>3</a></sup>

##### Inference

- Instead of predicting $Y$ from $X$, we may be more interested how $Y$ changes as a function of $X$. In inference, we usually do not treat $f$ as a black box. 

Examples of important inference questions:

- *Which predictors have the largest influence on the response?*
- *What is the relationship between the response and each predictor?*
- *Is f linear or non-linear?

### How to Estimate $f$?

##### Parametric methods

Steps for parametric method:

1. Assume a parametric model for $f$, that is assume a specific functional form<sup><a href='#foot4' id='ref4'>4</a></sup>

$$f = f(X, \boldsymbol{\beta}) $$

for some vector of ***parameters*** $\boldsymbol{\beta} = (\beta_1,\dots,\beta_p)^T$  

2. Use the training data to ***fit*** or ***train*** the model, that is to choose $\beta_i$ such that 

$$Y \approx f(X, \boldsymbol{\beta})$$

##### Non-parametric methods

These methods make no assumptions about the functional form of $f$.

### Accuracy vs. Interpretability

- In inference, generally speaking the more flexible the method, the less interpretable.

- In prediction, generally speaking the more flexible the method, the less accurate

### Supervised vs. Unsupervised Learning

- In ***supervised learning***, training data consists of pairs $(X, Y)$ where $X$ is a vector of predictors and $Y$ a response. Prediction and inference are supervised learning problems, and the response variable (or the relationship between the response and the predictors) *supervises* the analysis of model 

- In ***unsupervised learning***, training data lacks a response variable.

### Regression vs. Classification

- Problems with a quantitative response ($Y\in S \subseteq \mathbb{R}$) tend to be called ***regression*** problems

- Problems with a qualitative, or categorical response ($Y \in \{y_1, \dots, y_n\})$ tend to be called ***classification*** problems

## Assessing Model Accuracy

There is no free lunch in statistics

### Measuring Quality of Fit

- To evaluate the performance of a method on a data set, we need measure model accuracy (how well predictions match observed data). 

- In regression, the most common measure is the ***mean-squared error***

$$MSE = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{f}(x_i))^2$$

  where $y_i$ and $\hat{f}(x_i)$ are the $i$ true and predicting   
  responses, respectively.

- We are usually not interested in minimizing MSE with respect to training data but rather to test data. 

- There is no guarantee low training MSE will translate to low test MSE. 

- Having low training MSE but high test MSE is called ***overfitting***

### The Bias-Variance Tradeoff

- For a given $x_0$, the expected <sup><a href='#foot5' id='ref5'>5</a></sup> MSE can be written

\begin{align*}
\mathbb{E}\left[\left(y_0 - \hat{f}(x_0)\right)^2\right] 
&= \left(\mathbb{E}\left[\hat{f}(x) \right] - f(x)\right)^2 + \mathbb{E}\left[\left(\hat{f}(x_0) - \mathbb{E}\left[\hat{f}(x_0)\right]\right)^2\right] + \mathbb{E}\left[\left(\epsilon - \mathbb{E}[\epsilon]\right)^2\right]\\
&= \mathbf{bias}^2\left(\hat{f}(x_0))\right) + \mathbb{V}\left(\hat{f}(x_0)\right) + \mathbb{V}(\epsilon)
\end{align*}


- A good method minimizes variance and bias simultaneously. 

- As a general rule, these quantities are inversely proportional. More flexible methods have lower bias but higher variance, while less flexible methods have the opposite. This is the ***bias-variance tradeoff*** 

- In practice the mse, variance and bias cannot be calculated exactly but one must keep the bias-variance tradeoff in mind.

### The Classification Setting

- In the classification setting, the most common measure of model accuracy is the ***error rate*** <sup><a href='#foot6' id='ref6'>6</a></sup>

$$\frac{1}{n}\sum_{i=1}^n I(y_i \neq \hat{y}_i)$$ 

- As with the regression, we are interested in minimizing the test error rate, not the training error rate.

##### The Bayes Classifier

- Given $K$ classes, the ***Bayes Classifier*** predicts

$$ \hat{y_0} = \underset{1\leqslant j \leqslant K}{\text{argmax}\,} \mathbb{P}\left(Y=j\ |\ X = x_0\right)$$

- The set of points 
$$\{x_0\in\mathbb{R}^p\ |\ \mathbb{P}\left(Y=j\ |\ X = x_0\right) = \mathbb{P}\left(Y=k\ |\ X = x_0\right)\ \text{for all}\ 1\leqslant j,k \leqslant K\}$$

    is called the ***Bayes decision boundary***

- The test error rate of the Bayes classifier is the ***Bayes error rate***, which is minimal among classifiers. It is given by

$$ 1 - \mathbb{E}\left(\underset{j}{\max} \mathbb{P}\left(Y=j\ |\ X\right)\right)$$

- The Bayes classifier is optimal, but in practice we don't know $\mathbb{P}\left(Y\ |\ X\right)$.

##### K-Nearest Neighbors

- The ***K-nearest neighbors*** classifier works by estimating $\mathbb{P}\left(Y\ |\ X\right)$ as follows.

1. Given $K\geqslant 1$ and $x_0$,  find the set of points

$$ \mathcal{N}_0 = \{K\ \text{nearest points to}\ x_0\}\subseteq\mathbb{R}^p $$

2. For each class $j$ set 

$$ \mathbb{P}\left(Y=j\ |\ X\right) = \frac{1}{K}\sum_{x_i\in\mathcal{N}_0}I(y_i = j)$$

3. Predict

$$ \hat{y_0} = \underset{1\leqslant j \leqslant K}{\text{argmax}\,} \mathbb{P}\left(Y=j\ |\ X = x_0\right)$$

---
## Footnotes

<p>
</p>

<div id="foot0"> 0. Reading the rest of the chapter, one realized this is the situation for *supervised* learning, which is the vast majority of this book is concerned with. 
<a href="#ref0">↩</a>
</div>

<p>
</p>

<div id="foot1"> 1. Here $X=(X_1,\dots, X_p)^T$ is a vector.
<a href="#ref1">↩</a>
</div>

<p>
</p>

<div id="foot2"> 2. This is usual definition of the mean squared-error of $\hat{Y}$ as an estimator of the (non-parametric) quantity $Y=f(X)$.
<a href="#ref2">↩</a>
</div>

<p>
</p>

<div id="foot3"> 3. We can in principle control the reducible error by improving the estimate $\hat{f}$, but we cannot control the irreducible error.
<a href="#ref3">↩</a>
</div>

<p>
</p>

<div id="foot4"> 4. For example, a simple but popular assumption is that f is linear in both the parameters and the features, that is:
    
$$f(X) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p$$

This is linear regression.
<a href="#ref4">↩</a>
</div>

<p>
</p>

<div id="foot5"> 5. Here the random variable is $\hat{f}(x_0)$, so the average is taken over all data sets
<a href="#ref5">↩</a>
</div>

<p>
</p>


<div id="foot6"> 6. This is just the proportion of misclassified observations.
<a href="#ref6">↩</a>
</div>

<p>
</p>
